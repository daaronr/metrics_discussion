# Making inferences from previous work; Meta-analysis, combining studies {#metaanalysis}

My opinion on why this is so important (unfold):

```{block2,  type='fold'}

it is lame how often I see 'new experiments' and 'new studies' that tread most of the ground as old studies, spend lots of money, get a publication and ... ignore or pay lip service to the previous findings. There is tons of data out there that can inform new questions and bring better through re-analysis and combination with other data. Otherwise we are not actually building progress. This is why I became involved with a project I called 'ExpArchive', later working with projects such as GESIS' X-Econ to try to encourage and facilitate data sharing in experimental economics, as well as  the innovationsinfundraising.org project, which is now collaborating with the Lily Institute's "revolutionizing philanthropy research" (RPR) project.

```


## Notes: Christensen et al 2019, ch 5, 'Using all evidence, registration and meta-analysis

> how the research community can systematically collect, organize, and analyze a body of research work

- Limitations to the 'narrative literature review': subjectivity, too much info to narrate

### The origins [and importance] of study [pre-]registration

... Make details of planned and ongoing studies available to the community .... including those not (yet) published

- Required by FDA in 1997, many players in medical community followe d soon after

- Turner ea (08) and others documented massive publication bias and misrepresentation

... but registration far from fully enforced (Mathieu ea '09) found 46% clealy registered, and discrepancies between registered and published outcomes
!

### Social science study registries

Jameel 2009, AEA 2013, 2100 registrations to date
RIDIE, EGAP, AsPredicted, OSF allowing a DOI (25,000+)

### Meta-analsis

Key references: Borenstein ea '09, Cooper, Hedges, and V '09

#### Selecting studies

"some scholarly discretion regarding which measures are 'close enough' to be included... contemperanous meta-analyses on the same topic finding opposit e conclusions

'asses the robustness... to diffrent inclusion conditions'... see Doucouliagos ea '17 on inclusion options


<div class="marginnote">


My opinion: this is the key barrier to meta-analysis in social science! How do we weight studies using different methodologies and in different contexts? The Bayesian Random Effects approach seems to offer some help here (this not to be confused with the random effects panel-data models discussed in standard Econometrics texts).

</div>

#### Assembling estimates

- Which statistic to collect?

\


Studies $j \in J, j= 1..N_j$

Relevant estimate of stat from each study is $\hat{
\beta_j}$ with SE $\hat{\sigma_j}$

- Papers report several estimates (e.g., in robustness checks): which to choose, esp if author's preferred approach differs from other scholars.

\

*Ex from Hsiang, B, Miguel, '13*: links between extreme climate and violence

- how to classify outcomes... interpersonal and intergroup... normalised as pct changes wrt the meanoutcome in that dataset

- how to standardice climate varn measures... chose SD from local area mean
(DR: this choice implicitly reflects a behavioural assumption)

$\rightarrow$ 'pct  change in a conflict outcome as a fncn of a 1 SD schock to local climate'

### Combining estimates

'Fixed-effect meta-analysis approach': assumes a single true effect'

<div class="marginnote">

DR: I'm  not sure I agree on theis assesment of *why* this is unlikely to be true in practice... 'differences in measures' (etc) seem to be a different issue

</div>

*Equal weight approach*: (Simply the average across studies... ugh)

\

*Precision-weighted approach*:

$$\hat{\beta}_{PW}=
\sum_{j}p_j\hat{\beta}_j/
\sum_{j}p_j$$

where $p_j$ is the estimated precision for study $j$: $\frac{1}{\hat{\sigma_i}^2}$

Thus the weight $\omega_j$ placed on study $j$ is proportional to it's precision.


<div class="marginnote">

'implies weight in proportion to sample size'? I think that's loosely worded, it must be nonlinear.

</div>

$\rightarrow$ This minimises the variance in the resulting meta-analtical estimate:

$$Var(\hat{\beta}_{PW) =\sum_j \omega_j^\hat{\sigma_j}^2=\frac{1}{\sum_j(p_j)}$$

'inclusion of additional estimates always reduces the SE of $\hat{\beta_{PW}}$ [in expectation].' ... so more estimtes can't hurt as long as you know their precision.

(they give a numerical example here with 3 estimates)

<!-- Todo: add R code explicitly doing these calculations -->

### Heterogeneous estimates...

#### WLS estimate

(Stanley and Doucouliagos '15)

Interpreted as 'an estimate of the average of potentially heterogenous estimates'

This may feel like a more familiar to Economists but it is also seems to be far less useful than the Bayesian approach.

\


#### Random-effects (more common)

*Focus here on hierarchical Bayesian approach* (Gelman and Hill '06; Gelman ea '13)

'The magnitude and precision of the common component represents the generalizable conclusions we might draw from a literature'

... continuing from above notation

'cross-study differences we observe might not be driven solely by sampling variability... [even with] infinite data, they would not converge to the exact same [estimate] '

True Treatment Effect (TE) $\beta_j$ for study j drawn from a normal distribution...

$$\beta_j \sim N(\mu, \tau^2)$$

'Hyperparameters' $\mu$ determines central tendency of findings...
$\tau$ the extent of hety across contexts.

Considering $\tau$ vs $\mu$ is informative  in itself.
And a large $\mu$ may suggest looking into sample splits for hety on obsl lines.

\

Uniform prior for $\mu$ $\rightarrow$ conditional posterior:

$$\mu|\tau,y \sim N(\hat{\mu}, V_{\mu})$$ where the estimated common effect $\hat{\mu}$ is

$$\hat{\mu}=
\frac{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))\hat{\beta}}
{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))}$$

(Similar to precision-weighted approach but now the between-study dispersion is incorporated into the weights)

and where the estimated variance  of the generalizable component $V_\mu$ is:

$$Var(\hat{\mu})= \frac{1}{\sum_j\big(1/(\hat{\sigma_i}^2 + \hat{tau}^2)}$$


<div class="marginnote">

Confusion/correction? Is this the estimated variance or the variance of the estimate?

</div>


- and how do we estimate some of the components of these, like $\hat{\tau}$?

> Intuitively, if estimated [TE] in all studies are near one another and have relatively wide and overlapping [CI's], then most of the difference in estimates is likely the result of sampling variation [and $\tau$] is likely to be close to zero.


<div class="marginnote">

DR: But if the TE have wide CI's, do we have power to idfy btwn-study hety? ... I guess that's what the 'estimated TE are all near each other' gives us?

</div>

... Alternatively, if there is extensive variation in the estimated ATEs but each is precise... $\tau$ is likely to be relatively large.

```{block2,  type='note'}
Coding meta-analyses in R

"A Review of Meta-Analysis Packages in R" offers a helpful guide to the various packages, such as `metafor`.

[Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) appears extremely helpful; see, e.g., their chapter [Bayesian Meta-Analysis in R using the brms package](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-meta-analysis-in-r-using-the-brms-package.html)

```


<!-- TODO: some code exercises should be put or linked here? Perhaps drawn from the above references? -->


</div>

\ The $I^2$ stat is a measure of the proportion of total variation attributed to cross-study variation; if $\hat{\sigma}_j$ is the same across all studies we have:
$I^2(.) = \hat{\tau}^2/(\hat{\tau}^2 + \hat{\sigma}^2)$

<!-- *DR: more detail would be welcome here. Material from [this syllabus]() may be helpful.

https://docs.google.com/document/d/1oImg-ojUFqak5KyZ-ETD2qGvkvUgx8Ym6b8gG4GwfM8/edit?usp=drivesdk

-->

## Excerpts and notes from 'Doing Meta-Analysis in R: A Hands-on Guide' (Harrer et al) {#doing-meta}

Some notes follow excerpting and commenting on 

[Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/)


```{r, warning=FALSE, message=FALSE}

#devtools::install_github("MathiasHarrer/dmetar")
#install.packages("extraDistr")

library(meta)

# I followed a lot of steps before this worked! ... https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started ... 
#install.packages("brms")
library(brms)
library(dmetar)

library(extraDistr)

```

### Pooling effect sizes
FE model calculates weighted average: 

FIX THESE FORMULAS

$$\hat{\theta_F} = \frac{\sum\limits_{k=1}^K \hat{\theta_k} $$ 
$$\hat{\sigma^2_k}=\sum\limits_{k=1}^K \frac{1}{K}\hat{\sigma}^2_k $$



- note that this process does not 'dig in' to the raw data, it just needs the summary statistics, neither does the "RE model" they refer to:

> Both of these models only require an effect size, and a dispersion (variance) estimate for each study, of which the inverse is taken. This is why the methods are often called generic inverse-variance methods.

Nor the Bayesian models, apparently (they use the same 'madata' dataset)

### Bayesian Meta-analysis {#doing-bayes-meta}


```{block2,  type='note'}
"The model we apply in Bayesian Meta-Analysis is a so-called Bayesian Hierarchical Model...
every meta-analytical model inherently possesses a multilevel, and thus 'hierarchical', structure."

```  

#### The setup {-}
Underlying RE model (as before)

Study-specific estimate: 

$$ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) $$

True study-specific effects distributed: 

$$ \theta_k \sim \mathcal{N}(\mu,\tau^2) $$

... simplified to the 'marginal' form:

$$ \hat\theta_k | \mu, \tau, \sigma_k \sim \mathcal{N}(\mu,\sigma_k^2 + \tau^2)$$

\


And now we specify priors for these parameters, 'making it Bayesian'


$$(\mu, \tau^2) \sim p(.)$$
$$ \tau^2 > 0 $$
\

Estimation will...

> involve[] Markov Chain Monte Carlo based sampling procedures, such as the Gibbs Sampler. In the brms package we will be using in this chapter, the No-U-Turn Sampler, or NUTS (Hoffman and Gelman 2014), is used.

\

```{block2,  type='note'}

**Why use Bayesian?**

- to  "directly model the uncertainty when estimating [the between-study variance] $\tau^2$"

- "have been found to be superior in estimating the between-study heterogeneity and pooled effect, particularly when the number of included studies is small"

- "produce full posterior distributions for both $\mu$ and $\tau$" ... so we can make legitimate statements about the probabilities of true parameters

- "allow us to integrate prior knowledge and assumptions when calculating meta-analyses" (including methodological uncertainty perhaps)

```  
\

#### Setting weakly informative' priors for the mean and cross-study variance of the TE sizes {-}

> It has been argued that a good approach is to use weakly informative priors (Williams, Rast, and BÃ¼rkner 2018) [rather than 'non-informative priors'!]


**For $\mu$**: 

> include distributions which represent that we do indeed have **some confidence that some values are more credible than others**, while still not making any overly specific statements about the exact true value of the parameter. ... In most applied cases, it seems reasonable to assume that the true effect size we want to estimate must lie somewhere between, for example, Cohen's $d=-2.0$ and $d=2.0$, but will unlikely be hovering around $d=50$. A good starting point for our $\mu$ prior may therefore be a normal distribution with mean $0$ and variance $1$. This means that we grant a 95% prior probability that the true pooled effect size $\mu$ lies between $d=-2.0$ and $d=2.0$:

$$ \mu \sim \mathcal{N}(0,1)$$

**For  $\tau^2$**

- must be non-negative, but might be very close to zero.

- Recommended distribution for this case (for variances in general):  *Half-Cauchy prior* (a censored Cauchy)

$\mathcal{HC}(x_0,s)$

- with  *location parameter* $x_0$ (peak on x-axis)
- and  $s$, scaling parameter 'how heavy-tailed'

\

Half-Cauchy distribution for varying $s$, with $x_0=0$:

```{r, echo=FALSE, fig.width=6, fig.align='center'}
library(png)
library(grid)
img <- readPNG(here("images", "half_cauchy.png"))
grid.raster(img)
```


HC is 'heavy-tailed;... gives some probability to very high values but low values are still more likely.

One might consider $s=0.3$ 

<div class="marginnote">
$s$ corresponds to the std deviation here? ... so an SD of the effect size about 1/3 of it's mean size?
</div>


Checking the share of this distribution below 0.3...
```{r, message=F, warning=F}
phcauchy(0.3, sigma = 0.3) #cumulative share of distribution below 0.3 ... is 1/2 ... with sigma=0.3
```

\

... But they go for the 'more conservative' $s=0.5$.

> In general, it is advised to always conduct sensitivity analyses with different prior specifications to check if they affect the results substantially

\

Complete model:

$$ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) $$
$$ \theta_k \sim \mathcal{N}(\mu,\tau^2) $$
$$ \mu \sim \mathcal{N}(0,1)$$
$$ \tau \sim \mathcal{HC}(0,0.5)$$

#### Bayesian Meta-Analysis in R using the `brms` package



You specify the priors as a vector of elements, each of which invokes the 'prior' function, which makes some sort of data frame. The priors function takes a distribution function with parameters, and a 'class'. 

```{r priors}
priors <- c(prior(normal(0,1), class = Intercept), prior(cauchy(0,0.5), class = sd))
```



A quick look at the data we're using here: 

```{r}

str(ThirdWave[,1:3])
```


\

To actually run the model, he uses the following code:

(it seems to require Xcode to run on my mac)

```{r}

m.brm <- brm(TE|se(seTE) ~ 1 + (1|Author),
             data = ThirdWave,
             prior = priors,
             iter = 400)

```


- The *formula for the model* is specified using 'regression formula notation'

- As there is no 'predictor variable' here (unless it's meta-regression), `x` is replaced with `1`

- But we want to  give studies with greater precision of the effect size estimate a greather weight. 
  - Done using `y|se(se_y)`
  
- For  the *random effects terms* he adds  `(1|study)` to the predictor part.

-  `prior`: Plugs in the priors created above plug in the `priors` object we created previously here.

- `iter`:  Number of iterations of MCMC algorithm... the more complex your model, the higher this number should be. [DR: but what's a rule of thumb here?]




