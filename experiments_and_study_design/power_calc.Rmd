# (Ex-ante) Power calculations for (Experimental) study design {#power}

> Power is the ability to distinguish signal from noise. 
> - [Coppock](https://egap.org/resource/10-things-to-know-about-statistical-power/)


The 'statistical power' of an analysis is the probability that this analysis diagnoses that 'an effect is present' (or a parameter is nonzero). The power of an analysis can only considered in light of (as a function of) 

- a particular *true effect size* (parameter magnitude), 
- or an effect size stated as relative to the underlying dispersion, 
- or as 'the probability of achieving a certain desired precision'.\*

Thus one often sees the power of an analysis 'plotted' against particular effect sizes (sometimes 'alternative hypotheses'). 

<div class="marginnote">
\* E.g., we may see power calculated as a function of a measure of 'effect relative to variation' ... like Cohen's $d$ of ... effect size/SD.
</div>

Considering standard frequentist null hypothesis testing, 'the power of a test' (or analysis) represents one minus the probability of a type-II error (approximately 'one minus the false-negative rate').

<div class="marginnote">
Todo: an aside here about power calculations in a Bayesian context.
</div>
 
 
\

*A basic primer:* [Egap - 10 things to know about statistical power](https://egap.org/resource/10-things-to-know-about-statistical-power/)


## What is the point of doing a 'power analysis' or 'power calculations'?

There are several reasons to consider power and to do 'power calculations' in advance of running an experiment or doing an analysis. 

An 'underpowered study' is one with a low likelihood of diagnosing a 'substantial' effect is present when it *is* present. Such a study is also likely to be an *uninformative* study. Furthermore, if 'rejecting a particular null hypothesis' is important\*\*, an underpowered test is unlikely to reject this null hypothesis even when it is 'substantially and meaningfully false'. 

<div class="marginnote">
\*\* But please see McElreath and other's discussion about the follies of the ways NHST is used in science on this point.
</div>
 
An underpowered study is likely to yield wide 'confidence intervals' (or wide 'posterior Bayesian credible intervals'). Simply put, after an underpowered study (at least in isolation), we still won't have a good sense of what the true value (of the parameter or effect of interest) is. 

\


All else equal, you want to run a study that is *as high-powered as possible* (or a study that is part of a larger project that collectively yields substantial power) because:

1. you want your study to be informative and to contribute to science (empirical analysis) and

2. 'low powered studies' (at least in certain publication contexts) can potentially [harm the accuracy of our scientific consensus](underpowered).

<div class="marginnote">
Other than perhaps being 'too costly', is it bad for a study to be 'overpowered'? [I argue that this is *not* a problem.](https://twitter.com/GivingTools/status/1350259972786057216)
</div>
 


###  What are the practical benefits of doing a power analysis {#practical-power}

A power analysis may allow you to:

- consider the **cost/benefits of 'more data'**, to help you determine 'how much to collect',

-  consider and **optimise over the tradeoffs in design choices** (e.g., introducing more treatments usually involves a loss of power), and

- understand whether you '**have enough funds** to gather enough data to make it worth doing a study?', 

- (maybe) be more credible in making ex-post statements about null effects.\*


Furthermore, if you are trying to do a replication exercise to diagnose the credibility of previous work you want to be able to claim 'I have power to do this credibly'.\*
<div class="marginnote">
\* I'm not sure about these latter point, this needs to be stated more carefully
</div>
 
\

In general, power analyses and ensuring sufficient power is good for science, avoiding the harm from 'underpowered studies'. There are arguments that individual ['underpowered' studies may undermine science](#underpowered), particular in conjunction with 'publication bias'. 


## Key ingredients for doing a power analysis (and designing an experimental study in light of this) {#power-ingredients}

1. Our assumptions (or existing prior data used that we can use for simulations) over the data-generating-process.  In particular, 'what do we expect the distribution of the outcomes to look like' (e.g., 'normally distributed'), and 'with what dispersion'?\*

<div class="marginnote">
 
However, a measure of dispersion is not necessary for *all* power calculations. As noted throughout, we can calculate the power to detect an effect of a particular size *relative to the dispersion*, or to attain a particular confidence interval over such an effect. 

Also note that for binary outcomes the choice of a 'distribution function' is obvious and 'dispersion' only depends on the share of units with each outcome.

</div>

2. The nature of the sampling or assignment to experimental treatments \*, \*\*

<div class="marginnote">
\* E.g., ["complete random assignment](https://cran.r-project.org/web/packages/randomizr/vignettes/randomizr_vignette.html) to two a single treatments and a single control, each with probability 1/2".)

\*\* Remember, here we are focusing on power calculations in *experimental* contexts. However, power calculations are also relevant in other empirical contexts, particularly where data-collection is costly.
</div>
 

3. The specific proposed statistical test (or procedure) to be used (e.g., a t-test or a rank-sum test)

\

*and perhaps the most important and debated ingredient:* 

*4. Which 'metrics of power and effect size' are we considering, and what are our targets?**

- Are we simply seeking 'precise estimates', estimates with small confidence/credible intervals? If so, how precise, and how do we measure this precision? 

- Do we seek power to detect some 'minimal effect size of interest'?

If so, *what is this 'MESOI'?*

<div class="marginnote">
Note that there is (often? always?) a mathematical equivalency between the confidence interval and the standard 'power to detect X' criteria.

Another criterion might be "power to do an 'equivalency test'"... but I need to learn more about this. 
</div>
 \
 

#### Considering: 'should I use a function of previous estimated effect sizes to determine the MESOI?' {-}

From David Moss (unfold)

```{block2,  type='fold'}

Moss: 

> ... not basing power calculations on previously observed effect sizes?

> Lakens uses the SESOI approach, which we often do, but SESOI can be specified based on the effect sizes previous found in the literature [though obviously there are a bunch of ways to do it](https://journals.sagepub.com/doi/10.1177/2515245918770963) 

```

\

Moss, citing Lakens: ... use earlier work to decide which effect sizes are deemed to be 'meaningful', with particular specific recommendations:

```{block2,  type='fold'}

> Subjective justification of a smallest effect size of interest ... Second, the SESOI can be based on related studies in the literature. Ideally, researchers who publish novel research would always specify their SESOI, but this is not yet common practice. It is thus up to researchers who build on earlier work to decide which effect size is too small to be meaningful when they examine the same hypothesis. Simonsohn (2015) recently proposed setting the SESOI as the effect size that an earlier study would have had 33% power to detect.

> With this small-telescopes approach, the equivalence bounds are thus primarily based on the sample size in the original study. For example, consider a study in which 100 participants answered a question, and the results were analyzed with a one-sample t test. A two-sided test with an alpha of .05 would have had 33% power to detect an effect of d = 0.15. Another example of how previous research can be used to determine the SESOI can be found in Kordsmeyer and Penke (2017), who based the SESOI on the mean of effect sizes reported in the literature. Thus, in their replication study, they tested whether they could reject effects at least as extreme as the average reported in the literature. Given random variation and bias in the literature, a more conservative approach could be to use the lower end of a confidence interval around the meta-analytic estimate of the effect size (cf. Perugini, Gallucci, & Costantini, 2014).

Another justifiable option when choosing the SESOI on the basis of earlier work is to use the smallest observed effect size that could have been statistically significant in a previous study. In other words, the researcher decides that effects that could not have yielded a p less than $\alpha$ in an original study will not be considered meaningful in the replication study either, even if those effects are found to be statistically significant in the replication study. The assumption here is that the original authors were interested in observing a significant effect, and thus were not interested in observed effect sizes that could not have yielded a significant result. It might be likely that the original authors did not consider which effect sizes their study had good statistical power to detect, or that they were interested in smaller effects but gambled on observing an especially large effect in the sample purely as a result of random variation. Even then, when building on earlier research that does not specify a SESOI, a justifiable starting point might be to set the SESOI to the largest effect size that, when observed in the original study, would not have been statistically significant.
```
\


DR response: Why do we assume previous authors considered MESOI?

```{block2,  type='fold'}


I’m missing the logic in the quotes above as to “why the previously detected affects, or some bounds on these should represent the minimum effect size of interest”?

Perhaps there is some justification in “assuming that previous authors have powered their study correctly to detect such a minimum affects”, But to me this just seems like kicking the can down the road and I do not assume this in general. We know that people run under powered studies all the time (see the previous discussion on the harm to science)

```

DR: A second reason why one might see that as the “minimum effect size of interest” simply has to do with being able to publish a paper that can in some way “refute” previous claimed findings.

```{block2,  type='fold'}
But that is flawed, in my view, as a way of doing science. We should power the study that is most informative, either by itself, or when made part of a meta analysis. I don’t see the value of this adversarial back-and-forth approach.

```

\

DR preferred approach - power a study based on policy concerns, also considering it's use in meta-analysis.

```{block2,  type='fold'}
One basic argument is: I want to power a study as a practical goal based on my policy concerns. Typically the value of the study will depend on how precisely you are able to estimate and “bound” a parameter. (This may be expressed as a conference interval or a credible interval if we are thinking of a Bayesian posterior). 

So, in determining how much power I wish to achieve, I need to weigh the benefits of this precision against the cost of a larger sample size.

This is a very different considerations from “what power do I have to detect that a previously estimated effect size, if true, is ‘statistically significant’” (By the standard definition).

```



Of course “We should power” is subject to constraints and cost concerns. 

<div class="marginnote">
So, when I am designing the study I am almost never able to have the power I want for all possible tests/hypotheses. Where these considerations come into play, is whether to decide to run the study now or wait to get more funding, and in considering which hypotheses to test and which treatments to put in the study, et cetera
</div>
 



## The 'harm to science' from running underpowered studies {#underpowered}

> "One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.
- From an anonymous referee report

Perhaps most of us consider power largely in thinking about

1. “Is our analysis going to be fruitful for ourselves as researchers?” 

and perhaps also, where we find a null result...

2. “Is the analysis powerful enough to plausibly rule out an effect of a meaningful size?”

The conventional wisdom has been that, at least for papers reporting non-null effects, running a low-power study is mostly done at the authors’ *own* peril. We might think “if I am lucky enough to observe a strong effect in an low-powered study then I have managed to mine a vein of truth on a relatively unproductive plot, and have thus earned my reward.”

However [@buttonPowerFailureWhy2013] point out that running lower-powered studies reduces the positive predicted value—the probability that a “positive” research finding reflects a true effect—of a typical study reported to find a statistically significant result.


In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.

<div class="marginnote">
DR:  However, I speculate that this idea might be less clear-cut than it seems. E.g., if we consider a (“non-sparse”) world where every factor indeed has an effect, lower powered studies are more likely to detect effects that are truly larger, which are arguably more policy-relevant; moreover, overstated effect sizes might be adjusted with a standard correction.
</div>


```{block2,  type='note'}
[Ferraro discussion on magnitude error due to underpowered studies:](https://www.pauljferraro.com/publications/2020/2/1/is-there-a-replicability-crisis-on-the-horizon-for-environmental-and-resource-economics) {-}

... if you are looking at an under-powered design then sure, you might pick up a significant result which is actually spurious. But on top of that even if there is a genuine effect there, the effect that you actually pick up as being significant will (likely) be overestimated. The intuition behind that result is (I think) that for an effect to be picked up in a study then it has to be large enough to overcome the issue that you face with power. Low-powered studies can only detect really large effects, and so the large effect you pick up in such a study could be genuine, but it equally could be a poorly-estimated coefficient. By using a low powered study you sift through for these kind of effects.

```


## Power calculations without real data

[R 'Paramtest' package vignette](https://cran.r-project.org/web/packages/paramtest/vignettes/Simulating-Power.html) is helpful here.

## Power calculations using prior data

Adapt example in 'scopingwork.Rmd' to this

### From Reinstein upcoming experiment preregistration

> We are searching for a design and sample size that has sufficient power to detect (or ‘statistically rule out’) an effect of ‘minimal interest’ size, given our somewhat-limited budget. The ‘design parameters’ we can play with are given above.

> While conventional practice seems to involve completely simulated data based on parametric assumptions (normality, etc) we prefer to draw from comparable ‘untreated’ real-world data (see (Barrios 2014) for a related discussion). Assuming the general distribution of outcomes (and covariates) is in some sense constant or predictable over time (perhaps stationarity?), this should give us more accurate estimates of power.

> We will consider each design’s power to detect particular ‘treatment effects’ (of a minimum relevant size) on particular outcomes, which may be linear, proportional, or otherwise. Our calculations do not depend on any assumptions over the ‘true treatment effect’.


#### Assignment procedures to consider {-#assign-notes}

We consider three categories of possible assignment criteria: (We are coding these [below](#assign-functions).)

**1. Simple data-based**

(Coded [here](#power-simple))

Here we imagine a very simple dynamic assignment to treatments with alternation (or repeated from an urn with one ball per treatment, refilling the urn once empty). This procedure will essentially guarantee an equal share of observations in each treatment.

We will also consider an unbalanced design, both in this and in other categories, which may achieve greater power, especially considering the differential costs of our treatments.

<div class="marginnote">
Although as we have no evidence on the treatments and thus no reason to anticipate a differential variance between treatments and control, an unbalanced design may allow greater power for the same *cost*, as observing controls is costless, and the 'low-donation' treatment is lower cost.
</div>

\

**2. Ad-hoc (Reinstein adapts Barrios), using prediction.Rmd quantiles**

*General summary:* Fit a predictive model of the outcome (total donations) based on pre-treatment observables, using set-aside training data. Generate quantiles of 'predicted donation' (tuning parameter=number of quantiles?). Power-test block randomisation with these blocks as quantiles, using set-aside testing data.

*Caveats:* If we test the power with multiple models on the test data  (e.g., 'tuning' the number of quantiles) we will be overly optimistic and maybe overfitting.

Also, this assignment procedure is not necessarily robust to TE heterogeneity. By luck, it may assign substantial *imbalance* across any particular dimension.

\

Prediction algorithm (folded)
```{block2,  type='fold'}

- Adapt from code in CharitySubstitutionExperiment repo, in assignments_power.Rmd and analysis_subst.Rmd; examples of Elastic net etc

1. Define and organize the set of variables available at intervention

2. Define and calculate the outcome variables (total amount raised)

3. Split and set-aside validation and simulation data (?within prediction also)

4. Model the outcome, using a Ridge regression with all features.  The regularization/penalty parameter could be optimized for best fit. (Cross-fold).

```

Blocking process (folded)
```{block2,  type='fold'}

5. We can test block randomization by the predicted quantile of this model with a bootstrapped procedure using set-aside 'testing' data not used in the above regression.  We want to run the simulations until we find the optimal "block width" or quantile to use for blocking the randomization. Ideally, this procedure should take into account the fact that if we stopped at a random time we may have uneven cell sizes.

(Caveat -- this resampling may need to be done based a randomised 'start time' to address random time-specific effects. ) *Note*: Because of this and for feasibility, we may abridge step 5, and just try out a few reasonable large block widths (e.g., quartiles)

Probably the way to do this is, for each proposed block width (e.g., quartiles, deciles, 15 bins, etc) we draw random samples from the set-aside data according to this procedure,  and estimate an "effect size" for each sample. We then consider the 99% bounds of these simulated effect sizes.  The block width (and  regularization parameter?) that consistently gives us the tightest bounds  should be the one that allows us the greatest power to rule out an effect of a certain size, given the null hypothesis of no treatment effect.

The intuition: The smaller the largest difference in mean total donations (between treatment and control) that occurs by chance in 99% of draws... the smaller the *actual* effect that we will be powered to detect (able to judged as statistically significant a reasonable share of the time).    [Note: these latter notes may bot go with the procedure proposed below; these are older.]

```



**Kasy method?**

We may or may not get to considering the method proposed in @kasyWhyExperimentersMight2016. It is ideal for Bayesian approaches to policy, but it may make frequentist inference difficult (?). (See steps/notes folded below.)

```{block, type='fold'}

See kasy_2016_dont_randomise.md; kasy_dynamic.md may also be relevant.

- Prepare concise data set (csv?) to throw into his app, choose baseline covariates
- Estimator: Difference of means or Bayes
- Prior:  Squared exponential or Linear?
- Re-randomization draws (default=1000); Expected R-sq (default=0.7)

Notes:
- Stratify on 'discrete strata'
- Conservative: difference in means without controls or interactions
- More reasonable, fully general: Make estimator in Power calc regression with strata dummies and interactions with treatment, usual Robust standard errors
- But Kasy's technique makes frequentist inference difficult (Bayesian OK)
```


**See Guidance/code:**

- Simple; from  <https://egap.org/content/power-analysis-simulations-r>

https://egap.org/resource/script-power-analysis-simulations-in-r/

https://egap.org/resource/10-things-to-know-about-statistical-power/


#### Treatment assignment functions, used in power calculations {-#assign-functions}

1. simple_assign: assign treatment dummy to first t-share of n rows

```{r simple_assign}

simple_assign <- function(df, tshare=0.5, blockvar="NA") { #note: no blocking here, this is just to get a homogenous code
  mutate(df, d_t = row_number() <= (n() * tshare)) #note: the data must be randomised first!
}

```

\

**Data-based power calculation: create simulation function** {#power-calc-sim-func}

<!-- partly from https://stats.stackexchange.com/questions/37796/calculating-necessary-sample-size-using-bootstrap -->

```{r power-simulations}

power_data <- function(ds, reps, yvar, N, tshare=0.5, linTE=0, propTE=0, alpha=0.05, test_nm= wilcox.test, f_assign=simple_assign, bv) {

    results  <- sapply(1:reps, function(r) { #TODO - replace with purr::map (Toby)
#1. Sample size N from data for each iteration
    exp_sample <- sample_n(ds, size = N, replace = TRUE)

#2. Selection of control and treatment group using function `f_assign'
    exp_sample <- exp_sample %>%
      f_assign(tshare=tshare, blockvar=bv) %>%

#3. Add treatment effects (`propTE` and `linTE`) to treatgroup
      mutate({{yvar}} :=   ifelse(
        d_t == "TRUE", {{yvar}} * (1 + propTE) + linTE, {{yvar}})) %>%
#TODO: Do this for several y-variables in each run; 'map' these?
      mutate(yv := {{yvar}}) %>%  #reassign variable because I couldn't figure out how to get the unquoted argument to work in tests below #TODO-fix

#4. Run chosen test `test_nm`, output p-value
      dplyr::select(yv, d_t)
    test <- exp_sample %>%
      do(tidy(test_nm( yv ~ d_t, data = ., paired = FALSE )))
    test$p.value
    }
    )

#5. Output share of p-values below alpha
      sum(results < alpha) / reps
  }
```



2. block_1d_assign: assign treatment dummy to first t-share within each (pre-calculated) one-dimensional block group (`blockvar`)

*Note: I am using 'randomizr' here to assign blocks*

```{r block_1d_assign}

block_1d_assign <- function(df, tshare=0.5, blockvar) {
      block_rand <- as.tibble(
        randomizr::block_ra(
          blocks =  df[[glue("{blockvar}")]], conditions = c("control","treat"), prob_each=c(1-tshare, tshare)
          )
        )
      df <- as.tibble(bind_cols(df, block_rand)) %>%
        rename(d_t=value) %>%
        mutate(
          d_t=(d_t=="treat")
        )
}

```


#### Power calculation (just simple examples): adapt to built-in data {-}
```{r}

#pwr_n400_L50_p15 <- power_data(ds=df,reps=100,yvar=sum_don,N=400,tshare=0.5,linTE=50,propTE=0.15,alpha=0.05, f_assign=simple_assign)

```

#### Loop and plot over...

```{r}

linTE.try <- c(0,50,100)
propTE.try <- seq(from=0.05, to = 0.2, by = 0.05)

outcomes.try <- c("sum_don","count_don")
tests.try <- c(t.test, wilcox.test)

```


Testing equicost parings, determine necessary cost

With only control and treatment we have

$$cost = multiplier \times avgcost \times N \times tshare$$
$$\rightarrow N = cost/\big(multiplier \times avgcost \times tshare\big)$$

Setting a 2x multiplier (for the 'large') treatment, avgcost=£30 (hard coded) for now, and imagining a £6000 initial budget yields

$$\rightarrow N = 6000/\big(60 \times tshare\big) $$



```{r}

cost <- 6000 #make this an entry in the 'design_params' list
multip <- 2
avdon <- 30

tshare.try <- seq(from=0.1, to=0.5, by=0.1)
sample.try <- seq(from=400, to=1200, by=200)
equi_sample <- cost/(avdon*multip*tshare.try)

# now maybe make a vector of tshare.try and equi_sample, for iterating over
# I assume we can consider the 'optimal tradeoff' and this will be invariant to the cost; am I right?


```


#### For 'total x' outcome variable {-}

(Some sample code below, needs discussion)

```{r eval=FALSE}

#Unvarying parameters up here:
linTE <- 0

#power_vals: tibble to collect parameters and results #TODO: faster to generate a list?
power_vals <- tibble(
  n_try=NA,
  prop_te = NA,
  lin_te=NA,
  test=NA,
  power_est=NA
)

#tic() #timer

#for (o in seq_along(outcomes.try)) { #TODO: map instead of loops. See R4ds 21.7 'mapping over multiple arguments'

  #OUTCOME <- outcomes.try[[o]] #TODO: may be faster to generate all outcomes for each sample ; also, I haven't got the syntax to work

  df_x <- df %>%
    select(outcomes.try)
    #select(OUTCOME) #Minimal data set to speed it up; (seems to save about 50% of the time)

  #loop over tests
  for (t in seq_along(tests.try)) {
    TEST <- tests.try[t]

    #Loop over proportional TE
    for (p in seq_along(propTE.try)) {
      PT <- propTE.try[[p]]

      #Loop over sample sizes
      for (s in seq_along(sample.try)) {
        N <- sample.try[s]
        PW <- power_data(
            ds = dfX, reps = 80, yvar = sum_don, N = N, tshare = 0.5, linTE = linTE, propTE = PT, alpha = 0.05
          )
        power_vals <- add_row(
            power_vals, n_try = N, prop_te = PT, lin_te = linTE, test = as.character(TEST), power_est = PW
          ) ##TODO - more efficient to save the results in a list and combine it into a single vector or dataframe at end (see r4ds 21.3.3)
      }
    }
  }

  #}

#toc()
  
```

\


## Survey design digression: sample size for a "precise estimate of a 'population parameter'" (focus: mean of a Likert scale response) {#survey-power-likert}

### How to measure and consider the precision of Likert-item responses 

Considering ‘precision of Likert-item responses’ and sample-size calculations:

What are commonly used/justifiable measures of central tendency and dispersion for Likert-items?

How can we think about ‘precision of estimated Likert-item responses?’ and attaining sufficient precision, and a metric for this?


"How precise is precise, and by what metric?"

\

#### A simple naive approach? {-}

Interval coding:  $y=\[1,2,3,4,5\]$ for a 5-item

Outcome: $\bar{y} :=$ Sample mean of numeric-coded responses,

Measure of dispersion:  $\hat{s}$ := Sample standard deviation of y \*

<div class="marginnote">
Perhaps with the $n-1$ correction, but who cares/
</div>


Measure of (inverse of) precision: estimated standard error of the mean $\hat{SE_m} = s/\sqrt(n)$

If we assume $y$ is normally distributed (which obviously can’t be precisely the case)...\*\*

<div class="marginnote">
\* \* but Wiki (Derrick and White 2017?) claim "responses often show a quasi-normal distribution."
</div>
 

... then a 95% confidence interval for $\bar{y}$ would be

$$\bar{y} \pm  1.96 \: \hat{SE_m}$$

\

**A. ‘Absolute’ metric?:** Target a 95% CI range less than (e.g.) 1 ‘Likert scale unit’,
i.e.,

$$2 * 1.96 \: \hat{SE_m} < 1$$ \*

<div class="marginnote">
\* Or perhaps considering the actual rather than estimated 95% CI this should be  “$2 * 1.96 \: SE_m < 1$”.
</div>
 
Recall that $\hat{SE_m} = s/\sqrt(n)$.

Thus, to choose a sample size to achieve these bounds we need to have a measure/guess/estimate of $s$, the standard deviation of $y$, perhaps based on previous data.\*\*

<div class="marginnote">
To have (e.g.) an 80% probability of getting these bounds for the actual confidence intervals we would also need a measure of the dispersion of our estimate of this sd. (Hmm, it’s getting complicated).
</div>



\

**B. ‘Relative’ metric?**:  Target a 95% CI range below $B$ sd of the Likert-item-integer-response $y$:
i.e.,

$$2*1.96 SE_m < B*sd(y)$$

i.e., $2*1.96 * s/\sqrt(n) < B*s$,
i.e., $2*1.96/\sqrt(n) < B$
i.e., $\sqrt(n) > 2*1.96/B$
i.e.,

$$n >  15.3664/(B^2)$$

... where $sd(y)$ is the true standard deviation of the outcome.

<div class="marginnote">
Note: This $n$ gives should give us an estimated CI with a range of B standard deviations of the outcome. I’m not sure if it implies that, after collecting the sample, our estimates of the CI will always have a range equal to the estimated sd. Need to think about this more.
</div>
 
As you can see (caveat: calculations need doublechecking), if we assume the Likert-integer-thing is normally distributed, the calculation of ‘how big a sample size (n) we need in order to get, on average, a CI of 1 sd or smaller’ is straightforward.
 



### Computing sample size to achieve this precision

Initial thoughts (unfold):

```{block2,  type='fold'}

If we assume normality, there should be a simple analytical formula for

’Minimum sample size….
… for (e.g.) 80% likelihood …
… of achieving an (e.g.) 95% “confidence interval (CI) over the mean” of a variable…
… that is within (e.g.) 1 standard deviation of the variable on either side

*Notes:*
  
- This is to get CI bounds on the means stated relative to the SD of the variable. If we wanted to bounds in ‘units of the variable’ we would need to know, guess, or estimate the SD of the variable. 

- For a Likert variable normality is not a great assumption. We should probably make another assumption over the distribution (or even draw from past data), and then we can either do a similar analytical computation or a simulation based computation (which should be fairly easy)

- I put ‘CI over the mean’ in scare quotes because these are frequentist confidence intervals which are hard to interpret. A Bayesian approach might be more appropriate… worth thinking about

-  Not sure whether ‘mean of a Likert-item response’ is important anyway. I’ll read more on Likert scales.
```


## Digression: Power calculations/optimal sample size for 'lift' in a ranking case

We want to know what the 'best title for our new movie' is. Twenty titles have been suggested. We have funds to do a survey of a relevant representative audience. 

We need to decide on a general experimental design, a statistical analysis, and on sample sizes considering power (or perhaps 'lift').

Note that although we are mainly framing this in terms of statistical inference, it might also/instead be considered a 'reinforcement learning' problem.\*

<div class="marginnote">

\* See Max Kasy's slides and articles on [adaptive field experiments](https://maxkasy.github.io/home/files/slides/adaptive_field_slides_kasy.pdf), particularly considering 'exploration sampling'.

</div>
 

\

### Design: Which questions to ask the audience about the proposed titles, and in what order

This is an 'experimental design for internal identification and external generalisability' question. (See ['Identifying meaningful and useful (causal) relationships and parameters](#why_experiment_design))

\

Some possibilities: 

- Subjects asked to rank (or rate) all 20 titles
- Subjects asked to identify 'top N' and 'bottom N' (e.g., top and bottom 3) titles
- Subjects presented a series of pairwise comparisons
- Subjects asked to rate (or say whether they would attend) a single title, with between-subject variation


### Which statistical test(s)/analyses to run (if any) and what measures to report?  {-}

Suppose we asked each subject to rank all 20 titles. 

How could we **test if there were any 'substantial difference in the title rankings'** and what would be a meaningful measure of the 'extent' of this difference? We might want to consider some 'minimum effect size of interest' and ensure that we have a large enough sample to diagnose such an effect with (e.g.) 80% probability (while maintaining a false-positive type-1 error rate of less than 5%).\*

<div class="marginnote">
\* However, it is not clear why this is the most relevant question. Simply determining 'there is a difference of some minimum size' doesn't tell us how confident we are about the best title, nor how much value is gained by choosing that title. This suggests a reinforcement learning approach. 
</div>
 

[Friedman's Q](https://en.wikipedia.org/wiki/Friedman_test#:~:text=The%20Friedman%20test%20is%20used,by%20many%20statistical%20software%20packages), is a measure of whether 'any (at least one?) items are systematically ranked higher or lower'.  $Q$ can be normalized into, [Kendall's W](https://en.wikipedia.org/wiki/Kendall%27s_W), a measure of 'inter-rater agreement' going from 0 to 1. There is a significant test for W "against a null hypothesis of no agreement (i.e. random rankings)".

[Kendalls uses the Cohen’s interpretation guidelines of 0.1 to 0.3  being a 'small effect'](https://rdrr.io/cran/rstatix/man/friedman_effsize.html)


"A significant Friedman test can be followed up by pairwise Wilcoxon signed-rank tests for identifying which groups are different", with multiple testing corrections.  [datanovia website](https://www.datanovia.com/en/lessons/friedman-test-in-r/)


\

### How to assign the 'treatments', and how large a sample is optimal, considering 'power' (or 'lift')? {-}

#### Simple assignment {-}

Suppose we are restricted to a single allocation of treatments across the 20 titles.  Suppose we asked all subjects to rank all of the 20 titles, or perhaps only to focus on the 'best' and 'worst' titles. 

\

*We might frame our test and power calculation as the following:*

Suppose the 'Minimal effect of interest' that we want to be able to detect is (sort of the 'alternative hypothesis HA')...\*
 
HA: "One title is ranked by a share of the population that is one and a half times as high as any other title."

If all the other titles have the same (lower) ranking on average, this should offer the greatest chance of detecting such a difference. Thus, if assuming this, the computations below should *underestimate* the necessary sample size.

I.e., defining $r_{i,j}$ as the rank given title $i$ by subject $j$, and letting $\bar{R^1_j}=\frac{1}{n}\sum I(r_{i,j}=1)$ be the share of the sample ranking title $j$ as first, we may consider a case where $\bar{R^1_j} > \frac{3}{2}\frac{1}{19}\sum_{k\neq j}\bar{R^1_k}$. (I think the latter term may simply be \frac{3}{2}\frac{19}{2}, or something similar, the 'average rank'.) 

... perhaps, we want to power our test so, in such a case, we have an 80% chance that we ‘find an effect’. I.e., an 80% chance that our test statistic (whatever it is) tells us that "it is less than 5% likely that this title would have performed as well in our sample by chance if (H0) all titles been perceived as equally good and thus randomly ranked in the population.”

(But I don’t really think that that is what we are looking for.)

To test for this in we would follow a certain procedure, e.g., (and I am not sure this is an appropriate procedure) 

1. Find the title 'j' that has the most people ranking it first in our sample, share $\bar{R^1_j}$

2. Compute (perhaps through simulation) the probability that, if all titles were randomly ranked by the population, in a sample of size $N$ (our actual sample size), the average rank of the highest-ranked title would be as high as $\bar{R^1_j}$.

3. If this computed probability of such an extreme result, given our sample size, $P(N, \bar{R^1_j})$ is below our threshold $0.05$ we 'reject the null.'
\

Given such a procedure, we can now simulate (or perhaps calculate) the power of our design and test against the above HA.

For each simulated sample $t$:
- Draw $N$ observations from an imagined 'true population', 
  - with 20 ranks being drawn for each of the $N$ individuals 
  - with one title (the same one always) having a 2/20 probability of being drawn for the first rank, and the other 19 titles each having a 1/20 probability of being drawn for the first rank (OK, this doesn't add up, adjust the fraction slightly)
- Compute $[\bar{R^1_j}]^t$ and then $P(N, [\bar{R^1_j}]^t) \equiv [P]^t$

Over a sufficient number of simulations, determine the average probability of 'rejecting the null' in favor of the above HA (specifically for the 'correct' title $j$).\*\* This is the power of the test.
<div class="marginnote">
\*\* This last point is a wrinkle I've not seen in previous work involving power calculations, so I hope I am not missing something here. 
</div>
 

#### Sequential/adaptive designs, multi-armed bandits {-}

<div class="marginnote">
More generally, see \@ref(sequential). 
</div>
 




