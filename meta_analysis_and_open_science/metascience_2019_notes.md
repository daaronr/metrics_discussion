% metascience_2019 notes

#Dean Keith Simonton University of California, Davis Scientific Creativity: Discovery and Invention as Combinatorial

# Roberta Sinatra IT University of Copenhagen Quantifying the evolution of scientific careers

'random impact rule'  as well as  'hot streaks period'



Despite the frequent use of numerous quantitative indicators to gauge the professional impact of scientists, little is known about how scientific impact emerges and evolves in scientific careers. In this talk we present a series of findings from the analysis of a **large-scale dataset of scientific careers.** We tackle the following three questions:

How does impact evolve in a career? What is the role of scientific chaperones in achieving high impact? How interdisciplinary is our award system?

We show that impact, as measured by influential publications, is distributed randomly within a scientist’s sequence of publications, and formulate a stochastic model that uncouples the effects of productivity, individual ability, and luck in scientific careers. We show the role of chaperones in achieving high scientific impact and we study the relation between interdisciplinarity and scientific recognitions. Taken together, we contribute to the understanding of the principles governing the emergence of scientific success.

# Melissa Schilling New York University:  Where do breakthrough ideas come from?


Where do breakthrough ideas come from?
Most innovations and scientific discoveries are incremental advances of existing knowledge and technology. Some innovations and discoveries, however, appear to be much larger leaps, and can be very disconnected from, or disruptive to, existing technology trajectories and scientific theories and paradigms. Where do such breakthrough ideas come from, and is there any way to foster their discovery? This presentation will integrate results from two studies: A seven-year multiple case study research project on serial breakthrough innovators, and a large sample analysis of “outlier patents.” These studies reveal common patterns – including innovator characteristics, experiences, and search processes — underlying the generation and pursuit of breakthrough ideas.

# Jacob Foster
University of California, Los Angeles, USA

Made to Know: Science as the Social Production of Knowledge (and its Limits)
In this talk, I develop a view of science as the social production of collective knowledge by a complex adaptive system. Using data from millions of scientific papers, I illustrate that scientists’ research choices are shaped by the tension between tradition and innovation, which generates a distributed algorithm for directing scientists’ collective attention. I then show how this distributed algorithm leads to more (and less) efficient collective discovery. Such distributed algorithms are “programmed“ by scientific institutions. To clarify our understanding of these institutions, I describe a simple formal model of scientific problem choice and use it to show that taken-for-granted features of scientific institutions can have unexpected consequences on the pace of knowledge production. I draw together these results using ideas from computational learning theory to suggest how scientists’ strategies, though objectively adapted to social goals, can nonetheless support robust collective creation of knowledge about the natural world. In other words, the production of collective knowledge is made possible by the distinctive cultural technologies of science—which also produce limits to that same knowledge. I conclude by briefly considering the ominous possibility that the participation of (even quite modest) “machine knowers” in science could produce insurmountable limits to human understanding.

"epistemically sullied"
patent races

Wants to bring together 'science of science' and 'science studies' people


Lots of effort testing things that 'seem plausible'? How do you know what not to focus on? Usually it is via social networks, which are stratified. A lot of wasted work among those who 'haven't heard the news yet'. Correct environment: put out negative findings quickly, according less credit to these... awarding more credit to positive findings and determinative negative findings.

# Carole Lee University of Washington Gender-based homophily in collaborations across a heterogeneous scholarly landscape

Carole Lee
University of Washington, Seattle, USA/em>

Gender-based homophily in collaborations across a heterogeneous scholarly landscape
The tendency to associate with individuals of the same gender creates profound divisions within professional and social contexts. We investigate this tendency within scientific co-authorships using the JSTOR corpus of articles. We distinguish three components of gender homophily in collaborations: a structural component that is due to demographics and non-gendered authorship norms of a scientific community, a compositionalcomponent which is driven by varying gender representation across subdisciplines, and a behavioral component which we define as the remainder of observed homophily after its structural and compositional components have been taken into account. We find that significant behavioral homophily can be detected across the JSTOR corpus and show that this finding is robust to missing gender indicators in our data. In a secondary analysis, we show that the proportion of female representation in a field is positively associated with significant behavioral homophily.

Random swapping of composition, 'null distribution of alpha' ... holding composition at higher level constant... to consider

XXX homophily

'Compositional homophily'

'Behavioral homophily'

... the reversal due to gender imbalances and sructural differences across subpop

'simpson's paradox'



# Tim Errington COS, Charlottesville, USA Barriers to conducting replications – challenges or opportunities?

Interpretation of scientific findings requires the reader to understand what was being tested, how it was tes- ted, how it was analyzed, and what results were obtained. Scientific articles are the primary means to communicate these pieces of information. However, what constitutes as enough information is not always clear or known and varies by discipline, journal, and scientist. Additionally, this varies over time as established methodologies mature and new methodologies emer- ge. This presentation will focus on the barriers encountered when gathering this information as part of the Reproducibility Project: Cancer Biology (https://osf.io/ e81xl/wiki/home/), a project to assess reproducibility rates, predictors of reproducibility, and common obstacles to conducting replications in preclinical cancer biology. The replications illustrate some of the challenges and opportunities in how research is conducted and communicated.

'failed replications' were harder to publish


Q \& A on changing the system

'We shouldn't be in a position where someone's career depends on a research  outcome'  (paraphrase)

# Shirley Wang Harvard Medical School, Boston, USA

What does replicable ‘real world’ evidence from ‘real world’ data look like?
Regulatory and Health Technology Assessment (HTA) organizations are increasingly looking toward use of ‘real world’ evidence (RWE) from ‘real world’ data such as administrative claims and electronic health record databases to support decision-making. Ideally, decision-makers want to focus on RWE that uses valid methodology, has transparent reporting, and generates reproducible evidence. However, published database studies frequently do not report on key implementation parameters, making it difficult to detect flaws in design or analysis. To increase confidence in use of RWE, decision-makers need the means to effectively and efficiently distinguish between studies of high versus low validity. The REPEAT Initiative has projects focused on improving transparency, reproducibility and validity of database research. These pro- jects include large scale replication of 150 published database studies, evaluation of robustness of results to alternative study parameters, and development of a structured reporting template with design visualization to increase transparency of reporting and minimize misinterpretation.


These are 'database' studies! They should replicate exactly (?!)

Primary measures of interest
- difference in log effect sizes
- 'calibration' ... overlap of confidence intervals ('limits'?)

    "we were able to replicate some low quality studies  closely, because they reported their methods very well"

# Jonathan Schooler: How replicable can psychological science be?: A highly powered multi-site investigation of the robustness of newly discovered findings

University of California, Santa Barbara
Co-Authors: John Protzko, Brian Nosek, Charlie Ebersole, Jon Krosnick, Sebastian Lundmark, Matt Berent, Matt Debell, Bo MacInnis, Leif Nelson, Hannah Perfecto, Mike O’Donnell, Scott Roeder, Jan Walleczek

There has been an increasing concern among scientists regarding irreproducibility of scientific findings in general, and psychological findings in particular. To date, the understanding of reproducibility has been impeded by two related challenges: 1) the lack of transparency of the scientific record, and 2) the retrospective nature of reproducibility studies. In order to overcome these obstacles, four labs conducted a large scale multi-site prospective multi-replication study. Each lab independently discovered new psychological findings that were then systematically replicated by the originating laboratory and by the others, following a complex pre-specified sequence of various replications and analyses. In so doing, this project 1) developed a gold standard for replication protocol, in which every effort was made to design experiments and implement replications in a manner that simultaneously maintained ecological validity while maximizing the likelihood of full replicability, and 2) tested whether the replications of newly devised experimental protocols are associated with declining effect sizes, even when all reasonable efforts are made to minimize such declines. Although the project is still underway, preliminary analyses indicate that when a gold standard approach is applied psychological findings are highly robust.


Why do findings typically 'decline with replication'?


Regression to the mean and 'random inflation'

Publication bias ... esp to large effects

Selective reporting

Change in protocol -- why should this tend to lead to a decline?

Something about placebo effects becoming stronger in later studies

Crazy idea that observation actually changes things

Present study: Find souece of this... do 'newly developed' things still show the decline effect?

# Yang Yang Northwestern University, Evanston, USA The Replicability of Scientific Findings Using Human and Machine Intelligence

In top journals, more papers fail than pass replication tests and papers failing replications spread as widely as replicating papers. This dynamic raises research costs by over 20bn annually, jeopardizes the literature, and exposes the need for new methods for predicting replicability. Using 96 studies that underwent rigorous manual replication, we developed an artificial intelligence (AI) model that predicts a paper’s replicability. We then tested the model on 317 diverse out-of-sample studies that span disciplines, methods, and topics. We find that AI predicts replicability better than statistics and individual reviewers and as accurately as prediction markets, the gold standard of replicability methods. Further, AI generalizes to out-of-sample data at AUC levels up to 0.78. Finally, tests indicate that the AI model does not show biases common to human reviewers. We discuss how AI can address replication problems at scale in ways that current methods cannot and can advance research by combining human and machine intelligence.


# Michèle Nuijten Tilburg University, NL Checking Robustness in 4 Steps

Whether a published finding is robust is difficult to assess. Researchers often point at replication as a robustness check. However, conducting a replication on a new sample can cost a lot of time, effort, and money. In this talk, I propose a consecutive “four-step robustness check” that aims at the low-hanging fruit first. First, we check the internal consistency of statistical results (possibly using automated tools, such as “statcheck”). Second, we reanalyze the data using the original analytical strategy to see if the reported conclusions hold. Third, we check if the original result is robust to alternative analytical choices, for instance via a multi-verse analysis. Only then, in the fourth step, we perform a replication study on a new sample. This four-step approach allows detecting unreliable results, while wasting as little resources as possible. I will discuss potential advantages and limitations of this approach.

Go to her biography

Reproduceability .. reanalysse the original data before replication ... the former is a prerequisite

4 steps:

1. Checking the international consistency of the statistical results

'statcheck' !!

The GRIM test (Brown and Heathers)

2. Reanalyse the data using the original analytical strategy

3. Check if robust to alt analytical choices

- 2 -tailed
- remove one arbitrary covariate
- reanalyse following reported procedures
- exclude includeoutlier
- exclude last observation


4. Perform a replicaiton study in a new sample


IN your own work ... to show robustness

0 share well-documented data, analysis sccripts, in-chouise code review, code review during peer review, dynamic documents

report robustness to analytical choices.
... 'sensitivity analysis'

facili

!!Meta-science symposium 21 November Tilburg, also July 2020

#Andrew Gelman Columbia University, New York, USA Embracing Variation and Accepting Uncertainty: Implications for Science and Metascience

The world would be pretty horrible if your attitude on immigration could be affected by a subliminal smiley face, if elections were swung by shark attacks and college football games, if how you vote depended on the day within your monthly cycle, etc. Fortunately, there is no good evidence for these and other high-profile claims about the effects of apparently irrelevant stimuli on social and political attitudes and behaviors. Indeed, for theoretical reasons, we argue that it is not possible for these large and persistent effects to co-exist in the real world. But if the sorts of effects being studied vary greatly by person and scenario, then simple experiments will not yield reliable estimates of effect sizes. It is necessary to instead embrace variation, which, in turn, requires accepting uncertainty. This has implications for the practice of science and for the proper understanding of replication and other aspects of metascience.

"We are too polite"
... the goal of metascience is producing science not scientists
"being retracted *should* hurt my career ... publishing it helped my career"

# Zoltán Kekecs Eötvös Loránd University, Budapest, HU How to produce credible research on anything

How can we trust research produced by others, and how can we show others that the research we produce can be trusted? What would it take for you to trust a re- sult that directly contradicts your prior beliefs? A host of credibility enhancing approaches will be discussed including crowdsourced research design, automation, trusted third party oversight, tamper evident seals on data and software, documented training, lab logs, and radical transparency about the whole research pipeline. These techniques and their consequences are pre- sented in the context of a massive multilab replication of one of Bem’s 2011 parapsychology experiments investigating human precognitive abilities that shocked psychological science and contributed to the initiation of the reformist movement on the field.

"Consensus design process" ... proponents and adversaries

Agreed on
appropriate protocol
Safeguards
Interpretation of the possible results

# Edward Miguel University of California, Berkeley, USA,  Innovations in Pre-registration in Economics

The adoption of pre-registration has increased rapidly in Economics since the start of the American Economic Association registry in 2013. We discuss recent evidence on the practice of pre-registration in Economics, including opportunities for improvement. We survey frontier topics in pre-registration in the field, including the collection of expert forecasts, pre-specifying the research process, the pre-registration of prospective observational studies, and recent journal efforts to incorporate pre-results review.

Research transparency as the next step in 'credibility revolution'

Three leading concerns
1. Fraud
2. Publication Bias
3. ? Missed it

8 Health economics journa;s editorial statement ... to accept well-designed studies "regarless of whether... empirical findings do or do not reject the null hypothesis"

Blanco-Perez and Brodeur '19 -- increase in the share of null results published ... among those who participated in the editorial statement

New AEA data and code posting requirements, including in-house relutst cerification, will attain TOP Level III (Nosek ea, '15)
Journals copying this

Rise of study registration and 'pre-results review'
Believe emerging consensus in favour of Paps.
Debate about 'how detailed they should be'

## Pre-results review

Levine/Krueger '96 ... 2001 Neumark, David Levine committed to publish results in Industrial Relations


JDE launched pilot of pre-reults review in March 2018

-- recently made tis a standard permanent submission track
- ExpEcon Lanching a pilot

"Trends in open science attutudes and practices across social science'

- Sent out a survey to a representative sample... (top publishers and Phd's in top-20)

- Chronicles rise/rate of prereg, posting study insgtruments, data, any
