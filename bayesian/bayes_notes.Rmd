# Bayesian approaches




```{block2,  type='note'}
I take notes on several different resources/texts below. Ultimately I'll try to integrate these into a single set of notes.
```  

## My (David Reinstein's) uses for Bayesian approaches (brainstorm)



### Meta-analysis of previous evidence

- Of prior work, especially on motivators of (effective) charitable giving and responses to effectiveness information

- Of my own series' of experiments (potentially joint with prior work)

### Inference, particularly about 'null effects'

When/what can we say about the 'absence of an effect'

How to integrate into inferences from diagnostic testing (e.g., common-trend assumption)?  

### 'Policy' and business implications and recommendations

- E.g., for 'which pages to seed'

### Theory-driven inference about optimizing agents, esp. in strategic settings

- Especially in 'predicted contributions to public goods' settings and 2nd order beliefs

### Experimental design

- Optimal treatment assignment, with previous observables and a track record

- Sequential designs

- Bayesian Power calculation
\

```{r}
#sessionInfo()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Package loadings from Kurtz:
```{r, echo = F, message = F, warning = F, results = "hide"}

pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")

library(tidyverse) #adds in next chapter

```


## 'Statistical thinking' (McElreath) and [AJ Kurtz 'recoded' (bookdown)](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded): highlights and notes 


```{block2,  type='note'}

McElreath's course and text looks great. I'm taking selective notes here; I'll try to incorporate content from both text and [youtube video lectures](https://www.youtube.com/watch?v=4WVelCswXo4&list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI).

[AJ Kurtz has re-written the code](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded) using the `brms` package, which he finds superior. More crucially for me, he redoes the code using ggplot and tidyverse?
  
I'm planning to through this here, adding my own notes, questions, and considerations and (hopefully) incorporating some of my own work.
```  

<div class="marginnote">
I've also forked Kurtz's repo [here](https://github.com/daaronr/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse), which I may play with.
</div>
 
### The Golem of Prague (Chapter 1)

Don't let your model or approach turn into a Golem you can't control. Don't 'believe the model'; continuously validate it. The map is not the territory.

'Statistical decision trees' lend a false sense of security... and almost never fit the actual case we are dealing with. (fig 1.1)

\

Statistical models are non-unique maps to 'Process models' which are non-unique maps to Hypotheses. ('Nuetral evolutionary selection' example.) This makes strict falsification impossible ... How can you falsify a hypothesis/theory if it corresponds to a wide set of process models and statistical models, many of which overlap other hypotheses? 

\

But this warning is at least as relevant for Bayesian analyses, which must be based on specifically defined (term) models of the DGP etc. Thus he recommends caution and continuous (?) interplay between the model and the data. (See next chapter ... 'small worlds and large worlds'.)
\

He also suggests we refer not to 'Confidence intervals' or even 'Credible intervals', but to 'Consistent intervals' ... as in 'these intervals are consistent with the model and data'.

\

And... 
> [so you should] '...Explicitly compare predictions of more than one model'


#### Rethinking: Is NHST falsificationist? 

```{r failure_of_falsification.png, fig.cap='From McElreath video lecture 1', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("images/failure_of_falsification.png")

```

> Null hypothesis significance testing, NHST, is often identified with the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify a null hypothesis, not the actual research hypothesis. So the falsification is being done to something other than the explanatory model. This seems the reverse from Karl Popper’s philosophy.

<div class="marginnote">
 
I.e., scientists have turned things upside down; originally the idea was that you had substitute of hypotheses that you would want to falsify and now we try to falsify silly null hypotheses that “nothing is going on”. You should try to really build a hypothesis and test it not just reject that nothing is going on.

</div>


#### Book's foci

1. Bayesian data analysis
2. Multilevel modeling
3. Model comparison using information criteria

\

### Small Worlds and Large Worlds (Ch 2)

> ... The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20)

\

We imagine a bag filled with four marbles, each of which is blue or white. 

"So, if we're willing to code the marbles as 0 = "white" 1 = "blue", we can arrange the possibility data in a tibble as follows." I.e., we can consider the five possible worlds, in each of which the bag has a different number of white and blue marbles, and represent each of these worlds as a column vector:

```{r, warning = F, message = F}
d <-
  tibble(p_1 = 0,
         p_2 = rep(1:0, times = c(1, 3)),
         p_3 = rep(1:0, times = c(2, 2)),
         p_4 = rep(1:0, times = c(3, 1)),
         p_5 = 1)

d

```

\

We visualize this in the plot below, where each column is one 'world':

```{r,  out.width='50%'}
d %>% 
  gather() %>% #make it long, with an ket variable for the possibility 'world'
  mutate(x = rep(1:4, times = 5), #an index for 'which ball'
         possibility = rep(1:5, each = 4)) %>% #distributing the 'which world' index
  ggplot(aes(x = x, y = possibility, 
             fill = value %>% as.character())) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) +
  theme(legend.position = "none")
```

\

Simple combinatorics (permutations rule) tells us how many 'ways' we can draw 1, 2, and 3 marbles... Here we think about 'which' marble is drawn, and not just 'which color' it is. We can draw marble 1-4, the first time, then 1-4 the second time, and then 1-4 the third time... so `possibilities=marbles ^ draw`.

```{r}

tibble(draw    = 1:3,
       marbles = 4) %>% 
  mutate(possibilities = marbles ^ draw) %>% 
  knitr::kable()

```

```{block2,  type='note'}
Next, there is a huge amount of code explaining how to make the 'garden of forking paths' diagrams. I'm basically going to skip all that code, and paste in a few images. You can find  all the code [HERE](https://bookdown.org/content/3890/small-worlds-and-large-worlds.html#the-garden-of-forking-data)

```  
\

Suppose there is only one blue ball and three white balls, possibility '2' above. For this world, we see the full 'garden of forking paths' --- the number of ways to select 1, 2, and 3 balls (with replacement) --- below. 


Every path starting from the center is a possible (sequence of) draws. 

```{r, fig.cap='All possible draws of three balls', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-13-1.png")

```

\

Now the inferential exercise: we want to know (the likelihood) of each of the five possible 'worlds'. As we draw data we know we are  proceeding along one of some subset of the forking paths.
\
For example, under possible world 2,  if we draw Blue, then White, then Blue, this could have occured with any of the following paths (consider a draw of each of the white balls as distinct):

```{r, fig.cap='All possible draws of three balls', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-14-1.png")

```

We see that under World 2 there are 3 ways of getting this sequence. 3 out of $4^3$ equally likely paths under World 2, or a $3/64$ chance  (about 5%).

\

We can do similar for the other possible worlds; multiplying the 'ways to produce each draw' in the path yields the 'total ways to produce the path', under each world.  

```{r}
# if we make two custom functions, here, it will simplify the code within `mutate()`, below
n_blue <- function(x) {
  rowSums(x == "b")
}

n_white <- function(x) {
  rowSums(x == "w")
}

t <-
  # for the first four columns, `p_` indexes position
  tibble(p_1 = rep(c("w", "b"), times = c(1, 4)),
         p_2 = rep(c("w", "b"), times = c(2, 3)),
         p_3 = rep(c("w", "b"), times = c(3, 2)),
         p_4 = rep(c("w", "b"), times = c(4, 1))) %>% 
  mutate(`draw 1: blue`  = n_blue(.),
         `draw 2: white` = n_white(.),
         `draw 3: blue`  = n_blue(.)) %>% 
  mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`)

t %>% 
  knitr::kable()
```

Among of all possible worlds, we see the most number of ways to get B-W-B in a world 4; with three blues and one white -- here there are 9 ways in total to get B-W-B. Under world 4 this sequence occurs 9/64, or roughly 14% of the time. 


\

We can see this in the following plot. (We leave out the worlds with only one color ball, as these will have no paths that produce B-W-B). Below, each partitioned section represents one world, and the paths in that world that could produce B-W-B are shown.

\ 

```{r, fig.cap='All possible draws of three balls',  out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-20-1.png")

```

Three paths, versus 8 paths, versus nine paths...

Does this reveal world 4 to be the most likely contents of the present bag? Not necessarily. Suppose we knew ex-ante, from the factory, that '99 bags out of 100 have equal numbers of whites and blues.' Then, it would be much more likely that this bag was from an equal-color bag (world 3), even though this *draw* is more likely conditional on the bag being from world 4. We need will to consider the base-rate probabilities as well. 

<div class="marginnote">
This in turn motivates the standard 'false positive/false negative HIV test' example.
</div>
 

### Using prior information 

> We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25)

This seems to easy to be true, but our garden illustration helps us understand why it is the case. 


#### "Multiply in" new information

First consider, what if we had another draw from the bag, how would this adjust the 'number of paths' for each world? Remember, each draw is independent (replacement). We simply record the number of ways (permutations) that could lead to this draw in each world, and we *multiply* the previous count by this number. You can consider this visually in seeing how 'adding an additional fork at the end of each path' changes the count. 

\

This is given in the table below:

```{r}
t <-
  t %>% 
  rename(`previous counts` = `ways to produce`,
         `ways to produce` = `draw 1: blue`) %>% 
  select(p_1:p_4, `ways to produce`, `previous counts`) %>% 
  mutate(`new count` = `ways to produce` * `previous counts`)

t %>% 
  knitr::kable()
```
\
How to incorporate *prior* information about the probability of each world? 
\

Suppose your friend in the factory tells you (reliably) that 'we produce 3 bags with (just) 1 blue for every 2 bags with equal counts, for every 1 bag with 3 blues.

We can think of the 'factory choosing which bag to produce' as another draw, thus another path.

\
Here the *sequence* in which the information is recieved shouldn't matter. The draws are independent (we presume).  

We can thus multiply the number of paths for each marbles-in-bag world by the (relative) frequency with which the factory 'draws' that bag... as shown below:


```{r}
t <- t %>% 
  select(p_1:p_4, `new count`) %>% 
  rename(`prior count` = `new count`) %>% 
  mutate(`factory count` = c(0, 3:0)) %>% 
  mutate(`new count` = `prior count` * `factory count`)

t %>% 
  knitr::kable()
```

### From counts to probability.


## Title: "Introduction to Bayesian analysis in R and Stata - Katz, Qstep"

*Content from notes from this lecture*

### Why and when use Bayesian (MCMC) methods?

#### Pros
1. No need for asymptotics ... good when sample sizes are small

2. Incorporate previous information

You can consider the 'robustness to other priors'

3. Fit complex nonstandard models
... e.g., with difficult functional forms or likelihood settings (more computation, less thinking)

4. Easy to make predictions (e.g., simulate scenarios) after estimation

5. Incorporate evidence, results, expert judgement

('restrictions' with some lee-way?)

(ISn't this the same as number 2?)

6. Cleaner treatment/imputation of missing values ... these are just parameters

#### Cons

1. Must specify prior distributions ... allows subjective judgement

2. Different way of thinking about stats and inference; probability distributions and simulations, not much about p-values, point estimates and standard errors ... path dependence

3. Computational cost

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


#### Why more popular today?

- Starting from around 2005 in Political Science and Sociology

Computational revolution comes from Markov chain Monte Carlo (MCMC) methods ... don't need analytical solutions

Software implementations -- many in R, specialised software like EWinBugs, JAGS, STAN; also increasingly in Stata


### Theory

Bayes theorem ... inverting conditional probability thing ... 'inversion' to make inferences about the parameters

- In Bayesian stats the *parameters* (and sometimes missing values) are random variables, we make probability statements about them

$$P(A|B)=P(B|A)P(A)/P(B)$$


Frequentist: Point estimates, unknown fixed parameters, data from a hyol repeataable random sample

Bayesian: Fixed data (from the experiment), parameters are random variables ... results based on probability distributions about rthese

Classical statistics: likelihood of data given parameter: $p(y|\theta)$

Bayes we want, $p(\theta|y) = p(y|\theta)p(\theta)/p(y)$

$p(y)$  is a 'constant' in our estimation ... the data is fixed.

So it's proportional to $p(\theta|y) = p(y|\theta)\times p(\theta)$

$p(y|\theta)$ is what we max when we do ML

$ p(\theta)$: prior distribution capturing beliefs about $\theta$

#### So how do we estimate it?

1.  Specify a probability model, a distribution for Y (likelihood function) and the priors for $\theta$

2. Solve (find) the posterior distribution $p(\theta|Y)$ and summarise the parameters of interest

In practice, step 2 is usually done via MCMC simulation rather than analytically.

... via simulations, I approach the 'true' value on $\theta$

(Given 'regularity conditions')

#### Linear regression model example

$$Y = x'\beta+\epsilon$$ with n obs

only random term is epsilon ... natural candidate is a normal distribution, so $Y \sim N(x'\beta,\sigma^2_e)$


So we want to find $p(\beta, \sigma^2_\epsilon|Y,X)$. This depends on the choices of $p(\beta)$ and $p(\epsilon)$. Could choose conjugate priors, leading to a particular joint posterior, you can solve it analytically.

Can yield a joint posterior.

Instead, let's assume that the latter (variance) parameter is known, you can show that the posterior for $\beta$ is also normally distributed. (Conjugate)

Similarly, if we assume $\beta$ is known, if the variance term had an inverse gamma distribution (prior), so will the posterior.

In these conjugate priors, the posterior mean will be a weighted average of the priors and the data.

#### Gibbs

Needs closed form conditional posterior for every parameter.

What Gibbs sampler does is break the parameter space into sets of parameters

1. Choose starting values, $\theta^0_1,...\theta^0_k$

2. sample from the first parameter's distribution given the others
... the second one, ... the k'th one .

3. Repeat step 2 ... thousands of times (starting with the parameters from the previous iteration)
Eventually 'we obtain samples of $p(\theta|y)$'

*But if we don't have a closed form, we cannot simply sample from known distributions in each step*

E.g., in case of Logit distribution.

#### Metropolis Hastings

1. Choose 'proposal distribution' to sample parameter values (a candidate like normal, uniform)
1. Start w a prelim guess for parameter values $\theta_0$
1. At iteration t sample a proposal $\theta_t$  from $p(\theta_t|\theta_{t-1})$ ?? what does this come from?
1. If $p(\theta_t|y)>p(\theta_{t-1}|y)$ accept it as the new value of $\theta$. ??? how is this computed if we don't have conjugate closed-form posteriors?
1. Otherwise flip a coin  with probability  r = (ratio of those probabilities)
- if coin tosses heads, accept as new theta, otherwise stay at previous theta
-  allows algorithm to avoid getting stuck at local maxima

Commonly used proposal: random walk sample: $\theta_t=\theta_{t-1}+z_t$, $z_t \sim f$

?? I do this because there is no analytical way to derive this, unlike in the conjugate case, where we might use the Gibbs

- can combine Gibbs with Metropolis steps; relevant to some problems

#### Assessing convergence

- previous ... 'eyeballing'
- formal:
  - single-chain tests (Geweke/Heidel) ... is the last part of the chain stable (stationary)... compare simulation at middle and end, is there much variation?
  - multiple-chain test... (starting from different values), do they end similar ... Gelman-Rubin diagnosting $\hat{R}$
  - typically either a very long chain and use GH convergence, or multiple shorter chains and use $\hat{R}$

Gabriel: Gelman-Rubin is probably preferred; more conservative

?? What am I iterating towards? Converging on what?

#### Assesing 'fit' in Bayesian

- No r-squared
- Typical measure is 'posterior predictive comparisons'

$p(y_{replicated}|y_{observed}= ...$

1. Simulate data from estimated parameters
2. Compare to observed data
3. Use an overall fit measure to assess model fit

E.g.,   percent correct predictions (binary), whether the true data is within the 95\% CI of the replicates, deviance

For each replicate Choose statistic D, compare the replicated $D(y_s_{replicated})$ against $D(y_s_{observed})

Quantify the discrepancy ... percent of correct predictions, proportion of times replicated y is below true y ... compute 'bayesian p-value's'

Systematic differences between replicate and actual data indicate model limitations

(?? what are reasonable values here??)

### Comparing models ... Equivalent of 'likelihood'

'Deviance Information Criterion' (most used); specific for MCMC simulations: compares expected LL of the model (of the data given the estimated parameters; average here across much of the later points in the chain) against the llhd at the posterior parameter mean.   Always select model with lowest DIC.

Bayes Factor (less used): Ratio of llhd of the models; higher BF means model is more supported; BF>10 seen to provide strong evidence for model w higher value

### On choosing priors

Most social scientists use non-informative or vague priors; i.e., large variance... e.g., $\beta \sim N(0,1000)$

But its often useful to incorporate information into your priors

Small pilot to test, $\rightarrow$ data $Y_1$, another study gives data $Y_2$; repeated application of Bayes theorem gives the posterior.

Same result whether you obtained these together, or whether you did one and then updated (e.g., via an MCMC, starting with the first one as a prior)

Conjugate priors (mentioned before)

- Jeffrey's priors (??)

### Implementation

If you don't need to do fancy things, and don't want to (?) generate the full posterior distribution (or something)

Some Stata/R commands that make Bayesian look frequentist.

In Jags and Winbugs, we only have to specify the prior... rest is done for us

Jags is great ... you only need to do self-coding with lots of data and super complicated models as it can freeze up

We went through it the fancy way in Probit.R

Then the easy way with 'script probit Jags.R'

### Generate predictions from a WinBUGS model

You can just generate these outcomes ...

Prediction: generate a new observation #note, he is doing one per iteration, but since these are convergent it would be basically the same if you just chose a random iteration and did all the draws from that one

### Missing data case

One solution -- multiple imputation

- choose imputation model to predict missings,
- generate many copies of orig data set, imputing missibg value for each
- 2 more steps here

Need a model for X|alpha, because missing variables are random variables

### Stata

Has some rather simple implementations; e.g., just using commands like ```bayes: regress y x```

### R mcmc pac

Also simple code; great for standard use

Speedup with parallelization; see "script for parallel probit.R" and "parallelprobit.R"

More advanced: C++; can integrate it with Rcpp, or even use Exeter's ISCA cluster


```{r cars}
summary(cars)
```

