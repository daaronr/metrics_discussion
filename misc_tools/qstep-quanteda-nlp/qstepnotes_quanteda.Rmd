---
title: "Qstep: Quanteda"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(dplyr,magrittr,purrr,tidyverse,tidyr,broom,janitor,glue,dataMaid,glue,readr, lubridate,summarytools,gtools,knitr,pastecs, quanteda) 
```

#Introduction 

insights from text based on NLP

'Text-mining' but analysis should be theory-driven

Analytical methods from frequency analysis to predictive modeling

Usually each word is a variable in stat analysis of textual data ... a very sparse matrix

Ex: Sentiment analysis of news-- timeline of this, event study

Built in corpus-- presidential inaugural 

#data format

1. Raw text
2. Matrix representation: tokenization (what are the units), feature selection (which words count?)

- observation (column) is the text ('1789-Washington')

`r my_corpus1 <- corpus(data_corpus_inaugural[1:5])`

3. Analytics

- statistics: term frequencies , keyness (?), readability, lexical diversity, similarity, distance

- models: 

'Locally proximate feature co-ocurrence matrix' occurs for the 'word embeddings' only
plots


Lori: There are models that look at text as network data, but doesn't think it adds much

#Quanteda initiative quanteda.org

- quanteda
- readtext... to get the text in from a variety of input formats
- spacyr is a wrapper to a python library spacy
- stopwords... lists of words to ignore (the, and, etc)


#examples?

Twitters of european parliament

'relative frequency 'keyness''

Chi-sq of each word; reference vs target

Document similarity for a matrix

Hierarchical clustering.... most similar classification

`r tokens("I was born in Rhode Island, grew up on Long Island, NY from age 8, went to college in DC, and lived in California from 1999. .") %>% tokens_wordstem()`

quanteda.dictionaries --- UK to USA homogenisation  

'cosine similarity' for text
A 'corpus object' contains texts w document-level variables 

##corpus() constructs
corpus_reshape() recasts
corpus_sample randomlu samples
corpus_segment()
...subset (e.g, year>1980)

##tokens object, functions 

tokens() to tokenise
tokens_compound() convert sequences into compoind tokens
tokens_lookup() apply a dictionary ??
ngrams, skipgrams; forms combinations


#dfm object contains frequency of words/symbols in a matrix

pdf back to a document; need regexp syntax; 

...trim

```{r}
d <- data_corpus_inaugural %>%
  corpus_subset(Year >1960)

```

textstat_*() functions perform statistical analysis of text data --- yield data grames

textmodel_*() Machine learning functions

textplot_ for visual
textplot_xray: Such a cool command -- lexical dispersion plot

StackOverflow channel on Quanteda