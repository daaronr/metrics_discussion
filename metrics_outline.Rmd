# Introduction

- Focus on the practical tools I use and the challenges I ([David Reinstein](https://daaronr.github.io/markdown-cv/)) face

Microeconomics, behavioral economics, focus on charitable giving and 'returns to education' type of straightforward problems. (Limited to no focus on structural approaches.)

- Where we can *add value* to real econometric (and statistical and experimental design) practice??

\

Data:

- Observational (esp. web-scraped and API data and national surveys/admin data}

- Experimental: esp. where with multiple crossed arms, and where the 'cleanest design' may not be possible

\

Assume familiarity with most basic statistical concepts like 'bias', 'consistency', and 'null hypothesis testing.' However, I will focus on some concepts that seem to often be misunderstood and mis-applied.

## (Conceptual: approaches to statistics/inference and causality)[#conceptual]

### Bayesian vs. frequentist approaches

Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)

### Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

#### DAGs and Potential outcomes

### Theory, restrictions, and 'structural vs reduced form'

## Getting, cleaning and using data; project management and coding {#data-sci}

This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.  

### Data: What/why/where/how

### Organizing a project

### Dynamic documents (esp Rmd/bookdown)

### Good coding practices


#### New tools and approaches to data (esp 'tidyverse')

#### Style and consistency

Indenting, snake-case, etc

#### Using functions, variable lists, etc., for clean, concise, readable code

### Data sharing and integrity


## Basic regression and statistical inference: Common mistakes and issues

### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: 	Simonsohn18

http://datacolada.org/62

### OLS and heterogeneity

- OLS does *not* identify the ATE

http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT


- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"

"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

#### Confidence intervals and Bayesian credible intervals

#### Comparing relative parameters

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT)

See [@verkaik2016]

### Interaction terms and pitfalls

#### 'Moderators' Confusion with nonlinearity

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if outcome  'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative

#### MHT

### Choice of test statistics (including nonparametric)

<!-- (Or get to this in the experimetrics section) -->

### How to display and write about regression results and tests

### Bayesian interpretations of results

(see 'the Bayesian Approach')

## LDV and discrete choice modeling

## Robustness and diagnostics, with integrity

### (How) can diagnostic tests make sense? Where is the burden of proof?


### Estimating standard errors

### Sensitivity analysis: Interactive presentation

## [Control strategies and prediction; Machine Learning approaches](#control-ml)

### Machine Learning (statistical learning): Lasso, Ridge, and more

### Limitations to inference from learning approaches

## IV and its many issues


### Instrument validity

- Exogeneity vs. exclusion
- Very hard to 'powerfully test'


### Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

Focusing on a binary endogenous 'treatment' variable 

- With heterogeneity 
- With imperfect compliance 
- With one-way compliance

### Weak instruments, other issues

- With a 'weak instrument' ... why does that matter?

### Reference to the use of IV in experiments/mediation

## Causal pathways: [Mediation modeling and its massive limitations](#mediators)

An applied review

## Causal pathways: selection, corners, hurdles, and 'conditional on' estimates


### 'Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection

"Conditional on positive"/"intensive margin" analysis ignores selection *identification issue* 	See Angrist and Pischke on "Good CoP, bad CoP".  See also bounding approaches such as [@Lee2018]	AngristJ.D.2008a,

\

#### Bounding approaches (Lee, Manski, etc)

See [Notes on Lee bounds](#notes_lee)

## Other paths to observational identification

### Fixed effects and differencing

### DiD

FE/DiD does not rule out a correlated dynamic unobservable, causing a bias

### RD

### Time-series-ish panel approaches to micro

#### Lagged dependent variable and fixed effects --> 'Nickel bias'

## (Ex-ante) Power calculations

### What sort of 'power calculations' make sense, and what is the point?

#### The 'harm to science' from running underpowered studies

> "One worries about underpowered tests. Your result (may have)  relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.

> Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects."	verkaik2016, metzger2015

### Power calculations without real data

### Power calculations using prior data


## (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters

### Why run an experiment or study?

- Sugden and Sitzia critique here, give more motivation

### Causal channels and identification

- Ruling out alternative hypotheses, etc

### Types of experiments, 'demand effects' and more artifacts of artifical setups

### Generalizability (and heterogeneity)

## (Experimental) Study design: Background and quantitative issues

### Pre-registration and Pre-analysis plans

#### The hazards of specification-searching

### Sequential and adaptive designs

Needs to adjust significance tests for augmenting data/sequential analysis/peeking	Statistics/econometrics	new-statistics		sagarin_2014			http://www.paugmented.com/	resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/

Yet ...


P_augmented may *overstate* type-1 error rate	Statistics/econometrics	response to referees, new-statistics	"
A process involving stopping ""whenever the nominal $p.0.5$"" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5\%. Even if the subsequent data suggested a ""one in a million chance of arising under the null"" the overall process yields a 5\%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the ""likelihood of the null hypothesis"" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate.

Considering the calculations in \ref{sagarin2014}, it is clear that $p_{augmented}$ should \textit{overstate} the type-1 error of the process if there is a positive probability that after an initial experiment attains p$<0.05$, more data is collected.  A headline $p<0.05$ does \textit{not} imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded $p>0.05$, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though $p<0.05$. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine's experiment).  Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p<0.05, more data is collected, implying that $p_{augmented}$ as calculated above overstates the type I error rate that would arise  from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below $p=0.05$, in order to attain a type-1 error rate of 5% or less in our published corpus."

### Efficient assignment of treatments

(Links back to power analyses)

## 'Experimetrics' and measurement of treatment effects from RCTs

### Which error structure? Random effects?

### Randomization inference?

### Parametric and nonparametric tests of simple hypotheses

### Adjustments for exogenous (but non-random) treatment assignment

### IV in an experimental context to get at 'mediators'?

### Heterogeneity in an experimental context


## The Bayesian approach

## Making inferences from previous work; Meta-analysis, combining studies

### Publication bias

### Combining a few (your own) studies/estimates

### Full meta-analyses

- Models to address publication biases

## Some key resources and references 

[@AngristJ.D.2008a]

'The Mixtape' (Cunningham)

[@kennedyGuideEconometrics2003]

[@Tibshirani]

OSF guides

Christensen ea "Transparent and Reproducable Social Science Research"

[@Gentzkow2013; @wooldridgeEconometricAnalysisCross2002; @wooldridgeIntroductoryEconometricsModern2008]


An Introduction to Statistical Learning with Applications in R

R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org 

Statistical Rethinking: A Bayesian Course with Examples in R and Stan

### Consider: 

Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017

Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction .
Cambridge University Press, 2015

Judea Pearl


Imbens: Potential Outcomes versus DAGs