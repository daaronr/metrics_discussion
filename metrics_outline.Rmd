# Introduction

My goal in putting this resource is to focus on the practical tools I use and the challenges I ([David Reinstein](https://daaronr.github.io/markdown-cv/)) face. But I am open to collaboration with others on this.

\

*My focus:* Microeconomics, behavioral economics, focus on charitable giving and 'returns to education' type of straightforward problems. (Minimal focus on structural approaches.)

What I care about: Where we can *add value* to real econometric (and statistical and experimental design) practice?

\

The data I focus on:

- Observational (esp. web-scraped and API data and national surveys/admin data)

- Experimental: esp. where with multiple crossed arms, and where the 'cleanest design' may not be possible

\

I will assume familiarity with most basic statistical concepts like 'bias', 'consistency', and 'null hypothesis testing.' However, I will focus on some concepts that seem to often be misunderstood and mis-applied.

```{block2,  type='note'}

If you are involved with this project, you can find a brief guide (somewhat WIP) on how to add content (HERE)[https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html].  This is from a different project but the setup is basically the same.

```

\

A note to potential research assistants and student collaborators (unfold):

```{block2,  type='fold'}

```

## (Conceptual: approaches to statistics/inference and causality)[#conceptual]

### Bayesian vs. frequentist approaches {-}


Folder: bayesian

Notes: [bayes_notes](bayesian/bayes_notes.Rmd)

### Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

#### DAGs and Potential outcomes

### Theory, restrictions, and 'structural vs reduced form'

## [Getting, cleaning and using data; project management and coding](#data-sci)

This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.

### Data: What/why/where/how

### Organizing a project

### Dynamic documents (esp Rmd/bookdown)

### Good coding practices


#### New tools and approaches to data (esp 'tidyverse')

#### Style and consistency

Indenting, snake-case, etc

#### Using functions, variable lists, etc., for clean, concise, readable code

### Data sharing and integrity


## Basic regression and statistical inference: Common mistakes and issues

### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: [Simonsohn18]( http://datacolada.org/62)


### OLS and heterogeneity

- OLS does *not* identify the ATE

http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT


- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"

"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

- **Confidence intervals and Bayesian credible intervals**

- **Comparing relative parameters**

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT)

### Interaction terms and pitfalls

** 'Moderators' Confusion with nonlinearity**

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if the outcome 'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative


### Choice of test statistics (including nonparametric)

<!-- (Or get to this in the experimetrics section) -->

### How to display and write about regression results and tests

### Bayesian interpretations of results

(see 'the Bayesian Approach')

## LDV and discrete choice modeling

## Robustness and diagnostics, with integrity

### (How) can diagnostic tests make sense? Where is the burden of proof?

### Estimating standard errors

### Sensitivity analysis: Interactive presentation

## [Control strategies and prediction; Machine Learning approaches](#control-ml)

### Machine Learning (statistical learning): Lasso, Ridge, and more

### Limitations to inference from learning approaches

## IV and its many issues


### Instrument validity

- Exogeneity vs. exclusion
- Very hard to 'powerfully test'


### Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

Focusing on a binary endogenous 'treatment' variable

- With heterogeneity
- With imperfect compliance
- With one-way compliance

### Weak instruments, other issues

- With a 'weak instrument' ... why does that matter?

### Reference to the use of IV in experiments/mediation

## [Other paths to observational identification](#other_paths)

### Fixed effects and differencing

### DiD

FE/DiD does not rule out a correlated dynamic unobservable, causing a bias

### RD

### Time-series-ish panel approaches to micro

** Lagged dependent variable and fixed effects --> 'Nickel bias'**

## Causal pathways: [Mediation modeling and its massive limitations](#mediators)

An applied review

## Causal pathways: selection, corners, hurdles, and 'conditional on' estimates


### 'Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection

"Conditional on positive"/"intensive margin" analysis ignores selection *identification issue* 	See Angrist and Pischke on "Good CoP, bad CoP".  See also bounding approaches such as [@Lee2018]	AngristJ.D.2008a,

\

#### Bounding approaches (Lee, Manski, etc)

See [Notes on Lee bounds](#notes_lee)


## [(Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters](#why_experiment_etc)

<!-- experiments_and_study_design/why_experiment_design.Rmd -->

### Why run an experiment or study?

<!-- - Sugden and Sitzia critique here, give more motivation -->

### Causal channels and identification

- Ruling out alternative hypotheses, etc

### Types of experiments, 'demand effects' and more artifacts of artifical setups

### Generalizability (and heterogeneity)

## (Experimental) Study design: Background and quantitative issues

<!-- experiments_and_study_design/quant_design_power.Rmd -->

### Pre-registration and Pre-analysis plans

#### The hazards of specification-searching

### Sequential and adaptive designs

### Efficient assignment of treatments

(Links back to [power analyses](#power)


## (Experimental) Study design: (Ex-ante) Power calculations

### What sort of 'power calculations' make sense, and what is the point?

#### The 'harm to science' from running underpowered studies

### Power calculations without real data

### Power calculations using prior data

## ['Experimetrics' and measurement of treatment effects from RCTs](#experimetrics_te)

### Which error structure? Random effects?

### Randomization inference?

### Parametric and nonparametric tests of simple hypotheses

### Adjustments for exogenous (but non-random) treatment assignment

### IV in an experimental context to get at 'mediators'?

### Heterogeneity in an experimental context

## [Making inferences from previous work; Meta-analysis, combining studies](#metaanalysis)

### Publication bias

### Combining a few (your own) studies/estimates

### Full meta-analyses

- Models to address publication biases

## The Bayesian approach

## Some key resources and references

[@AngristJ.D.2008a]

'The Mixtape' (Cunningham)

[@kennedyGuideEconometrics2003]

[@Tibshirani]

OSF guides

Christensen ea "Transparent and Reproducable Social Science Research"

[@Gentzkow2013; @wooldridgeEconometricAnalysisCross2002; @wooldridgeIntroductoryEconometricsModern2008]


An Introduction to Statistical Learning with Applications in R

R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org

Statistical Rethinking: A Bayesian Course with Examples in R and Stan

### Consider:

Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017

Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction .
Cambridge University Press, 2015

Judea Pearl

Imbens: Potential Outcomes versus DAGs
