# Introduction

My goal in putting this resource is to focus on the practical tools I use and the challenges I ([David Reinstein](https://daaronr.github.io/markdown-cv/)) face. But I am open to collaboration with others on this. (Other contributors so far include Gerhard Riener, Oska Fentem, and Scott Dickerson)

\

*My focus:* Microeconomics, behavioral economics, focus on charitable giving and 'returns to education' type of straightforward problems. (Minimal focus on structural approaches.)

What I care about: Where we can *add value* to real econometric (and statistical and experimental design) practice?


\

The data I focus on:

- Observational (esp. web-scraped and API data and national surveys/admin data)

- Experimental: esp. where with multiple crossed arms, and where the 'cleanest design' may not be possible

\

I will assume familiarity with most basic statistical concepts like 'bias', 'consistency', and 'null hypothesis testing.' However, I will focus on some concepts that seem to often be misunderstood and mis-applied.

```{block2,  type='note'}

If you are involved with this project, you can find a brief guide (somewhat WIP) on how to add content (HERE)[https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html].  This is from a different project but the setup is basically the same.

```

\

A note to potential research assistants and student collaborators (unfold):

```{block2,  type='fold'}

```

## (Conceptual: approaches to statistics/inference and causality)[#conceptual] {-}

-  Bayesian vs. frequentist approaches

Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)

-  Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

<!-- #### DAGs and Potential outcomes -->

-  Theory, restrictions, and 'structural vs reduced form'

## [Getting, cleaning and using data; project management and coding](#data-sci) {-}

<div class="marginnote">
This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.
</div>
 
- Data: What/why/where/how

- Organizing a project

-  Dynamic documents (esp Rmd/bookdown)

-  Good coding practices

<!--- #### New tools and approaches to data (esp 'tidyverse')

#### Style and consistency

Indenting, snake-case, etc

#### Using functions, variable lists, etc., for clean, concise, readable code

--> 

-  Data sharing and integrity


## Basic regression and statistical inference: Common mistakes and issues {-}

- "Bad control" ("colliders")
- Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

- Choices of lhs and rhs variables
- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

-  Functional form
  - Logs and exponentials
  - Nonlinear modeling (and interpreting coefficients)


- 'Testing for nonlinear terms'


-  OLS and heterogeneity
  - OLS does *not* identify the ATE
  - See: ['your go-to regression specification is biased...'](http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT)

  - Modeling heterogeneity: the limits of Quantile regression

-  "Null effects"

- Confidence intervals and Bayesian credible intervals

- Comparing relative parameters

-  Multiple hypothesis testing (MHT)

-  Interaction terms and pitfalls

- 'Moderators' Confusion with nonlinearity



-  Choice of test statistics (including nonparametric)

<!-- (Or get to this in the experimetrics section) -->

-  How to display and write about regression results and tests

- Bayesian interpretations of results (see also '[Bayesian Approaches](#bayes)')

## LDV and discrete choice modeling {-}

## Robustness and diagnostics, with integrity {-}

- (How) can diagnostic tests make sense? Where is the burden of proof?

- Estimating standard errors

-  Sensitivity analysis: Interactive presentation

## [Control strategies and prediction; Machine Learning approaches](#control-ml) {-}

-  Machine Learning (statistical learning): Lasso, Ridge, and more

-  Limitations to inference from learning approaches

## IV and its many issues {-}

-  Instrument validity
- Exogeneity vs. exclusion
- Very hard to 'powerfully test'


-  Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

-  Weak instruments, other issues

- Reference to the use of IV in experiments/mediation

## [Other paths to observational identification](#other_paths) {-}

- Fixed effects and differencing

-  DiD

-  RD

-  Time-series-ish panel approaches to micro

## Causal pathways: [Mediation modeling and its massive limitations](#mediators) {-}

An applied review

## Causal pathways: [selection, corners, hurdles, and 'conditional on' estimates](#selection_cop) {-}


- Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection


\

- Bounding approaches (Lee, Manski, etc). See [Notes on Lee bounds](#notes_lee)


## [Survey design and implementation; analysis of survey data](#surveys)



## [(Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters](#why_experiment_etc) {-}

<!-- experiments_and_study_design/why_experiment_design.Rmd -->

- Why run an experiment or study?

<!-- - Sugden and Sitzia critique here, give more motivation -->

- Causal channels and identification

- Ruling out alternative hypotheses, etc

-Types of experiments, 'demand effects' and more artifacts of artifical setups

-Generalizability (and heterogeneity)

## (Experimental) Study design: Background and quantitative issues {-}

<!-- experiments_and_study_design/quant_design_power.Rmd -->

- Pre-registration and Pre-analysis plans

-The hazards of specification-searching

-Sequential and adaptive designs

-Efficient assignment of treatments

(Links back to [power analyses](#power))


## (Experimental) Study design: (Ex-ante) Power calculations

- What sort of 'power calculations' make sense, and what is the point?

- The 'harm to science' from running underpowered studies

- Power calculations without real data

- Power calculations using prior data

## ['Experimetrics' and measurement of treatment effects from RCTs](#experimetrics_te) {-}

<!-- Which error structure? Random effects?

- Randomization inference?

- Parametric and nonparametric tests of simple hypotheses

- Adjustments for exogenous (but non-random) treatment assignment


-  IV in an experimental context to get at 'mediators'?

- Heterogeneity in an experimental context

--> 

## [Making inferences from previous work; Meta-analysis, combining studies](#metaanalysis)

### Publication bias

### Combining a few (your own) studies/estimates

### Full meta-analyses

- Models to address publication biases

## The Bayesian approach

## Some key resources and references

[@angrist2008mostly]

'The Mixtape' (Cunningham)

[@kennedyGuideEconometrics2003]


Tibshirani, Robert. n.d. “Statistical Learning with Sparsity the Lasso and Generalizations.”

<!--[Tibshirani] -->

OSF guides

Christensen ea "Transparent and Reproducable Social Science Research"

[@wooldridgeEconometricAnalysisCross2002; @wooldridgeIntroductoryEconometricsModern2008]

Gentzkow and Shapiro (2013) <!-- Add reference -->


An Introduction to Statistical Learning with Applications in R

R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org

Statistical Rethinking: A Bayesian Course with Examples in R and Stan

### Consider:

Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017

Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction .
Cambridge University Press, 2015

Judea Pearl

Imbens: Potential Outcomes versus DAGs
