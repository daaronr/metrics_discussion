# Survey design and implementation; analysis of survey data {#surveys}


**Resources consulted/to consult**

Wikipedia entries on

- ['survey sampling'](https://en.wikipedia.org/wiki/Survey_sampling)


 Carl-Erik Särndal; Bengt Swensson; Jan Wretman (2003). Model assisted survey sampling. Springer. pp. 9–12. ISBN 978-0-387-40620-6. Retrieved 2 January 2011.


## Survey sampling/intake

### Probability sampling

<div class="marginnote">
Here: naive notes based on wikipedia, will be improved.
</div>

In my own words:

```{block2,  type='fold'}

Probability sampling’ has been the standard approach in survey sampling since random-digit dialing was possible.

The basic idea, if I understand correctly, is to define a population of interest and a ‘sample frame’ (the place we are actually drawing from, the empirical analogue of the population of interest perhaps).
Essentially, rather than advertising or trying to recruit everyone, or have people enter by self selection, probability sampling select from the sample frame with a particular probability, and then actively tries to get the selected individuals to complete the survey. As only a smaller number of people are selected to be interviewed/fill out the survey, you can spend more time and money/incentives, trying to make sure they respond.

Probability sampling also allows ‘stratification’, and oversampling of harder to reach groups.  We  potentially divide up (‘stratify’) that frame by observable groups. We randomly draw (sample) within each strata with a certain probability.

*If we have an informative estimate of the  TRUE shares in each strata* we can sample/re-weight so that the heterogeneous parameter of interest can be said to represent the average value for the true population of interest.
    
```

\



> A probability-based survey sample is created by constructing a list of the target population, called the sampling frame, a randomized process for selecting units from the sample frame, called a selection procedure, and a method of contacting selected units to enable them to complete the survey, called a data collection method or mode (lookup in [Wiki](https://en.wikipedia.org/wiki/Survey_sampling#Probability_sampling))

<div class="marginnote">
Note that the 'sampling frame', 'the source material or device from which a sample is drawn', e.g, a telephone directory, may not exactly contain all elements of the 'population of interest' (e.g., the population with and without listed numbers).

On the other hand this terminology doesn't seem to be everywhere consistent, e.g., Salganik and H refer to it as 'a list of all of the members in the population'.

See 'Missing elements', 'Foreign elements', 'Duplicate entries' and 'Groups or clusters' ...

Also note that 'Not all frames explicitly list population elements; some list only 'clusters'. For example, a street map can be used as a frame for a door-to-door survey.'

</div>

- Simple random versus stratified and cluster sampling


\

*Issues*\*

<div class="marginnote">
Many of these also seem relevant for our ['social movement' case of interest below.](#jazz-case)
</div>


- Non-response bias (biggest issue?)



### Non-probability sampling

@bakerSummaryReportAAPOR2013

[Non-Probability Sampling - report of the aapor task force on non-probability sampling](https://www.aapor.org/Education-Resources/Reports/Non-Probability-Sampling.aspx)


'River sampling'

\


#### Respondent-driven sampling (from rare or 'hidden' populations) {-}

** [sampling and estimation in hidden populations using respondent-driven sampling, matthew j. salganik* douglas d.    heckathorny](https://www.bebr.ufl.edu/sites/default/files/Sampling%20and%20Estimation%20in%20Hidden%20Populations.pdf)** @salganikSamplingEstimationHidden2004
 
 
 >  Standard sampling and estimation techniques require the researcher to select sample members with a known probability of selection. In most cases this requirement means that researchers must have a sampling frame, a list of all members in the population. However, for many populations of interest such a list does not exist.
 
- 'Standard' approach ... Construct a sample frame of 'all the injection drug users in a large city'; 'probably impossible' 

- Or 'reach a large number of people via random digit dialing and then screen them for membership in the hidden population. Again, this approach is extremely costly and potentially very inaccurate.' 

- Or 'take a sample of target population members in an institutional setting—for example, injection drug users in a drug rehabilitation program.'
  - But 'a nonrandom sample from the hidden population, ... impossible to use to ... make accurate estimates about the entire hidden population'

Targeted sampling ('street outreach') is cost-effective but very hard to know what you are getting. Time-space sampling can deal with certain limitations to this, but its far from a complete remedy

The authors propose  

> respondent-driven sampling, is a variation of chain-referral sampling methods that were first introduced by Coleman (1958) under the name snowball sampling.

> select a small number of seeds who are the first people to participate in the study. These seeds then recruit others to participate in the study. This process of existing sample members recruiting future sample members continues until the desired sample size is reached   

While.. 

> chain-referral methods produce samples that are not even close to simple random samples 

> researchers believed that any small bias in selecting the seeds would be compounded in unknown ways as the sampling process continued

and also ... 
>  This research is fairly well summarized by Berg(1988) when he writes, ‘as a rule, a snowball sample will be strongly biased toward inclusion of those who have many interrelationships with or are coupled to, a large number of individuals. In the absence of knowledge of individual inclusion probabilities in different waves of the snowball sample, unbiased estimation is not possible.

<div class="marginnote">
**"Biased toward inclusion of those who have many interrelationships..."**

This would seem to suggest a solution, perhaps driving the approach the authors use. If we can measure the likelihood that an individual is reached as a function of their network, we may be able to downweight those individuals that are more likely to be sampled.
</div>
 

But the authors believe they have a solution

>  We believe that previous researchers have been overly pessimistic about chain-referral samples. In this paper we will show that it is possible to make unbiased estimates about hidden populations from
these types of samples. Further, we will show that these estimates are asymptotically unbiased no matter how the seeds are select

How does it work?: 

> First ,the sample is used to make estimates about the social network connectingthe population. Then, this information about the social network is used to derive the proportion of the population in different groups (forexample, HIV- or HIV+)

There are some intricate and interesting calculations involving network analysis 

They show the unbiasedness of this approach under specified conditions, and its robustness to the seed choice. They also give a monte-carlo simulation illustrating the same. 
Finally, they do some sort of comparison of their approach and previous approaches in real settings (I didn't read it carefully).


## Case: Surveying an unmeasured and rare population surrounding a 'social movement' {#jazz-case}

[Related question/post on ResearchGate here](https://www.researchgate.net/post/Measuring_composition_attitudes_of_rare_hidden_population_with_convenience_samples_river_sample_opt-in_web-based_what_to_do_read)

### Background and setup {-}

*Consider a case where:*

1. We have a **population-of-interest** based on an affiliation, certain actions, or a set of ideas.
Some examples:

- Vegetarians
- 'Tea party conservatives' in the US
- Jews, both religious and 'culturally Jewish'
- Jazz musicians
- Goths ... 'ethnography'; Paul Hodkinson

For this writeup, we will call the targeted group 'the Jazz Movement' or 'the Jazz population'. Individuals will either be 'Jazzy' (J) or 'non-Jazzy' (NJ).


There are some **disagreements about how to define this group**.

\

2. We have **no 'gold standard' to benchmark against.**

- There is no 'actual targeted and measured outcome' such as voting in an election.
- There are no other surveys or enumerations (e.g., Censuses) to inform our results.

\

3. We have collected survey responses from  **self-selected 'convenience' samples** (='internet surveys'?) across several years; this most resembles **'river sampling'**

... based on advertising and word-of-mouth in a variety of outlets ('referrers') associated with the 'movement'.

<div class="marginnote">
Particularly:

- A discussion forum
- A newsletter
- A popular website and hub for the movement

</div>


- We can identify *which* 'referrer' lead someone to our survey.

- All participants are given a similar 'donation' incentive, an incentive that might tend to particularly attract members of the Movement.

<div class="marginnote">
Given the context, we might reasonably expect that willingness to complete the survey might be associated with depth of support for the movement.
</div>


- We can link some individuals across years.

- Some questions repeat across years.


\

4. We have (self-reported) **measures** of

- Demographics (age, gender, etc),
- Attitudes and beliefs (e.g., support for the death penalty),
- Retrospectives (esp. 'year you became Jazzy'), and
- Behaviors (e.g., charitable donations).


\

5. Our **research goals include** measuring:

- The size of the movement (challenging),
- The demographics (and economic status, psychographics, etc) of the movement,
- The attitudes, beliefs and behaviors of people in the movement,
- The (causal) drivers of joining the movement and actively participating in the movement (or leaving the movement),

- ... and the trends/changes in all of the above.


And particularly interested in the most avid and engaged Jazzers.
Knowing about self-reported challenges. 

\


**Why do we care?** *We want to know these things for several reasons, including...*


To find ways to build membership (perhaps 'expand and diversify') and increase participation in the movement (including specific behaviors like donating), especially through...

- Funding causal drivers (policies) that 'work', and
- Profiling and targeting 'likely Jazzers' from outside the movement

\

To understand and better represent the attitudes of the movement's members in our movement-wide activities. <!-- See -->

\

'For general understanding of the movement and its members', to inform a wide range of decisions across the movement, and further research into the movement.


### Our 'convenience' method; issues, alternatives


Our current approach seems to be some combination of what is called 'convenience sampling' and 'snowball sampling'. The major distinction from probability sampling (as I see it) is...

*Probability sampling* identifies a *population of interest* and a *sample frame* meant to capture this population. Rather than appealing to this entire population/frame, PS randomly (or stratified/clustered) samples a 'probability share' (e.g., 1/1000) from this frame. Selected participants are (hopefully) given strong incentives to complete the survey. One can carefully analyze ---and perhaps adjust for---the rate of non-response.

In contrast,

<div class="marginnote">
 I have heard that 'internet surveys', if done right, with proper adjustments, are seen as increasingly reliable, especially in the context of electoral polling. Is our approach similar enough to this to be able to adopt these approaches.
</div>

\


Wikipedia entry on 'convenience sampling'

> Another example would be a gaming company that wants to know how one of their games is doing in the market one day after its release. Its analyst may choose to create an online survey on Facebook to rate that game.

> **Bias** The results of the convenience sampling cannot be generalized to the target population because of the potential bias of the sampling technique due to under-representation of subgroups in the sample in comparison to the population of interest. The bias of the sample cannot be measured. Therefore, inferences based on the convenience sampling should be made only about the sample itself.[9] (Wikipedia, on 'Convenience sampling',  cites Borenstein et al, 2017)


*This statement is deeply pessimistic...* 'the bias cannot be measured'. We might dig more deeply to see if there are potential approaches to dealing with this


\

### Potential alternative approaches


**Analysis adjustments**

**Design adjustments**

- Respondent-driven sampling (with network-analysis based adjustments)



### Notes and considerations:

- 'Whether we can do better’ sort of questions ... we should try to pin down what our metrics and targets are

- What do we mean when we say the ‘actual Jazz population’?

- What sorts of questions/claims are we interested in most/what ‘parameters’ are we trying to measure?

- For the second point, I think that some methods might yield plausibly unbiased/reliable estimates of certain parameters (e.g., year-to-year changes) but not others.

\

Moss:

Some flexibility/two-way interaction here in that we have a lot of different goals with the survey (i.e. different kinds of questions being posed) and depending on the answers you discern, we might give up on some of these questions and focus on others
