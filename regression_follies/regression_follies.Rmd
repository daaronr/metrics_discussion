# Basic statistical inference and regressions: Common mistakes and issues {#reg-follies}


## Basic regression and statistical inference: Common mistakes and issues briefly listed


Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)	Identification

Random effects estimators show a lack of robustness	Specification	Clustering SE  is more standard practice

OLS/IV estimators not 'mean effect' in presence of heterogeneity

Power calculations/underpowered

Selection bias due to attrition

Selection bias due to missing variables -- impute these as a solution

Signs of p-hacking and specification-hunting

Weak diagnostic/identification tests

Dropping zeroes in a "loglinear" model is problematic

Random effects estimators show a lack of robustness

Dropping zeroes in a "loglinear" model is problematic

Random effects estimators show a lack of robustness

-(Some notes on multi-level modeling) and RE linked [in this Twitter thread](https://twitter.com/DavidPoe223/status/1239447381172727809)
 
With heterogeneity the simple OLS estimator is not the 'mean effect'

P_augmented may *overstate* type-1 error rate

Impact size from regression of "log 1+gift amount"

Lagged dependent variable and fixed effects --> 'Nickel bias'

Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)

Weak IV bias

Bias from selecting instruments and estimating using the same data

Failure to adjust for multiple-hypothesis testing

### Bad control

From MHE:

> some variables are bad controls and should not be included in a regression model even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notational experiment at hand. That is, bad controls might just as well be dependent variables too."

– They could also be interpreted as endogenous variables.

Example of looking at a regression of wages in schooling, controlling for college degree completion:

> Once we acknowledge the fact that college affects occupation, comparison of wages by college degree status within occupation are no longer apples to apples, even if college degree completion is randomly assigned."

– The question here was whether to control for the category of occupation, not the college degree.

> It is also incorrect to say that the conditional comparison captures the part of the effect of college that is 'not explained by occupation' ... so we would do better to control only for variables that are not themselves caused by education."



### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: 	Simonsohn18

http://datacolada.org/62

### OLS and heterogeneity

#### OLS does *not* identify the ATE (-)

In general, with heterogeneity, OLS does *not* identify the ATE. It weights observations from different parts of the sample differently. Parts with greater residual variation in the treatment (outcome) variable are more (less) heavily weighted. 

<div class="marginnote">
E.g., if the treatment is binary, the estimator will most heavily weight those parts of the sample where the probability of treatment is closest to 1/2.
</div>
 
The formula is ...

\

```{block2,  type='note'}
**Some intuition**

Why is this the case?  The OLS type estimators we are taught in Econometrics are 'BLUE'  under the assumption of a *single homogenous 'effect'* (the 'slope'...  although the discussion itself is often agnostic as to whether this represents a causal effect). 

\
 
It is 'best' in a minimizing MSE sense under certain assumptions;  in particular, we must also know the true functional form and the set of variables to be included. See 'overfitting' issues.
 
In  order to have the estimate of the true slope that minimizes the squared errors, OLS (and related estimators like FGLS; as well as 2SLS in a more complicated sense) weights  some observations more than others. The 'influence' of an observation on the estimated slope depends on the nature of the variation in the  dependent and independent variables in the region that observation is drawn from.  Think of drawing a line  through a set of points  that were drawn with some noise from the true distribution.  If you drew it based on a bunch of points (from a region where) the treatment varies very little and the outcomes have a lot of noise,  the line you draw will be very sensitive to the latter noise and thus unreliable. So,  would optimally 'down-weight'  these observations in drawing the line. 

\

However, if the *actual* slope varies by region, this also means you are under-representing certain regions, and  thus getting  a biased  estimate of the average slope.

```

How can we deal with this?  If we think  that the  treatment effect varies with *observable* variables, we could include 'interactions';  essentially making separate estimates of the slope for each share of the population (but potentially  allowing other control variables to have a homogenous effects, and pooled or clustered estimation of underlying variance.)

<div class="marginnote">
 ...Although we may want to consider both overfitting here and the idea that there may be *some* shared component, so the fully-interacted model may be sub-optimal. See mixed modeling (?) 
</div>

\

However, this does not tell us how to recover the *average* of these slopes (approximately, the ATE).  Should we weight  each of the slopes by the share of the population that this group represents?

Mechanically,  the standard way of estimating and representing these  interactions and economics has been with simple dummies (0,1) for each compared group. This yields a 'base group' (e.g., males aged 35-60) --  this obviously does not recover the average slope-- as well as the 'adjustment' coefficients.

\

Another way of expressing interactions, particularly helpful with multi-level interactions is called 'effect coding': each group is coded as a 'difference from 0' (e.g,. males are -1/2 and females +1/2), before doing the interactions. This could allow for a more straightforward interpretation: at each level, the uninteracted term represents the average treatment effects, and the interacted terms  represent adjustments relative to this average. *But under which conditions is this in fact the case?*


[insert here].

[WB blog - your-go-to regression-specification is -biased-here-s-simple-way-fix-it](http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT)

A key paper: http://www.jcsuarez.com/Files/Suarez_Serrato-BFE.pdf

> In particular, we compare treatment effect estimates using a fixed effects estimator (FE) to the average treatment effect (ATE) by replicating eight influential papers from the American Economic Review published between 2004 and 2009.1 Using these examples, we consider a randomized experiment in Section 1 as a case study and, in Section 3, we show generally that heterogeneous treatment effects are common and that the FE and ATE are often different in statistically and economically significant degrees. In all but one paper, there is at least one statistically significant source of treatment effect heterogeneity. In five papers, this heterogeneity induces the ATE to be statistically different from the FE estimate at the 5% level (7 of 8 are statistically different at the 10% level). Five of these differences are economically significant, which we define as an absolute difference exceeding 10%. Based upon these results, we conclude that methods that consistently estimate the ATE offer more interpretable results than standard FE models

By "FE" here I think they mean group dummies; they are focused on cross-sectional and not panel data!

> While fixed effects permit different mean outcomesamong groups,  the estimates of treatment effects are typically required to be the same;  in more colloquial  terms,  the  intercepts  of  the  conditional  expectation  functions  may  differ,  but  not  the slopes

DGP

$$y_i = x_i \beta_{g(i)} + \mathbf{z_i}' \gamma + \epsilon_i$$

> where $y_i is the outcome for observation i among N [N what?],
$x_i$ is treatment or another variable of interest, and $z_i$ contains control variables, including group-specific fixed effects.  

> The treatment effects aregroup-specific  for  each  of  the $g=1,...,G$ groups,  where  group  membership  is  known  for  each observation. 

Defining ATE

$$\beta^{ATE}=\sum_g \pi_g \beta_g $$

where the $\pi$ terms are population frequencies

\


The use of interaction terms is delicate... 

<!-- 
On the book, I saw the section on the problems of using OLS as an estimator of FE without fully interacting the variables (4.2.4, this blog post), but d
By idn’t really understand the issue/intuition behind the problem/solution – I would be keen to chat about what it means.. and if I then get it, would more than happily contribute a lay summary for the book. Also there are three papers by Ferraro (links below) that I think you might find interesting/offer well explained insights into difficulties in how to do empirical econ/problems within it for the Book. 

Plus there is this paper – https://www.nber.org/papers/w25636 which applies the changes-in-changes method of Athey and Imbens 2006 a method which (proponents claims) is able to get at heterogeneous treatment effects better than simple DiD by bin/something similar.
--> 


<!-- #TODO: recover conversations with Winston Lin and write these up --> 

- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"

"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

#### Confidence intervals and Bayesian credible intervals

#### Comparing relative parameters

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT) {#mht}

A typical conversation (unfold)

```{block2,  type='fold'}

This isn't really on the above discussion of the value of pre-reg, but on a topic that i think is at least somehwat related. Namely that when we have multiple hypotheses testing in a paper how do we control for that and what do our p-values really mean. Particulary, there is [this paper](https://academic.oup.com/qje/article/134/2/557/5195544) by Alwyn Young which (I think) is basically saying that when we have a load of hypotheses within a study and then one has a significant p value we are quite a lot of the time picking up a false result. Obviously that isn't very much of a surprise. But i think it also talks somewhat to the Steffano DellaVigna paper that was presented at the VAFE - that the publication bias really creeps in when we are publishing studies that have just one or two p values around 0.05. After all, it is those studies which have a real pay-off from getting that significant result.
```


### Interaction terms and pitfalls

See also 'effect coding'

#### 'Moderators' Confusion with nonlinearity

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if outcome  'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative


#### MHT

### Choice of test statistics (including nonparametric)

(Or get to this in the experimetrics section)

### How to display and write about regression results and tests

### Bayesian interpretations of results
