<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Statistics, econometrics, experiment and survey methods, data science: Notes</title>
  <meta name="description" content="Statistics, econometrics, experiment and survey methods, data science: Notes" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Statistics, econometrics, experiment and survey methods, data science: Notes" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="daaronr/metrics_discussion_work" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistics, econometrics, experiment and survey methods, data science: Notes" />
  
  
  

<meta name="author" content="Dr. David Reinstein; contributions from Gerhard Riener, Scott Dickinson, Oska Fentem, and others" />


<meta name="date" content="2021-06-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.8.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- <script src="js/hideOutput.js"></script> -->

<!-- Mathjax -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>




<script async defer src="https://hypothes.is/embed.js"></script>

<!-- Folding text box javascript thing -->

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold" ? "Unfold" : "Fold");  // if the text equals "Fold", change it to "Unfold"or else to "Fold"
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>

<!--
<script type="text/javascript">

// toggle visibility of R source blocks in R Markdown output
function toggle_R() {
  var x = document.getElementsByClassName('r');
  if (x.length == 0) return;
  function toggle_vis(o) {
    var d = o.style.display;
    o.style.display = (d == 'block' || d == '') ? 'none':'block';
  }

  for (i = 0; i < x.length; i++) {
    var y = x[i];
    if (y.tagName.toLowerCase() === 'pre') toggle_vis(y);
  }

    var elem = document.getElementById("myButton1");
    if (elem.value === "Hide Global") elem.value = "Show Global";
    else elem.value = "Hide Global";
}

document.write('<input onclick="toggle_R();" type="button" value="Hide Global" id="myButton1" style="position: absolute; top: 10%; right: 2%; z-index: 200"></input>')

</script> -->

<!-- Global site tag (gtag.js) - Google Analytics
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-148137970-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-148137970-3');
</script>
-->

<!-- FOLDING TEXT BOXES -->

<script>
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan, pre.js');
  rCodeBlocks.each(function() {

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}
.open > .dropdown-menu {
    display: block;
}
.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>


<script>
document.write('<div class="btn-group pull-right" style="position: absolute; top: 10%; right: 15%; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>')
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="support/tufte_plus.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Statistics, econometrics, experiment and survey methods, data science: Notes</h1>
<p class="author"><em>Dr. David Reinstein; contributions from Gerhard Riener, Scott Dickinson, Oska Fentem, and others</em></p>
<p class="date" style="margin-top: 1.5em;"><em>2021-06-18</em></p>
<div class="abstract">
<p class="abstract">Abstract</p>
This ‘book’ organizes my/our notes and helps others engage, understand, learn, and respond
</div>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="#basic-statistical-approaches-and-frameworks"><span>Basic statistical approaches and frameworks</span></a></li>
<li><a href="#regression-and-control-approaches-robustness"><span>Regression and control approaches, robustness</span></a></li>
<li><a href="#causal-inference-through-observation-caus_inf_obs">Causal inference through observation{-#caus_inf_obs}</a></li>
<li><a href="#causal-paths-and-levels-of-aggregation">Causal paths and levels of aggregation</a></li>
<li><a href="#experiments-and-surveys-design-and-analysis">Experiments and surveys: design and analysis</a></li>
</ul></li>
<li><a href="#other-approaches-techniques-and-applications"><span>Other approaches, techniques, and applications</span></a>
<ul>
<li><a href="#some-key-resources-and-references">Some key resources and references</a></li>
</ul></li>
<li><a href="#conceptual"><span class="toc-section-number">2</span> <strong>BASIC STATISTICAL APPROACHES AND FRAMEWORKS</strong></a>
<ul>
<li><a href="#learning-and-optimization-as-an-alternative-to-statistical-inference"><span class="toc-section-number">2.1</span> ‘Learning and optimization’ as an alternative to statistical inference</a></li>
<li><a href="#statistical-inference"><span class="toc-section-number">2.2</span> Statistical inference</a></li>
<li><a href="#bayesian-vs.-frequentist-approaches"><span class="toc-section-number">2.3</span> Bayesian vs. frequentist approaches</a>
<ul>
<li><a href="#interpretation-of-frequentist-cis-aside"><span class="toc-section-number">2.3.1</span> Interpretation of frequentist CI’s (aside)</a></li>
</ul></li>
<li><a href="#causal-vs.-descriptive-treatment-effects-and-the-potential-outcomes-causal-model"><span class="toc-section-number">2.4</span> Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model</a>
<ul>
<li><a href="#dags-and-potential-outcomes"><span class="toc-section-number">2.4.1</span> DAGs and Potential outcomes</a></li>
</ul></li>
<li><a href="#theory-restrictions-and-structural-vs-reduced-form"><span class="toc-section-number">2.5</span> Theory, restrictions, and ‘structural vs reduced form’</a></li>
<li><a href="#hypothesis-testing"><span class="toc-section-number">2.6</span> ‘Hypothesis testing’</a>
<ul>
<li><a href="#mcelreaths-critique"><span class="toc-section-number">2.6.1</span> McElreath’s critique</a></li>
<li><a href="#bayesian-vs.-frequentist-hypothesis-testing"><span class="toc-section-number">2.6.2</span> Bayesian vs. frequentist hypothesis ‘testing’</a></li>
<li><a href="#individual-vs.-joint-hypothesis-testing-what-does-it-mean"><span class="toc-section-number">2.6.3</span> Individual vs. joint hypothesis testing: what does it mean?</a></li>
<li><a href="#other-issues"><span class="toc-section-number">2.6.4</span> Other issues</a></li>
</ul></li>
</ul></li>
<li><a href="#reg_control"><strong>REGRESSION AND CONTROL APPROACHES, ROBUSTNESS</strong></a></li>
<li><a href="#reg-follies"><span class="toc-section-number">3</span> Basic statistical inference and regressions: Common mistakes and issues</a>
<ul>
<li><a href="#basic-regression-and-statistical-inference-common-mistakes-and-issues-briefly-listed"><span class="toc-section-number">3.1</span> Basic regression and statistical inference: Common mistakes and issues briefly listed</a>
<ul>
<li><a href="#bad-control"><span class="toc-section-number">3.1.1</span> Bad control</a></li>
<li><a href="#bad-control-colliders"><span class="toc-section-number">3.1.2</span> “Bad control” (“colliders”)</a></li>
<li><a href="#choices-of-lhs-and-rhs-variables"><span class="toc-section-number">3.1.3</span> Choices of lhs and rhs variables</a></li>
<li><a href="#functional-form"><span class="toc-section-number">3.1.4</span> Functional form</a></li>
<li><a href="#ols-and-heterogeneity"><span class="toc-section-number">3.1.5</span> OLS and heterogeneity</a></li>
<li><a href="#null-effects"><span class="toc-section-number">3.1.6</span> “Null effects”</a></li>
<li><a href="#multi-var-tests"><span class="toc-section-number">3.1.7</span> Multivariate tests and ‘tests for non-independence’</a></li>
<li><a href="#mht"><span class="toc-section-number">3.1.8</span> Multiple hypothesis testing (MHT)</a></li>
<li><a href="#interaction-terms-and-pitfalls"><span class="toc-section-number">3.1.9</span> Interaction terms and pitfalls</a></li>
<li><a href="#choice-of-test-statistics-including-nonparametric"><span class="toc-section-number">3.1.10</span> Choice of test statistics (including nonparametric)</a></li>
<li><a href="#how-to-display-and-write-about-regression-results-and-tests"><span class="toc-section-number">3.1.11</span> How to display and write about regression results and tests</a></li>
<li><a href="#bayesian-interpretations-of-results"><span class="toc-section-number">3.1.12</span> Bayesian interpretations of results</a></li>
</ul></li>
<li><a href="#aside-effect-and-contrast-coding-of-categorical-variables"><span class="toc-section-number">3.2</span> Aside: effect and contrast coding of categorical variables</a></li>
</ul></li>
<li><a href="#robust-diag"><span class="toc-section-number">4</span> Robustness and diagnostics, with integrity; Open Science resources</a>
<ul>
<li><a href="#how-can-diagnostic-tests-make-sense-where-is-the-burden-of-proof"><span class="toc-section-number">4.1</span> (How) can diagnostic tests make sense? Where is the burden of proof?</a>
<ul>
<li><a href="#further-discussion-the-did-approach-and-parallel-trends"><span class="toc-section-number">4.1.1</span> Further discussion: the DiD approach and ‘parallel trends’</a></li>
</ul></li>
<li><a href="#estimating-standard-errors"><span class="toc-section-number">4.2</span> Estimating standard errors</a></li>
<li><a href="#sensitivity-analysis-interactive-presentation"><span class="toc-section-number">4.3</span> Sensitivity analysis: Interactive presentation</a></li>
<li><a href="#supplement-open-science-resources-tools-and-considerations"><span class="toc-section-number">4.4</span> Supplement: open science resources, tools and considerations</a></li>
<li><a href="#diagnosing-p-hacking-and-publication-bias-see-also-meta-analysis"><span class="toc-section-number">4.5</span> Diagnosing p-hacking and publication bias (see also <span>meta-analysis</span>)</a>
<ul>
<li><a href="#publication-bias-see-also-considering-publication-bias-in-meta-analysis"><span class="toc-section-number">4.5.1</span> Publication bias – see also <span>considering publication bias in meta-analysis</span></a></li>
</ul></li>
<li><a href="#multiple-hypothesis-testing---see-above"><span class="toc-section-number">4.6</span> <span>Multiple hypothesis testing - see above</span></a></li>
</ul></li>
<li><a href="#control-ml"><span class="toc-section-number">5</span> Control strategies and prediction, Machine Learning (Statistical Learning) approaches</a>
<ul>
<li><a href="#see-also-notes-on-data-science-for-business"><span class="toc-section-number">5.1</span> See also <span>“notes on Data Science for Business”</span></a>
<ul>
<li><a href="#limitations-to-inference-from-learning-approaches"><span class="toc-section-number">5.1.1</span> Limitations to inference from learning approaches</a></li>
<li><a href="#tree-models"><span class="toc-section-number">5.1.2</span> Tree models</a></li>
</ul></li>
<li><a href="#notes-hastie-statistical-learning-with-sparsity"><span class="toc-section-number">5.2</span> Notes Hastie: Statistical Learning with Sparsity</a>
<ul>
<li><a href="#introduction-1"><span class="toc-section-number">5.2.1</span> Introduction</a></li>
<li><a href="#ch2-lasso-for-linear-models"><span class="toc-section-number">5.2.2</span> Ch2: Lasso for linear models</a></li>
<li><a href="#chapter-3-generalized-linear-models"><span class="toc-section-number">5.2.3</span> Chapter 3: Generalized linear models</a></li>
<li><a href="#chapter-4-generalizations-of-the-lasso-penalty"><span class="toc-section-number">5.2.4</span> Chapter 4: Generalizations of the Lasso penalty</a></li>
</ul></li>
<li><a href="#notes-mullainathan"><span class="toc-section-number">5.3</span> Notes: Mullainathan</a></li>
</ul></li>
<li><a href="#caus_inf_obs"><strong>CAUSAL INFERENCE THROUGH OBSERVATION</strong></a></li>
<li><a href="#iv_limitations"><span class="toc-section-number">6</span> Causal inference: IV (instrumental variables) and its limitations</a>
<ul>
<li><a href="#some-casual-discussion">Some casual discussion</a></li>
<li><a href="#instrument-validity"><span class="toc-section-number">6.1</span> Instrument validity</a></li>
<li><a href="#heterogeneity-and-late"><span class="toc-section-number">6.2</span> Heterogeneity and LATE</a></li>
<li><a href="#weak-instruments-other-issues"><span class="toc-section-number">6.3</span> Weak instruments, other issues</a></li>
<li><a href="#instrumenting-interactions"><span class="toc-section-number">6.4</span> Instrumenting Interactions</a></li>
<li><a href="#reference-to-the-use-of-iv-in-experimentsmediation"><span class="toc-section-number">6.5</span> Reference to the use of IV in experiments/mediation</a></li>
</ul></li>
<li><a href="#causal-inference-other-paths-to-observational-identification"><span class="toc-section-number">7</span> <span id="other_paths">Causal inference: Other paths to observational identification</span></a>
<ul>
<li><a href="#fixed-effects-and-differencing"><span class="toc-section-number">7.1</span> Fixed effects and differencing</a></li>
<li><a href="#did"><span class="toc-section-number">7.2</span> DiD</a></li>
<li><a href="#rd"><span class="toc-section-number">7.3</span> RD</a></li>
<li><a href="#time-series-ish-panel-approaches-to-micro"><span class="toc-section-number">7.4</span> Time-series-ish panel approaches to micro</a>
<ul>
<li><a href="#lagged-dependent-variable-and-fixed-effects-nickel-bias"><span class="toc-section-number">7.4.1</span> Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’</a></li>
<li><a href="#apc-effects"><span class="toc-section-number">7.4.2</span> Age-period-cohort effects</a></li>
</ul></li>
</ul></li>
<li><a href="#causal-paths-and-levels-of-aggregation-1"><strong>CAUSAL PATHS AND LEVELS OF AGGREGATION</strong></a></li>
<li><a href="#mediators"><span class="toc-section-number">8</span> Mediation modeling and its massive limitations</a>
<ul>
<li><a href="#mediators-and-selection-and-roy-models-a-review-considering-two-research-applications"><span class="toc-section-number">8.1</span> Mediators (and selection and Roy models): a review, considering two research applications</a></li>
<li><a href="#dr-initial-thoughts-for-nl-education-paper"><span class="toc-section-number">8.2</span> DR initial thoughts (for NL education paper)</a></li>
<li><a href="#econometric-mediation-analyses-heckman-and-pinto"><span class="toc-section-number">8.3</span> Econometric Mediation Analyses (Heckman and Pinto)</a>
<ul>
<li><a href="#relevance-to-parey-et-al">Relevance to Parey et al</a></li>
<li><a href="#summary-and-key-modeling"><span class="toc-section-number">8.3.1</span> Summary and key modeling</a></li>
<li><a href="#common-assumptions-and-their-implications"><span class="toc-section-number">8.3.2</span> Common assumptions and their implications</a></li>
</ul></li>
<li><a href="#pinto-2015-selection-bias-in-a-controlled-experiment-the-case-of-moving-to-opportunity"><span class="toc-section-number">8.4</span> Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity</a>
<ul>
<li><a href="#summary">Summary</a></li>
<li><a href="#relevance-to-parey-et-al-1">Relevance to Parey et al</a></li>
<li><a href="#introduction-2">Introduction</a></li>
<li><a href="#identification-strategy-brief">Identification strategy brief</a></li>
<li><a href="#results-in-brief">Results in brief</a></li>
<li><a href="#framework-first-for-binarybinary-simplification">Framework: first for binary/binary (simplification)</a></li>
<li><a href="#framework-for-mto-multiple-treatment-groups-multiple-choices">Framework for MTO multiple treatment groups, multiple choices</a></li>
</ul></li>
<li><a href="#antonakis-approaches"><span class="toc-section-number">8.5</span> Antonakis approaches</a></li>
</ul></li>
<li><a href="#selection_cop"><span class="toc-section-number">9</span> Selection, corners, hurdles, and ‘conditional on’ estimates</a>
<ul>
<li><a href="#corner-solution-or-hurdle-variables-and-conditional-on-positive"><span class="toc-section-number">9.1</span> ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’</a></li>
<li><a href="#bounding-approaches-lee-manski-etc"><span class="toc-section-number">9.2</span> Bounding approaches (Lee, Manski, etc)</a>
<ul>
<li><a href="#notes-training-wages-and-sample-selection-estimating-sharp-bounds-on-treatment-effects-david-lee-2009-restud"><span class="toc-section-number">9.2.1</span> Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD</a></li>
</ul></li>
</ul></li>
<li><a href="#mlm"><span class="toc-section-number">10</span> Multi-level models</a>
<ul>
<li><a href="#introduction-qstep"><span class="toc-section-number">10.1</span> Introduction (Qstep)</a></li>
<li><a href="#some-basic-theory"><span class="toc-section-number">10.2</span> Some basic theory</a>
<ul>
<li><a href="#level-1-model"><span class="toc-section-number">10.2.1</span> Level 1 model</a></li>
<li><a href="#level-2"><span class="toc-section-number">10.2.2</span> Level 2</a></li>
<li><a href="#alternativenaive-approaches"><span class="toc-section-number">10.2.3</span> Alternative/Naive approaches</a></li>
<li><a href="#old-way-two-stage-regression"><span class="toc-section-number">10.2.4</span> ‘old way’: two-stage regression</a></li>
<li><a href="#how-many-higher-level-units-do-you-need"><span class="toc-section-number">10.2.5</span> How many higher-level units do you need?</a></li>
</ul></li>
<li><a href="#fitting-mlm-in-practice"><span class="toc-section-number">10.3</span> Fitting mlm in practice</a></li>
<li><a href="#stimuli-treatments-as-a-random-factor"><span class="toc-section-number">10.4</span> “Stimuli” (treatments) as a random factor</a></li>
</ul></li>
<li><a href="#experiments-and-surveys-design-and-analysis-1"><strong>EXPERIMENTS AND SURVEYS: DESIGN AND ANALYSIS</strong></a></li>
<li><a href="#surveys"><span class="toc-section-number">11</span> Survey design and implementation; analysis of survey data</a>
<ul>
<li><a href="#survey-samplingintake"><span class="toc-section-number">11.1</span> Survey sampling/intake</a>
<ul>
<li><a href="#probability-sampling">Probability sampling</a></li>
<li><a href="#np-sampling">Non-probability sampling</a></li>
</ul></li>
<li><a href="#jazz-case"><span class="toc-section-number">11.2</span> Case: Surveying an unmeasured and rare population surrounding a ‘social movement’</a>
<ul>
<li><a href="#background-and-setup">Background and setup</a></li>
<li><a href="#our-convenience-method-issues-alternatives">Our ‘convenience’ method; issues, alternatives</a></li>
<li><a href="#our-methodological-questions">Our methodological questions</a></li>
<li><a href="#sketched-model-and-approach-bayesian-inferenceupdating-for-estimating-demographics-and-attitudes-of-an-rarehidden-population"><span class="toc-section-number">11.2.1</span> Sketched model and approach: Bayesian inference/updating for estimating demographics and attitudes of an rare/hidden population</a></li>
</ul></li>
</ul></li>
<li><a href="#why_experiment_design"><span class="toc-section-number">12</span> Experimental design: Identifying meaningful and useful (causal) relationships and parameters</a>
<ul>
<li><a href="#why-run-an-experiment-or-study"><span class="toc-section-number">12.1</span> Why run an experiment or study?</a>
<ul>
<li><a href="#sitzia-and-sugden-on-what-theoretically-driven-experiments-can-and-should-do"><span class="toc-section-number">12.1.1</span> Sitzia and Sugden on what theoretically driven experiments can and should do</a></li>
</ul></li>
<li><a href="#causal-channels-and-identification"><span class="toc-section-number">12.2</span> Causal channels and identification</a></li>
<li><a href="#artifacts"><span class="toc-section-number">12.3</span> Types of experiments, ‘demand effects’ and more artifacts of artificial setups</a></li>
<li><a href="#ws-bs"><span class="toc-section-number">12.4</span> Within vs between-subject designs</a></li>
<li><a href="#generalizability-and-heterogeneity"><span class="toc-section-number">12.5</span> Generalizability (and heterogeneity)</a></li>
</ul></li>
<li><a href="#quant_design_power"><span class="toc-section-number">13</span> Robust experimental design: pre-registration and efficient assignment of treatments</a>
<ul>
<li><a href="#pre-reg-pap"><span class="toc-section-number">13.1</span> Pre-registration and Pre-analysis plans</a>
<ul>
<li><a href="#the-benefits-and-costs-of-pre-registration-a-typical-discussion"><span class="toc-section-number">13.1.1</span> The benefits and costs of pre-registration: a typical discussion</a></li>
<li><a href="#the-hazards-of-specification-searching"><span class="toc-section-number">13.1.2</span> The hazards of specification-searching</a></li>
</ul></li>
<li><a href="#designs-for-decision-making"><span class="toc-section-number">13.2</span> Designs for <em>decision-making</em></a>
<ul>
<li><a href="#notes-on-bandit-vs-exploration-problemsthompson-vs-exploration-sampling"><span class="toc-section-number">13.2.1</span> Notes on Bandit vs Exploration problems/Thompson vs Exploration sampling</a></li>
<li><a href="#sequential">Sequential</a></li>
<li><a href="#adaptive"><span class="toc-section-number">13.2.2</span> Adaptive</a></li>
</ul></li>
<li><a href="#efficient-assignment-of-treatments"><span class="toc-section-number">13.3</span> Efficient assignment of treatments</a>
<ul>
<li><a href="#see-also-multiple-hypothesis-testing"><span class="toc-section-number">13.3.1</span> See also <span>multiple hypothesis testing</span></a></li>
<li><a href="#how-many-treatment-arms-can-you-afford"><span class="toc-section-number">13.3.2</span> How many treatment arms can you ‘afford?’</a></li>
<li><a href="#other-notes-and-resources"><span class="toc-section-number">13.3.3</span> Other notes and resources</a></li>
</ul></li>
</ul></li>
<li><a href="#power"><span class="toc-section-number">14</span> (Ex-ante) Power calculations for (Experimental) study design</a>
<ul>
<li><a href="#what-is-the-point-of-doing-a-power-analysis-or-power-calculations"><span class="toc-section-number">14.1</span> What is the point of doing a ‘power analysis’ or ‘power calculations?’</a>
<ul>
<li><a href="#practical-power"><span class="toc-section-number">14.1.1</span> What are the practical benefits of doing a power analysis</a></li>
</ul></li>
<li><a href="#power-ingredients"><span class="toc-section-number">14.2</span> Key ingredients for doing a power analysis (and designing an experimental study in light of this)</a></li>
<li><a href="#underpowered"><span class="toc-section-number">14.3</span> The ‘harm to science’ from running underpowered studies</a></li>
<li><a href="#power-calculations-without-real-data"><span class="toc-section-number">14.4</span> Power calculations without real data</a></li>
<li><a href="#power-calculations-using-prior-data"><span class="toc-section-number">14.5</span> Power calculations using prior data</a>
<ul>
<li><a href="#from-reinstein-upcoming-experiment-preregistration"><span class="toc-section-number">14.5.1</span> From Reinstein upcoming experiment preregistration</a></li>
</ul></li>
<li><a href="#lift-test"><span class="toc-section-number">14.6</span> Digression: Power calculations/optimal sample size for ‘lift’ in a ranking case</a>
<ul>
<li><a href="#design-which-questions-to-ask-the-audience-about-the-proposed-titles-and-in-what-order"><span class="toc-section-number">14.6.1</span> Design: Which questions to ask the audience about the proposed titles, and in what order?</a></li>
<li><a href="#which-statistical-testsanalyses-to-run-if-any-and-what-measures-to-report">Which statistical test(s)/analyses to run (if any) and what measures to report?</a></li>
<li><a href="#how-to-assign-the-treatments-and-how-large-a-sample-is-optimal-considering-power-or-lift">How to assign the ‘treatments,’ and how large a sample is optimal, considering ‘power’ (or ‘lift’)?</a></li>
</ul></li>
<li><a href="#survey-power-likert"><span class="toc-section-number">14.7</span> Survey design digression: sample size for a “precise estimate of a ‘population parameter’” (focus: mean of a Likert scale response)</a>
<ul>
<li><a href="#how-to-measure-and-consider-the-precision-of-likert-item-responses"><span class="toc-section-number">14.7.1</span> How to measure and consider the precision of Likert-item responses</a></li>
<li><a href="#computing-sample-size-to-achieve-this-precision"><span class="toc-section-number">14.7.2</span> Computing sample size to achieve this precision</a></li>
</ul></li>
</ul></li>
<li><a href="#experimetrics_te"><span class="toc-section-number">15</span> ‘Experimetrics’ and measurement of treatment effects from RCTs</a>
<ul>
<li><a href="#which-error-structure-random-effects"><span class="toc-section-number">15.1</span> Which error structure? Random effects?</a></li>
<li><a href="#randomization-inference"><span class="toc-section-number">15.2</span> Randomization inference?</a></li>
<li><a href="#parametric-and-nonparametric-tests-of-simple-hypotheses"><span class="toc-section-number">15.3</span> Parametric and nonparametric tests of simple hypotheses</a>
<ul>
<li><a href="#parametric-tests"><span class="toc-section-number">15.3.1</span> Parametric tests</a></li>
<li><a href="#non-parametric-tests"><span class="toc-section-number">15.3.2</span> Non-parametric tests</a></li>
</ul></li>
<li><a href="#adjustments-for-exogenous-but-non-random-treatment-assignment"><span class="toc-section-number">15.4</span> Adjustments for exogenous (but non-random) treatment assignment</a></li>
<li><a href="#iv-in-an-experimental-context-to-get-at-mediators"><span class="toc-section-number">15.5</span> IV in an experimental context to get at ‘mediators?’</a></li>
<li><a href="#heterogeneity-in-an-experimental-context"><span class="toc-section-number">15.6</span> Heterogeneity in an experimental context</a></li>
<li><a href="#incorporate-above-notes-on-the-econometrics-of-randomised-experiments-athey-and-imbens"><span class="toc-section-number">15.7</span> Incorporate above: Notes on “The econometrics of randomised experiments” (Athey and Imbens)</a>
<ul>
<li><a href="#abstract-and-intro"><span class="toc-section-number">15.7.1</span> Abstract and intro</a></li>
<li><a href="#randomised-experiments-and-validity"><span class="toc-section-number">15.7.2</span> Randomised experiments and validity</a></li>
<li><a href="#potential-outcomes-rubin-causal-model-framework-covered-earlier"><span class="toc-section-number">15.7.3</span> Potential outcomes/ Rubin causal model framework (covered earlier)</a></li>
<li><a href="#classification-of-assignment-mechanisms"><span class="toc-section-number">15.7.4</span> 3.2 Classification of assignment mechanisms</a></li>
<li><a href="#the-analysis-of-completely-randomized-experiments"><span class="toc-section-number">15.7.5</span> The analysis of Completely randomized experiments</a></li>
<li><a href="#randomization-inference-for-average-treatment-effects"><span class="toc-section-number">15.7.6</span> Randomization inference for Average treatment effects</a></li>
<li><a href="#quantile-treatment-effect-infinite-population-context"><span class="toc-section-number">15.7.7</span> Quantile treatment effect (Infinite population context)</a></li>
<li><a href="#covariates-if-not-stratified-in-completely-randomized-experiments"><span class="toc-section-number">15.7.8</span> Covariates (if not stratified) in completely randomized experiments</a></li>
<li><a href="#randomization-inference-and-regression-estimators"><span class="toc-section-number">15.7.9</span> Randomization inference and regression estimators</a></li>
<li><a href="#regression-estimators-with-additional-covariates-dr-seems-important"><span class="toc-section-number">15.7.10</span> Regression Estimators with Additional Covariates [DR: seems important]</a></li>
<li><a href="#stratified-randomized-experiments-analysis"><span class="toc-section-number">15.7.11</span> Stratified randomized experiments: analysis</a></li>
<li><a href="#the-design-of-randomised-experiments-and-the-benefits-of-stratification"><span class="toc-section-number">15.7.12</span> 7 The Design of randomised experiments and the benefits of stratification</a></li>
<li><a href="#power-calculations"><span class="toc-section-number">15.7.13</span> 7.1 Power calculations</a></li>
<li><a href="#stratified-randomized-experiments-benefits"><span class="toc-section-number">15.7.14</span> Stratified randomized experiments: Benefits</a></li>
<li><a href="#re-randomization"><span class="toc-section-number">15.7.15</span> Re-randomization</a></li>
<li><a href="#analysis-of-clustered-randomised-experiments"><span class="toc-section-number">15.7.16</span> Analysis of Clustered Randomised Experiments</a></li>
<li><a href="#noncompliance-in-randomized-experiments-dr-relevant-to-nl-lottery-not-to-charity-experiments"><span class="toc-section-number">15.7.17</span> Noncompliance in randomized experiments (DR: Relevant to NL lottery, not to charity experiments)</a></li>
<li><a href="#heterogenous-treatment-effects-and-pretreatment-variables"><span class="toc-section-number">15.7.18</span> Heterogenous Treatment Effects and Pretreatment Variables</a></li>
<li><a href="#data-driven-subgroup-analysis-recursive-partitioning-for-treatment-effects"><span class="toc-section-number">15.7.19</span> Data-driven Subgroup Analysis: Recursive Partitioning for Treatment Effects</a></li>
<li><a href="#non-parametric-estimation-of-treatment-effect-heterogeneity"><span class="toc-section-number">15.7.20</span> 10.3.2 Non-Parametric Estimation of Treatment Effect Heterogeneity</a></li>
<li><a href="#treatment-effect-heterogeneity-using-regularized-regression"><span class="toc-section-number">15.7.21</span> 10.3.3 Treatment Effect Heterogeneity Using Regularized Regression</a></li>
<li><a href="#comparison-of-methods"><span class="toc-section-number">15.7.22</span> 10.3.4 Comparison of Methods</a></li>
</ul></li>
</ul></li>
<li><a href="#other_approaches"><strong>OTHER APPROACHES, TECHNIQUES, AND APPLICATIONS</strong></a></li>
<li><a href="#psychometrics"><span class="toc-section-number">16</span> Boiling down: Construct validation/reliability, dimension reduction, factor analysis, and Psychometrics</a>
<ul>
<li><a href="#constructs-and-construct-validation-and-reliability"><span class="toc-section-number">16.1</span> Constructs and construct validation and reliability</a>
<ul>
<li><a href="#validity-general-discussion"><span class="toc-section-number">16.1.1</span> Validity: general discussion</a></li>
<li><a href="#reliability-general-discussion"><span class="toc-section-number">16.1.2</span> Reliability: general discussion</a></li>
<li><a href="#raykovmetaanalysisscalereliability2013"><span class="toc-section-number">16.1.3</span> <span class="citation"><span>Raykov and Marcoulides</span> (<span>2013</span>)</span></a></li>
</ul></li>
<li><a href="#factor-analysis-and-principal-component-analysis"><span class="toc-section-number">16.2</span> Factor analysis and principal-component analysis</a></li>
<li><a href="#other"><span class="toc-section-number">16.3</span> Other</a></li>
</ul></li>
<li><a href="#metaanalysis"><span class="toc-section-number">17</span> Meta-analysis and combining studies: Making inferences from previous work</a>
<ul>
<li><a href="#introduction-4"><span class="toc-section-number">17.1</span> Introduction</a></li>
<li><a href="#christensen-meta"><span class="toc-section-number">17.2</span> An overview of meta-analysis, from Christensen et al 2019, ch 5, ’Using all evidence, registration and meta-analysis</a>
<ul>
<li><a href="#the-origins-and-importance-of-study-pre-registration"><span class="toc-section-number">17.2.1</span> The origins [and importance] of study [pre-]registration</a></li>
<li><a href="#social-science-study-registries"><span class="toc-section-number">17.2.2</span> Social science study registries</a></li>
<li><a href="#meta-analysis"><span class="toc-section-number">17.2.3</span> Meta-analysis</a></li>
<li><a href="#combining-estimates"><span class="toc-section-number">17.2.4</span> Combining estimates</a></li>
<li><a href="#heterogeneous-estimates"><span class="toc-section-number">17.2.5</span> Heterogeneous estimates…</a></li>
</ul></li>
<li><a href="#doing-meta"><span class="toc-section-number">17.3</span> Excerpts and notes from <span>‘Doing Meta-Analysis in R: A Hands-on Guide’</span> (Harrer et al)</a>
<ul>
<li><a href="#pooling-effect-sizes"><span class="toc-section-number">17.3.1</span> Pooling effect sizes</a></li>
<li><a href="#doing-bayes-meta"><span class="toc-section-number">17.3.2</span> Bayesian Meta-analysis</a></li>
<li><a href="#forest-plots"><span class="toc-section-number">17.3.3</span> Forest plots</a></li>
</ul></li>
<li><a href="#pubbias"><span class="toc-section-number">17.4</span> Dealing with publication bias</a>
<ul>
<li><a href="#diagnosis-and-responses-p-curves-funnel-plots-adjustments"><span class="toc-section-number">17.4.1</span> Diagnosis and responses: P-curves, funnel plots, adjustments</a></li>
</ul></li>
<li><a href="#other-notes-links-and-commentary"><span class="toc-section-number">17.5</span> Other notes, links, and commentary</a></li>
<li><a href="#other-resources-and-tools"><span class="toc-section-number">17.6</span> Other resources and tools</a>
<ul>
<li><a href="#institutional-and-systematic-guidelines"><span class="toc-section-number">17.6.1</span> Institutional and systematic guidelines</a></li>
</ul></li>
<li><a href="#example-discussion-of-meta-analyses-of-the-paleolithic-diet-below"><span class="toc-section-number">17.7</span> Example: discussion of meta-analyses of the Paleolithic diet <span>BELOW</span></a></li>
</ul></li>
<li><a href="#bayes"><span class="toc-section-number">18</span> Bayesian approaches</a>
<ul>
<li><a href="#my-david-reinsteins-uses-for-bayesian-approaches-brainstorm"><span class="toc-section-number">18.1</span> My (David Reinstein’s) uses for Bayesian approaches (brainstorm)</a>
<ul>
<li><a href="#meta-analysis-of-previous-evidence"><span class="toc-section-number">18.1.1</span> Meta-analysis of previous evidence</a></li>
<li><a href="#inference-particularly-about-null-effects"><span class="toc-section-number">18.1.2</span> Inference, particularly about ‘null effects’</a></li>
<li><a href="#policy-and-business-implications-and-recommendations"><span class="toc-section-number">18.1.3</span> ‘Policy’ and business implications and recommendations</a></li>
<li><a href="#theory-driven-inference-about-optimizing-agents-esp.-in-strategic-settings"><span class="toc-section-number">18.1.4</span> Theory-driven inference about optimizing agents, esp. in strategic settings</a></li>
<li><a href="#experimental-design"><span class="toc-section-number">18.1.5</span> Experimental design</a></li>
</ul></li>
<li><a href="#statistical-thinking-mcelreath-and-aj-kurtz-recoded-bookdown-highlights-and-notes"><span class="toc-section-number">18.2</span> ‘Statistical thinking’ (McElreath) and <span>AJ Kurtz ‘recoded’ (bookdown)</span>: highlights and notes</a>
<ul>
<li><a href="#the-golem-of-prague-map-ant-the-territory"><span class="toc-section-number">18.2.1</span> 1. The Golem of Prague (map ant the territory)</a></li>
<li><a href="#small-worlds-and-large-worlds"><span class="toc-section-number">18.2.2</span> 2. Small Worlds and Large Worlds</a></li>
</ul></li>
<li><a href="#title-introduction-to-bayesian-analysis-in-r-and-stata---katz-qstep"><span class="toc-section-number">18.3</span> Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep”</a>
<ul>
<li><a href="#why-and-when-use-bayesian-mcmc-methods"><span class="toc-section-number">18.3.1</span> Why and when use Bayesian (MCMC) methods?</a></li>
<li><a href="#theory"><span class="toc-section-number">18.3.2</span> Theory</a></li>
<li><a href="#comparing-models-equivalent-of-likelihood"><span class="toc-section-number">18.3.3</span> Comparing models … Equivalent of ‘likelihood’</a></li>
<li><a href="#on-choosing-priors"><span class="toc-section-number">18.3.4</span> On choosing priors</a></li>
<li><a href="#implementation"><span class="toc-section-number">18.3.5</span> Implementation</a></li>
<li><a href="#generate-predictions-from-a-winbugs-model"><span class="toc-section-number">18.3.6</span> Generate predictions from a WinBUGS model</a></li>
<li><a href="#missing-data-case"><span class="toc-section-number">18.3.7</span> Missing data case</a></li>
<li><a href="#stata"><span class="toc-section-number">18.3.8</span> Stata</a></li>
<li><a href="#r-mcmc-pac"><span class="toc-section-number">18.3.9</span> R mcmc pac</a></li>
</ul></li>
<li><a href="#other-resources-and-notes-to-integrate"><span class="toc-section-number">18.4</span> Other resources and notes to integrate</a></li>
</ul></li>
<li><a href="#n_ds4bs"><span class="toc-section-number">19</span> Notes on Data Science for Business by Foster Provost and Tom Fawcett (2013)</a>
<ul>
<li><a href="#evaluation-of-this-resource"><span class="toc-section-number">19.1</span> Evaluation of this resource</a></li>
<li><a href="#ch-1-introduction-data-analytic-thinking">Ch 1 Introduction: Data-Analytic Thinking</a>
<ul>
<li><a href="#example-during-hurricane-frances-predicting-demand-to-gear-inventory-and-avoid-shortages-lead-to-huge-profit-for-wal-mart">Example: During Hurricane Frances… predicting demand to gear inventory and avoid shortages … lead to huge profit for Wal-Mart</a></li>
<li><a href="#example-predicting-customer-churn">Example: Predicting Customer Churn</a></li>
<li><a href="#data-science-engineering-and-data-driven-decision-making"><span class="toc-section-number">19.1.1</span> Data Science, Engineering, and Data-Driven Decision Making</a></li>
<li><a href="#data-processing-and-big-data"><span class="toc-section-number">19.1.2</span> Data Processing and “Big Data”</a></li>
<li><a href="#data-asset"><span class="toc-section-number">19.1.3</span> Data and Data Science Capability as a <strong>Strategic Asset</strong></a></li>
<li><a href="#da-thinking"><span class="toc-section-number">19.1.4</span> Data-Analytic Thinking</a></li>
<li><a href="#data-mining-and-data-science-revisited"><span class="toc-section-number">19.1.5</span> Data Mining and Data Science, Revisited</a></li>
</ul></li>
<li><a href="#ds4bs-ch2"><span class="toc-section-number">19.2</span> Ch 2 Business Problems and Data Science Solutions</a>
<ul>
<li><a href="#types-of-problems-and-approaches"><span class="toc-section-number">19.2.1</span> Types of problems and approaches</a></li>
<li><a href="#data-mining-process"><span class="toc-section-number">19.2.2</span> The Data Mining Process</a></li>
</ul></li>
<li><a href="#ch-3-introduction-to-predictive-modeling-from-correlation-to-supervised-segmentation"><span class="toc-section-number">19.3</span> Ch 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation</a>
<ul>
<li><a href="#models-induction-and-prediction"><span class="toc-section-number">19.3.1</span> Models, Induction, and Prediction</a></li>
<li><a href="#supervised-segmentation"><span class="toc-section-number">19.3.2</span> Supervised Segmentation</a></li>
<li><a href="#summary-1"><span class="toc-section-number">19.3.3</span> Summary</a></li>
<li><a href="#note-check-if-there-is-a-gap-here"><span class="toc-section-number">19.3.4</span> NOTE – check if there is a gap here</a></li>
</ul></li>
<li><a href="#ds4bs-model-to-data"><span class="toc-section-number">19.4</span> Ch. 4: Fitting a Model to Data</a>
<ul>
<li><a href="#classification-via-mathematical-functions"><span class="toc-section-number">19.4.1</span> Classification via Mathematical Functions</a></li>
<li><a href="#regression-via-mathematical-functions"><span class="toc-section-number">19.4.2</span> Regression via Mathematical Functions</a></li>
<li><a href="#class-probability-estimation-and-logistic-regression"><span class="toc-section-number">19.4.3</span> Class Probability Estimation and Logistic Regression</a></li>
<li><a href="#logistic-regression-some-technical-details"><span class="toc-section-number">19.4.4</span> Logistic Regression: Some Technical Details</a></li>
<li><a href="#example-logistic-regression-versus-tree-induction"><span class="toc-section-number">19.4.5</span> Example: Logistic Regression versus Tree Induction</a></li>
<li><a href="#nonlinear-functions-support-vector-machines-and-neural-networksthe-two-most-common-families-of-techniques-that-are-based-on-fitting-the-parameters-of-complex-nonlinear-functions-are-nonlinear-supportvector-machines-and-neural-networks."><span class="toc-section-number">19.4.6</span> Nonlinear Functions, Support Vector Machines, and Neural NetworksThe two most common families of techniques that are based on fitting the parameters of complex, nonlinear functions are nonlinear supportvector machines and neural networks.</a></li>
</ul></li>
<li><a href="#ds4bs-overfitting"><span class="toc-section-number">19.5</span> Ch 5: Overfitting and its avoidance</a>
<ul>
<li><a href="#generalization"><span class="toc-section-number">19.5.1</span> Generalization</a></li>
<li><a href="#holdout-data-and-fitting-graphs"><span class="toc-section-number">19.5.2</span> Holdout Data and Fitting Graphs</a></li>
<li><a href="#example-overfitting-linear-functions"><span class="toc-section-number">19.5.3</span> Example: Overfitting Linear Functions</a></li>
<li><a href="#example-why-is-overfitting-bad"><span class="toc-section-number">19.5.4</span> Example: Why Is Overfitting Bad?</a></li>
<li><a href="#from-holdout-evaluation-to-cross-validation"><span class="toc-section-number">19.5.5</span> From Holdout Evaluation to Cross-Validation</a></li>
<li><a href="#learning-curves"><span class="toc-section-number">19.5.6</span> Learning Curves</a></li>
<li><a href="#avoiding-overfitting-with-tree-induction"><span class="toc-section-number">19.5.7</span> Avoiding Overfitting with Tree Induction</a></li>
<li><a href="#a-general-method-for-avoiding-overfitting"><span class="toc-section-number">19.5.8</span> A General Method for Avoiding Overfitting</a></li>
<li><a href="#a-general-method-for-avoiding-overfitting-1"><span class="toc-section-number">19.5.9</span> A General Method for Avoiding Overfitting</a></li>
<li><a href="#avoiding-overfitting-for-parameter-optimization"><span class="toc-section-number">19.5.10</span> Avoiding Overfitting for Parameter Optimization</a></li>
</ul></li>
<li><a href="#ds4bs-similarity"><span class="toc-section-number">19.6</span> Ch 6.: Similarity, Neighbors, and Clusters</a>
<ul>
<li><a href="#similarity-and-distance"><span class="toc-section-number">19.6.1</span> Similarity and Distance</a></li>
<li><a href="#similarity-and-distance-1"><span class="toc-section-number">19.6.2</span> Similarity and Distance</a></li>
<li><a href="#example-whiskey-analytics"><span class="toc-section-number">19.6.3</span> Example: Whiskey Analytics</a></li>
<li><a href="#nearest-neighbors-for-predictive-modeling"><span class="toc-section-number">19.6.4</span> Nearest Neighbors for Predictive Modeling</a></li>
<li><a href="#how-many-neighbors-and-how-much-influence"><span class="toc-section-number">19.6.5</span> How Many Neighbors and How Much Influence?</a></li>
<li><a href="#geometric-interpretation-overfitting-and-complexity-control"><span class="toc-section-number">19.6.6</span> Geometric Interpretation, Overfitting, and Complexity Control</a></li>
<li><a href="#issues-with-nearest-neighbor-methods"><span class="toc-section-number">19.6.7</span> Issues with Nearest-Neighbor Methods</a></li>
<li><a href="#other-distance-functions"><span class="toc-section-number">19.6.8</span> Other Distance Functions</a></li>
<li><a href="#stepping-back-solving-a-business-problem-versus-data-exploration"><span class="toc-section-number">19.6.9</span> Stepping Back: Solving a Business Problem Versus Data Exploration</a></li>
<li><a href="#summary-2"><span class="toc-section-number">19.6.10</span> Summary</a></li>
</ul></li>
<li><a href="#ds4bs-decision-thinking"><span class="toc-section-number">19.7</span> Ch. 7. Decision Analytic Thinking I: What Is a Good Model?</a>
<ul>
<li><a href="#evaluating-classifier"><span class="toc-section-number">19.7.1</span> Evaluating Classifier</a></li>
<li><a href="#the-confusion-matrix"><span class="toc-section-number">19.7.2</span> The Confusion Matrix</a></li>
<li><a href="#problems-with-unbalanced-classes"><span class="toc-section-number">19.7.3</span> Problems with Unbalanced Classes</a></li>
<li><a href="#generalizing-beyond-classification"><span class="toc-section-number">19.7.4</span> Generalizing Beyond Classification</a></li>
<li><a href="#a-key-analytical-framework-expected-value"><span class="toc-section-number">19.7.5</span> A Key Analytical Framework: Expected Value</a></li>
<li><a href="#using-expected-value-to-frame-classifier-use"><span class="toc-section-number">19.7.6</span> Using Expected Value to Frame Classifier Use</a></li>
<li><a href="#using-expected-value-to-frame-classifier-evaluation"><span class="toc-section-number">19.7.7</span> Using Expected Value to Frame Classifier Evaluation</a></li>
<li><a href="#evaluation-baseline-performance-and-implications-for-investments-in-data"><span class="toc-section-number">19.7.8</span> Evaluation, Baseline Performance, and Implications for Investments in Data</a></li>
<li><a href="#summary-3"><span class="toc-section-number">19.7.9</span> Summary</a></li>
<li><a href="#ranking-instead-of-classifying"><span class="toc-section-number">19.7.10</span> Ranking Instead of Classifying</a></li>
<li><a href="#profit-curves"><span class="toc-section-number">19.7.11</span> Profit Curves</a></li>
</ul></li>
<li><a href="#ds4bs-contents"><span class="toc-section-number">19.8</span> Contents and consideration</a></li>
</ul></li>
<li><a href="#paleo-example">Meta-analysis arbitrary example: the ‘Paleo diet’</a>
<ul>
<li><a href="#conceptual"><span class="toc-section-number">19.9</span> Conceptual: Thoughts on nutritional studies and meta-analysis issues</a>
<ul>
<li><a href="#compliance"><span class="toc-section-number">19.9.1</span> Limited compliance; ‘what are we aiming to measure and why?’</a></li>
<li><a href="#control-group-what-is-being-measured"><span class="toc-section-number">19.9.2</span> Control group: what is being measured?</a></li>
<li><a href="#what-is-being-tested-and-how-broadly-should-we-interpret-the-results"><span class="toc-section-number">19.9.3</span> What is being tested and how broadly should we interpret the results?</a></li>
</ul></li>
<li><a href="#manheimer"><span class="toc-section-number">19.10</span> Manheimer et al</a>
<ul>
<li><a href="#strengths-and-limitations"><span class="toc-section-number">19.10.1</span> Strengths and limitations</a></li>
<li><a href="#overall-results-interpretation-consideration-of-evidence-presented-in-manheimerpaleolithicnutritionmetabolic2015"><span class="toc-section-number">19.10.2</span> Overall results, interpretation, consideration of evidence presented in <span class="citation"><span>Manheimer et al.</span> (<span>2015</span>)</span></a></li>
<li><a href="#my-rough-conclusions-from-manheimer-et-al"><span class="toc-section-number">19.10.3</span> My rough conclusions from Manheimer et al</a></li>
<li><a href="#critiques"><span class="toc-section-number">19.10.4</span> External critiques and evaluations of Manheimer et al, (esp Fenton) authors’ response</a></li>
</ul></li>
<li><a href="#other-meta-analyses-and-consideration-of-the-paleo-diet"><span class="toc-section-number">19.11</span> Other meta-analyses and consideration of the Paleo diet</a>
<ul>
<li><a href="#process-of-finding-relevant-work-informal"><span class="toc-section-number">19.11.1</span> Process of finding relevant work (informal)</a></li>
</ul></li>
<li><a href="#boers"><span class="toc-section-number">19.12</span> Focus: Boers et al</a></li>
<li><a href="#overall-analysis"><span class="toc-section-number">19.13</span> Overall analysis</a>
<ul>
<li><a href="#limitations-p"><span class="toc-section-number">19.13.1</span> Limitations and uncertainties to my own analysis; proposed future steps</a></li>
</ul></li>
</ul></li>
<li><a href="#data-sci"><span class="toc-section-number">20</span> Getting, cleaning and using data</a>
<ul>
<li><a href="#data-whatwhywherehow"><span class="toc-section-number">20.1</span> Data: What/why/where/how</a></li>
<li><a href="#organizing-a-project"><span class="toc-section-number">20.2</span> Organizing a project</a></li>
<li><a href="#dynamic-documents-esp-rmdbookdown"><span class="toc-section-number">20.3</span> Dynamic documents (esp Rmd/bookdown)</a>
<ul>
<li><a href="#managing-referencescitations"><span class="toc-section-number">20.3.1</span> Managing references/citations</a></li>
<li><a href="#an-example-of-dynamic-code"><span class="toc-section-number">20.3.2</span> An example of dynamic code</a></li>
</ul></li>
<li><a href="#project-management-tools-esp.-gitgithub"><span class="toc-section-number">20.4</span> Project management tools, esp. Git/Github</a></li>
<li><a href="#good-coding-practices"><span class="toc-section-number">20.5</span> Good coding practices</a>
<ul>
<li><a href="#new-tools-and-approaches-to-data-esp-tidyverse"><span class="toc-section-number">20.5.1</span> New tools and approaches to data (esp ‘tidyverse’)</a></li>
<li><a href="#style-and-consistency"><span class="toc-section-number">20.5.2</span> Style and consistency</a></li>
<li><a href="#using-functions-variable-lists-etc.-for-clean-concise-readable-code"><span class="toc-section-number">20.5.3</span> Using functions, variable lists, etc., for clean, concise, readable code</a></li>
<li><a href="#mapping-over-lists-to-produce-results"><span class="toc-section-number">20.5.4</span> Mapping over lists to produce results</a></li>
<li><a href="#building-results-based-on-lists-of-filters-of-the-data-set"><span class="toc-section-number">20.5.5</span> Building results based on ‘lists of filters’ of the data set</a></li>
<li><a href="#coding-style-and-indenting-in-stata-one-approach"><span class="toc-section-number">20.5.6</span> Coding style and indenting in Stata (one approach)</a></li>
</ul></li>
<li><a href="#additional-tips-integrate"><span class="toc-section-number">20.6</span> Additional tips (integrate)</a>
<ul>
<li><a href="#some-key-points-from-r-for-data-science-see-my-hypothesis-notes">Some key points from <span>R for data science</span> (see my hypothesis notes)</a></li>
<li><a href="#from-a-named-list"><span class="toc-section-number">20.6.1</span> From a named list</a></li>
<li><a href="#list-to-vector"><span class="toc-section-number">20.6.2</span> List to vector</a></li>
<li><a href="#unnesting"><span class="toc-section-number">20.6.3</span> Unnesting</a></li>
<li><a href="#section"><span class="toc-section-number">20.6.4</span> </a></li>
</ul></li>
<li><a href="#making-tidy-data-with-broom"><span class="toc-section-number">20.7</span> Making tidy data with broom</a></li>
</ul></li>
<li><a href="#references"><span class="toc-section-number">21</span> List of references</a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics, econometrics, experiment and survey methods, data science: Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<!--
base file created from

`pandoc -f docx -t gfm -o writing_econ_gfm.md "bookoutline3-cutting examples down-cutnamesd.docx" `

and similar from


`pandoc -f docx -t gfm -o writing_econ_gfm1.md "Adapting back for BOOK --Ec831 outline-fillingindetails_forslides_edMiriam-conflict.docx"`

replacements needed:

- "\[ \]" surrounds math -- square brackets do not need 'escape' in main text
- colors need adjusting to 'format_with_col'

-->
<p>Try downloading and accessing some functions here:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>   try_download <span class="ot">&lt;-</span> <span class="cf">function</span>(url, path) {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>      new_path <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;[.]&quot;</span>, <span class="st">&quot;X.&quot;</span>, path)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">tryCatch</span>({</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="fu">download.file</span>(<span class="at">url =</span> url,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">destfile =</span> new_path)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>      }, <span class="at">error =</span> <span class="cf">function</span>(e) {</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="fu">print</span>(<span class="st">&quot;You are not online, so we can&#39;t download&quot;</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>      })</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">tryCatch</span>(</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">file.rename</span>(new_path, path</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(here)</span></code></pre></div>
<pre><code>## here() starts at /Users/yosemite/githubs/metrics_discussion_work</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>    here <span class="ot">&lt;-</span> here<span class="sc">::</span><span class="fu">here</span>()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">try_download</span>(<span class="st">&quot;https://raw.githubusercontent.com/daaronr/dr-rstuff/master/functions/project_setup.R&quot;</span>, here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;code&quot;</span>,<span class="st">&quot;project_setup.R&quot;</span>))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;code&quot;</span>, <span class="st">&quot;project_setup.R&quot;</span>))</span></code></pre></div>
<pre><code>## Registered S3 method overwritten by &#39;pryr&#39;:
##   method      from
##   print.bytes Rcpp</code></pre>
<pre><code>## Warning in p_load(arm, arsenal, bettertrace, blockTools, broom, car, citr, : Failed to install/load:
## bettertrace, citr, ggplot</code></pre>
<p>#Basic options used across files and shortcut functions, e.g., ‘pp()’ for print
#functions grabbed from web and created by us for analysis/output</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;bookdown&quot;</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tufte&quot;</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># or the development version</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;rstudio/bookdown&quot;)</span></span></code></pre></div>
<p><strong>Pulling in key files from other repos; don’t edit them here</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">try_download</span>(<span class="st">&quot;https://raw.githubusercontent.com/daaronr/data_acad_materials/main/other_content_notes/ds_for_bsns_notes.Rmd?token=AB6ZCMECAFDEIYJ72VEGU4K74YGOE&quot;</span>, here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;data_sci&quot;</span>,<span class="st">&quot;ds_for_bsns_notes_remote.Rmd&quot;</span>))</span></code></pre></div>
<pre><code>## [1] &quot;You are not online, so we can&#39;t download&quot;</code></pre>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">#possibly move these to a separate file</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#multi-output text color</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">#https://dr-harper.github.io/rmarkdown-cookbook/changing-font-colour.html#multi-output-text-colour</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">#We can then use the code as an inline R expression format_with_col(&quot;my text&quot;, &quot;red&quot;)</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>format_with_col <span class="ot">=</span> <span class="cf">function</span>(x, color){</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(knitr<span class="sc">::</span><span class="fu">is_latex_output</span>())</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(<span class="st">&quot;</span><span class="sc">\\</span><span class="st">textcolor{&quot;</span>,color,<span class="st">&quot;}{&quot;</span>,x,<span class="st">&quot;}&quot;</span>,<span class="at">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span>(knitr<span class="sc">::</span><span class="fu">is_html_output</span>())</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(<span class="st">&quot;&lt;font color=&#39;&quot;</span>,color,<span class="st">&quot;&#39;&gt;&quot;</span>,x,<span class="st">&quot;&lt;/font&gt;&quot;</span>,<span class="at">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    x</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<!-- BUILDING IT? SERVING IT? 
bookdown::serve_book(dir = ".", output_dir = "_book", preview = TRUE, 
           in_session = TRUE, quiet = FALSE)
-->
<!---
Can define text blocks here, refer to them again and again if desired
-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <html> -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLKFNFTGXX"></script> -->
<!-- <script> -->
<!--   window.dataLayer = window.dataLayer || []; -->
<!--   function gtag(){dataLayer.push(arguments);} -->
<!--   gtag('js', new Date()); -->
<!--   gtag('config', 'G-QLKFNFTGXX'); -->
<!-- </script> -->
<!-- </html> -->
<!--chapter:end:index.Rmd-->
<div id="introduction" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>My goal in putting this resource is to focus on the practical tools I use and the challenges I (<a href="https://daaronr.github.io/markdown-cv/">David Reinstein</a>) face. But I am open to collaboration with others on this. (Other contributors so far include Gerhard Riener, Oska Fentem, and Scott Dickerson.)</p>
<p><br />
</p>
<p><em>My focus:</em> Microeconomics, behavioral economics, focus on charitable giving and ‘returns to education’ type of straightforward problems. (Minimal focus on structural approaches.)</p>
<p>What I care about: Where we can <em>add value</em> to real econometric (and statistical, experimental, survey design, and data science) practice?</p>
<p><br />
</p>
<p>The data I focus on:</p>
<ul>
<li><p>Observational (esp. web-scraped and API data and national surveys/admin data)</p></li>
<li><p>Experimental: esp. with multiple crossed arms, and where the ‘cleanest design’ may not be possible</p></li>
</ul>
<p><br />
</p>
<p>I will assume familiarity with most basic statistical concepts like ‘bias,’ ‘consistency,’ and ‘null hypothesis testing.’ However, I will focus on some concepts that seem to often be misunderstood and mis-applied, and I will give and link definitions as time permits.</p>

<div class="note">
<p>If you are involved with this project, you can find a brief guide (somewhat WIP) on how to add content (HERE)[<a href="https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html" class="uri">https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html</a>]. This is from a different project but the setup is basically the same.</p>
</div>
<div id="basic-statistical-approaches-and-frameworks" class="section level2 unnumbered">
<h2 class="unnumbered"><a href="#conceptual">Basic statistical approaches and frameworks</a></h2>
<ul>
<li>Bayesian vs. frequentist approaches</li>
</ul>
<div class="marginnote">
<p>Folder: bayesian
Notes: <a href="bayesian/bayes_notes.Rmd">bayes_notes</a></p>
</div>
<ul>
<li>Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model</li>
</ul>
<!-- #### DAGs and Potential outcomes -->
<ul>
<li>Theory, restrictions, and ‘structural vs reduced form’</li>
</ul>
</div>
<div id="regression-and-control-approaches-robustness" class="section level2 unnumbered">
<h2 class="unnumbered"><a href="#reg_control">Regression and control approaches, robustness</a></h2>
<!--

[Getting, cleaning and using data; project management and coding](#data-sci) {-}

<div class="marginnote">
This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.
</div>
 
- Data: What/why/where/how
- Organizing a project
- Dynamic documents (esp Rmd/bookdown)
- Good coding practices
- Data sharing and integrity

-->
<!--- #### New tools and approaches to data (esp 'tidyverse')

#### Style and consistency

Indenting, snake-case, etc

#### Using functions, variable lists, etc., for clean, concise, readable code

-->
<!--
## Basic regression and statistical inference: Common mistakes and issues {-}

Simple identification issues: "Bad control" ("colliders"); Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

odMeling choices: Choices of lhs and rhs variables, choice of control variables and interactions, which outcome variable/variables; i

Functional form: Logs and exponentials, Nonlinear modeling (and interpreting coefficients), 'Testing for nonlinear terms'

Missing data


OLS and heterogeneity:  OLS does *not* identify the ATE. See: ['your go-to regression specification is biased...'](http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT). Modeling heterogeneity: the limits of Quantile regression.

Interpretation and inference: 

- "Null effects" (or move to previous 'conceptual' chapter), confidence intervals and Bayesian credible intervals
-  Interaction terms and pitfalls
- Comparing relative parameters
-  Multiple hypothesis testing (MHT)
- 'Moderators', confusion with nonlinearity
- Choice of test statistics (including nonparametric)
-->
<!-- (Or get to this in the experimetrics section) 

How to display and write about regression results and tests

Bayesian interpretations of results (see also '[Bayesian Approaches](#bayes)')

-->
<!-- ## LDV and discrete choice modeling {-}

## Robustness and diagnostics, with integrity {-}

- (How) can diagnostic tests make sense? Where is the burden of proof?
- Estimating standard errors
-  Sensitivity analysis: Interactive presentation

## [Control strategies and prediction; Machine Learning approaches](#control-ml) {-}

-  Machine Learning (statistical learning): Lasso, Ridge, and more
-  Limitations to inference from learning approaches

-->
</div>
<div id="causal-inference-through-observation-caus_inf_obs" class="section level2 unnumbered">
<h2 class="unnumbered">Causal inference through observation{-#caus_inf_obs}</h2>
<!-- 
##  [Causal inference: IV (instrumental variables) and its limitations]  {-#iv_limitations}

- Instrument validity
- Exogeneity vs. exclusion
- Very hard to 'powerfully test'
-  Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

-  Weak instruments, other issues
- Reference to the use of IV in experiments/mediation

## [Causal inference: Other paths to observational identification](#other_paths) {-}

- Fixed effects and differencing
-  DiD
-  RD
-  Time-series-ish panel approaches to micro


-->
</div>
<div id="causal-paths-and-levels-of-aggregation" class="section level2 unnumbered">
<h2 class="unnumbered">Causal paths and levels of aggregation</h2>
<!-- 
## Causal paths: [Mediation modeling and its massive limitations](#mediators) {-}

An applied review

## Causal paths: [selection, corners, hurdles, and 'conditional on' estimates](#selection_cop) {-}

'Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection

\

Bounding approaches (Lee, Manski, etc). See [Notes on Lee bounds](#notes_lee)
-->
</div>
<div id="experiments-and-surveys-design-and-analysis" class="section level2 unnumbered">
<h2 class="unnumbered">Experiments and surveys: design and analysis</h2>
<!-- 
## [Survey design and implementation; analysis of survey data](-#surveys)

## [(Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters](#why_experiment_etc) {-}

-->
<!-- experiments_and_study_design/why_experiment_design.Rmd 
Why run an experiment or study?

- Sugden and Sitzia critique here, give more motivation -->
<!--
Causal channels and identification, ruling out alternative hypotheses, etc

Types of experiments, 'demand effects' and more artifacts of artifical setups

Generalizability (and heterogeneity)


## (Experimental) Study design: Background and quantitative issues {-}
-->
<!-- experiments_and_study_design/quant_design_power.Rmd -->
<!-- 
- Pre-registration and Pre-analysis plans
- The hazards of specification-searching
- Sequential and adaptive designs
- Efficient assignment of treatments

(Links back to [power analyses](#power))



## (Experimental) Study design: (Ex-ante) Power calculations {-}

- What sort of 'power calculations' make sense, and what is the point?
- The 'harm to science' from running underpowered studies
- Power calculations without real data
- Power calculations using prior data

## ['Experimetrics' and measurement of treatment effects from RCTs](-#experimetrics_te) {-}
-->
<!-- Which error structure? Random effects?

- Randomization inference?

- Parametric and nonparametric tests of simple hypotheses

- Adjustments for exogenous (but non-random) treatment assignment


-  IV in an experimental context to get at 'mediators'?

- Heterogeneity in an experimental context

-->
</div>
</div>
<div id="other-approaches-techniques-and-applications" class="section level1 unnumbered">
<h1 class="unnumbered"><a href="#other_approaches">Other approaches, techniques, and applications</a></h1>
<!--
## [Making inferences from previous work; Meta-analysis, combining studies](-#metaanalysis)

- Publication bias 

- Combining a few (your own) studies/estimates

- Full meta-analyses

- Models to address publication biases

## The Bayesian approach {-}

-->
<div id="some-key-resources-and-references" class="section level2 unnumbered">
<h2 class="unnumbered">Some key resources and references</h2>
<p><span class="citation">(<a href="#ref-angrist2008mostly" role="doc-biblioref">Joshua D. Angrist and Pischke 2008</a>)</span></p>
<p>‘The Mixtape’ (Cunningham)</p>
<p><span class="citation">(<a href="#ref-kennedyGuideEconometrics2003" role="doc-biblioref">P. Kennedy 2003</a>)</span></p>
<p>Tibshirani, Robert. n.d. “Statistical Learning with Sparsity the Lasso and Generalizations.”</p>
<!--[Tibshirani] -->
<p>OSF guides</p>
<p>Christensen ea “Transparent and Reproducable Social Science Research”</p>
<p><span class="citation">(<a href="#ref-wooldridgeEconometricAnalysisCross2002" role="doc-biblioref">Jeffrey M. Wooldridge 2002</a>; <a href="#ref-wooldridgeIntroductoryEconometricsModern2008" role="doc-biblioref">J. M. Wooldridge 2008</a>)</span></p>
<p>Gentzkow and Shapiro (2013) <!-- Add reference --></p>
<p>An Introduction to Statistical Learning with Applications in R</p>
<p>R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org</p>
<p>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</p>
<div class="marginnote">
<p>Consider:</p>
<p>Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017</p>
<p>Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction . Cambridge University Press, 2015</p>
<p>Judea Pearl</p>
<p>Imbens: Potential Outcomes versus DAGs</p>
</div>
<!--chapter:end:metrics_outline.Rmd-->
</div>
</div>
<div id="conceptual" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> <strong>BASIC STATISTICAL APPROACHES AND FRAMEWORKS</strong></h1>
<p>The first theme in this book: <em>what is statistics and what is trying to do?</em></p>
<p>After a conceptual discussion, we go over some</p>
<p><em>Conceptual: approaches to statistics/inference and causality</em></p>
<div id="learning-and-optimization-as-an-alternative-to-statistical-inference" class="section level2" number="2.1">
<h2 number="2.1"><span class="header-section-number">2.1</span> ‘Learning and optimization’ as an alternative to statistical inference</h2>
<p>In many real-world cases we use data and ‘statistics’ <em>not</em> to learn about the world for its own sake, but simply in order to make the ‘best’ decision.</p>
<p>These ‘decision optimization’ cases are referred to as ‘reinforcement learning’ (I think). In collecting data and planning our analysis and experimental interventions (if any), we need not be directly concerned with ‘statistical power’ nor with hypothesis testing for its own sake. We might prefer to learn <em>a little bit</em> about each of many possible options, with a very low chance of finding a ‘statistically significant result,’ over learning a lot about one or two particular options. See the ‘movie titles’ example discussed in section @ref{#lift-test}.</p>
<!-- also see https://docs.google.com/document/d/1v_LflrtZP8ZEP8Xk_L8pKqHazmKcqEn6j4MYrTKrir4/edit -->
</div>
<div id="statistical-inference" class="section level2" number="2.2">
<h2 number="2.2"><span class="header-section-number">2.2</span> Statistical inference</h2>
</div>
<div id="bayesian-vs.-frequentist-approaches" class="section level2" number="2.3">
<h2 number="2.3"><span class="header-section-number">2.3</span> Bayesian vs. frequentist approaches</h2>
<!-- 
Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)
-->
<div id="interpretation-of-frequentist-cis-aside" class="section level3" number="2.3.1">
<h3 number="2.3.1"><span class="header-section-number">2.3.1</span> Interpretation of frequentist CI’s (aside)</h3>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
The fact that 95% of all (correct) CIs contain the true value does not mean that 95% of those that exclude zero do so correctly. You could have (say) 59% correct coverage for 10% excluding zero and 99% for 90% including zero.
</p>
— Stephen John Senn (@stephensenn) <a href="https://twitter.com/stephensenn/status/1219680319114227714?ref_src=twsrc%5Etfw">January 21, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</div>
<div id="causal-vs.-descriptive-treatment-effects-and-the-potential-outcomes-causal-model" class="section level2" number="2.4">
<h2 number="2.4"><span class="header-section-number">2.4</span> Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model</h2>
<div id="dags-and-potential-outcomes" class="section level3" number="2.4.1">
<h3 number="2.4.1"><span class="header-section-number">2.4.1</span> DAGs and Potential outcomes</h3>
</div>
</div>
<div id="theory-restrictions-and-structural-vs-reduced-form" class="section level2" number="2.5">
<h2 number="2.5"><span class="header-section-number">2.5</span> Theory, restrictions, and ‘structural vs reduced form’</h2>
</div>
<div id="hypothesis-testing" class="section level2" number="2.6">
<h2 number="2.6"><span class="header-section-number">2.6</span> ‘Hypothesis testing’</h2>
<div id="mcelreaths-critique" class="section level3" number="2.6.1">
<h3 number="2.6.1"><span class="header-section-number">2.6.1</span> McElreath’s critique</h3>
</div>
<div id="bayesian-vs.-frequentist-hypothesis-testing" class="section level3" number="2.6.2">
<h3 number="2.6.2"><span class="header-section-number">2.6.2</span> Bayesian vs. frequentist hypothesis ‘testing’</h3>
</div>
<div id="individual-vs.-joint-hypothesis-testing-what-does-it-mean" class="section level3" number="2.6.3">
<h3 number="2.6.3"><span class="header-section-number">2.6.3</span> Individual vs. joint hypothesis testing: what does it mean?</h3>
</div>
<div id="other-issues" class="section level3" number="2.6.4">
<h3 number="2.6.4"><span class="header-section-number">2.6.4</span> Other issues</h3>
<p>(Mention, link to later discussion: issues of the overall false error rate/coverage with MHT)</p>
<!--chapter:end:conceptual/conceptual.Rmd-->
</div>
</div>
</div>
<div id="reg_control" class="section level1 unnumbered">
<h1 class="unnumbered"><strong>REGRESSION AND CONTROL APPROACHES, ROBUSTNESS</strong></h1>
</div>
<div id="reg-follies" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> Basic statistical inference and regressions: Common mistakes and issues</h1>
<div id="basic-regression-and-statistical-inference-common-mistakes-and-issues-briefly-listed" class="section level2" number="3.1">
<h2 number="3.1"><span class="header-section-number">3.1</span> Basic regression and statistical inference: Common mistakes and issues briefly listed</h2>
<p>Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Identification</p>
<p>Random effects estimators show a lack of robustness Specification Clustering SE is more standard practice</p>
<p>OLS/IV estimators not ‘mean effect’ in presence of heterogeneity</p>
<p>Power calculations/underpowered</p>
<p>Selection bias due to attrition</p>
<p>Selection bias due to missing variables – impute these as a solution</p>
<p>Signs of p-hacking and specification-hunting</p>
<p>Weak diagnostic/identification tests</p>
<p>Dropping zeroes in a “loglinear” model is problematic</p>
<p>Random effects estimators show a lack of robustness</p>
<p>Dropping zeroes in a “loglinear” model is problematic</p>
<p>Random effects estimators show a lack of robustness</p>
<p>-(Some notes on multi-level modeling) and RE linked <a href="https://twitter.com/DavidPoe223/status/1239447381172727809">in this Twitter thread</a></p>
<p>With heterogeneity the simple OLS estimator is not the ‘mean effect’</p>
<p>P_augmented may <em>overstate</em> type-1 error rate</p>
<p>Impact size from regression of “log 1+gift amount”</p>
<p>Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’</p>
<p>Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)</p>
<p>Weak IV bias</p>
<p>Bias from selecting instruments and estimating using the same data</p>
<p>Failure to adjust for multiple-hypothesis testing</p>
<div id="bad-control" class="section level3" number="3.1.1">
<h3 number="3.1.1"><span class="header-section-number">3.1.1</span> Bad control</h3>
<p>From MHE:</p>
<blockquote>
<p>some variables are bad controls and should not be included in a regression model even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notational experiment at hand. That is, bad controls might just as well be dependent variables too."</p>
</blockquote>
<p>– They could also be interpreted as endogenous variables.</p>
<p>Example of looking at a regression of wages in schooling, controlling for college degree completion:</p>
<blockquote>
<p>Once we acknowledge the fact that college affects occupation, comparison of wages by college degree status within occupation are no longer apples to apples, even if college degree completion is randomly assigned."</p>
</blockquote>
<p>– The question here was whether to control for the category of occupation, not the college degree.</p>
<blockquote>
<p>It is also incorrect to say that the conditional comparison captures the part of the effect of college that is ‘not explained by occupation’ … so we would do better to control only for variables that are not themselves caused by education."</p>
</blockquote>
</div>
<div id="bad-control-colliders" class="section level3" number="3.1.2">
<h3 number="3.1.2"><span class="header-section-number">3.1.2</span> “Bad control” (“colliders”)</h3>
<p>Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)</p>
</div>
<div id="choices-of-lhs-and-rhs-variables" class="section level3" number="3.1.3">
<h3 number="3.1.3"><span class="header-section-number">3.1.3</span> Choices of lhs and rhs variables</h3>
<ul>
<li>Missing data</li>
<li>Choice of control variables and interactions</li>
<li>Which outcome variable/variables</li>
</ul>
</div>
<div id="functional-form" class="section level3" number="3.1.4">
<h3 number="3.1.4"><span class="header-section-number">3.1.4</span> Functional form</h3>
<ul>
<li>Logs and exponentials</li>
<li>Nonlinear modeling (and interpreting coefficients)</li>
</ul>
<div id="testing-for-nonlinear-terms" class="section level4" number="3.1.4.1">
<h4 number="3.1.4.1"><span class="header-section-number">3.1.4.1</span> ‘Testing for nonlinear terms’</h4>
<p>Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18</p>
<p><a href="http://datacolada.org/62" class="uri">http://datacolada.org/62</a></p>
</div>
</div>
<div id="ols-and-heterogeneity" class="section level3" number="3.1.5">
<h3 number="3.1.5"><span class="header-section-number">3.1.5</span> OLS and heterogeneity</h3>
<div id="ols-does-not-identify-the-ate" class="section level4 unnumbered">
<h4 class="unnumbered">OLS does <em>not</em> identify the ATE</h4>
<p>In general, with heterogeneity, OLS does <em>not</em> identify the ATE. It weights observations from different parts of the sample differently. Parts with greater residual variation in the treatment (outcome) variable are more (less) heavily weighted.</p>
<div class="marginnote">
<p>E.g., if the treatment is binary, the estimator will most heavily weight those parts of the sample where the probability of treatment is closest to 1/2.</p>
</div>
<p>The formula is …</p>
<p><br />
</p>

<div class="note">
<p><strong>Some intuition</strong></p>
<p>Why is this the case? The OLS type estimators we are taught in Econometrics are ‘BLUE’ under the assumption of a <em>single homogenous ‘effect’</em> (the ‘slope’… although the discussion itself is often agnostic as to whether this represents a causal effect).</p>
<p><br />
</p>
<p>It is ‘best’ in a minimizing MSE sense under certain assumptions; in particular, we must also know the true functional form and the set of variables to be included. See ‘overfitting’ issues.</p>
<p>In order to have the estimate of the true slope that minimizes the squared errors, OLS (and related estimators like FGLS; as well as 2SLS in a more complicated sense) weights some observations more than others. The ‘influence’ of an observation on the estimated slope depends on the nature of the variation in the dependent and independent variables in the region that observation is drawn from. Think of drawing a line through a set of points that were drawn with some noise from the true distribution. If you drew it based on a bunch of points (from a region where) the treatment varies very little and the outcomes have a lot of noise, the line you draw will be very sensitive to the latter noise and thus unreliable. So, would optimally ‘down-weight’ these observations in drawing the line.</p>
<p><br />
</p>
<p>However, if the <em>actual</em> slope varies by region, this also means you are under-representing certain regions, and thus getting a biased estimate of the average slope.</p>
</div>
<p>How can we deal with this? If we think that the treatment effect varies with <em>observable</em> variables, we could include ‘interactions’; essentially making separate estimates of the slope for each share of the population (but potentially allowing other control variables to have a homogenous effects, and pooled or clustered estimation of underlying variance.)</p>
<div class="marginnote">
<p>…Although we may want to consider both overfitting here and the idea that there may be <em>some</em> shared component, so the fully-interacted model may be sub-optimal. See mixed modeling (?)</p>
</div>
<p><br />
</p>
<p>However, this does not tell us how to recover the <em>average</em> of these slopes (approximately, the ATE). Should we weight each of the slopes by the share of the population that this group represents?</p>
<p>Mechanically, the standard way of estimating and representing these interactions and economics has been with simple dummies (0,1) for each compared group. This yields a ‘base group’ (e.g., males aged 35-60) – this obviously does not recover the average slope– as well as the ‘adjustment’ coefficients.</p>
<p><br />
</p>
<p>Another way of expressing interactions, particularly helpful with multi-level interactions is called ‘effect coding’: each group is coded as a ‘difference from 0’ (e.g,. with bunarybinary gender data males are -1/2 and females +1/2), before doing the interactions. This could allow for a more straightforward interpretation: at each level, the uninteracted term represents the average treatment effects, and the interacted terms represent adjustments relative to this average. <em>But under which conditions is this in fact the case?</em></p>
<div class="marginnote">
<p><a href="https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis/#:~:text=Unlike%20dummy%20coding%2C%20effect%20coding,variable%20must%20sum%20to%20zero.">There are various forms of ‘effect coding’ of which ‘effect contrast coding’ is one.</a></p>
</div>
<p>[insert here].</p>
<p><a href="http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT">WB blog - your-go-to regression-specification is -biased-here-s-simple-way-fix-it</a></p>
<p>A key paper: <a href="http://www.jcsuarez.com/Files/Suarez_Serrato-BFE.pdf" class="uri">http://www.jcsuarez.com/Files/Suarez_Serrato-BFE.pdf</a></p>
<blockquote>
<p>In particular, we compare treatment effect estimates using a fixed effects estimator (FE) to the average treatment effect (ATE) by replicating eight influential papers from the American Economic Review published between 2004 and 2009.1 Using these examples, we consider a randomized experiment in Section 1 as a case study and, in Section 3, we show generally that heterogeneous treatment effects are common and that the FE and ATE are often different in statistically and economically significant degrees. In all but one paper, there is at least one statistically significant source of treatment effect heterogeneity. In five papers, this heterogeneity induces the ATE to be statistically different from the FE estimate at the 5% level (7 of 8 are statistically different at the 10% level). Five of these differences are economically significant, which we define as an absolute difference exceeding 10%. Based upon these results, we conclude that methods that consistently estimate the ATE offer more interpretable results than standard FE models</p>
</blockquote>
<p>By “FE” here I think they mean group dummies; they are focused on cross-sectional and not panel data!</p>
<blockquote>
<p>While fixed effects permit different mean outcomesamong groups, the estimates of treatment effects are typically required to be the same; in more colloquial terms, the intercepts of the conditional expectation functions may differ, but not the slopes</p>
</blockquote>
<p>DGP</p>
<p><span class="math display">\[y_i = x_i \beta_{g(i)} + \mathbf{z_i}&#39; \gamma + \epsilon_i\]</span></p>
<blockquote>
<p>where $y_i is the outcome for observation i among N [N what?],
<span class="math inline">\(x_i\)</span> is treatment or another variable of interest, and <span class="math inline">\(z_i\)</span> contains control variables, including group-specific fixed effects.</p>
</blockquote>
<blockquote>
<p>The treatment effects aregroup-specific for each of the <span class="math inline">\(g=1,...,G\)</span> groups, where group membership is known for each observation.</p>
</blockquote>
<p>Defining ATE</p>
<p><span class="math display">\[\beta^{ATE}=\sum_g \pi_g \beta_g \]</span></p>
<p>where the <span class="math inline">\(\pi\)</span> terms are population frequencies</p>
<p><br />
</p>
<p>The use of interaction terms is delicate…</p>
<!--
On the book, I saw the section on the problems of using OLS as an estimator of FE without fully interacting the variables (4.2.4, this blog post), but d
By idn’t really understand the issue/intuition behind the problem/solution – I would be keen to chat about what it means.. and if I then get it, would more than happily contribute a lay summary for the book. Also there are three papers by Ferraro (links below) that I think you might find interesting/offer well explained insights into difficulties in how to do empirical econ/problems within it for the Book.

Plus there is this paper – https://www.nber.org/papers/w25636 which applies the changes-in-changes method of Athey and Imbens 2006 a method which (proponents claims) is able to get at heterogeneous treatment effects better than simple DiD by bin/something similar.
-->
<!-- #TODO: recover conversations with Winston Lin and write these up -->
<ul>
<li>Modeling heterogeneity: the limits of Quantile regression</li>
</ul>
</div>
</div>
<div id="null-effects" class="section level3" number="3.1.6">
<h3 number="3.1.6"><span class="header-section-number">3.1.6</span> “Null effects”</h3>
<p>"While the classical statistical framework is not terribly clear about when one should ‘accept’ a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ‘zero or minimal effect’ one would want to find these closely bounded around zero under a variety of conditions for generalizability.</p>
<p>In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks” <span class="citation"><a href="#ref-harmsMakingNullEffects2018" role="doc-biblioref">Harms and Lakens</a> (<a href="#ref-harmsMakingNullEffects2018" role="doc-biblioref">2018</a>)</span>.</p>
<blockquote>
<p>Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.
(Lakens et al, 2018)</p>
</blockquote>
<div id="confidence-intervals-and-bayesian-credible-intervals" class="section level4" number="3.1.6.1">
<h4 number="3.1.6.1"><span class="header-section-number">3.1.6.1</span> Confidence intervals and Bayesian credible intervals</h4>
</div>
<div id="comparing-relative-parameters" class="section level4" number="3.1.6.2">
<h4 number="3.1.6.2"><span class="header-section-number">3.1.6.2</span> Comparing relative parameters</h4>
<p>E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men.” This doesn’t cut it: we need to see a <em>statistical test</em> for the <em>difference</em> in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).</p>
<p>See <span class="citation">(<a href="#ref-gelmanDifferenceSignificantNot2006" role="doc-biblioref">Gelman and Stern 2006</a>)</span></p>
</div>
</div>
<div id="multi-var-tests" class="section level3" number="3.1.7">
<h3 number="3.1.7"><span class="header-section-number">3.1.7</span> Multivariate tests and ‘tests for non-independence’</h3>
<p>‘Multiple’ tests typically consider</p>
<p>H0: “None of the following coefficients are different from 0”</p>
<p>H1: “At least one of these coefficients are different from 0”</p>
<p>But of course, ‘rejecting H0’ doesn’t tell us <em>which</em> of these coefficients are distinct from 0, nor how different from zero we should expect the average coefficients to be<br />
</p>
<p>Other tests look for ‘associations’ between categorical variables, each potentially involving many categories.</p>
<p>From a discussion on the appropriate adjustment of Chi-Sq tests for categorical variables, and the possible adjustments to this test for <a href="https://experts.nebraska.edu/en/publications/mrcv-a-package-for-analyzing-categorical-variables-with-multiple-">multiple response categorical variables</a></p>
<blockquote>
<p>If I am correct, I don’t see a huge value in doing/adjusting any ‘aggregate tests for association between (e.g.) ethnicity and involve mode.’
I say this because I think most people would have a strong belief that there will be at least some association here overall, even if it is a small one, and even if it isn’t ‘caused by ethnicity,’ whatever that could mean.o</p>
</blockquote>
<p>The general idea that ‘we expect an association in most cases, so finding one is not particularly informative (without further results) … is mentioned <a href="http://sumsar.net/blog/2014/06/bayesian-first-aid-prop-test/">HERE</a> and discussed by Gelman <a href="https://statmodeling.stat.columbia.edu/2011/04/02/so-called_bayes/">here</a> (context of ’Bayesian hypothesis testing’) and <a href="https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/">here</a> (in terms of a new typeology of ‘errors’).</p>
</div>
<div id="mht" class="section level3" number="3.1.8">
<h3 number="3.1.8"><span class="header-section-number">3.1.8</span> Multiple hypothesis testing (MHT)</h3>
<p>A typical conversation (unfold)</p>

<div class="fold">
<p>Namely that when we have multiple hypotheses testing in a paper how do we control for that and what do our p-values really mean. Particulary, there is <a href="https://academic.oup.com/qje/article/134/2/557/5195544">this paper</a> by Alwyn Young which (I think) is basically saying that when we have a load of hypotheses within a study and then one has a significant p value we are quite a lot of the time picking up a false result. Obviously that isn’t very much of a surprise. But I think it also talks somewhat to the Steffano DellaVigna paper that was presented at the VAFE - that the publication bias really creeps in when we are publishing studies that have just one or two p values around 0.05. After all, it is those studies which have a real pay-off from getting that significant result.</p>
</div>
<p><br />
</p>
<p>From List et al (2019), discussing experiments reporting tests of multiple treatments, on multiple subgroups, and considering multiple outcomes:</p>
<blockquote>
<p>it is uncommon for the analyses of these data to account for the multiple hypothesis testing. As a result, the probability of a false rejection may be much higher than desired. To illustrate this point, consider testing <span class="math inline">\(N\)</span> null hypotheses simultaneously. Suppose that for each null hypothesis a p value is available whose distribution is uniform on the unit interval when the corresponding null hypothesis is true. Suppose further that all null hypotheses are true and that the p values are independent. In this case, if we were to test each null hypothesis in the usual way at level <span class="math inline">\(\alpha \in (0, 1)\)</span>, then the probability of one or more false rejections equals <span class="math inline">\(1 − (1 − \alpha)N\)</span>, which may be much greater than <span class="math inline">\(\alpha\)</span> and in fact tends rapidly to one as <span class="math inline">\(N\)</span> increases. For instance, with <span class="math inline">\(\alpha = 0.05\)</span>, it equals <span class="math inline">\(0.226\)</span> when <span class="math inline">\(N = 5\)</span> equals 0.401 when N = 10 and 0.994 when N = 100. In order to control the probability of a false rejection, it is therefore important to account appropriately for multiplicity of null hypotheses being tested.</p>
</blockquote>
<p><br />
</p>
<p>Winston Lin’s response to an email:</p>

<div class="fold">
<blockquote>
<p>On multiple comparisons corrections, there are some thorny philosophical issues and there isn’t one right answer:</p>
</blockquote>
<blockquote>
<p>mixed feelings about the recent popularity of multiplicity corrections in the social sciences. I think there’s a tendency to fetishize p-values and null-hypothesis significance testing, and multiplicity-corrected p-values can be hard to interpret. (In principle, one could also report multiplicity-corrected confidence intervals, but I haven’t seen people doing that.) To me, the real value of p-values and NHST (if there is any) is as a restraining device, and if this restraining device isn’t working because of the multiple comparisons issue, there’s more than one way to address the problem. In medical research, people often pre-specify one or two primary comparisons (which implies a commitment that these are the comparisons that’ll be highlighted in the abstract, and the rest are basically treated as exploratory) without doing multiplicity corrections.</p>
</blockquote>
<blockquote>
<p>On the other hand, maybe you have to do multiplicity corrections because your reviewers want them, or maybe you don’t have one or two primary comparisons between your 7 treatments. When I last followed the multiple comparisons literature (about 4 years ago), my feeling was that if I had to report multiplicity-corrected p-values, I’d use either the Westfall-Young or the Romano-Wolf procedure.</p>
</blockquote>
<p><a href="https://combinatronics.com/egap/methods-guides/master/hte/heteffects.html#multiple-comparisons">There are some explanations and references in section 7 and footnotes 5-12 of this EGAP guide that Al Fang, Don Green, and I worked on</a>
DR: I will try to start a hypothes.is or forum conversation in that Egap guide.</p>
</div>
<p><br />
</p>
<p><strong>Considerations:</strong></p>
<p>Frequentist NHST: Understated type-one error rate overall if no adjustment, overly narrow confidence intervals (coverage)</p>
<ul>
<li><p>“Family-wise error rate” (FWER)</p></li>
<li><p>“False discovery rate” (FDR)</p></li>
</ul>
<p><br />
</p>
<p><strong>Responses:</strong></p>
<p>‘FWER Control methods’</p>
<ul>
<li>Bonferroni correction: divides the threshold p-value by the number of tests (<span class="math inline">\(k\)</span>)
<ul>
<li>Extremely conservative, assumes independence of hypotheses/outcomes</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method#:~:text=The%20Holm%E2%80%93Bonferroni%20method%20is%20%22uniformly%22%20more%20powerful%20than,always%20at%20least%20as%20powerful.&amp;text=Thus%2C%20The%20Hochberg%20procedure%20is,powerful%20than%20the%20Holm%20procedure">Holm-Bonferroni</a>
<ul>
<li>slightly less conservative than Bonferroni – orders p-values, applies Bonferroni to the “most significant” result (smallest p-value), and the correction reduces the denominator by 1 for each ‘remaining result’</li>
</ul></li>
<li>Westfall-Young Step-down:
<ul>
<li>Rank-orders p-values, simulate p-values with random assignment of ‘null treatments’; adjusted p-value share of ‘k-ranked simulated p-value is below k’th rank of original p-value’</li>
<li><a href="https://blogs.worldbank.org/impactevaluations/overview-multiple-hypothesis-testing-commands-stata">“Romano and Wolf note that the Westfall-Young procedure requires an additional assumption of subset pivotality”</a></li>
</ul></li>
<li>List, Shaikh, Xu (2019): “bootstrap-based procedure for testing these null hypotheses simultaneously using experimental data in which simple random sampling is used to assign treatment status to units.”
<div class="marginnote">
<p>(Does it apply to blocked sampling?)</p>
</div></li>
</ul>
<blockquote>
<p>our procedure (1) asymptotically controls the familywise error rate—the probability of one or more false rejections—and (2) is asymptotically balanced in that the the marginal probability of rejecting any true null hypothesis is approximately equal in large samples. Importantly, by incorporating information about dependence ignored in classical multiple testing procedures, such as the Bonferroni (1935) and Holm (1979) corrections, our pro- cedure has much greater ability to detect truly false null hypotheses. In the presence of multiple treatments, we additionally show how to exploit logical restrictions across null hypotheses to further improve power.</p>
</blockquote>
<p><br />
</p>
<p>FDR-Control</p>
<ul>
<li>Benjamini and Hochberg … but this one seems to be conservative in the sense that it implicitly assumes independent hypotheses in making the correction (?)<br />
</li>
</ul>
</div>
<div id="interaction-terms-and-pitfalls" class="section level3" number="3.1.9">
<h3 number="3.1.9"><span class="header-section-number">3.1.9</span> Interaction terms and pitfalls</h3>
<p>See also ‘effect coding’ or ‘contrast coding.’</p>
<div id="moderators-and-their-confusion-with-nonlinearity" class="section level4" number="3.1.9.1">
<h4 number="3.1.9.1"><span class="header-section-number">3.1.9.1</span> ‘Moderators’ and their confusion with nonlinearity</h4>
<p>Moderators: Heterogeneity mixed with nonlinearity/corners</p>
<p>In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in.’ Related to <a href="https://datacolada.org/57" class="uri">https://datacolada.org/57</a> [57] Interactions in Logit Regressions: Why Positive May Mean Negative</p>
</div>
</div>
<div id="choice-of-test-statistics-including-nonparametric" class="section level3" number="3.1.10">
<h3 number="3.1.10"><span class="header-section-number">3.1.10</span> Choice of test statistics (including nonparametric)</h3>
<p>(Or get to this in the experimetrics section)</p>
</div>
<div id="how-to-display-and-write-about-regression-results-and-tests" class="section level3" number="3.1.11">
<h3 number="3.1.11"><span class="header-section-number">3.1.11</span> How to display and write about regression results and tests</h3>
</div>
<div id="bayesian-interpretations-of-results" class="section level3" number="3.1.12">
<h3 number="3.1.12"><span class="header-section-number">3.1.12</span> Bayesian interpretations of results</h3>
</div>
</div>
<div id="aside-effect-and-contrast-coding-of-categorical-variables" class="section level2" number="3.2">
<h2 number="3.2"><span class="header-section-number">3.2</span> Aside: effect and contrast coding of categorical variables</h2>
<p>Suppose we want to do a simple ‘descriptive model of income.’ Suppose we have three groups, North, Central, and South (think US regions).</p>
<p>Comparing otherwise similar groups, auppose average income in the North is 130, Central is 80, and South is 60. Suppose equal group sizes, so mean is 90</p>
<p>There should be a way, in a (linear regression) model, to report coefficients as differences from overall means (in a multivariate context, ‘all else equal’) and get one for each
.
<span class="math inline">\(\beta_{North} = 40\)</span></p>
<p><span class="math inline">\(\beta_{Central} = -10\)</span>
<span class="math inline">\(\beta_{South} = -30\)</span></p>
<p>And skip the intercept, obviously.</p>
<p>This seems most intuitive to me. But I can’t for the life of me figure out how to get this with R’s ‘contrast coding.’ (And also, that seems to mess up the variable names).</p>
<p>Set parameters</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>m_inc <span class="ot">&lt;-</span> <span class="dv">90</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>b_n <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>b_c  <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>b_s <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">30</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>sd_prop <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="co">#sd as share of mean</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>pop_per <span class="ot">&lt;-</span> <span class="dv">1000</span></span></code></pre></div>
<p>Simulated data</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">#later, do this with a list for good code practice</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>n_income <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(pop_per, m_inc <span class="sc">+</span> b_n, (m_inc <span class="sc">+</span> b_n)<span class="sc">*</span>sd_prop)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>c_income <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(pop_per, m_inc <span class="sc">+</span> b_c, (m_inc <span class="sc">+</span> b_s)<span class="sc">*</span>sd_prop)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>s_income <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(pop_per, m_inc <span class="sc">+</span> b_s, (m_inc <span class="sc">+</span> b_s)<span class="sc">*</span>sd_prop)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>noise_var <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(pop_per<span class="sc">*</span><span class="dv">3</span>, <span class="dv">0</span>, (m_inc <span class="sc">+</span> b_s)<span class="sc">*</span>sd_prop)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>i_df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">region =</span> <span class="fu">rep</span>( <span class="fu">c</span>(<span class="st">&quot;n&quot;</span>, <span class="st">&quot;c&quot;</span>, <span class="st">&quot;s&quot;</span>), <span class="fu">c</span>(pop_per, pop_per, pop_per) ),</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">income =</span> <span class="fu">c</span>(n_income, c_income, s_income),</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  noise_var</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">region =</span> <span class="fu">as.factor</span>(region))</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>i_df <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(region) <span class="sc">%&gt;%</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">n =</span> <span class="fu">n</span>() ,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>              <span class="at">mean =</span> <span class="fu">mean</span>(income),</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">sd =</span> <span class="fu">sd</span>(income))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-9">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-9) </caption><col><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">region</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">n</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">mean</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">sd</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1000</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">80.1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">29.4</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">n</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1000</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">131&nbsp;&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">67&nbsp;&nbsp;</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">s</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1000</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">59.6</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">31.5</td></tr>
</table>

<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>i_df <span class="sc">%&gt;%</span>                               <span class="co"># Summary by group using purrr</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">split</span>(.<span class="sc">$</span>region) <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  purrr<span class="sc">::</span><span class="fu">map</span>(summary)</span></code></pre></div>
<pre><code>## $c
##  region       income      noise_var   
##  c:1000   Min.   :-10   Min.   :-104  
##  n:   0   1st Qu.: 59   1st Qu.: -20  
##  s:   0   Median : 80   Median :   0  
##           Mean   : 80   Mean   :   0  
##           3rd Qu.: 99   3rd Qu.:  20  
##           Max.   :175   Max.   :  81  
## 
## $n
##  region       income      noise_var  
##  c:   0   Min.   :-86   Min.   :-89  
##  n:1000   1st Qu.: 88   1st Qu.:-20  
##  s:   0   Median :132   Median :  0  
##           Mean   :131   Mean   :  0  
##           3rd Qu.:176   3rd Qu.: 20  
##           Max.   :345   Max.   : 88  
## 
## $s
##  region       income      noise_var   
##  c:   0   Min.   :-36   Min.   :-101  
##  n:   0   1st Qu.: 39   1st Qu.: -17  
##  s:1000   Median : 60   Median :   1  
##           Mean   : 60   Mean   :   1  
##           3rd Qu.: 80   3rd Qu.:  21  
##           Max.   :158   Max.   :  90</code></pre>
<p>Looks close enough.</p>
<p>Now I want to ‘model income’ to examine the differences by region ‘controlling for other factors’ (which here are noise, but let’s do it anyways; will make this exercise richer later).</p>
<p>Just to be difficult, let’s make the south the base group.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">contrasts =</span> <span class="fu">rep</span> (<span class="st">&quot;contr.treatment&quot;</span>, <span class="dv">2</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>i_df <span class="ot">&lt;-</span> i_df <span class="sc">%&gt;%</span>   <span class="fu">mutate</span>(<span class="at">region =</span> <span class="fu">relevel</span>(region, <span class="at">ref=</span><span class="st">&quot;s&quot;</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  basic_lm <span class="ot">&lt;-</span> i_df <span class="sc">%&gt;%</span> <span class="fu">lm</span>(income <span class="sc">~</span> region <span class="sc">+</span> noise_var, .)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = income ~ region + noise_var, data = .)
## 
## Coefficients:
## (Intercept)      regionc      regionn    noise_var  
##    59.56067     20.56332     71.53226     -0.00566</code></pre>
<p>The standard thing: intercept is (approximately) the mean for the ‘base group,’ the south, and the coefficients ‘regionc’ and ‘regionn’ represent the relative adjustments for these, roughly +20 and +70.</p>
<p>This is standard ‘dummy coding,’ i.e., ‘treatment coding.’ This is the default in R:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="st">&quot;contrasts&quot;</span>)</span></code></pre></div>
<pre><code>## $contrasts
## [1] &quot;contr.treatment&quot; &quot;contr.treatment&quot;</code></pre>
<p>We can adjust this default (for unordered variables) to something called ‘sum contrast coding’ for both unordered and ordered</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">contrasts =</span> <span class="fu">rep</span> (<span class="st">&quot;contr.sum&quot;</span>, <span class="dv">2</span>))</span></code></pre></div>
<p>Running the regression again</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  basic_lm_cc <span class="ot">&lt;-</span> i_df <span class="sc">%&gt;%</span> <span class="fu">lm</span>(income <span class="sc">~</span> region <span class="sc">+</span> noise_var, .)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = income ~ region + noise_var, data = .)
## 
## Coefficients:
## (Intercept)      region1      region2    noise_var  
##    90.25920    -30.69853    -10.13521     -0.00566</code></pre>
<p>Now this seems to get us the adjustment coefficients we are looking for, but</p>
<ol style="list-style-type: decimal">
<li>The names of the regions are lost; how do I know which is which?</li>
<li>It is apparently reporting the adjustment coefficients for s (south) and c (central). Not very intuitive.</li>
</ol>
<p>This seems to be the case no matter how we relevel the region to set a particular base group (I tried it) … the coefficients don’t change.</p>
<p>What if we de-mean the outcome (income) variable, and force a 0 intercept?</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>i_df <span class="sc">%&gt;%</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">m_inc =</span> <span class="fu">mean</span>(income)) <span class="sc">%&gt;%</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">lm</span>(income <span class="sc">-</span> m_inc <span class="sc">~</span> <span class="dv">0</span>  <span class="sc">+</span> region <span class="sc">+</span> noise_var, .)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = income - m_inc ~ 0 + region + noise_var, data = .)
## 
## Coefficients:
##   regions    regionc    regionn  noise_var  
## -30.69581  -10.13249   40.83645   -0.00566</code></pre>
<p>Yay! This is what I wanted, and miraculously the variable names are preserved. But it seems a weird way to do it. Also note that, with the above code, this set of coefficients appears no matter which contrast matrix I force, whether sum or treatment.</p>
<p>However, this does not carry over easily to multiple dimensions of contrast coding (illustrate).</p>
<!--chapter:end:regression_follies/regression_follies.Rmd-->
</div>
</div>
<div id="robust-diag" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> Robustness and diagnostics, with integrity; Open Science resources</h1>
<div id="how-can-diagnostic-tests-make-sense-where-is-the-burden-of-proof" class="section level2" number="4.1">
<h2 number="4.1"><span class="header-section-number">4.1</span> (How) can diagnostic tests make sense? Where is the burden of proof?</h2>
<p>Where a particular assumption is critical to identification and inference … iFailure to reject the violation of an assumption is not sufficient to give us confidence that it is satisfied and the results are credible. Authors frequently cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles).</p>
<p>I am concerned with the interpretation of diagnostic testing, both in model selection, and in the defense of the exclusion restrictions or identification assumptions. It is problematic, when the basic consistency of the estimator (or a main finding of the paper) critically depends on such tests failing to reject a null hypothesis, to merely state that the ‘test failed to reject, therefore we maintain the null hypothesis.’</p>
<p><br />
</p>
<ul>
<li><p>How powerful are these tests?</p></li>
<li><p>I.e. what is the probability of a false negative Type II error?</p></li>
<li><p>How large a bias would be compatible with reasonable confidence intervals for these tests?</p></li>
</ul>
<div id="further-discussion-the-did-approach-and-parallel-trends" class="section level3" number="4.1.1">
<h3 number="4.1.1"><span class="header-section-number">4.1.1</span> Further discussion: the DiD approach and ‘parallel trends’</h3>
<p>BB 17/06:</p>
<blockquote>
<p>[In general] economists do … suggest that they have evidence which allows them to accept null hypotheses. The motivating example would be in the case of DiD estimation, where we have to assume parallel trends. My hunch is that fairly frequently people just say “yes we have evidence the trends were parallel pre-intervention” and then point away to something that shows a <span class="math inline">\(p&gt;0.05\)</span> on pre-interenvetion differences in trend. Sometimes this is done better where they say ‘yeah here is a (relatively) tightly estimated zero effect.’ But what do we accept as good enough to show that?</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>Building on that point, it seems that we do need something to tell us when DiD is valid and yet don’t yet have something. My hunch is that the answer may well lie in Bayesian approaches. DR’s suggestion was, I think, that we could maybe simultaneously try and estimate the difference in trends both before and after intervention simultaneously.</p>
</blockquote>
<p>DR: My idea was that a Bayesian approach would have involve a parameter for ‘differential trend,’ with a prior (and then posterior) distribution over both this parameter and the ultimate estimate. I’ll look to find comparable approaches in the literature.</p>
<blockquote>
<p>BB: I was wondering about a less sophisticated approach in which we set up the prior to be not-parallel trends and see over what range of “not parallel” priors the posterior is pulled closer to the idea of there actually being a parallel trend.</p>
</blockquote>
<p>DR: It’s an interesting reframe, to place the burden of proof on the researcher; sort of ‘rejecting the null of a non-parallel trend.’ But how can this work? And how to deal with the idea that the null could be a differential trend <em>in either direction</em>?</p>
<div class="marginnote">
<p>@BB: And perhaps we should try to have this discussion using the specific equations and assumptions of DiD?</p>
</div>
<blockquote>
<p>BB: I think similar to what DR suggested above is what we used in the Bayesian approaches to do for Covid. Seems to me that the intervention in this case is the lockdown. There we assume that when a country locks down it reduces the R, and we estimate two different Rs.</p>
</blockquote>
<p>DR: Can you define this with a few equations to fix ideas? It’s hard to follow.</p>
<blockquote>
<p>BB: Thinking about that approach here, we could try and estimate a pre-intervention relationship between the two groups, and then introduce a post-intervention term which is asked to explain the residual of the variation in the intervention group post-treatment.</p>
</blockquote>
<blockquote>
<p>BB: Again, I think they key would be trying to establish ‘over what prior assumptions for the pre-treatment trend is the posterior outcome for post-treatment effect in a particular direction.’</p>
</blockquote>
<p>DR: Yes, essentially, we need to consider the sensitivity of the estimates to the assumption!</p>
<blockquote>
<p>But I think before we get into this we should definitely look at the tweet on DiD that you included as well as the drive folder of Patrick Button</p>
</blockquote>
<p>Just to add that there seem to some further papers we should take a look at.</p>
<p>Specifically:
<a href="https://www.nber.org/papers/w25018">This one</a>
<a href="https://academic.oup.com/qje/article-abstract/119/1/249/1876068">and this one</a></p>
</div>
</div>
<div id="estimating-standard-errors" class="section level2" number="4.2">
<h2 number="4.2"><span class="header-section-number">4.2</span> Estimating standard errors</h2>
</div>
<div id="sensitivity-analysis-interactive-presentation" class="section level2" number="4.3">
<h2 number="4.3"><span class="header-section-number">4.3</span> Sensitivity analysis: Interactive presentation</h2>
</div>
<div id="supplement-open-science-resources-tools-and-considerations" class="section level2" number="4.4">
<h2 number="4.4"><span class="header-section-number">4.4</span> Supplement: open science resources, tools and considerations</h2>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
I'm in psychology research (just finished PhD) and I want to get into <a href="https://twitter.com/hashtag/OpenScience?src=hash&amp;ref_src=twsrc%5Etfw">#OpenScience</a> to make sure I'm following best practices. But this is something that wasn't explicitly taught to me. What are some good resources? Thanks! <a href="https://twitter.com/AcademicChatter?ref_src=twsrc%5Etfw">@AcademicChatter</a> <a href="https://twitter.com/OSFramework?ref_src=twsrc%5Etfw">@OSFramework</a> <a href="https://twitter.com/OpenAcademics?ref_src=twsrc%5Etfw">@OpenAcademics</a> <a href="https://twitter.com/dsquintana?ref_src=twsrc%5Etfw">@dsquintana</a>
</p>
— Alessa Teunisse (@alessateunisse) <a href="https://twitter.com/alessateunisse/status/1252481892240125952?ref_src=twsrc%5Etfw">April 21, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br />
</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
A couple of months ago I made a guide on how to use Binder to make our <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> code <a href="https://twitter.com/hashtag/reproducible?src=hash&amp;ref_src=twsrc%5Etfw">#reproducible</a>. E.g., Binder will make your code runnable using the versions of R and r-packages used when you analyzed your data. <a href="https://t.co/srYNazwy0q">https://t.co/srYNazwy0q</a> <a href="https://twitter.com/hashtag/reproducibility?src=hash&amp;ref_src=twsrc%5Etfw">#reproducibility</a> <a href="https://twitter.com/hashtag/openscience?src=hash&amp;ref_src=twsrc%5Etfw">#openscience</a> <a href="https://t.co/gcTlVFpaY5">pic.twitter.com/gcTlVFpaY5</a>
</p>
— Erik Marsja (Emarsja) <a href="https://twitter.com/Emarsja/status/1206850126444204032?ref_src=twsrc%5Etfw">December 17, 2019</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div id="diagnosing-p-hacking-and-publication-bias-see-also-meta-analysis" class="section level2" number="4.5">
<h2 number="4.5"><span class="header-section-number">4.5</span> Diagnosing p-hacking and publication bias (see also <a href="#metaanalysis">meta-analysis</a>)</h2>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
Ever wonder “Were those results p-hacked?” Brodeur et al. propose a useful new check (“speccheck” on Stata. R/etc. coming soon). <a href="https://twitter.com/hashtag/ASSA2020?src=hash&amp;ref_src=twsrc%5Etfw">#ASSA2020</a> <a href="https://t.co/NCZ1jZTaO5">pic.twitter.com/NCZ1jZTaO5</a>
</p>
— Eva Vivalt (evavivalt) <a href="https://twitter.com/evavivalt/status/1213608282486525952?ref_src=twsrc%5Etfw">January 4, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<div id="publication-bias-see-also-considering-publication-bias-in-meta-analysis" class="section level3" number="4.5.1">
<h3 number="4.5.1"><span class="header-section-number">4.5.1</span> Publication bias – see also <a href="#pubbias">considering publication bias in meta-analysis</a></h3>
</div>
</div>
<div id="multiple-hypothesis-testing---see-above" class="section level2" number="4.6">
<h2 number="4.6"><span class="header-section-number">4.6</span> <a href="#mht">Multiple hypothesis testing - see above</a></h2>
<!--chapter:end:meta_anal_and_open_science/robust_diagnos.Rmd-->
</div>
</div>
<div id="control-ml" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> Control strategies and prediction, Machine Learning (Statistical Learning) approaches</h1>
<div id="see-also-notes-on-data-science-for-business" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> See also <a href="#n_ds4bs">“notes on Data Science for Business”</a></h2>
<blockquote>
<p>‘Identification’ of causal effects with a control strategy not
credible</p>
</blockquote>
<p>Essentially a ‘control strategy’ is “control for all or most of the
reasonable determinants of the independent variable so as to make the
remaining unobservable component very small, minimizing the potential
for bias in the coefficient of interest.” All of the controls must still
be exogenous, otherwise this itself can lead to a bias. There is some
discussion of how to validate this approach; see, e.g.,
<span class="citation">(<a href="#ref-osterUnobservableSelectionCoefficient2019" role="doc-biblioref">Oster 2019</a>)</span>. ## Machine Learning
(statistical learning): Lasso, Ridge, and more</p>
<p>Machine learning seeks to fit models to data in order to <em>learn</em>
patterns and thus make predictions about the future. This learning is
split into categories:</p>
<ul>
<li><p><strong>Supervised learning</strong>: Here our data consists of <em>labelled</em>
examples. This means that we have data on the outcome variable of
interest. Linear regression is a form of supervised learning
algorithm as the data used will contain values for <span class="math inline">\(Y\)</span> and is thus
labeled.</p></li>
<li><p><strong>Unsupervised learning</strong>: In this type of learning data has no
label. Clustering algorithms are examples of unsupervised learning,
where the model seeks to group objects into collections with other
similar observations.</p></li>
<li><p><strong>Semi-supervised learning</strong>: This method is used in order to
account for the high cost of labeling data. Where a data set may
have labels for some observations and not for others we use
semi-supervised learning.</p></li>
<li><p><strong>Reinforcement learning</strong>: Reinforcement learning works by
assigning rewards (or errors) to actions taken by an algorithm.
Typically this reward will be an estimate of how much reward is
received at the end of a <em>round</em>. This method is popular in
artificial intelligence programs, such as the AlphaGo algorithm by
DeepMind. <!-- Add example for economics --></p></li>
</ul>
<div id="limitations-to-inference-from-learning-approaches" class="section level3" number="5.1.1">
<h3 number="5.1.1"><span class="header-section-number">5.1.1</span> Limitations to inference from learning approaches</h3>
<p>From a discussion on the EA forum about predicting donation choices</p>
<blockquote>
<p>… the ML approaches Greg is suggesting here are all about
<strong>prediction</strong>, not about drawing meaningful inferences.For prediction
it’s good to start with the largest amount of features (variables) you
can find (as long as they are truly ex-ante) and then do a fancy dance
of cross-validation and regularisation, before you do your final
‘validation’ of the model on set-aside data.But that doesn’t easily
give you the ability to make strong inferential statements (causal or
not), about things like ‘age is likely to be strongly associated with
satisfaction measures in the true population.’ Why not? If I
understand correctly:<em>The model you end up with, which does a great
job at predicting your outcome</em><br />
</p>
<ol style="list-style-type: decimal">
<li><p>… may have dropped <code>age</code> entirely or “regularized it” in a way
that does not yield an unbiased or consistent estimator of the
actual impact of <code>age</code>  on your outcome. Remember, the goal here
was prediction, not making inferences about the relationship
between of any particular variable or sets of variables …</p></li>
<li><p>… may include too many variables that are highly correlated with
the <code>age</code>  variable, thus making the age coefficient very
imprecise</p></li>
<li><p>…  may include variables that are actually ‘part of the age
effect you cared about, because they are things that go naturally
with age, such as mental agility’</p></li>
<li><p>Finally, the standard ‘statistical inference’ (how you can
quantify your uncertainty) does not work for these learning models
(although there are new techniques being developed)</p></li>
</ol>
</blockquote>
</div>
<div id="tree-models" class="section level3" number="5.1.2">
<h3 number="5.1.2"><span class="header-section-number">5.1.2</span> Tree models</h3>
<div class="marginnote">
<p>This section was mainly written by Oska Fentem.</p>
</div>
<p>Decision trees are a type of supervised machine learning algorithm.
While they are very simple and intuitive to understand, they form the
basis for a variety of more complex models. Here we will discuss the
model in the context of a classification problem; however they can also
be used for continuous outcome (‘regression’) settings.</p>
<p>Decision trees are a type of supervised machine learning algorithm.
“Recursive binarry splitting” is used; the algorithm repeatedly splits
the data into subsets and subsets of these subsets, finally making a
prediction based on the dominant group in each subset.
<!-- Guess who example --> A decision tree consists of:</p>
<ol style="list-style-type: decimal">
<li><strong>Root node</strong>: this is the first node in the tree.</li>
<li><strong>Interior nodes</strong>: these are the nodes on which the data is split.</li>
<li><strong>Leaf nodes</strong>: these contain the final classifications made by the
model.</li>
</ol>
<p>In splitting a data set by it’s features (variables) the decision tree
algorithm seeks to split using the most informative features first. In
order to calculate which features are the most informative we can
calculate <strong>information gain</strong> or <strong>Gini impurity</strong>.</p>
<p><br />
</p>
<div id="information-gain" class="section level4" number="5.1.2.1">
<h4 number="5.1.2.1"><span class="header-section-number">5.1.2.1</span> Information Gain</h4>
<p>Information gain is used to measure the feature which provides the
largest increase in information about the classification of our outcome
variable. This metric is based on the notion of entropy, a measure of
set impurity as well as the remaining information.</p>
<p><strong>Entropy</strong>: For <span class="math inline">\(X = x_1,…, x_n\)</span> occurring with probability
<span class="math inline">\(P(x_1),…, P(x_n)\)</span> entropy is defined as:
<span class="math display">\[\text{Entropy} = -\sum_{i=1}^n P(x_i)\log_2 P(x_i)\]</span> Entropy is high
when there is more uncertainty around the result if we were to guess a
member of the set at random. An example of a set with high entropy would
be a pack of playing cards. Each card in the set has a low probability
of being picked. This means that we would be very uncertain about the
outcome of picking a card at random. By graphing the function
<span class="math inline">\(- P(x_i) \log_2 P(x_i)\)</span> we see that as the probability of an each
outcome decreases the total entropy increases:</p>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-15-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Using the entropy function we are able to compute information gain with
the following three equations:</p>
<p><span class="math display">\[
\begin{aligned}&amp;\operatorname{rem}(d, \mathcal{D})=\sum_{I \in \text {levels}(d)} \underbrace{\frac{\left|\mathcal{D}_{d=I}\right|}{|\mathcal{D}|}}_{\text {weighting }} \times \underbrace{H\left(t, \mathcal{D}_{d=I}\right)}_{\text {entropy of partition} \space \mathcal{D}_{d=I}}\\&amp;I G(d, \mathcal{D})=H(t, \mathcal{D})-\operatorname{rem}(d, \mathcal{D})\end{aligned}
\]</span></p>
<p>Source: <span class="citation">(<a href="#ref-kelleherFundamentalsMachineLearning2015" role="doc-biblioref">Kelleher, Mac Namee, and D’Arcy 2015</a>)</span></p>
</div>
<div id="ensemble-learning" class="section level4" number="5.1.2.2">
<h4 number="5.1.2.2"><span class="header-section-number">5.1.2.2</span> Ensemble learning</h4>
<p>Decision trees provide an easy and intuitive way to represent a complex
relationships between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. While these models have very low bias
they have high variance. This means that changes in the training data
may cause large changes in the predictions made by the model.</p>
<p><strong>Random forests</strong> are a form of ensemble learning. This method works by
combining multiple weaker classifiers into one strong one. By doing this
we average the predictions made across all the decision trees, which can
often result in better performance than a single tree. In a random
forest we fit multiple decision tree models on random bootstrap
sub-samples of our data, limiting ourselves, in each forest, to a
randomly-restricted subset of variables (features). The algorithm works
as follows:</p>
<p>For <span class="math inline">\(b=1\)</span> to <span class="math inline">\(B\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Draw a bootstrap sample <span class="math inline">\(Z^*\)</span> of size <span class="math inline">\(N\)</span> from our training data</p>
<ul>
<li>Select a subset of <span class="math inline">\(m\)</span> variables by random out of a total of <span class="math inline">\(p\)</span>
variables</li>
</ul></li>
<li><p>Use this sample to fit a tree, <span class="math inline">\(T_b\)</span>:</p>
<ol style="list-style-type: lower-roman">
<li><p>Pick the best variable to split the data on out of <span class="math inline">\(m\)</span></p></li>
<li><p>Split the node</p></li>
</ol>
<p>Steps i-ii are repeated until the minimum node size is reached.</p></li>
<li><p>Output ensemble</p></li>
</ol>
<p>Note that this procedure is similar to bagging. However, by using a
random set of <span class="math inline">\(m\)</span> variables to consider for splitting our nodes the
random forest algorithm achieves lower correlation between the
individual trees. Standard bagging instead makes use of all <span class="math inline">\(p\)</span>
variables when deciding on tree splits, which could result in trees that
are correlated. If we have a collection of identically distributed trees
then the expectation of all these models is equal to the expectation of
any one of them. If one tree is more likely to make a wrong prediction
this is averaged out by the lack of correlation between this and the
other models.</p>
<p>As forests contain a collection of smaller trees the prediction process
is slightly different than for individual trees. Consider that each tree
gives a prediction <span class="math inline">\(\hat{C}_b(X)\)</span>. Our random forest prediction is thus:
<span class="math display">\[\hat{C}_{rf}^B = \text{Majority vote} \{ \hat{C}_n)x_i \}^B_1\]</span></p>
<p>Source: <span class="citation">(<a href="#ref-friedmanElementsStatisticalLearning2001" role="doc-biblioref">Friedman et al. 2001</a>)</span></p>
</div>
</div>
</div>
<div id="notes-hastie-statistical-learning-with-sparsity" class="section level2" number="5.2">
<h2 number="5.2"><span class="header-section-number">5.2</span> Notes Hastie: Statistical Learning with Sparsity</h2>
<p><a href="https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=f-A_CQAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;ots=G4RMC-gZU-&amp;sig=u_EoI3-ZcX0phwsZgLt2Ux7DXyc#v=onepage&amp;q&amp;f=true">google books
link</a></p>
<div id="introduction-1" class="section level3" number="5.2.1">
<h3 number="5.2.1"><span class="header-section-number">5.2.1</span> Introduction</h3>
<blockquote>
<p>One form of simplicity is sparsity, the central theme of this book.
Loosely speaking, a sparse statistical model is one in which only a
relatively small number of parameters (or predictors) play an
important role.</p>
</blockquote>
<p>“the <span class="math inline">\(\ell_1\)</span> norm is special” (abs value). Other norms yield nonconvex
problems, hard to minimize.</p>
<blockquote>
<p>“bet on sparsity” principle: Use a procedure that does well in sparse
problems, since no procedure does well in dense problems.</p>
</blockquote>
<ul>
<li>Examples from gene mapping</li>
</ul>
<div id="book-roadmap" class="section level4" number="5.2.1.1">
<h4 number="5.2.1.1"><span class="header-section-number">5.2.1.1</span> Book roadmap</h4>
<ul>
<li><p>Chapter 2 … lasso for linear regression, and a simple coordinate
descent algorithm for its computation.</p></li>
<li><p>Chapter 3 application of <span class="math inline">\(\ell_1\)</span> [lasso-type] penalties to
generalized linear models such as multinomial and survival models,
as well as support vector machines. [?]</p></li>
<li><p>Chapter 4: Generalized penalties such as the elastic net and group
lasso are discussed in Chapter 4.</p></li>
<li><p>Chapter 5: numerical methods for optimization (skip for now]</p></li>
<li><p>Chapter 6: statistical inference for fitted (lasso) models,
including the bootstrap, Bayesian methods and more recent stuff</p></li>
<li><p>Chapter 7: Sparse matrix decomposition [?] (Skip?)</p></li>
<li><p>Ch 8: sparse multivariate analysis of that (Skip?)</p></li>
<li><p>Ch 9: Graphical models and their selection (Skip?)</p></li>
<li><p>Ch 10: compressed sensing (Skip?)</p></li>
<li><p>Ch 11: a survey of theoretical results for the lasso (Skip?)</p></li>
</ul>
</div>
</div>
<div id="ch2-lasso-for-linear-models" class="section level3" number="5.2.2">
<h3 number="5.2.2"><span class="header-section-number">5.2.2</span> Ch2: Lasso for linear models</h3>
<ul>
<li>N samples (?N observations), want to approx the response variable
using a linear combination of the predoctors</li>
</ul>
<hr />
<p><strong>OLS</strong> minimizes squared-error loss but</p>
<ol style="list-style-type: decimal">
<li>Prediction accuracy</li>
</ol>
<ul>
<li><p>OLS unbiased but ‘often has large variance’</p></li>
<li><p>prediction accuracy can be improved by shrinking coefficients (even
to zero)</p>
<ul>
<li>yielding biased but perhaps better predictive estimators</li>
</ul></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Interpretation: too many predictors hard to interpret</li>
</ol>
<ul>
<li>DR: I do not care about this for fitting background noise in
experiments</li>
</ul>
<div id="the-lasso-estimator" class="section level4" number="5.2.2.1">
<h4 number="5.2.2.1"><span class="header-section-number">5.2.2.1</span> 2.2 The Lasso Estimator</h4>
<p>Lasso bounds the sum of the abs values of coefficients, an “$_1”
constraint.</p>
<p>Lasso is OLS subject to</p>
<p><br />
</p>
<p><span class="math inline">\(\sum_{j=1..p}{\abs(\beta_j)}\leq t\)</span></p>
<p><br />
</p>
<p>“compactly” <span class="math inline">\(||\beta||_1\leq t\)</span></p>
<p>with notation for the “<span class="math inline">\(\ell_1\)</span> norm”</p>
<p><br />
</p>
<ul>
<li><p>Bound <span class="math inline">\(t\)</span> acts as a ‘budget,’ must be specified by an ‘external
procedure’ such as cross-validation</p></li>
<li><p>typically we must <em>standardize the predictors</em> $** so
that each column is centered with unit variance … as well as the
outcome variables (?) … can ignore intercept</p>
<ul>
<li>DR: Not clear here whether standardisation is necessary for the
procedure to be valid or just convenient for explaining and
deriving its properties.</li>
</ul></li>
</ul>
<p><br />
</p>
<div class="marginnote">
<p>Aside: Can re-write Lasso minimization st constraint as a Lagrangian.
<span class="math inline">\(\lambda\)</span> plays the same role as <span class="math inline">\(t\)</span> in the constraint. Thus we can
speak of the solution to the Lagrangian minimisation problem
<span class="math inline">\(\hat{\beta)_{\lambda}\)</span> which also solves the bound problem with
<span class="math inline">\(t=||\hat_{\lambda}||_1\)</span>.</p>
</div>
<div class="marginnote">
<p>Aside: We often remove the <span class="math inline">\(1/2n\)</span> term at the beginning of the
minimization problem. Same minimization, minimizing sum of squared
deviations rather than something like an average of this.</p>
</div>
<p><br />
</p>
<p>Express (Karush-Kuhn-Tucker) optimisation conditions for this …</p>
<hr />
<p>Example from Thomas (1990) on crime data</p>
<blockquote>
<p>Typically … lasso is most useful for much larger problems, including
“wide” data for which <span class="math inline">\(p&gt;&gt;N\)</span></p>
</blockquote>
<p><br />
</p>
<p>Fig 2.1: Lasso vs ridge regression; coefficients of each for a set of
considered variables plotted against their respective norms (as shares
of maximal bound on coefficient sum measure, i.e., ols, for each)</p>
<ul>
<li><p>Note ridge regression penalises <em>squared</em> sums of betas</p>
<ul>
<li>Fig 2.2., in <span class="math inline">\(\beta_1,\beta_2\)</span> space illustrates the difference
well: contour lines of Resid SS elipses, ‘budget constraint’ for
each (disc vs diamond)</li>
</ul></li>
</ul>
<p>(Note: lasso bound was chosen via cross-validation)</p>
<ul>
<li>No analytical statistical inference after lasso (some being
developed?), bootstrap is common</li>
</ul>
<blockquote>
<p>lasso sets two of the five coefficients to zero, and tends to shrink
the coefficients of the others toward zero relative to the full
least-squares estimate.</p>
</blockquote>
<ul>
<li><p>DR: analytically and intuitively, I do not yet understand why lasso
should shrink coefficients but not all the way to zero.</p>
<ul>
<li>The penalty is linear in the coefficient size, so I would think
the solution would be bang-bang, either drop a coeficient or
leave it unchanged. But it is not.</li>
<li>Adding an increment to a <span class="math inline">\(\hat{\beta}\)</span> when it is below the OLS
estimate should have a linear effect on the RSS (according to my
memory and according to Sebastian).</li>
<li>But that would mean that shrinking one parameter always yields a
better benefit to cost ratio. Thus I should shrink each
parameter to zero before beginning to shrink any others. This
cannot be right!</li>
</ul></li>
</ul>
<p>I looked up this derivative wrt the beta vector (one needs to set this
to 0 to get the ols estimates</p>
<p><a href="https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression">stackexchange</a></p>
<p><span class="math inline">\(\frac{d RSS}{d \beta}=-2X^{T}(y-X\beta}\)</span></p>
<p>or</p>
<p><span class="math inline">\(−\frac{d e&#39;e}{d b}=2X′y+2X′Xb\)</span></p>
<p>The answer to this question: while the impact of changing each
coefficient on SSR is in fact constant (a constant own-derivative),
there is <em>also</em> an impact of changing one coefficient on the <em>other</em>
derivatives. As one coefficient shrinks to zero the marginal impact of
the other coefficients on the SSR may (will?) increase.</p>
<pre><code>- At the same time, we need that the effect of increasing it from zero need not be infinite, so it might not outweigh the linear penalty, thus some coefficients might be set to zero</code></pre>
<p><strong>Relaxed lasso</strong></p>
<blockquote>
<p>the least-squares fit on the subset of the three predictors tends to
expand the lasso estimates away from zero. The nonzero estimates from
the lasso tend to be biased toward zero, so the debiasing in the right
panel can often improve the prediction error of the model. This
two-stage process is also known as the relaxed lasso (Meinshausen
2007).</p>
</blockquote>
<ul>
<li><p>DR: When is this likely to help/hurt relative to pure lasso?</p></li>
<li><p><a href="https://stats.stackexchange.com/questions/285501/why-do-we-use-ols-to-estimate-the-final-model-chosen-by-lars/285518#285518">Stackexchange
discussion</a>
Contrasts a ‘relaxed-lasso’ from a ‘lars-ols’</p></li>
</ul>
<hr />
<p>Aside: which seems better for <em>Control variable selection for
prediction/reducing noise to enable better inference of treatment
effects</em>?</p>
<p>Ridge? better than Lasso here? We do not care about <em>interpreting</em> the
predictors here… so if we allow <span class="math inline">\(\beta\)</span>’s to be shrunk towards zero
for each coefficient maybe that should yield better prediction than
making them exactly zero?</p>
<p><br />
</p>
<p>On the other hand if we know the true model is ‘parsimonious’ (as in the
genes problem) it might boost efficiency to allow inference about
coefficients that should be exactly zero (edited)</p>
<hr />
</div>
<div id="cross-validation-and-inference" class="section level4" number="5.2.2.2">
<h4 number="5.2.2.2"><span class="header-section-number">5.2.2.2</span> 2.3 Cross-Validation and Inference</h4>
<dl>
<dt>Generalization ability</dt>
<dd><p>accuracy for predicting independent test data from the same
population</p>
</dd>
</dl>
<p>… find the value of t that does best</p>
<p>**Cross-validation procedure*</p>
<ol style="list-style-type: decimal">
<li>randomly divide … dataset into K groups.</li>
</ol>
<p>“Typical choices … might be 5 or 10, and sometimes N.”</p>
<ol start="2" style="list-style-type: decimal">
<li><p>One ‘test,’ remaining K-1 ‘training’</p></li>
<li><p>Apply lasso to training data for a range of t values,</p>
<ul>
<li>use each fitted model to predict the responses in the test set,
recording mean-squared prediction errors for each value of t.</li>
</ul></li>
<li><p>Repeat the previous step K times</p>
<ul>
<li>each time, one of the K groups is the test data, remaining K − 1
are training data.</li>
<li>yields K different estimates of the prediction error over a
range of t values.</li>
</ul></li>
<li><p>Average K estimates of prediction error for each value of t
<span class="math inline">\(\rightarrow\)</span> cross-validation error curve.</p></li>
</ol>
<p>Fig 2.3 plots an example with K=10 splits for cross validation</p>
<ul>
<li>… of the estimated MS prediction error vs the relative bound
<span class="math inline">\(\tilde{t}\)</span>(summed absolute value of Lasso betas divided by summed
abs value of OLS betas).</li>
<li>Also draw dotted line at the 1-std-error rule choice of <span class="math inline">\(\tilde{t\)</span>}</li>
<li>Number of nonzero coefficients plotted at top</li>
</ul>
</div>
<div id="computation-of-the-lasso-solution" class="section level4" number="5.2.2.3">
<h4 number="5.2.2.3"><span class="header-section-number">5.2.2.3</span> 2.4 Computation of the Lasso solution</h4>
<p>DR: I think I will skip this for now</p>
<p>least angle/LARS is mentioned at the bottom as a ‘homotopy method’ which
“produce the entire path of solutions in a sequential fashion, starting
at zero”</p>
</div>
<div id="degrees-of-freedom" class="section level4" number="5.2.2.4">
<h4 number="5.2.2.4"><span class="header-section-number">5.2.2.4</span> 2.5 Degrees of freedom</h4>
<p>…</p>
<p>Jumping to</p>
</div>
<div id="some-perspective" class="section level4" number="5.2.2.5">
<h4 number="5.2.2.5"><span class="header-section-number">5.2.2.5</span> 2.10 Some perspective</h4>
<p><strong>Good properties of the Lasso (</strong><span class="math inline">\(\ell_1\)</span> penalty)</p>
<ul>
<li><p>Natural interpretation (enforce sparsity and simplicity)</p></li>
<li><p>Statistical efficiency … if the underlying true signal is sparse
(but if it is not sparse “no method can do well relative to the
Bayes error”)</p></li>
<li><p>Computational efficiency, as <span class="math inline">\(\ell_1\)</span> penalties are convex</p></li>
</ul>
</div>
</div>
<div id="chapter-3-generalized-linear-models" class="section level3" number="5.2.3">
<h3 number="5.2.3"><span class="header-section-number">5.2.3</span> Chapter 3: Generalized linear models</h3>
</div>
<div id="chapter-4-generalizations-of-the-lasso-penalty" class="section level3" number="5.2.4">
<h3 number="5.2.4"><span class="header-section-number">5.2.4</span> Chapter 4: Generalizations of the Lasso penalty</h3>
<blockquote>
<p>lasso does not handle highly correlated variables very well; the
coefficient paths tend to be erratic and can sometimes show wild
behavior.</p>
</blockquote>
<p>The elastic net makes a compromise between the ridge and the lasso
penalties (Zou and Hastie 2005)1] is a parameter that can be varied.</p>
<p>For an individual coefficient the penalty is
<span class="math inline">\(\frac{1}{2} (1-\alpha)\beta_j^2 + \alpha|\beta_j|\)</span></p>
<p>(a convex combo of the lasso and ridge penalties)</p>
<p>multiplied by a ‘regularization weight’ <span class="math inline">\(\lambda&gt;0\)</span> which plays the same
role (I think) as in lasso</p>
<ul>
<li>elastic net is also <em>strictly convex</em></li>
</ul>
</div>
</div>
<div id="notes-mullainathan" class="section level2" number="5.3">
<h2 number="5.3"><span class="header-section-number">5.3</span> Notes: Mullainathan</h2>
<blockquote>
<p>The fundamental insight behind these breakthroughs is as much
statistical as computational. Machine intelligence became possible
once researchers stopped approaching intelligence tasks procedurally
and began tackling them empirically. Face recognition algorithms, for
example, do not consist of hard-wired rules to scan for certain pixel
combinations, based on human understanding of what constitutes a face.
Instead, these algorithms use a large dataset of photos labeled as
having a face or not to estimate a function f (x) that predicts the
presence y of a face from pixels x</p>
</blockquote>
<p>(p2) &gt; supervised- machine learning, the focus of this article)
revolves around the problem of prediction: produce predictions of y from
x</p>
<p>…</p>
<blockquote>
<p>manages to fit complex and very flexible functional forms to the data
without simply overfitting; it finds functions that work well
out-of-sample</p>
</blockquote>
<blockquote>
<p>danger in using these tools is taking an algorithm built for
[predicting <span class="math inline">\(y\)</span>-] and presuming their [parameters <span class="math inline">\(\beta\)</span>] - have the
properties we typically associate with estimation output</p>
</blockquote>
<blockquote>
<p>One category of such applications appears when using new kinds of data
for traditional questions; for example, in measuring economic activity
using satellite images or in classifying industries using corporate
10-K filings. Making sense of complex data such as images and text
often involves a prediction pre-processing step</p>
</blockquote>
<div class="marginnote">
<p>This middle category is most relevant for me</p>
</div>
<blockquote>
<p>In another category of applications, the key object of interest is
actually a parameter … but the inference procedures (often
implicitly) contain a prediction task. For example, the first stage of
a linear instrumental variables regression is effectively prediction.
The same is true when estimating heterogeneous treatment effects,
testing for effects on multiple outcomes in experiments, and flexibly
controlling for observed confounders</p>
</blockquote>
<blockquote>
<p>A final category is in direct policy applications. Deciding which
teacher to hire implicitly involves a prediction task (what added
value will a given teacher have?), one that is intimately tied to the
causal question of the value of an additional teacher.</p>
</blockquote>
<p>(p3)</p>
<p><strong>A useful (interactive?) example:</strong></p>
<blockquote>
<p>We consider 10,000 randomly selected owner-occupied units from the
2011 metropolitan sample of the American Housing Survey. In addition
to the values of each unit, we also include 150 variables that contain
information about the unit and its location, such as the number of
rooms, the base area, and the census region within the United States.
To compare different prediction techniques, we evaluate how well each
approach predicts (log) unit value on a separate hold-out set of
41,808 units from the same sample. All details on the sample and our
empirical exercise can be found in an online appendix available with
this paper at <a href="http://e-jep.org" class="uri">http://e-jep.org</a></p>
</blockquote>
<blockquote>
<p>In-sample performance may overstate performance; this is especially
true for certain machine learning algorithms like random forests that
have a strong tendency to overfit. Second, on out-of-sample
performance, machine learning algorithms such as random forests can do
significantly better than ordinary least squares, even at moderate
sample sizes and with a limited number of covariates</p>
</blockquote>
<p>(p4)</p>
<blockquote>
<p>algorithms are fitted on the same, randomly drawn training sample of
10,000 units and evaluated on the 41,808 remaining held-out units.</p>
</blockquote>
<blockquote>
<p>Simply including all pairwise interactions would be infeasible as it
produces more regressors than data points (especially considering that
some variables are categorical</p>
</blockquote>
<blockquote>
<p>Machine learning searches for these interactions automatically</p>
</blockquote>
<p>(p5)</p>
<blockquote>
<p>Shallow Regression Tree Predicting House Values</p>
</blockquote>
<div class="marginnote">
<p>not sure what’s going on here. is this the random forest thing?</p>
</div>
<blockquote>
<p>The prediction function takes the form of a tree that splits in two at
every node. At each node of the tree, the value of a single variable
(say, number of bathrooms) determines whether the left (less than two
bathrooms) or the right (two or more) child node is considered next.
When a terminal node-a leaf—is reached, a prediction is returned. An</p>
</blockquote>
<p>So how does machine learning manage to do out-of-sample prediction? The
first part of the solution is regularization. In the tree case, instead
of choosing the -best" overall tree, we could choose the best tree among
those of a certain depth.</p>
<p>(p5) Tree depth is an example of a regularizer. It measures the
complexity of a function. As we regularize less, we do a better job at
approximating the in-sample variation, but for the same reason, the
wedge between in-sample and out-of-sample</p>
<p>(p6) how do we choose the level of regularization (-tune the
algorithm")? This is the second key insight: empirical tuning.</p>
<p>(p6) -tuning within the training sample In empirical tuning, we create
an out-of-sample experiment inside the original sample. We fit on one
part of the data and ask which level of regularization leads to the best
performance on the other part of the data.4 We can increase the
efficiency of this procedure through cross-validation: we randomly
partition the sample into equally sized subsamples (-folds"). The
estimation process then involves successively holding out one of the
folds for evaluation while fitting the prediction function for a range
of regularization parameters on all remaining folds. Finally, we pick
the parameter with the best estimated average performance.5 The</p>
<p>(p6) -! This procedure works because prediction quality is observable:
both predictions y- and outcomes y are observed. Contrast this with
parameter estimation, where typically we must rely on assumptions about
the data-generating process to ensure consistency</p>
<p>(p7) Some Machine Learning Algorithms Function class - (and its
parametrization) Regularizer R( f ) Global/parametric predictors Linear
-′x (and generalizations) Subset selection|</p>
<p>(p7) -very useful table Some Machine Learning Algorithms Function class
- (and its parametrization) Regularizer R( f ) Global/parametric
predictors Linear -′x (and generalizations) Subset selection||β|</p>
<p>(p7) Random forest (linear combination of trees</p>
<p>(p7) -kernel in an ml framework! Kernel regression</p>
<p>(p6) -but can we make inferences about the structure? hypothesis
testing? Regularization combines with the observability of prediction
quality to allow us to fit flexible functional forms and still find
generalizable structure.</p>
<p>(p7) Picking the prediction function then involves two steps: The first
step is, conditional on a level of complexity, to pick the best
in-sample loss-minimizing function.8 The second step is to estimate the
optimal level of complexity using empirical tuning (as we saw in
cross-validating the depth of the tree).</p>
<p>(p8) -but they forgot to mention that others are shrunk linear
regression in which only a small number of predictors from all possible
variables are chosen to have nonzero values: the absolute-value
regularizer encourages a coefficient vector where many are exactly zero.</p>
<p>(p4) -why no ridge or elastic net? LASSO</p>
<p>(p8) -ensembles usually win contests While it may be unsurprising that
such ensembles perform well on average- after all, they can cover a
wider array of functional forms-it may be more surprising that they come
on top in virtually every prediction competition</p>
<p>(p8) -neural nets broadly explained neural nets are popular prediction
algorithms for image recognition tasks. For one standard implementation
in binary prediction, the underlying function class is that of nested
logistic regressions: The final prediction is a logistic transformation
of a linear combination of variables (-neurons") that are themselves
such logistic transformations, creating a layered hierarchy of logit
regressions. The complexity of these functions is controlled by the
number of layers, the number of neurons per layer, and their
connectivity (that is, how many variables from one level enter each
logistic regression on the next)</p>
<p>(p9) These choices about how to represent the features will interact
with the regularizer and function class: A linear model can reproduce
the log base area per room from log base area and log room number
easily, while a regression tree would require many splits to do so.</p>
<p>(p9) In a traditional estimator, replacing one set of variables by a set
of transformed variables from which it could be reconstructed would not
change the predictions, because the set of functions being chosen from
has not changed. But with regularization, including these variables can
improve predictions because-at any given level of regularization-the set
of functions might change</p>
<p>(p9) -!! Economic theory and content expertise play a crucial role in
guiding where the algorithm looks for structure first. This is the sense
in which -simply throw it all in- is an unreasonable way to understand
or run these machine learning algorithms</p>
<p>(p9) -I need hear of using adjusted r square for this Should
out-ofsample performance be estimated using some known correction for
overfitting (such as an adjusted R2 when it is available) or using
cross-validation</p>
<p>(p9) -big unknowns available finite-sample guidance on its
implementation-such as heuristics for the number of folds (usually five
to ten) or the -one standard-error rule" for tuning the LASSO (Hastie,
Tibshirani, and Friedman 2009)-has a more ad-hoc flavor</p>
<p>(p9) firewall principle: none of the data involved in fitting the
prediction function-which includes crossvalidation to tune the
algorithm—is used to evaluate the prediction function that is produced</p>
<p>(p10) -how? First, econometrics can guide design choices, such as the
number of folds or the function class</p>
<p>(p10) with the fitted function. Why not also use it to learn something
about the -underlying model</p>
<p>(p10) -!! the lack of standard errors on the coefficients. Even when
machine-learning predictors produce familiar output like linear
functions, forming these standard errors can be more complicated than
seems at first glance as they would have to account for the model
selection itself. In fact, Leeb and P-tscher (2006, 2008) develop
conditions under which it is impossible to obtain (uniformly) consistent
estimates of the distribution of model parameters after data-driven
selection</p>
<p>(p11) -lasso chosen variables are unstable because of multicollinearity.
a problem for making inferences from estimated coefficients the
variables are correlated with each other (say the number of rooms of a
house and its square-footage), then such variables are substitutes in
predicting house prices. Similar predictions can be produced using very
different variables. Which variables are actually chosen depends on the
specific finite sample</p>
<p>(p11) this creates an Achilles- heel: more functions mean a greater
chance that two functions with very different</p>
<p>(p12) coefficients can produce similar prediction quality</p>
<p>(p12) In econometric terms, while the lack of standard errors
illustrates the limitations to making inference after model selection,
the challenge here is (uniform) model selection consistency itself</p>
<p>(p12) -is this equally a problem for non sparsity based procedures like
ridge? First, it encourages the choice of less complex, but wrong
models. Even if the best model uses interactions of number of bathrooms
with number of rooms, regularization may lead to a choice of a simpler
(but worse) model that uses only number of fireplaces. Second, it can
bring with it a cousin of omitted variable bias, where we are typically
concerned with correlations between observed variables and unobserved
ones. Here, when regularization excludes some variables, even a
correlation between observed variables and other observed (but excluded)
ones can create bias in the estimated coefficients</p>
<p>(p12) Some econometric results also show the converse: when there is
structure, it will be recovered at least asymptotically (for example,
for prediction consistency of LASSO-type estimators in an approximately
sparse linear framework, see Belloni, Chernozhukov, and Hansen 2011).</p>
<p>(p12) -unrealistic for micro economic applications Zhao and Yu (2006)
who establish asymptotic model-selection consistency for the LASSO.
Besides assuming that the true model is -sparse“—only a few variables
are relevant-they also require the”irrepresentable condition" between
observables: loosely put, none of the irrelevant covariates can be even
moderately related to the set of relevant ones. In practice, these
assumptions are strong.</p>
<p>(p13) Machine learning can deal with unconventional data that is too
high-dimensional for standard estimation methods, including image and
language information that we conventionally had not even thought of as
data we can work with, let alone include in a regression</p>
<p>(p13) satellite data</p>
<p>(p13) they provide us with a large x vector of image-based data; these
images are then matched (in what we hope is a representative sample) to
yield data which form the y variable. This translation of satellite
images to yield measures is a prediction problem</p>
<p>(p13) particularly relevant where reliable data on economic outcomes are
missing, such as in tracking and targeting poverty in developing
countries (Blumenstock 2016</p>
<p>(p13) cell-phone data to measure wealth</p>
<p>(p13) Google Street View to measure block-level income in New York City
and Boston</p>
<p>(p13) online posts can be made meaningful by labeling them with machine
learning</p>
<p>(p14) extract similarity of firms from their 10-K business description
texts, generating new time-varying industry classifications for these
firms</p>
<p>(p14) and imputing even in traditional datasets. In this vein,
Feigenbaum (2015a, b) applies machine-learning classifiers to match
individuals in historical records</p>
<p>(p13) -the first prediction applications New Data</p>
<p>(p14) Prediction in the Service of Estimation</p>
<p>(p14) linear instrumental variables understood as a two-stage procedure</p>
<p>(p14) The first stage is typically handled as an estimation step. But
this is effectively a prediction task: only the predictions x- enter the
second stage; the coefficients in the first stage are merely a means to
these fitted values. Understood this way, the finite-sample biases in
instrumental variables are a consequence of overfitting</p>
<p>(p14) -ll overfitting. Overfitting means that the in-sample fitted
values x- pick up not only the signal -′z, but also the noise δ. As a
consequence, xˆ is biased towards x, and the second-stage instrumental
variable estimate - - is thus biased towards the ordinary least squares
estimate of y on x. Since overfit will be larger when sample size is
low, the number of instruments is high, or the instruments are weak, we
can see why biases arise in these cases</p>
<p>(p14) same techniques applied here result in split-sample instrumental
variables (Angrist and Krueger 1995) and -jackknife" instrumental
variables (Angrist, Imbens, and Krueger 1999)</p>
<p>(p15) -worth referencing In particular, a set of papers has already
introduced regularization into the first stage in a high-dimensional
setting, including the LASSO (Belloni, Chen, Chernozhukov, and Hansen
2012) and ridge regression (Carrasco 2012; Hansen and Kozbur 2014). More
recent extensions include nonlinear functional forms, all the way to
neural nets (Hartford, Leyton-Brown, and Taddy 2016</p>
<p>(p15) Practically, even when there appears to be only a few instruments,
the problem is effectively high-dimensional because there are many
degrees of freedom in how instruments are actually constructed</p>
<p>(p15) -a note of caution It allows us to let the data explicitly pick
effective specifications, and thus allows us to recover more of the
variation and construct stronger instruments, provided that predictions
are constructed and used in a way that preserves the exclusion
restriction</p>
<p>(p15) -this seems similar to my idea of regularising on a subset
Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016) take
care of high-dimensional controls in treatment effect estimation by
solving two simultaneous prediction problems, one in the outcome and one
in the treatment equation</p>
<p>(p15) the problem of verifying balance between treatment and control
groups (such as when there is attrition</p>
<p>(p15) -! Or consider the seemingly different problem of testing for
effects on many outcomes. Both can be viewed as prediction problems
(Ludwig, Mullainathan, and Spiess 2017). If treatment assignment can be
predicted better than chance from pretreatment covariates, this is a
sign of imbalance. If treatment assignment can be predicted from a set
of outcomes, the treatment must have had an effect</p>
<p>(p15) prediction task of mapping unit-level attributes to individual
effect estimates</p>
<p>(p15) Athey and Imbens (2016) use sample-splitting to obtain valid
(conditional) inference on</p>
<p>(p16) treatment effects that are estimated using decision trees,</p>
<p>(p16) -look into the implication for treatment assignment with
heterogeneity heterogenous treatment effects can be used to assign
treatments; Misra and Dub- (2016) illustrate this on the problem of
price targeting, applying Bayesian regularized methods to a large-scale
experiment where prices were randomly assigned</p>
<p>(p16) -caveat Suppose the algorithm chooses a tree that splits on
education but not on age. Conditional on this tree, the estimated
coefficients are consistent. But that does not imply that treatment
effects do not also vary by age, as education may well covary with age;
on other draws of the data, in fact, the same procedure could have
chosen a tree that split on age instead</p>
<p>(p16) Prediction in Policy</p>
<p>(p16) -no .. can we predict who will gain most from admission? but even
if we can what can we report? Prediction in Policy</p>
<!--chapter:end:ml-and-mlreading-group/control_prediction_ml.Rmd-->
</div>
</div>
<div id="caus_inf_obs" class="section level1 unnumbered">
<h1 class="unnumbered"><strong>CAUSAL INFERENCE THROUGH OBSERVATION</strong></h1>
</div>
<div id="iv_limitations" class="section level1" number="6">
<h1 number="6"><span class="header-section-number">6</span> Causal inference: IV (instrumental variables) and its limitations</h1>
<div id="some-casual-discussion" class="section level3 unnumbered">
<h3 class="unnumbered">Some casual discussion</h3>
<p>Ben Balmford:</p>
<blockquote>
<p>… IV really is pretty unreliable</p>
</blockquote>
<blockquote>
<p>So <span class="citation">(<a href="#ref-ConsistencyInferenceInstrumental" role="doc-biblioref"><span>“Consistency Without <span>Inference</span>: <span>Instrumental Variables</span> in <span>Practical Application</span>* - <span>Google Search</span>”</span> n.d.</a>)</span> <a href="http://scholar.google.co.uk/scholar_url?url=https://uh-ir.tdl.org/bitstream/handle/10657/4338/2019_ConsistencyWithoutInferenceInstrumentalVariables.pdf%3Fsequence%3D1&amp;hl=en&amp;sa=X&amp;scisig=AAGBfm05pDW1C-mrVX9Sdp5WUPHeqmXSdQ&amp;nossl=1&amp;oi=scholarr">Young, 2019</a> by Young highlights that the point estimates seem to not differ that much from OLS estimates…
which i think suggests that IV is not (generally) dealing with the problem of OVB that it claims to.</p>
</blockquote>
<p><br />
</p>
<p>DR: My concerns tend to be more:</p>
<ol style="list-style-type: decimal">
<li><p>With heterogeneity IV is not capturing the <em>average</em> effect, and often finds results that are very much driven by a particular subsample, often accentuating the supposed bias</p></li>
<li><p>Many IV’s are not defensible and testing the identifying assumptions is basically impossible</p></li>
</ol>
<p><br />
</p>
<p>BB:</p>
<blockquote>
<p>And then <span class="citation">(<a href="#ref-brodeurMethodsMatterPhacking2018" role="doc-biblioref">Brodeur, Cook, and Heyes 2018</a>)</span> (<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3249910">link</a>) is I think quite a neat comparison of IV and other methods. The general point being that IV seems to have more publication bias than the other approaches to causal inference. Not sure that is that much of a surprise. In my chat with one of the authors he pointed out that in IV any changes are less obvious - in DiD (which doesnt perform great either)/RD the assumptions are much clearer to visualise on a graph. I also think that - in kind of a related way - there are many more places where there is [researcher] discretion in the IV approach.</p>
</blockquote>
<div class="marginnote">
<p>BB: Whereas in DiD (which doesn’t perform great either)/RD the its really easy to just graph the basic data and you can kind of eyeball that to see if there is any effect and if the assumptions are met (I actually think this kind of speaks somewhat for the need to really be able to test the assumptions at the heart of DiD and RD - it’s so hard to provide a valid statistical test that just the fact we can eye-ball test the assumptions is useful). DR: Don’t always trust your eyeballs!</p>
</div>
<p><br />
</p>
</div>
<div id="instrument-validity" class="section level2" number="6.1">
<h2 number="6.1"><span class="header-section-number">6.1</span> Instrument validity</h2>
<ul>
<li>Exogeneity vs. exclusion</li>
<li>Very hard to ‘powerfully test’</li>
</ul>
<p>IV not credible? Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a <em>direct</em> effect on the outcome (only an indirect effect through the endogenous variable</p>
</div>
<div id="heterogeneity-and-late" class="section level2" number="6.2">
<h2 number="6.2"><span class="header-section-number">6.2</span> Heterogeneity and LATE</h2>
<p><em>Basic consideration: what does IV identify and when:</em>?</p>
<p>Focusing on a binary endogenous ‘treatment’ variable</p>
<ul>
<li>With heterogeneity</li>
<li>With imperfect compliance</li>
<li>With one-way compliance</li>
</ul>
</div>
<div id="weak-instruments-other-issues" class="section level2" number="6.3">
<h2 number="6.3"><span class="header-section-number">6.3</span> Weak instruments, other issues</h2>
<ul>
<li>With a ‘weak instrument’ … why does that matter?</li>
</ul>
</div>
<div id="instrumenting-interactions" class="section level2" number="6.4">
<h2 number="6.4"><span class="header-section-number">6.4</span> Instrumenting Interactions</h2>
<p>In randomized controlled trials it sometimes arises the issue that one is particularly interested in a potentially endogeneous explanatory variable and its interaction with the experimental treatments, that are exogenous by construction. From the stata list and mostly harmless econometrics <a href="http://www.mostlyharmlesseconometrics.com/2010/02/multiple-endogenous-variables-what-now/"></a>
&gt;If you have <span class="math inline">\(X_1\)</span> endogenous and an exogenous instrument <span class="math inline">\(Z\)</span> , and your model includes two terms involving <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_1 \times X_2\)</span>, then you should use two instruments: <span class="math inline">\(Z\)</span>, and <span class="math inline">\(Z \times X_2\)</span>.</p>
</div>
<div id="reference-to-the-use-of-iv-in-experimentsmediation" class="section level2" number="6.5">
<h2 number="6.5"><span class="header-section-number">6.5</span> Reference to the use of IV in experiments/mediation</h2>
<!--chapter:end:causal_inference_general_notes/iv-issues.Rmd-->
</div>
</div>
<div id="causal-inference-other-paths-to-observational-identification" class="section level1" number="7">
<h1 number="7"><span class="header-section-number">7</span> <span id="other_paths">Causal inference: Other paths to observational identification</span></h1>
<div id="fixed-effects-and-differencing" class="section level2" number="7.1">
<h2 number="7.1"><span class="header-section-number">7.1</span> Fixed effects and differencing</h2>
</div>
<div id="did" class="section level2" number="7.2">
<h2 number="7.2"><span class="header-section-number">7.2</span> DiD</h2>
<p>Key issue: FE/DiD does not rule out a correlated dynamic unobservable, causing a bias</p>
<p><br />
</p>
<p>Helpful links from a Twitter thread:</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
Ok, let’s say you’ve neither written nor refereed a Diff-in-Diff paper in the last two yeasts. What are the key methodological papers I need to brush up on? <a href="https://twitter.com/hashtag/EconTwitter?src=hash&amp;ref_src=twsrc%5Etfw">#EconTwitter</a><br><br>Didn’t someone put together a list?
</p>
— Damon Jones (nomadj1s) <a href="https://twitter.com/nomadj1s/status/1263225306275614722?ref_src=twsrc%5Etfw">May 20, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Some other resources and discussion (unfold)</p>

<div class="fold">
<p>BB:</p>
<p>From that Twitter thread i also found this really useful <a href="https://drive.google.com/drive/folders/14VgSFPsmv2H8ZngW-P2xEQE7g4WbWXfH">google drive folder</a> of recent developments.</p>
<p>This Freyaldenhoven <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20180609">paper</a> might also be handy. And it looks like there is quite a lot by Susan Athey that we should look at too.</p>
<p>Re Athey, she has a paper with Imbens (2006) which proposes the change in changes approach which allows estimation of hetergoenous treatment effects in a “better” way than quantile DiD. The assumptions of CiC are pretty similar to those of DiD.</p>
<p>That method (CiC) is applied by <a href="https://www.nber.org/papers/w25636">this Assuncao paper</a> to the Amazon. We actually looked at this paper at the journal club in LEEP and the point was raised that actually you could see the observed effect (reduced deforestation in targetted municipalities) simply owing to the fact there is very little other forest left to harvest.</p>
<p>On DiD more generally, I think it suffers from the flaw that we have to somehow establish pre-treatment parallel trends which is basically an issue of ‘accepting a null hypothesis’ which is clearly not possible with traditional statistical inference.</p>
<p>DR: Yes, the problem of <a href="#robust-diag">diagnostic testing</a> the assumption that the identification entirely relies on. Does CiC suffer from the same issue?</p>
</div>
</div>
<div id="rd" class="section level2" number="7.3">
<h2 number="7.3"><span class="header-section-number">7.3</span> RD</h2>
</div>
<div id="time-series-ish-panel-approaches-to-micro" class="section level2" number="7.4">
<h2 number="7.4"><span class="header-section-number">7.4</span> Time-series-ish panel approaches to micro</h2>
<p>See esp:</p>
<ul>
<li><p>Wooldridge on distinct structures in panel data, First differences vs Fixed effects, etc.</p></li>
<li><p>Nickell bias and Arellano-Bond and related procedures</p></li>
<li><p>Diagnosing structural breaks… outside of Macroeconomics</p></li>
</ul>
<div class="marginnote">
<p>Ben: Ferraro has a couple of paper which together i think basically just caution against just accepting panel regression approaches on the basis that the covariates are matched well. They are <a href="https://www.journals.uchicago.edu/doi/10.1086/689868">here</a> and <a href="https://www.sciencedirect.com/science/article/pii/S016517651630489X">here</a></p>
</div>
<div id="lagged-dependent-variable-and-fixed-effects-nickel-bias" class="section level3" number="7.4.1">
<h3 number="7.4.1"><span class="header-section-number">7.4.1</span> Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’</h3>
</div>
<div id="apc-effects" class="section level3" number="7.4.2">
<h3 number="7.4.2"><span class="header-section-number">7.4.2</span> Age-period-cohort effects</h3>
<p>From <a href="https://www.publichealth.columbia.edu/research/population-health-methods/age-period-cohort-analysis">Columbia Public health</a>, in the context of epidemiology:</p>
<blockquote>
<p><strong>Age effects</strong> are variations linked to biological and social processes of aging specific to individuals … unrelated to the time period or birth cohort to which an individual belongs.</p>
</blockquote>
<p>We might want to further disentangle this (at least conceptually) into:</p>

<div class="fold">
<ul>
<li><p>overall-age effects: effect of an individual, organization, or any “unit’s” age in years <em>vs</em></p></li>
<li><p>‘tenure effects’: the effect of time spent participating (e.g., time in the labor force or as part of a political movement)</p></li>
</ul>
</div>
<blockquote>
<p><strong>Period effects</strong> result from external factors that equally affect all age groups at a particular calendar time. … [e.g.] environmental, social and economic factors, e.g. war, famine, economic crisis.</p>
</blockquote>
<blockquote>
<p><strong>Cohort effects</strong> are variations resulting from the unique experience/exposure of a group of subjects (cohort) as they move across time. The most commonly defined group in epidemiology is the birth cohort based on year of birth and it is described as difference in the risk of a health outcome based on birth year.</p>
</blockquote>
<blockquote>
<p>“identification problem” in APC… s due to the exact linear dependency among age, period, and cohort: Period – Age = Cohort; that is, given the calendar year and age, one can determine the cohort (birth year)</p>
</blockquote>
<p><a href="https://www.nuffield.ox.ac.uk/economics/Papers/2014/nielsen_apc_package.pdf">This apc package writeup looks promising both as a tool and an explanation</a></p>
<p><br />
</p>
<p>A standard APC model of the age-period-specific statistic (e.g., the average income) for age group <span class="math inline">\(i\)</span> in period <span class="math inline">\(j\)</span>: (O’Brien et al, 2008)</p>
<p><span class="math display">\[Y_{ij} = \mu  + \alpha_i + \pi_j + \chi_k + \epsilon_{ij} \]</span>
They refer to the “effect of the ith age group” <span class="math inline">\(\alpha_i\)</span> but this need not be interpreted causally.</p>
<p><span class="math inline">\(\pi_j\)</span> captures period <span class="math inline">\(j\)</span>, <span class="math inline">\(\chi_k\)</span> the kth cohort <span class="math inline">\(\mu\)</span> the intercept, and <span class="math inline">\(\epsilon_{ij}\)</span> the
“random disturbance.”</p>
<blockquote>
<p>[The] linear dependency among the variables representing age, period, and cohort, since knowing the values of any two of these variables allows one to know the value of the third … means that we cannot estimate the A – 1 age effects, the P − 1 period effects, and the A + P – 1 cohort effects in a single OLS model.</p>
</blockquote>
<p>(I believe) this holds whether or not we restrict the age, period, or cohort effects to being linear in a time variable.</p>
<p><br />
</p>
<p>The above equation also depicts the “age–period–cohort-characteristic (APCC)” model, letting <span class="math inline">\(\chi_k\)</span> represent “the value of [here, a single] cohort characteristic for the kth cohort.”
In other words, we replace a set of simple cohort dummies with (I presume) a variable meant to capture the meaningful characteristic of the cohort leading to the aforementioned ‘cohort effects.’</p>
<p>" This formulation eliminates the linear dependency … since the cohorts’ values on the cohort characteristic are unlikely to be perfectly linearly related to the age and period."</p>
<p><br />
</p>
<p>They next depict an ‘age-period’ model:</p>
<p><span class="math display">\[Y_{ij} = \mu + \alpha_i + \pi_j + \epsilon_{ij}\]</span></p>
<p>Estimating this model leads to the ‘observed residuals’ <span class="math inline">\(e_{ij}\)</span></p>
<blockquote>
<p>We could then examine the pattern of these residuals to see if they indicate an effect of cohort membership (after controlling for the effects of age and period).</p>
</blockquote>
<!--chapter:end:causal_inference_general_notes/other_paths.Rmd-->
</div>
</div>
</div>
<div id="causal-paths-and-levels-of-aggregation-1" class="section level1 unnumbered">
<h1 class="unnumbered"><strong>CAUSAL PATHS AND LEVELS OF AGGREGATION</strong></h1>
</div>
<div id="mediators" class="section level1" number="8">
<h1 number="8"><span class="header-section-number">8</span> Mediation modeling and its massive limitations</h1>
<div id="mediators-and-selection-and-roy-models-a-review-considering-two-research-applications" class="section level2" number="8.1">
<h2 number="8.1"><span class="header-section-number">8.1</span> Mediators (and selection and Roy models): a review, considering two research applications</h2>
<div class="marginnote">
<p>Originally focused on issues relevant to Parey et al project on ‘returns to HE institution’ using data from the Netherlands (flagged as @NL); also relevant to Reinstein et al work on substitution in charitable giving (flagged as @subst).</p>
</div>
</div>
<div id="dr-initial-thoughts-for-nl-education-paper" class="section level2" number="8.2">
<h2 number="8.2"><span class="header-section-number">8.2</span> DR initial thoughts (for NL education paper)</h2>
<div class="marginnote">
Here were my initial thoughts as pertaining to our paper on the returns to university.
</div>
<p>Suppose we observe treatment <span class="math inline">\(T\)</span> (e.g., ‘allowed to enter first-choice institution and course’),</p>
<p>intermediate outcome <span class="math inline">\(M\)</span> (e.g., completion of degree in first-choice course and institution),</p>
<p>and final outcome <span class="math inline">\(Y\)</span> (e.g., lifetime income.)<br />
</p>
<p><em>Alternately, in the “substitution between charities” (@subst) context… (unfold)</em></p>

<div class="fold">
<p>The treatment <span class="math inline">\(T\)</span> is</p>
<ol style="list-style-type: decimal">
<li><p>‘asked to donate in the first round’ (in Reinstein, Riener and Vance-McMullen, henceforth ‘RRV’ experiments, and perhaps in Schmitz 2019)’,</p></li>
<li><p>a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?),</p></li>
<li><p>the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others),</p></li>
</ol>
<p>the intermediate outcome <span class="math inline">\(M\)</span> is the amount given to that (first-round) charity,</p>
<p>and the final outcome <span class="math inline">\(Y\)</span> is the amount given to that charity (or other charities) in round 2 (experiments “3”: other charities in that round ).</p>
</div>
<p>The treatment <span class="math inline">\(T\)</span> (may) directly affect the final outcome <span class="math inline">\(Y\)</span>.</p>
<div class="marginnote">
<p>Do: show a diagram here</p>
</div>
<p><span class="math display">\[T\rightarrow Y\]</span></p>
<p><br />
</p>
<p><span class="math inline">\(T\)</span> also may affect an intermediate outcome <span class="math inline">\(M\)</span>.</p>
<p><span class="math display">\[T \rightarrow M\]</span></p>
<p>The intermediate outcome also may affect the final outcome <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[M \rightarrow Y\]</span></p>
<p><br />
</p>
<p>With exogenous variation in <span class="math inline">\(T\)</span> <em>and</em> <span class="math inline">\(M\)</span> (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions.</p>
<p>With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of <span class="math inline">\(T\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>We could also compute the share of this effect that occurs <em>via</em> the intermediate effect, i.e., <span class="math inline">\(T \rightarrow M\rightarrow Y\)</span>.
This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients.</p>
<p><br />
</p>
<p>However, there are two major challenges to this estimation.</p>
<ol style="list-style-type: decimal">
<li><p>We (may) have a valid instrument for (exogenous variation in) <span class="math inline">\(T\)</span> only, and <span class="math inline">\(M\)</span> may arise through a process involving selection on unobserved variables.</p></li>
<li><p>Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects.</p></li>
</ol>
<p>Thus we consult the relevant literature, discussed below.</p>
<p>The most influential paper in Economics has been <span class="citation">(<a href="#ref-Heckman2013" role="doc-biblioref">Heckman, Pinto, and Heckman 2013</a>)</span>.</p>
<p>It is cited in more recent applied work such as Fagereng, 2018 (unfold).</p>

<div class="fold">
<p>… We follow Heckman et al. (2013) and Heckman and Pinto (2015) in using mediation analysis. The goal of this analysis is to disentangle the average causal effect on outcomes that operate through two channels: a) Indirect effects arising from the effect of treatment on measured mediators, and b) direct effects that operate through channels other
than changes in the measured mediators (including changes in mediators that are not observed by the analyst and changes in the mapping between mediators and outcomes).</p>
<p>It is therefore necessary to assume that the mediators we do not
observe are uncorrelated with both <span class="math inline">\(\mathbf{X}\)</span> and the measured mediators for all values of <span class="math inline">\(D\)</span>.</p>
</div>
<p><br />
</p>
<p>Antonakis, coming from the Psychology and Leadership disciplines, considers the mediation question in a much simpler set of models.</p>
</div>
<div id="econometric-mediation-analyses-heckman-and-pinto" class="section level2" number="8.3">
<h2 number="8.3"><span class="header-section-number">8.3</span> Econometric Mediation Analyses (Heckman and Pinto)</h2>
<p><strong>Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs</strong></p>
<p><br />
</p>
<div id="relevance-to-parey-et-al" class="section level3 unnumbered">
<h3 class="unnumbered">Relevance to Parey et al</h3>
<p>We have an instrument for admission to one’s first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these ‘intermediate outcomes,’ including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these.</p>
<div class="marginnote">
<p>a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well</p>
</div>
</div>
<div id="summary-and-key-modeling" class="section level3" number="8.3.1">
<h3 number="8.3.1"><span class="header-section-number">8.3.1</span> Summary and key modeling</h3>
<p>There is a ‘production function’</p>
<ul>
<li><p>cf income as a function of human capital, opportunities, etc.</p></li>
<li><p>cf donation as a function of income, prices, mood, framing, etc.</p></li>
</ul>
<p><br />
</p>
<p>Treatments (e.g., RCTs) may affect outcomes through the following channels:</p>
<ol style="list-style-type: decimal">
<li>observable or proxied inputs</li>
</ol>
<ul>
<li><p>Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities</p></li>
<li><p>Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood<br />
</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>unobservable/unmeasured inputs</li>
</ol>
<ul>
<li>cf human capital, social connections</li>
<li>cf unobservable generosity, wealth, or temporary mood</li>
</ul>
<p><br />
</p>
<ol start="3" style="list-style-type: decimal">
<li>the production function itself, the ‘map between inputs and outputs for treatment group members’</li>
</ol>
<ul>
<li><p>Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital ‘matter more’ at some institutions?</p></li>
<li><p>Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else?</p></li>
</ul>
<p>If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs.</p>
<blockquote>
<p>RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs ... [nor distinguish effects from changes in production functions].</p>
</blockquote>
<p><br />
</p>
<p>Here “we can test some of the strong assumptions implicitly invoked.”</p>
<p>“Direct effects” as commonly stated refer to the impact of both channels 2 and 3 above.</p>
<div class="marginnote">
<p>DR: Channel 2 isn’t really a direct effect imho (what was this?)</p>
</div>
<p><br />
</p>
<p><strong>Standard potential outcomes framework:</strong></p>
<p><span class="math display">\[Y=DY_{1}+(1-D)Y_{0}\]</span></p>
<p><span class="math display">\[ATE=E(Y_{1}-Y_{0})\]</span></p>
<p><strong>Production function</strong></p>
<p><span class="math display">\[Y_{d}=f_{d}(\mathbf{\mathbf{{\theta}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}\]</span></p>
<p>... the function under treatment <span class="math inline">\(d\)</span>; of proxied and unobserved inputs that occur under state <span class="math inline">\(d\)</span>, and baseline variables.<br />
</p>
<p>The production function implies:</p>
<p><span class="math display">\[ATE=E\Big(f_{1}(\mathbf{\mathbf{{\theta}}}_{1}^{p},\mathbf{\mathbf{{\theta}}}_{1}^{u},\mathbf{{X}})-f_{0}(\mathbf{\mathbf{{\theta}}}_{0}^{p},\mathbf{\mathbf{{\theta}}}_{0}^{u},\mathbf{{X}})\Big)\]</span><br />
</p>
<p>We also consider counterfactual outputs, fixing treatment status and
proxied inputs:</p>
<p><span class="math display">\[Y_{d,\bar{\theta_{d}}^{p}}=f_{d}(\mathbf{\mathbf{{\bar{{\theta}}}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}\]</span><br />
</p>
<p>This allows us to decompose (‘as in the mediation literature’):</p>
<p><span class="math display">\[ATE(d)=IE(d)+DE(d)\]</span></p>
<ul>
<li><p><em>IE, Indirect effect</em>: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment
statuses)</p></li>
<li><p><em>DE, Direct effect</em>: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed
at one of the two treatment statuses)<br />
</p>
<p><br />
</p></li>
</ul>
<p>HP further decompose the direct effect into:</p>
<ul>
<li><p><span class="math inline">\(DE&#39;(d,d&#39;)\)</span>: The impact of letting the treatment vary the map only
(fixing the rest at one of the two appropriate values)</p></li>
<li><p><span class="math inline">\(DE&#39;&#39;(d,d&#39;)\)</span>: The impact of letting the treatment vary the
unmeasured inputs only (fixing the rest at one of the two
appropriate values)</p></li>
</ul>
<p>They use this to give two further ways of decomposing the ATE.<br />
</p>
</div>
<div id="common-assumptions-and-their-implications" class="section level3" number="8.3.2">
<h3 number="8.3.2"><span class="header-section-number">8.3.2</span> Common assumptions and their implications</h3>
<p>“The <u>standard literature</u> on mediation analysis in psychology regresses outputs on mediator inputs” ... often adopts the strong assumptions of:</p>
<ol style="list-style-type: decimal">
<li>no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy)
and</li>
</ol>
<div class="marginnote">
<p>Cf ‘winning institution’ impacts human capital, social networks,
etc identically for everyone; e.g., not a greater effect for men
then for women, nor a greater effect for those entering particular
specializations.</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>full invariance of the production function: <span class="math inline">\(f_{1}=f_{0}\)</span>.</li>
</ol>
<p>... which implies <span class="math inline">\(Y_{d}=f(\mathbf{\theta}_{d}^{p},d,\mathbf{X})\)</span>.<br />
</p>
<p><u>Sequential ignorability (Imai et al, 10, ’11)</u>:
Essentially, independent randomization of both treatment status and measured inputs.</p>
<div class="marginnote">
<p>Cf ‘winning institution’ does not effect the specialization
entered nor the location of residence, nor are both determined by a
third factor.</p>
</div>
<p><br />
</p>
<p><em>This sentence is hard to follow:</em></p>
<blockquote>
<p>In other words, input <span class="math inline">\(\theta_{d&#39;}^{p}\)</span> is statistically independent of potential outputs when treatment is fixed at <span class="math inline">\(D=d\)</span> and measured inputs are fixed at <span class="math inline">\(\bar{\theta_{d&#39;}^{p}}\)</span> conditional on treatment assignment <span class="math inline">\(D\)</span> and same preprogram characteristics <span class="math inline">\(X\)</span>.</p>
</blockquote>
<p><br />
</p>
<p>This assumption yields the ‘mediation formulas’:</p>
<span class="math display">\[\begin{aligned}
E(IE(d)|X)= &amp; \int E(Y|\theta^{p}=t,D=d,X)\underbrace{\Big(dF_{(\theta^{p}|D=1,X)}(t)-dF_{(\theta^{p}|D=1,X)}(t)\Big)}_{{\text{Difference in distribution of proxy inputs}}} &amp; (9)\\
E(DE(d)|X)= &amp; \int\underbrace{\Big(E(Y|\theta^{p}=t,D=1,X)-E(Y|\theta^{p}=t,D=0,X)\Big)}_{\text{Dfc in expectations (unobservables, function) between treatments given proxy inputs }}expe\underbrace{{dF_{(\theta^{p}|D=1,X)}(t)}}_{\text{Distn proxy inputs for D=1}} &amp; (10)
\end{aligned}\]</span>
<p><em>(??F is presumably the distribution over the observables; where did the
unobservables go? They are in the expectations, I guess.)</em><br />
<u>Difference from RCT</u></p>
<p><em>What RCT doesn’t do:</em></p>
<blockquote>
<p>[sequential ignorability] translates into ... <u>no confounding
effects</u> on both treatments and measured inputs ... does
not follow from a randomized assignment of treatment ...[which]
ensures independence between treatment status and counterfactual
inputs/outputs ... [but <em>not</em>] between proxied inputs
<span class="math inline">\(\theta_{d}^{p}\)</span> and unmeasured inputs <span class="math inline">\(\theta_{d}^{u}\)</span>. [Thus <em>not</em>
between counterfactual outputs and measured inputs is assumed in
condition (ii).]</p>
</blockquote>
<p>Cf, randomizing ‘win first-choice institution’ does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution.</p>
<p><em>What RCT</em> <em><u>does</u></em> <em>do:</em></p>
<p>RCT ensures “independence between treatment status and counterfactual
inputs/outputs,” thus identifying ’treatment effects for proxied inputs
and for outputs.</p>
<p>CF, we can identify the impact of the treatment ‘win first chosen
institution’ on proxied input like ‘enters a specialization’ and on
outputs like ‘income in observed years.’</p>
<p><br />
</p>
</div>
</div>
<div id="pinto-2015-selection-bias-in-a-controlled-experiment-the-case-of-moving-to-opportunity" class="section level2" number="8.4">
<h2 number="8.4"><span class="header-section-number">8.4</span> Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity</h2>
<div id="summary" class="section level3 unnumbered">
<h3 class="unnumbered">Summary</h3>
<ul>
<li><p>... 4000+ families targeted, incentive to relocate from projects to
better neighbourhoods.</p></li>
<li><p>Easy to identify impact of vouchers</p></li>
<li><p>Challenge (here) is to assess impact of <em>neighborhoods</em> on outcomes.</p></li>
<li><p>Method here to decompose the TEOT into unambiguously interpreted
effects. Method applicable to ‘unordered choice models with
categorical instrumental variables and multiple treatments’</p></li>
<li><p>Finds significant causal effect on labour market outcomes</p></li>
</ul>
</div>
<div id="relevance-to-parey-et-al-1" class="section level3 unnumbered">
<h3 class="unnumbered">Relevance to Parey et al</h3>
<ol style="list-style-type: decimal">
<li><p>We also have an instrument (DUO lottery numbers) cleanly identifying
the effect of the ‘opportunity to do something’ (in our case, to
enter the course at your preferred institution). However, we also
want to measure the impact of choices ‘encouraged’ by the
instrument, such as (i) attending the first choice course and
institution and (ii) completing this course. We also deal with
unordered choices (i. enter course and institution, enter course at
other institution, enter other course at institution, enter neither)
(ii. choice of medical specialisation)</p></li>
<li><p>The geographic outcome is relevant to our second paper (impact on
‘lives close to home’)</p></li>
</ol>
</div>
<div id="introduction-2" class="section level3 unnumbered">
<h3 class="unnumbered">Introduction</h3>
<p>The causal link between neighborhood characteristics and resident’s
outcomes has seldom been assessed.</p>
<p><strong>Treatments:</strong></p>
<ul>
<li><p>Control (no voucher)</p></li>
<li><p>Experimental: could use voucher to lease in low-poverty neighborhood</p></li>
<li><p>Section 8: Could use voucher in any () neighborhood</p></li>
</ul>
<p><em>Many papers evaluate the ITT or TOT effects of MTO.</em></p>
<ul>
<li><p>ITT: effect of being <em>offered</em> voucher</p>
<ul>
<li>estimated as difference in average outcome of experimental vs control families</li>
</ul></li>
<li><p>TOT: effect for ‘voucher compliers’ (assuming no effect of simply
being <em>offered</em> voucher on those who don’t use it)</p>
<ul>
<li>estimated as ITT/compliance rate</li>
</ul></li>
</ul>
<blockquote>
<p>[ITT and TOT] are the most useful parameters to investigate the
effects of <em>offering</em> [EA] rent subsidising vouchers to families.</p>
</blockquote>
</div>
<div id="identification-strategy-brief" class="section level3 unnumbered">
<h3 class="unnumbered">Identification strategy brief</h3>
<ul>
<li>Vouchers as IVs for choice among 3 neighborhood alternatives (no
relocation, relocate bad, relocate good)</li>
</ul>
<div class="marginnote">
<p>Cf @NL: enter course and
fp-institution, enter course at other institution, do not enter
course</p>
</div>
<ul>
<li><p>Neighborhood causal effects as difference in counterfactual outcomes
among 3 categories</p></li>
<li><p>Challenge: “MTO vouchers are insufficient to identify the expected
outcomes for all possible counterfactual relocation decisions”</p>
<ul>
<li>... “compliance with the terms of the program was highly
selective [Clampet-Lundquist and M, 08]”</li>
</ul></li>
<li><p>Solution: Uses theory and ‘tools of causal inference. Invokes SARP
to identify ’set of counterfactual relocation choices that are
economically justifiable’</p></li>
<li><p><u>Identifying assumption</u>: “the overall quality of the
neighborhood is not directly caused by the unobserved family
variables even though neighborhood quality correlates with these
unobserved family variables due to network sorting”</p></li>
<li><p>‘Partition sample ... into unobserved subsets associated with
economically justified counterfactual relocation choices and
estimate the causal effect of neighborhood relocation conditioned on
these partition sets.’ [<em>what does this mean?</em>]</p></li>
</ul>
</div>
<div id="results-in-brief" class="section level3 unnumbered">
<h3 class="unnumbered">Results in brief</h3>
<p>“Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes ... 65% higher than the TOT effect for adult earnings.”</p>
</div>
<div id="framework-first-for-binarybinary-simplification" class="section level3 unnumbered">
<h3 class="unnumbered">Framework: first for binary/binary (simplification)</h3>
<p><strong>First, for binary outcomes (simplified)</strong></p>
<p><span class="math inline">\(Z_{\omega}\)</span>: whether family <span class="math inline">\(\omega\)</span> receives a voucher <em>(cf
institution-winning lottery number)</em></p>
<p><span class="math inline">\(T_{\omega}\)</span>: whether family <span class="math inline">\(\omega\)</span> relocates (<em>cf enters first choice
institution and course)</em><br />
</p>
<p><strong>Counterfactuals</strong></p>
<ul>
<li><p><span class="math inline">\(T_{\omega}(z)\)</span>: relocation decision <span class="math inline">\(\omega\)</span> would choose if it had
been assigned voucher <span class="math inline">\(z\in{0,1}\)</span>’: vector of potential relocation
decisions (<em>cf education choices)</em> for each voucher assignment (<em>cf
lottery number)</em></p>
<ul>
<li>Can partition into never-takers, compliers, always takers, and
defiers</li>
</ul></li>
<li><p><span class="math inline">\((Y_{\omega}(0);Y_{\omega}(1\)</span>)): (Potential counterfactual) outcomes
(<em>cf income, residence, etc</em>) when relocation decision is fixed at 0
and 1, respectively</p></li>
</ul>
<p><strong>Key ( standard) identification assumption: instrument independent of
counterfactual variables</strong></p>
<p><span class="math display">\[(Y_{\omega}(0),Y_{\omega}(1),T_{\omega}(0),T_{\omega}(1))\perp Z_{\omega}\]</span></p>
<p><strong>Standard result 1: ITT</strong></p>
<p><span class="math display">\[\begin{aligned}
ITT=E(Y_{\omega}|Z_{\omega}=1)-(Y_{\omega}|Z_{\omega}=0)\\
=E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]&#39;)P(S_{\omega}=[0,1])+E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[1,0]&#39;)P(S_{\omega}=[0,1])\end{aligned}\]</span></p>
<p>i.e., ITT computation yields the sum of the ‘causal effect for
compliers’ and the ’causal effect for defiers, weighted by the
probability of each.</p>
<p><strong>Standard result 2: LATE</strong></p>
<p><span class="math display">\[\begin{aligned}
LATE=\frac{{ITT}}{P(T_{\omega}=1|Z_{\omega}=1)-P(T_{\omega}=1|Z_{\omega}=0)}= &amp;  &amp; E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]&#39;)\\
if\:P(S_{\omega}=[0,1])=0\end{aligned}\]</span></p>
<p>i.e., the LATE, computed as the ITT divided by the ‘first stage’ impact
of the instrument, is the causal effect for compliers if there are no
defiers.</p>
</div>
<div id="framework-for-mto-multiple-treatment-groups-multiple-choices" class="section level3 unnumbered">
<h3 class="unnumbered">Framework for MTO multiple treatment groups, multiple choices</h3>
<ul>
<li><p><span class="math inline">\(Z_{\omega}\in\{z_{1,}z_{2,}z_{3}\}\)</span> for no voucher, experimental
voucher, and section 8 voucher, respectively</p></li>
<li><p><span class="math inline">\(T_{\omega}\in\{1,2,3\}\)</span> ... no relocation, low poverty
neighborhood relocation, high poverty relocation</p></li>
<li><p><span class="math inline">\(T_{\omega}(z)\)</span>: relocation decision for family <span class="math inline">\(\omega\)</span> if assigned
voucher <span class="math inline">\(z\)</span></p></li>
</ul>
<p><span class="math inline">\(\rightarrow\)</span> Response type for each family <span class="math inline">\(\omega\)</span> is a three-dimensional vector:</p>
<p><span class="math display">\[S_{\omega}=[T_{\omega}(z_{1}),T_{\omega}(z_{2}),T_{\omega}(z_{3})]\]</span>.</p>
<p><span class="math inline">\(\rightarrow\)</span></p>
<p><strong>ITT</strong> computation now measures a weighted sum of effects across a subset of those response types whose responses vary between the assignments being compared.<br />
<br />
</p>
<p><em>Cf:</em></p>
<ul>
<li><p>Considering the ‘treatments’: ‘1: enter other course at fp-inst, ’2:
enter course at fp-inst,’ ‘3: enter course at non-fp inst’</p>
<ul>
<li>(I ignore other course at other institution for now)</li>
</ul></li>
<li><p>Looking among those who won the course lottery (so we have a binary
instrument: wininst <span class="math inline">\(Z_{\omega}\in{0,1\}}\)</span></p></li>
<li><p>Our reduced-form estimates (regressions on the ‘lottery number wins
institution’ dummy) measures the probablility-weighted sum of:</p>
<ul>
<li><p>impact of institution within course ($T_{}=$2 versus 3);
for those who would ‘fully comply’ (enter course at institution
if <span class="math inline">\(Z_{\omega}=1\)</span>, enter course at other institution if 0)</p></li>
<li><p>impact of the course at fp-institution versus second-best course
at fp-institution for ‘institution-loving’ noncompliers; those
who would enter the course <em>only</em> if they get the fp-institution
and otherwise another course at the same institution</p></li>
<li><p>effects for perverse defiers</p></li>
</ul></li>
</ul>
</div>
</div>
<div id="antonakis-approaches" class="section level2" number="8.5">
<h2 number="8.5"><span class="header-section-number">8.5</span> Antonakis approaches</h2>
<p>Insert notes here</p>
<!--chapter:end:mediation/mediators_lit_pinto_etc.Rmd-->
</div>
</div>
<div id="selection_cop" class="section level1" number="9">
<h1 number="9"><span class="header-section-number">9</span> Selection, corners, hurdles, and ‘conditional on’ estimates</h1>
<div id="corner-solution-or-hurdle-variables-and-conditional-on-positive" class="section level2" number="9.1">
<h2 number="9.1"><span class="header-section-number">9.1</span> ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’</h2>
<p>“Conditional on positive”/“intensive margin” analysis ignores selection</p>
<p>“Conditional on positive”/“intensive margin” analysis ignores selection <em>identification issue</em> See <span class="citation">(<a href="#ref-angrist2008mostly" role="doc-biblioref">Joshua D. Angrist and Pischke 2008</a>)</span> on “Good CoP, bad CoP.” See also bounding approaches such as <span class="citation">(<a href="#ref-leeTrainingWagesSample2009" role="doc-biblioref">Lee 2009</a>)</span>.</p>
<p><br />
</p>
</div>
<div id="bounding-approaches-lee-manski-etc" class="section level2" number="9.2">
<h2 number="9.2"><span class="header-section-number">9.2</span> Bounding approaches (Lee, Manski, etc)</h2>
<p>See <a href="#notes_lee">Notes on Lee bounds</a></p>
<ol class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div id="notes-training-wages-and-sample-selection-estimating-sharp-bounds-on-treatment-effects-david-lee-2009-restud" class="section level3" number="9.2.1">
<h3 number="9.2.1"><span class="header-section-number">9.2.1</span> Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD</h3>
<p>Notes David Reinstein</p>
<div id="introduction-3" class="section level5" number="9.2.1.0.1">
<h5 number="9.2.1.0.1"><span class="header-section-number">9.2.1.0.1</span> Introduction</h5>
<blockquote>
<p>even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research</p>
</blockquote>
<ul>
<li><p>Intuitive trimming procedure for bounding average treatment effects in the presence of sample selection…</p></li>
<li><p>Requires neither exclusion restrictions nor a bounded support for the outcome of interest."</p></li>
<li><p>(Also) applicable to “nonrandom sample selection/attrition,” as well as to the ‘conditional on positive’/hurdle/mediation effect discussed here</p></li>
</ul>
<blockquote>
<p>analyses and evaluations typically focus on "reduced form impacts on total earnings, a first-order issue for cost-benefit analysis. Unfortunately, exclusively studying the effect on total earnings leaves open the question of whether any earnings gains are achieved through raising individuals hypothesis wage rates (price affects or hours of work (quantity effects).</p>
</blockquote>
<p><em>Important methodological point to constantly bring up:</em> “even a randomized experiment cannot guarantee the treatment and control individuals will be comparable conditional on being employed.”</p>
<p>Claims that standard “parametric or semi-parametric methods for correcting sample selection require exclusion restrictions that have little justification in this case.” Notes that most of the baseline variables could affect employment probabilities or have a direct impact on wage rates.</p>
<p><br />
</p>
<p><em>Summary of the method</em>: “…amounts to first identifying the excess number of individuals who were induced to be selected (employed) because of the treatment and then trimming the upper and lower tails of the outcome… distribution by this number, yielding a worst-case scenario bound.”</p>
<p>Uses same assumptions as in “conventional models for sample selection”</p>
<ol style="list-style-type: decimal">
<li><p>regressor of interest is independent of the errors in the outcome and selection models selection equations – this is ensured by random assignment.</p></li>
<li><p>“the selection equation can be written as a standard latent variable binary response model”</p></li>
</ol>
<p>– what meaningful restriction does this impose?</p>
<p>He proves this procedure “yields the tightest bounds for the average treatment effect that are consistent with the observed data.”</p>

<div class="note">
<p>The bounds estimator is shown to be <span class="math inline">\(\sqrt(n)\)</span> consistent and asymptotically normal with an intuitive expression for its asymptotic variance which depends on the variance of the trimmed outcome and the trimming threshold, an estimated quantifiable; (and an added term accounting for the estimation of which quantile to trim on)</p>
</div>
<p>Note for charity experiment (unfold) (@subst)</p>

<div class="fold">
<p>– <em>DR, Note, charity data: We can make confidence statements over the bounds themselves. Will this procedure be easy to bring into our code?</em>
– In our (charity) experiment we in fact do have upper bounds on the outcome variable. Could this yield even greater efficiency?</p>
</div>
<p>Note for the Netherlands data: (unfold, @NL)</p>

<div class="fold">
<p>it is not immediately clear how this could be adapted to instrumental variables; we shall see. Can we recover something meaningful from the reduced form model they are? Can it be applied to the (instrumental variables) estimates to disentangle the impact of changing courses from the impact of the institution itself?</p>
</div>
<p>In Lee’s paper, the estimate seems to give very narrow and informative bounds even though they have a great many people who do not earn any wages as a share of the population, about 54%. These are much narrower than the bounds proposed by Horowitz and M then what those bounds produce.</p>
<!-- ask Gerhard whether his Horowitz/Manski estimator incorporated the natural bounds on the outcome. -->
</div>
<div id="the-national-job-corps-study-and-sample-selection-prior-approaches" class="section level5" number="9.2.1.0.2">
<h5 number="9.2.1.0.2"><span class="header-section-number">9.2.1.0.2</span> The National Job Corps Study and Sample Selection [prior approaches]</h5>
<blockquote>
<p>In the experiment discussed here those in the control group were embargoed from the program for three years but could join afterwards, thus “when I use the phrase ‘effect of the program’ I am referring to this reduced-form treatment effect,” i.e., the intent to treat effect.</p>
</blockquote>
<p>– “some subpopulations were randomized into the program group with differing, but known probabilities. Thus analyzing the data requires the use of design weights.”</p>
<div class="marginnote">
<p><em>Note:</em> (\<span class="citation">(<a href="#ref-NL" role="doc-biblioref"><strong>NL?</strong></a>)</span>) this bears some resemblance to our Dutch data situation, and we can probably use examples from analyses of these programs. We can check their code against ours.</p>
</div>
<p>– Note also that they impute means of the baseline variables with their means; this seems to be an accepted practice.</p>
<div class="marginnote">
<p>Lee notes that he focuses exclusively on the “sample selection on wages caused by employment” and not the attrition/nonresponse problem, to focus attention on this, but they could have used it for the other as well.</p>
</div>
<p>– <em>DR:</em> (@NL) Note again that their desire to separate the employment hours and wage effects of the program is very similar to our desire to separate out different margins of the impact of winning an institution. …Namely the impact on completing a course or starting a course versus other impacts and the impact of entering a specialization versus remaining impacts. …Similar decompositions for the geography outcomes.</p>
<pre><code>– To do: check whether any papers cite Lee using an IV approach, extending the technique and the estimation of variance.</code></pre>
<hr />
<blockquote>
<p>“the problem of nonrandom sample selection is well-understood in the training literature; … may be one of the reasons why most evaluations of job-training programs focus on total earnings, including zeros for those without a job, rather than on wages conditional on employment” “of the 24 studies referenced in a survey … (Heckman et al.)… Most examine annual, quarterly, or monthly earnings without discussing the sample selection problem examining rage rates.”</p>
</blockquote>
<p>– <em>DR:</em> (@NL)Note that this is relevant to our question of whether to exclude zeros in log models, etc. While there will be less unemployment in our data, it still may be a relevant influence made have a strong effect on the estimates.</p>
<hr />
<p><strong>…previous conventional approaches to the sample selection problem (skip if desired).</strong> One may explicitly model the process determining selection, such as in Heckman (1979) …</p>
<p>Separate equations for the wage and the propensity to be employed, where employment occurs if the latter crosses a particular threshold, in which case a wage is observed. It is reasonable to think that the treatment variable can have effects on both terms..</p>
<p>“sample selection bias can be seen as specification error in the conditional expectation…”</p>
<p>The expected wage conditional on treatment exogenous variables and the selection into working (that is the underlying propensity to work variable exceeding zero) his status is equal to the true effect of the treatment an adjustment for the differences in the observable’s exogenous variables and a bias term representing the expectation of the idiosyncratic unobservables given the treatment and the exogenous variables exceeding the value necessary to induce work participation. The unobservable term needs to exceed the prediction based on the observable term for the entire term to exceed zero inducing labor force participation.</p>
<p>One may assume the data are missing at random, perhaps conditional on a set of covariates (Rubin, 1976; essentially assuming the error terms in each equation are independent of one another, here “employment status is unrelated to the determination of wages”… This “is strictly inconsistent with standard models of labor supply that account for the participation decision (Heckman, 1974).”</p>
<p>A more common assumption is that some exogenous variables “determine sample selection but do not have their own direct impact on the outcome of interest…. Exclusion restrictions are used in parametric and semi-parametric models…”</p>
<p>but “there may not exist credible ’instruments… excluded from the outcome equation”</p>
<hr />
<p>– <em>DR, aside:</em> We can return to (our) previous papers to impose these Lee bounds! One example would be the Siskel and Ebert your reviews paper and perhaps incorporating us with subsequent approaches, considering the “selection to review” equation.</p>
<hr />
<p><strong>Second approach “the construction of worst-case scenario bounds of the treatment effect”</strong></p>
<p>“Impute missing data with either the largest or smallest possible values to compute the largest and smallest possible treatment effects consistent with the data” as in Horwitz and Manski (2000a) who provide a general framework for this.</p>
<ul>
<li>Particularly useful with binary outcomes.</li>
</ul>
<p>This cannot be used when the support is unbounded. … note in their replication example they are actually using the equivalent of the bottom 5th percentile and the top 95th percentile. Strictly using a procedure would provide even wider bounds.</p>
<p>Lee considers his approach to be a hybrid of the two previous general approaches.</p>
<p>…end of section 2.. .a statement of the Horwitz upper bound for the treatment effect; very intuitive: “what if everyone in the treatment who dropped out would have had the largest possible wage and everyone in the control group that drop out would’ve had the smallest possible wage; this will give the upper bound.” Switching this the other way around will give a lower bound.</p>
<hr />
<p><em>DR, an aside thought:</em> (@NL) Something akin to the Horwitz and M approach (or maybe Lee bounds) could be applied to our issue of swapping into institutions directly. Suppose we only focus on those who <em>actually</em> complied: those assigned to an institution who also went to that institution. Our concern was that this would under-represent those who had particularly strong institutional preferences. Suppose you are interested in looking at the impact of winning the lottery (for once preferred institution) itself, as that was our most simply identifiable outcome.</p>
<p>… Let’s consider evaluating a treatment effect for those who happened to swap in. Those who swapped in might be assigned a counter-factual outcome of the lowest value of the lifetime income among those who did not get their institution of choice. Similarly, the small group who swapped out might be assigned a counterfactual outcome (had they no swapped out) representing the highest outcome value for those who did get their institution of choice. This should give us an upper bound on the treatment effect for these two groups of what we might call non-compliers. Making the opposite assumptions, precisely that those who swapped <em>into</em> their institution of choice would’ve had a very good counterfactual outcome (if they had not got their institution of choice) that comes from the highest outcomes for those who didn’t get their institution of choice (and also reversing this for those who swapped out of their preferred institution) would give us a lower bound for the treatment effect for this group. We can then combine these bounded treatment effects for these non-compliers with the treatment effect for the compliers to get a measure of the average treatment effect with bounds for this sort of behavior.</p>
<p>This will also allow us to come up with estimates with bounds <em>without</em> having to use the instrumental variable strategy which has issues of its own.</p>
</div>
<div id="section-3-identification-of-bounds-on-treatment-effects-the-main-meat-of-the-model" class="section level5" number="9.2.1.0.3">
<h5 number="9.2.1.0.3"><span class="header-section-number">9.2.1.0.3</span> Section 3: identification of bounds on treatment effects; the main meat of the model</h5>
<p>He starts with a simple example. He begins with a model with a treatment indicator and no other covariates, and a continuous outcome variable, but notes that this will clearly apply to discrete outcome variables and will also apply conditional on controls.</p>
<p>Nest, he brings forward the statement… from the earlier selection models. In each case the latent variable must overcome a hurdle for the outcome to be observed and in fact <em>the hurdle differs depending on the impact of the treatment itself</em>. In general <em>when the errors in the selection and outcome equations are correlated the difference in these means differs from the actual treatment effect</em>. In other words through a slightly complicated story, when those who have unobservables that make them more likely to work also tend to have unobservables that would make them likely to earn more the standard difference in outcomes between control and treatment will <em>not</em> describe the true treatment effect.</p>
<p><br> </p>
<p><em>A key insight</em> seems to be that we could identify the treatment effect if we could estimate the expected outcome given treatment <em>and</em> given that the unobservable component in the selection equation would lead to an observable outcome had the person <em>not</em> been given treatment. If so, we could subtract the observed mean control outcome from the above to yield the true treatment effect (for those who would be observed always). However, we obviously do not observe this because we only observe the outcomes for those who are treated where the selection equation <em>is</em> in fact positive and not “where the selection equation <em>would have</em> been positive had they not been treated.”</p>
<p>However, the insight here is that this term can in fact be bounded. We <em>do</em> observe these outcomes for the treated people (note we are assuming without loss of generality that the treatment raises the probability of selection for this discussion) but we don’t know exactly which ones they are. In other words, we observe outcomes for more people in the treatment group than we need; we wish we could figure out what is the subset of these that would have <em>also</em> been observed had they not been treated, so we could compare like-to-like. The observed treatment mean is a weighted average of the thing we are seeking (to difference from the control) and “the mean for a subpopulation of marginal individuals… that are induced to be selected into the sample <em>because</em> of the treatment”</p>
<p>This then gets us the upper bound for the term expressing the treatment outcome for those who would have been observed even if they had been in the control. The upper bound for this is the expected outcome for those in the treated group (who are observed of course) and who are in quantile-p or above of the outcome, where this <span class="math inline">\(p\)</span> is the share of the treated population that are in the marginal group we referred to that were only induced to be selected into the sample because of the treatment.</p>
<p>In other words the worst case scenario is that the smallest share <span class="math inline">\(p\)</span> values of <span class="math inline">\(Y\)</span> are in the marginal group and the largest one (which is share 1-pone are in the inframarginal group. We don’t know which observations are inframarginal and which ones are marginal.</p>
<p><span class="math inline">\(p\)</span>: the share of marginal individuals and (1-p) the share of inframarginal individuals (the latter is group we want the average outcome for). The highest could be would be the average outcomefor the largest (1-p) share of this group. We are looking for the expectation given that they are at or above at will at or above percentile p within this group.</p>
<p>In other words we trim the lower tail of the Y distribution by the portion <span class="math inline">\(p\)</span>, (so what remains is the 1-p share) to get the upper bound for the inframarginal groups mean. We can then subtract the mean for the control group to get an upper bound for the treatment effect.</p>
<p>To compute this “trimming proportion p”: this p is equal to the share of the treated group whose outcome is observed minus the share of the control group whose outcome is deserved is observed, divided by the share of the treatment group where the outcome is observed.
Something like the <em>increased likelihood of observation that is driven by the treatment, as a share of the total number as a share of the probability of observation in the treatment group</em>.</p>
<p>The average observed outcome for the treatment group is including too many observations; we need to difference out the share of observations that are observed only because the treatment caused them to be observed; this share is certainly no larger than the increased probability of observation in the treatment group as a share of the probability of observations the treatment group.</p>
<p>Another much simpler way of saying this is “trimming the data by the known proportion of excess individuals” in the treatment group. (To gain bounds on the mean for the inframarginal group which we can then difference from the control-group mean get the treatment effect).</p>
<p>Perhaps some intuition for why this improves on the Horwitz model: we don’t need to assume that those observed in the treatment group that wouldn’t have been observed in the control would’ve had the highest possible outcomes. No, we only need to assume (to get the upper bound) that these came from the highest <em>distribution</em> because they had to come from somewhere. These were the people in the upper tail of the relevant group but they couldn’t <em>all</em> have been the individual highest achiever.</p>
<hr />
<p>The model is extended to heterogeneity and heteroscedasticity. This begins with the independence of treatment assignment the “potential sample selection indicators” for either treatment or control, in other words whether that individual will have an observed outcome under treatment and whether that the individual would have an observed outcome under control, and the latent potential outcomes.</p>
<p>Experimental or random assignment ensures that each of the potential outcomes (and the correspondence to observability under each treatment) is independent of the actual assigned treatment.</p>
<p>The second assumption is monotonicity: treatment assignment can only affect sample selection in one direction.</p>
<p><br />
</p>
*Some discussion from my own research projects on this …<br />

<div class="fold">
<p>DR: For our (substitution) experiments, it is in fact not clear to me whether this should necessarily be the case, as some (less generous?) people may be induced to leave because of having been asked to donate, while potentially other (more generous people) might be induced to return given that they were asked to donate. (This proposed nonmonotonicity implies that the ‘asked twice’ sample tends to weed out the less generous, which would lead to a bias <em>against</em> substitution, strengthening the case for our result.)</p>
<p>DR, aside: However, even though the paper doesn’t say it, I suspect this assumption could be weakened and you would still get some similar bounds.
To put it another way, I would imagine that these bounds could be adjusted based on some reasonable ad hoc assumptions about the share of the population who is affected in either direction.</p>
– NL: I’m coming to think that our Dutch data problems are more things involving “hurdle models.” Can this technique also be applied to such hurdle models?
</div>
<p>Next proposition 1a states that given these assumptions we can derive sharp lower and upper bounds for the average treatment effect (conditional on ‘would be observed in both states’). Note that for this estimator if the probability of observation is greater under the treatment we need to trim the treatment groups outcome distribution and if the probability of observation is greater under the control we need to trim the control group’s outcome distribution.</p>
<ul>
<li>DR, aside comment: we seem to be throwing out a bit of the data in these estimates, which would suggest that something more efficient could be generated.</li>
</ul>
<p>(The stated bounds you can estimate are exactly the same as the bounds from the previous specification, at least as I had interpreted the way they would be produced.)</p>
<p>Their remark 2 notes that an implication is that as <span class="math inline">\(P_0\)</span>, that as the “difference between the relative probability of observation of an outcome under treatment versus control” tends to zero,
i.e., as the probability of having an observed outcome (or the conditional probability of this) is the same for treatment and control) then there is no sample selection bias.</p>
<p>Their estimate convergences to the estimate he calls an estimate for the “always takers subpopulation… except that taking… is selection into the [outcome-observed] sample.”</p>
<p><em>So, a very vanilla estimator is acceptable if we find the same conditional probability of selection for each group, under monotonicity, which, for this case, we can test (see Remark 4 below).</em></p>
<p>– (DR: To me this suggests that there might be something wrong going on here. Intuitively, If I simply observe the same rate of attrition in the treatment and control groups this <em>shouldn’t</em> be enough to tell me that attrition did not matter, as it could occur differentially for both groups, but it seems to be a result here; this is probably due to the assumption of monotonicity of the selection/observation term, as well as the random/exogenous assignment to each group.)</p>
<hr />
<p>Remark 3 discusses the importance of monotonicity for the bounds, saying this assumption is “minimally sufficient” (I think it would be better to say minimally sufficient for these particular bounds that he computed). To demonstrate this he gives an extreme example. Without monotonicity it could be (note: this would seem like a very unlikely outcome!) that every observation in the control group comes from the population in the treatment group that would <em>not</em> have been observed had they been treated and every observation in the treatment group happens to come from the set of people that would <em>not</em> have been observed had they been in the control group. These two “subpopulations do not overlap, so the difference in the means could not be interpreted as a causal effect.”</p>
<p>– DR, aside : there must be some way to impose some restrictions on this even allowing for this non-monotonicity. (He notes that this can be improved upon somewhat by thinking about the total the idea that the total masses of unobserved that would’ve been observed in the other group can’t be greater than the share that is not observed in the other treatment group, but this doesn’t seem like a particularly fruitful route as it in most reasonable cases will still allow for very wide bounds.)</p>
<hr />
<p>Remark 4 suggests that if we can assume (or somehow observe?) that the conditional probabilities of selection are the same for treatment and control, we can <em>test whether monotonicity in fact holds</em> and the simple difference in means will be an appropriate estimate of the treatment effect. Here, the assumption implies that everyone in the treatment or control group would have been observed under the opposite treatment as well. This in fact implies that the distribution of the exogenous variables should be the same in the treatment and control groups conditional on being selected. This seems fairly intuitive, we look at whether selection seems to be occurring in different ways are on different margins for the two groups treatment versus control.</p>
<p>Apparently for this test to have <em>power</em> we need that the subpopulations of “noncompliant errors in opposite directions” (quotation mine) must have <em>distinct</em> distributions of baselines exogenous characteristics. If these were the same then whether or not monotonicity holds the test doesn’t tell us anything.</p>
<p>– DR: <em>I wonder if anyone uses this test for Monotonicity under non-differential selection?</em></p>
<p>Another relevant note that he bundles in this remark is that the technique here only yields estimates <em>for those who would be with an observed outcome for either treatment or control.</em> One could <em>additionally</em> try to bound this as an estimate for the entire population using the Horwitz and Manski bounds for this latter thing. However, in many contexts there are reasons that the bounded estimates they mainly use are the relevant ones, such as “the impact of the program on wage rates for those whose employment status was not affected by the program.”</p>
<ul>
<li>DR: In our substitution experiment case, the substitution patterns for those for whom attrition was not affected by the first-round-charity treatment</li>
<li>@NL: E.g., the impact of an institution on income for those whose choice to remain in the course was not affected by their institutional assignment</li>
</ul>
<hr />
<p>“Narrowing bounds using covariates”</p>
<p>All of the above could be done conditional on a particular set of baseline characteristics such as gender or race. The average treatment effect could be estimated separately for each. (Note: and perhaps combined in a fruitful way?)</p>
<p>One can alternately use covariates to reduce the width of these bounds. To give intuition, we can imagine a baseline covariate that perfectly predicts an individual’s wage. Because treatments are randomly assigned the maintained assumptions will still hold conditionally on this X. The results the methods can be applied separately for each value of this covariate, and for each such value the trimming procedure will actually have no impact on the estimate.</p>
<p>DR: I think this is the “estimate and sum things up in a weighted way” procedure I thought about a moment ago.</p>
<p><br> </p>
<p>Proposition 1B gives the balance estimator for a model involving exogenous variables. Essentially, this computes the corresponding bounds estimator at each X, where the differential selection probability is computed for that particular X, the upper quantile value of the outcome is given conditional on the same X and on being in the treatment group. These are then integrated (or summed up) weighted by the distribution or the cdf of this covariate in the control group. These bounds will necessarily be sharper than the balance without controls.</p>
</div>
<div id="section-4-estimation-and-inference" class="section level5" number="9.2.1.0.4">
<h5 number="9.2.1.0.4"><span class="header-section-number">9.2.1.0.4</span> Section 4: estimation and inference</h5>
<p>The asymptotic variance depends on components reflecting the variance of the trimmed distribution, the variance of the estimated trimming threshold, and the variance in the estimate of “how much of the distribution to trim” (the relative selection probability differential).</p>
<p>Equation 6 formally defines the estimator</p>
<p>Estimated bounds consistent for ‘true bounds’ under standard conditions</p>
<p>Two ways to compute CI’s – CI’s for the ‘true bounds’ or CI’s for the TE itself. A 95% CI for the former will contain the latter with even greater probability.</p>
<p>Imbens and Manski ‘04 can be used to derive the latter which are ’more apppropriate here’ since the object of interest is the TA and not the ’region of all rationalizable treatment effects.
These are built off of a transformation of the estimate UB and LB and max estimated sd of each of these.</p>
<ul>
<li>the latter are reported by the ‘cie’ option in ‘leebounds’</li>
</ul>
<p>Generalisation to monotonicity (without knowing direction of impact of treatment on selection)…</p>
<blockquote>
<p>As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative… [do similar]</p>
</blockquote>
<blockquote>
<p>though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small and p0 is “close” to zero
… A simple, conservative approach to combining the trimmed and untrimmed intervals is to compute their union</p>
</blockquote>
</div>
<div id="section-5-empirical-results" class="section level5" number="9.2.1.0.5">
<h5 number="9.2.1.0.5"><span class="header-section-number">9.2.1.0.5</span> Section 5: Empirical Results</h5>
<p>Table 4 gives a step-by-step that is a good way of seeing and understanding the construction of the estimator, and where the ‘action’ is, in treimming, in components of the SE, etc.</p>
<p>Intervals are 1/14 the width of the equivalent Horowitz/Manski bounds</p>
<div id="using-covariates-to-narrow-bounds" class="section level7" number="9.2.1.0.5.0.1">
<p class="heading" number="9.2.1.0.5.0.1"><span class="header-section-number">9.2.1.0.5.0.1</span> 5.2 using covariates to narrow bounds</p>
<blockquote>
<p>Any baseline covariate will do, as will any function of all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to be a proxy for the predicted wage potential for each individual—is constructed from a linear combination of all observed baseline characteristics. This single covariate is then discretized, so that effectively five groups are formed according to whether the predicted wage is within intervals defined by $6·75, $7, $7·50, and $8·50.</p>
</blockquote>
<ul>
<li>(Substitution): this is essentially what I propose we do, but using Ridge Regressions or something similar</li>
</ul>
<blockquote>
<p>To compute the bounds for the overall average…the group-specific bounds must be averaged, weighted by the proportion (sPr Group J|S0=1,S1=1)</p>
</blockquote>
<blockquote>
<p>The estimated asymptotic variance for these overall averages is the sum of (1) a weighted average of the group-specific variances and (2) the (weighted-) mean squared deviation of the group-specific estimates from the overall mean. This second term takes into account the sampling variability of the weights</p>
</blockquote>
<p><span class="math inline">\(\rightarrow\)</span> result: 11% narrower bounds</p>
<p><br> </p>
<p><em>Interesting; possibly do similar for @NL-ed</em>:</p>
<blockquote>
<p>By statistically ruling out any effect more negative than −0·037, this suggests that after 4 years, the Job Corps enabled program group members to offset at least 35% (and perhaps more) of the potential 0·058 loss in wages due to lost labour market experience that could have been caused by the program</p>
</blockquote>
</div>
</div>
<div id="section-6-conclusions-implications-and-applications" class="section level4" number="9.2.1.1">
<h4 number="9.2.1.1"><span class="header-section-number">9.2.1.1</span> Section 6: Conclusions: implications and applications</h4>
<p>Interesting intuitive argument:</p>
<blockquote>
<p>Another reason to interpret the evidence as pointing to positive wage effects is that the lower bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly negatively correlated with the propensity to be employed. From a purely theoretical standpoint, a simple labour supply model suggests that, all other things equal, those on the margin of being employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be the smallest wage that draws the individual into the labour force). In addition, the empirical evidence in Table 2 suggests that there is positive selection into employment: those who are predicted to have higher wages are more likely to be employed (i.e., U and V are positively correlated). If this is true, it seems relatively more plausible to trim the lower rather than the upper tail of the distribution to get an estimate of the treatment effect.</p>
</blockquote>
<!--chapter:end:sample_selection/selection_models_lee.Rmd-->
</div>
</div>
</div>
</div>
<div id="mlm" class="section level1" number="10">
<h1 number="10"><span class="header-section-number">10</span> Multi-level models</h1>
<p>Sources:</p>
<ul>
<li>qstep, Gabriel Katz</li>
<li><a href="https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multilevel-models.html">Statistical rethinking</a>
<span class="citation"><a href="#ref-McElreath2015" role="doc-biblioref">McElreath</a> (<a href="#ref-McElreath2015" role="doc-biblioref">2015</a>)</span></li>
</ul>
<p>McElreath:</p>
<blockquote>
<p>Multilevel models… remember features of each cluster in the data as they learn about all of the clusters.</p>
</blockquote>
<blockquote>
<p>When it comes to regression, multilevel regression deserves to be the default approach.</p>
</blockquote>
<div class="marginnote">
<p>DR: When I first heard about this MLM stuff, coming from an econometric background, it seemed like trying to ‘have your cake and eat it too.’ If you ‘control’ at the level of clusters, how can you also make inferences about things that vary within clusters. I think I was wrong about this.</p>
</div>
<p>See also: gelmanChapterMRPNoncensus</p>
<div id="introduction-qstep" class="section level2" number="10.1">
<h2 number="10.1"><span class="header-section-number">10.1</span> Introduction (Qstep)</h2>
<p>(Gabriel or MLM?) came from test scores modeling, education literature. Determinants at multiple levels … e.g,. students’ own characteristics and school characteristics</p>
<p>Standard response: clustered standard errors; this is essentially the same as the ‘random intercept model.’</p>
<p>NoteThere is not necessarily a hierarchy; school versus neighborhood… may overlap <span class="math inline">\(\rightarrow\)</span> ‘cross-nested’/‘cross-classified’; can have a hierarchy within some levels, not others</p>
<p>Panel/longitudinal (time/individual)</p>
<p>Accounts for heterogeneity and dependence</p>
<ul>
<li>correlated behaviors within the same group (??)</li>
</ul>
</div>
<div id="some-basic-theory" class="section level2" number="10.2">
<h2 number="10.2"><span class="header-section-number">10.2</span> Some basic theory</h2>
<p>Example: student i, score <span class="math inline">\(y_i\)</span>, i=1,…l</p>
<p>Characteristics of i <span class="math inline">\(X_i\)</span> and of school j</p>
<div id="level-1-model" class="section level3" number="10.2.1">
<h3 number="10.2.1"><span class="header-section-number">10.2.1</span> Level 1 model</h3>
<p><span class="math inline">\(y_i = \alpha_j + \beta x_i + \epsilon_i\)</span></p>
<p>Intercept varies across schools</p>
<p>New approach allows you to make the random effect term as much of a function of ?? to give it more flexibility</p>
</div>
<div id="level-2" class="section level3" number="10.2.2">
<h3 number="10.2.2"><span class="header-section-number">10.2.2</span> Level 2</h3>
<p><span class="math inline">\(\alpha_j=\lambda + \delta z_j + v_j\)</span></p>
<p>Together with the level-1 term, a mlm</p>
<p>‘Bias-adjusted FE’ is that other fancier approach that Sebastian does; requires stronger assumptions</p>
<p>The <span class="math inline">\(v_j\)</span> term allows an explicit heterogeneity of the effect</p>
<p>Total variation in y_i unexplained by observed factors is <span class="math inline">\(\sigma^2_\epsilon + \sigma^2_v\)</span></p>
<p>Proportion of variation in outcome accounted for by ‘unobserved contextual factors’ (the level-2 stuff): ‘Intra-class correlation coefficient’</p>
<p>Any mlm can be written as a sungle level model with a bunch of random effects … it’s a ‘variance component model’</p>
<p>Estimating a MLE with only an intercept is like a standard model with clustered se</p>
<p>… the basic mlm allows <strong>correlation </strong>**** between students in the same classroom</p>
<p>Even without a ‘measured’ second level we still have an mlm</p>
<p>Suppose schools only have an impact on the <em>average</em> student score…
Could also have a ‘random slope’ model; the model with heterogeneity</p>
<p>Could affect the impact of student effort; could affect students’ average marks, or both</p>
<div id="intra-cluster-correlations" class="section level4 unnumbered">
<h4 class="unnumbered">Intra-cluster correlations</h4>
<p>From David McKenzie’s WorldBank blog. <a href="https://blogs.worldbank.org/impactevaluations/tools-of-the-trade-intra-cluster-correlations">Tools of the Trade: Intra-cluster Correlations</a></p>
<p>You can calculate the ICC using ANOVA estimation.</p>
<pre><code>icc_anova &lt;- aov(outcome ~ group,data = df) %&gt;% summary
icc &lt;- icc_anova[[1]][1,2]/sum(icc_anova[[1]][,2])
</code></pre>
</div>
</div>
<div id="alternativenaive-approaches" class="section level3" number="10.2.3">
<h3 number="10.2.3"><span class="header-section-number">10.2.3</span> Alternative/Naive approaches</h3>
<ol style="list-style-type: decimal">
<li>Could also do ‘one regression per school’</li>
</ol>
<ul>
<li>some will have very few obs</li>
<li>In contrast, schools with few obs will have an estimated RE very close to the mean; those further will have a larger RE estimated</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>School-level predictors in the single-level regression</li>
</ol>
<p>… this assumes the schools influence is merfectly measured by z_j terms</p>
<ol start="3" style="list-style-type: decimal">
<li>Add both? no, these are perfectly collinear (although there are some attempts to relax this)</li>
</ol>
</div>
<div id="old-way-two-stage-regression" class="section level3" number="10.2.4">
<h3 number="10.2.4"><span class="header-section-number">10.2.4</span> ‘old way’: two-stage regression</h3>
<ul>
<li><ol style="list-style-type: lower-roman">
<li>school dummies in individual regression</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>regress dummies on school covariates</li>
</ol></li>
</ul>
<p>Problems:</p>
<ul>
<li>few students/school, imprecise estimates;</li>
<li>ignoring error terms of dummy estimates <span class="math inline">\(\rightarrow\)</span> spurious significance</li>
</ul>
</div>
<div id="how-many-higher-level-units-do-you-need" class="section level3" number="10.2.5">
<h3 number="10.2.5"><span class="header-section-number">10.2.5</span> How many higher-level units do you need?</h3>
<ul>
<li>Debate in the literature … conventional answers ask for 25; this only pertains to frequentist work, not Bayesian
… because it is not based on asymptotics</li>
</ul>
<p>?can’t you also use simulation in frequentist models (bootstrap the MSE)…?</p>
<p><span class="citation"><a href="#ref-juddTreatingStimuliRandom2012" role="doc-biblioref">Judd, Westfall, and Kenny</a> (<a href="#ref-juddTreatingStimuliRandom2012" role="doc-biblioref">2012</a>)</span>
### Other names: Random effects hierarchical, etc.</p>
</div>
</div>
<div id="fitting-mlm-in-practice" class="section level2" number="10.3">
<h2 number="10.3"><span class="header-section-number">10.3</span> Fitting mlm in practice</h2>
<p>R and Stata good for simple standard models</p>
<p>Winbugs enables complicated models (there is a package that derives the posteriors for you)</p>
<p>Stata: ‘xtreg, re’ is random intercept
… there is also ‘mreg’</p>
</div>
<div id="stimuli-treatments-as-a-random-factor" class="section level2" number="10.4">
<h2 number="10.4"><span class="header-section-number">10.4</span> “Stimuli” (treatments) as a random factor</h2>
<p>“Treating Stimuli as a Random Factor in Social Psychology: A New and Comprehensive Solution to a Pervasive but Largely Ignored Problem” - Judd et al, 2012</p>
<blockquote>
<p>In this article, we present a comprehensive solution using mixed models for the
analysis of data with crossed random factors (e.g., participants and stimuli).</p>
</blockquote>
<p>(John List talks a lot about ‘randomizing across context’ maybe there’s an experimental econ literature on this too?)</p>
<!--chapter:end:multilevel-models/multilevel_models_qstep.Rmd-->
</div>
</div>
<div id="experiments-and-surveys-design-and-analysis-1" class="section level1 unnumbered">
<h1 class="unnumbered"><strong>EXPERIMENTS AND SURVEYS: DESIGN AND ANALYSIS</strong></h1>
</div>
<div id="surveys" class="section level1" number="11">
<h1 number="11"><span class="header-section-number">11</span> Survey design and implementation; analysis of survey data</h1>
<p>The <a href="https://en.wikipedia.org/wiki/Survey_sampling">Wikipedia entry on ‘survey sampling’</a> provides a good overview.*</p>
<div class="marginnote">
<p>* Also see: Carl-Erik Särndal; Bengt Swensson; Jan Wretman (2003). Model assisted survey sampling. Springer. pp. 9–12. ISBN 978-0-387-40620-6. Retrieved 2 January 2011.</p>
</div>
<div id="survey-samplingintake" class="section level2" number="11.1">
<h2 number="11.1"><span class="header-section-number">11.1</span> Survey sampling/intake</h2>
<div id="probability-sampling" class="section level3 unnumbered">
<h3 class="unnumbered">Probability sampling</h3>
<p>In the fold below, I offer a rough characterization of the rationale for probabiliity sampling, in my own words, based on my reading of the aforementioned Wikipedia entry:</p>

<div class="fold">
<p>‘Probability sampling’ has been the standard approach in survey sampling since random-digit dialing was possible.</p>
<p>The basic idea, if I understand correctly, is to define a population of interest and a ‘sample frame’ (the place we are actually drawing from, the empirical analogue of the population of interest perhaps).
Essentially, rather than advertising or trying to recruit everyone, or have people enter by self selection, probability sampling select from the sample frame with a particular probability, and then actively tries to get the selected individuals to complete the survey. As only a smaller number of people are selected to be interviewed/fill out the survey, you can spend more time and money/incentives, trying to make sure they respond.</p>
<p>Probability sampling also allows ‘stratification,’ and oversampling of harder to reach groups. We potentially divide up (‘stratify’) that frame by observable groups. We randomly draw (sample) within each strata with a certain probability.</p>
<em>If we have an informative estimate of the TRUE shares in each strata</em> we can sample/re-weight so that the heterogeneous parameter of interest can be said to represent the average value for the true population of interest.
</div>
<p><br />
</p>
<blockquote>
<p>A probability-based survey sample is created by constructing a list of the target population, called the sampling frame, a randomized process for selecting units from the sample frame, called a selection procedure, and a method of contacting selected units to enable them to complete the survey, called a data collection method or mode (lookup in <a href="https://en.wikipedia.org/wiki/Survey_sampling#Probability_sampling">Wiki</a>)</p>
</blockquote>
<div class="marginnote">
<p>Note that the ‘sampling frame,’ ‘the source material or device from which a sample is drawn,’ e.g, a telephone directory, may not exactly contain all elements of the ‘population of interest’ (e.g., the population with and without listed numbers).</p>
<p>On the other hand this terminology doesn’t seem to be everywhere consistent, e.g., Salganik and H refer to it as ‘a list of all of the members in the population.’</p>
<p>See ‘Missing elements,’ ‘Foreign elements,’ ‘Duplicate entries’ and ‘Groups or clusters’</p>
<p>Also note that</p>
<blockquote>
<p>Not all frames explicitly list population elements; some list only ‘clusters.’ For example, a street map can be used as a frame for a door-to-door survey.</p>
</blockquote>
</div>
<p><br />
</p>
<ul>
<li>Simple random versus stratified and cluster sampling</li>
</ul>
<p><br />
</p>
<p><em>Issues</em>**</p>
<div class="marginnote">
<p>** Many of these also seem relevant for our <a href="#jazz-case">‘social movement’ case of interest below.</a></p>
</div>
<ul>
<li>Non-response bias (biggest issue?)</li>
</ul>
</div>
<div id="np-sampling" class="section level3 unnumbered">
<h3 class="unnumbered">Non-probability sampling</h3>
<blockquote>
<p>There is a wide range of non-probability designs that include case-control studies, clinical trials, evaluation research designs, intercept surveys, and opt-in panels, to name a few.</p>
</blockquote>
<p><span class="citation"><a href="#ref-bakerSummaryReportAAPOR2013" role="doc-biblioref">Baker et al.</a> (<a href="#ref-bakerSummaryReportAAPOR2013" role="doc-biblioref">2013</a>)</span></p>
<p><a href="https://www.aapor.org/Education-Resources/Reports/Non-Probability-Sampling.aspx">Non-Probability Sampling - report of the aapor task force on non-probability sampling</a></p>
<p>‘River sampling’ is closest to the case we are dealing with <a href="#jazz-case">below</a>.</p>
<p><br />
</p>
<div id="respondent-driven-sampling-from-rare-or-hidden-populations" class="section level4 unnumbered">
<h4 class="unnumbered">Respondent-driven sampling (from rare or ‘hidden’ populations)</h4>
<p><a href="https://www.bebr.ufl.edu/sites/default/files/Sampling%20and%20Estimation%20in%20Hidden%20Populations.pdf">sampling and estimation in hidden populations using respondent-driven sampling</a> <span class="citation"><a href="#ref-salganikSamplingEstimationHidden2004" role="doc-biblioref">Salganik and Heckathorn</a> (<a href="#ref-salganikSamplingEstimationHidden2004" role="doc-biblioref">2004</a>)</span></p>
<div class="marginnote">
They suggest a way to use ‘RDS’ and then recover representativeness, under certain assumptions.
</div>
<blockquote>
<p>Standard sampling and estimation techniques require the researcher to select sample members with a known probability of selection. In most cases this requirement means that researchers must have a sampling frame, a list of all members in the population. However, for many populations of interest such a list does not exist.</p>
</blockquote>
<ul>
<li><p>‘Standard’ approach … Construct a sample frame of ‘all the injection drug users in a large city’; ‘probably impossible’</p></li>
<li><p>Or ‘reach a large number of people via random digit dialing and then screen them for membership in the hidden population. Again, this approach is extremely costly and potentially very inaccurate.’</p></li>
<li><p>Or “take a sample of target population members in an institutional setting—for example, injection drug users in a drug rehabilitation program.” But “a nonrandom sample from the hidden population, … impossible to use to … make accurate estimates about the entire hidden population”</p></li>
</ul>
<p>Targeted sampling (‘street outreach’) is cost-effective but it is very hard to know what you are getting. Time-space sampling can deal with certain limitations to this, but it’s far from a complete remedy.</p>
<p><br />
</p>
<p>The authors propose:</p>
<blockquote>
<p>respondent-driven sampling, is a variation of chain-referral sampling methods that were first introduced by Coleman (1958) under the name snowball sampling.</p>
</blockquote>
<blockquote>
<p>select a small number of seeds who are the first people to participate in the study. These seeds then recruit others to participate in the study. This process of existing sample members recruiting future sample members continues until the desired sample size is reached</p>
</blockquote>
<p><br />
</p>
<p>While..</p>
<blockquote>
<p>chain-referral methods produce samples that are not even close to simple random samples</p>
</blockquote>
<blockquote>
<p>researchers believed that any small bias in selecting the seeds would be compounded in unknown ways as the sampling process continued.</p>
</blockquote>
<p>and also …</p>
<blockquote>
<p>This research is fairly well summarized by Berg(1988) when he writes, “as a rule, a snowball sample will be strongly biased toward inclusion of those who have many interrelationships with or are coupled to, a large number of individuals.” In the absence of knowledge of individual inclusion probabilities in different waves of the snowball sample, unbiased estimation is not possible.</p>
</blockquote>
<div class="marginnote">
<p><strong>“Biased toward inclusion of those who have many interrelationships…”</strong></p>
<p>This would seem to suggest a solution, perhaps driving the approach the authors use. If we can measure the likelihood that an individual is reached as a function of their network, we may be able to downweight those individuals that are more likely to be sampled.</p>
</div>
<p><br />
</p>
<p>But the authors believe they have a solution</p>
<blockquote>
<p>We believe that previous researchers have been overly pessimistic about chain-referral samples. In this paper we will show that it is possible to make unbiased estimates about hidden populations from
these types of samples. Further, we will show that these estimates are asymptotically unbiased no matter how the seeds are select</p>
</blockquote>
<p>How does it work?:</p>
<blockquote>
<p>First, the sample is used to make estimates about the social network connectingthe population. Then, this information about the social network is used to derive the proportion of the population in different groups (forexample, HIV- or HIV+)</p>
</blockquote>
<p><br />
</p>
<p>There are some intricate and interesting calculations involving network analysis .</p>
<p>They show the unbiasedness of this approach under specified conditions, and its robustness to the seed choice. They also give a monte-carlo simulation illustrating the same.</p>
<p>Finally, they do some sort of comparison of their approach and previous approaches in real settings.**</p>
<div class="marginnote">
<p>** DR: I didn’t read the latter carefully. Worth following up, perhaps.</p>
</div>
</div>
</div>
</div>
<div id="jazz-case" class="section level2" number="11.2">
<h2 number="11.2"><span class="header-section-number">11.2</span> Case: Surveying an unmeasured and rare population surrounding a ‘social movement’</h2>
<div class="marginnote">
<p>See: <a href="https://www.researchgate.net/post/Measuring_composition_attitudes_of_rare_hidden_population_with_convenience_samples_river_sample_opt-in_G-based_what_to_do_read">Related question/post on ResearchGate here</a></p>
</div>
<div id="background-and-setup" class="section level3 unnumbered">
<h3 class="unnumbered">Background and setup</h3>
<p><em>Consider a case where:</em></p>
<ol style="list-style-type: decimal">
<li>We have a <strong>population-of-interest</strong> based on an affiliation, certain actions, or a set of ideas. E.g., Vegetarians; ‘Tea party conservatives’ in the US; Jews, both religious and ‘culturally Jewish,’ Jazz musicians, “Goths” (‘ethnography’; Paul Hodkinson)</li>
</ol>
<p>For this writeup, we will call the targeted group ‘the Jazz Movement’ or ‘the Jazz population.’ Individuals will either be ‘Jazzy’ (J) or ‘non-Jazzy’ (NJ).</p>
<p>There are some <strong>disagreements about how to define this group</strong>.</p>
<p><br />
</p>
<ol start="2" style="list-style-type: decimal">
<li>We have <strong>no ‘gold standard’ to benchmark against.</strong></li>
</ol>
<ul>
<li>There is no ‘actual targeted and measured outcome’ such as voting in an election.</li>
<li>There are no other surveys or enumerations (e.g., censuses) to inform our results.</li>
</ul>
<p><br />
</p>
<ol start="3" style="list-style-type: decimal">
<li>We have collected survey responses from <strong>self-selected ‘convenience’ samples</strong> (‘internet surveys’) across several years; this most resembles <em>‘river sampling’</em></li>
</ol>
<p>… based on advertising and word-of-mouth in a variety of outlets (‘referrers’) associated with the ‘movement.’*</p>
<div class="marginnote">
<p>*Particularly:</p>
<ul>
<li>A discussion forum</li>
<li>A newsletter</li>
<li>A popular website and hub for the movement</li>
</ul>
</div>
<ul>
<li>We can identify <em>which</em> ‘referrer’ lead someone to our survey.</li>
<li>All participants are given a similar ‘donation’ incentive, an incentive that might tend to particularly attract members of the Movement.**</li>
</ul>
<div class="marginnote">
<p>** Given the context, we might reasonably expect that willingness to complete the survey might be associated with depth of support for the movement.</p>
</div>
<ul>
<li>We can link some individuals across years.</li>
<li>Some questions repeat across years.</li>
</ul>
<p><br />
</p>
<ol start="4" style="list-style-type: decimal">
<li>We have (self-reported) <strong>measures</strong> of</li>
</ol>
<ul>
<li>Demographics (age, gender, etc),</li>
<li>Attitudes and beliefs (e.g., support for the death penalty),</li>
<li>Retrospectives (esp. ‘year you became Jazzy’), and</li>
<li>Behaviors (e.g., charitable donations).</li>
</ul>
<p><br />
</p>
<ol start="5" style="list-style-type: decimal">
<li>Our <strong>research goals include</strong> measuring:</li>
</ol>
<p>The size of the movement (challenging),</p>
<p>… The demographics (and economic status, psychographics, etc) of the movement,</p>
<p>… The attitudes, beliefs and behaviors of people in the movement,</p>
<p>… The (causal) drivers of joining the movement and actively participating in the movement (or leaving the movement),</p>
<p>… and the trends/changes in all of the above.</p>
<p>We are particularly interested in the most avid and engaged Jazzers, and in knowing about self-reported challenges to participation.</p>
<p><br />
</p>
<p><strong>Why do we care?</strong> <em>We want to know these things for several reasons, including…</em></p>
<div class="marginnote">
<p>Our ‘theory of change.’</p>
</div>

<div class="fold">
<p>To find ways to build membership (perhaps ‘expand and diversify’), and increase participation in the movement (including specific behaviors like donating), especially through…</p>
<ul>
<li>Funding causal drivers (policies) that ‘work,’ and</li>
<li>Profiling and targeting ‘likely Jazzers’ from outside the movement</li>
</ul>
<p>To understand and better represent the attitudes of the movement’s members in our movement-wide activities. <!-- See --></p>
<p>‘For general understanding of the movement and its members,’ to inform a wide range of decisions across the movement, and further research into the movement.</p>
</div>
<p><br />
</p>
</div>
<div id="our-convenience-method-issues-alternatives" class="section level3 unnumbered">
<h3 class="unnumbered">Our ‘convenience’ method; issues, alternatives</h3>
<p>Our current approach may be described as a combination of ‘convenience sampling’ (‘river sampling’ and ‘opt-in’) and ‘snowball sampling.’ The major distinction from probability sampling (as I see it) is…</p>
<p><em>Probability sampling</em> identifies a <em>population of interest</em> and a <em>sample frame</em> meant to capture this population. Rather than appealing to this entire population/frame, probability sampling randomly (or using stratification/clustering) samples a ‘probability share’ (e.g., 1/1000) from this frame. Selected participants are (hopefully) given strong incentives to complete the survey. One can carefully analyze —and perhaps adjust for—the rate of non-response.</p>
<p>In contrast,</p>
<div class="marginnote">
<p>I have heard that ‘internet surveys,’ if done right, with proper adjustments, are seen as increasingly reliable, especially in the context of electoral polling. Is our approach similar enough to this to be able to adopt these approaches?</p>
</div>
<p><br />
</p>
<p>Wikipedia entry on ‘convenience sampling’</p>
<blockquote>
<p>Another example would be a gaming company that wants to know how one of their games is doing in the market one day after its release. Its analyst may choose to create an online survey on Facebook to rate that game.</p>
</blockquote>
<blockquote>
<p><strong>Bias</strong> The results of the convenience sampling cannot be generalized to the target population because of the potential bias of the sampling technique due to under-representation of subgroups in the sample in comparison to the population of interest. The bias of the sample cannot be measured. Therefore, inferences based on the convenience sampling should be made only about the sample itself.[9] (Wikipedia, on ‘Convenience sampling,’ cites Borenstein et al, 2017)</p>
</blockquote>
<p><em>This statement is deeply pessimistic…</em> ‘the bias cannot be measured.’ We might dig more deeply to see if there are potential approaches to dealing with this</p>
<p><br />
</p>
</div>
<div id="our-methodological-questions" class="section level3 unnumbered">
<h3 class="unnumbered">Our methodological questions</h3>
<p><strong>Analysis:</strong> Are there any approaches that would be better than ‘reporting the unweighted raw results’ (e.g., weighting, cross-validating something or other) to using this “convenience/river” sample to either:</p>
<ol style="list-style-type: lower-roman">
<li><p>Getting results (either levels or changes) likely ‘to be more ’representative of the movement as a whole’ than our unweighted raw measures of the responses in each year?</p></li>
<li><p>Getting measures of the extent to which our reports are likely to be biased due to undercoverage, ‘overcoverage,’ and differential participation rates, … perhaps bounds on this bias.</p></li>
</ol>
<p><br />
</p>
<p><strong>Survey design:</strong> In designing future years’ surveys, is there a better approach?</p>
<ul>
<li><p>Probability sampling of a ‘large group’ or within each outlet (with larger incentives/inducements), or even a nationally representative sample</p></li>
<li><p>Respondent-driven sampling (with network-analysis based adjustments)</p></li>
<li><p>Comparisons to other data points from other surveys and measures, comparisons to other groups and within groups</p></li>
</ul>
<!-- 
Notes:

- 'Whether we can do better’ sort of questions ... we should try to pin down what our metrics and targets are

- What do we mean when we say the ‘actual Jazz population’?

- What sorts of questions/claims are we interested in most/what ‘parameters’ are we trying to measure?

- For the second point, I think that some methods might yieldplausibly unbiased/reliable estimates of certain parameters (e.g., year-to-year changes) but not others.

-->
<!-- Some flexibility/two-way interaction here in that we have a lot of different goals with the survey (i.e. different kinds of questions being posed) and depending on the answers you discern, we might give up on some of these questions and focus on others
-->
<div id="sensitivity-testing-ideas" class="section level4 unnumbered">
<h4 class="unnumbered">Sensitivity testing ideas</h4>
<p>As we can separately measure demographics (as well as stated beliefs/attitudes) for respondents from each referrer, we could consider testing the sensitivity of the results to “how we weight responses from each referrer.”</p>
<ul>
<li><p>How much would results vary if we compare the widest group to the most motivated “first responders” to our regular posted survey?</p></li>
<li><p>Use demographics derived from some weighted estimate of surveys in all previous years to re-weight the survey data in the present year to be “more representative?” (But how to weight and judge previous surveys and referrers?)</p></li>
<li><p>A Bayesian meta-analytic approach where the ‘true population parameters’ are unknown and each survey provides an imperfect window.</p>
<ul>
<li>As noted we subjectively think that some referrers are more representative than others, so maybe we can do something with this using Bayesian tools.)
We may have some measures of the demographics of participants on some of the referrers, which might be used to consider weighting to deal with differential non-response</li>
</ul></li>
</ul>

<div class="note">
<p>Weighting may not be appropriate as a means of gaining ‘complete representativeness,’ as we have no gold standard. Even if we had a nationally-representative sample/survey this could yield differential response rates among different types of EA-ers. An approach to ‘convergent validation’ may be possible in the future, but it will take time to learn/develop a methodology.</p>
<p>However, we can measure and test the sensitivity of our results to variation along several ‘dimensions’</p>
<ul>
<li><p>Ease of response/dedication (maybe compare first to later responders, responders after reminders, future – compare those with additional incentives and pressure)</p></li>
<li><p>Referrers with different characteristics (‘large pool’ vs ‘small pool’ or something)</p></li>
<li><p>Demographics and ‘clusters/vectors of demographics’</p></li>
<li><p>(Possibly) re-weighting to match the demographics of some known group</p></li>
</ul>
<em>We may do this in an a-theoretical ad-hoc way for now, unless we can identify and use a systematic framework for this.</em>
</div>
</div>
</div>
<div id="sketched-model-and-approach-bayesian-inferenceupdating-for-estimating-demographics-and-attitudes-of-an-rarehidden-population" class="section level3" number="11.2.1">
<h3 number="11.2.1"><span class="header-section-number">11.2.1</span> Sketched model and approach: Bayesian inference/updating for estimating demographics and attitudes of an rare/hidden population</h3>
<!-- Private case-specific work: see https://docs.google.com/document/d/1_z7OcUvhqY3NvdKBfmQhcq_u6cr_zgEt5eqpiaJr3fk/edit#   -->
<div class="marginnote">
<p>I suspect there must be substantial work on this, but I’m just going to flesh out how I think it would probably go:</p>
</div>
<p><br />
</p>
<p><strong>Parameters/values:</strong>*</p>
<p>We consider both latent/unobservable ‘true’ population parameters and our observed values.</p>
<p><em>Movement population and composition (latent)</em>: The movement has a population (henceforth ‘our population’) of unknown size <span class="math inline">\(\Pi\)</span> and composition <span class="math inline">\(\delta\)</span>.</p>
<p>‘Demographics’ <span class="math inline">\(\delta\)</span> can either represent a single proportion, e.g., the female share of our population, or a vector of these proportions for various demographics.* For now, for simplicity, we will focus on a single demographic. Let <span class="math inline">\(\delta_0\)</span> be the male share and <span class="math inline">\(\delta_1 = 1- \delta_0\)</span> the non-male share. For indexing this simple case, we refer to demographics <span class="math inline">\(\delta_k\)</span> for <span class="math inline">\(k=0,1\)</span>.</p>
<div class="marginnote">
<p>* …or even a matrix or array describing each pairing of demographics.</p>
</div>
<p>E.g., if the male share is 70%, we have be <span class="math inline">\(\Pi_0= \delta_0 \Pi = 0.7\times 100 = 70\)</span> males in our population.**</p>
<div class="marginnote">
<p>Naturally, <span class="math inline">\(\Pi_1 = \delta_1 \Pi = (1-\delta_0)\Pi = \Pi - \Pi_0 = 100-30 = 30\)</span> non-males in our population.</p>
</div>
<p><br />
</p>
<p>As noted below, we will be targeting the population via several “referrers” <span class="math inline">\(r=0,1,...,R\)</span>. For now, we will assume each member of the population also only “belongs” to a single referrer. This is also described by <em>latent</em> parameters: we do not know what share of the population belongs to each referrer, nor how this breaks down by demographics.</p>
<p>For simplicity, consider only two referrers (<span class="math inline">\(r=0,1\)</span>), the ‘wide and narrow referrers,’ respectively, and let <span class="math inline">\(\rho_0\)</span> denote the share of our population that belongs to the wider referrer (and <span class="math inline">\(\rho_1 = 1-\rho_0\)</span> the share belonging to the narrower referrer).***</p>
<div class="marginnote">
<p>*** For the case in mind when I built this example, this embodies in a place of assumption that is too strong. In fact each of our members may belong to one or more referrers, or in fact to none at all.</p>
</div>
<p>Thus, supposing 80% of our population belongs to the wider referrer, we have <span class="math inline">\(R_0= \rho_0 \Pi = 0.8\times 100 = 80\)</span> members of our population belonging to the wider referrer. (Note that we let <span class="math inline">\(R_0\)</span> denote the ‘number of members of our population belonging to the wider referrer.’)</p>
<p>Again, we do not know how this breaks down by demographics, and it may not be even. Thus, we need to define <em>the shares of each demographic belonging to each referrer</em>:</p>
<p><span class="math inline">\(\rho_{r_k}=pr(k=k|r=r)\)</span>: The share of the ‘population of referrer r’ that also both belongs to demographic <span class="math inline">\(k\)</span>.</p>
<div class="marginnote">
<p>(Note – I’m not sure if I should use this ‘cross proportion’ or instead use two separate proportions, i.e., conditional probabilities.)</p>
</div>
<p>In general (and defining the corresponding ‘number of observations’ notation):</p>
<p><span class="math inline">\(\Pi_{rk}=\Pi\times\rho_r\times\rho_{r_k}\)</span></p>
<p>Thus, e.g., given the shares mentioned above, and assuming that 70% of the narrower referrer’s population is male (<span class="math inline">\(\rho_{1_0}=pr(k=0|r=1)) = 0.7\)</span>, our true population of ‘males in the narrower referrer’ is</p>
<p><span class="math inline">\(\Pi_{10}=\Pi\times\rho_1\times\rho_{1_0}=100\times0.2\times0.7 = 14\)</span></p>
<p><br />
</p>
<p><em>Surveys/referrers and response/participation rates (observed)</em></p>
<p>We have an opt-in web survey that has been administered yearly in years <span class="math inline">\(t=0,...,T\)</span>.</p>
In each year, the survey has been advertised in one of the two referrers noted above.*.
<div class="marginnote">
<p>More generally, in <span class="math inline">\(R\)</span> referrers … <span class="math inline">\(r=0,1,...,R\)</span>.</p>
</div>
<p>We observe the number of participants coming from each referrer <span class="math inline">\(r\)</span> with each demographic <span class="math inline">\(k\)</span> in each year <span class="math inline">\(t\)</span></p>
<p><span class="math inline">\(n_{r,k,t} \forall \: r,k,t\)</span></p>
<p>Suppose that the survey has only been run for 2 years.</p>
<p>Thus, e.g., we may observe <span class="math inline">\(n_{1,0,1}=20\)</span> from the narrower referrer, who are male, in the first year (year ‘0’).</p>
<p><br />
</p>
<p><em>Participation ‘model’:</em></p>
<p>I suggested above, we assume that each member of our population only observes (or only “meaningfully observes”) the survey from at most one referrer, the one they ‘belong to.’</p>
If an individual indeed observes the survey, they then decide whether to participate.*
<div class="marginnote">
<p>* In future, we might model these participation decisions as the outcome of optimization problems, perhaps giving us insight into the direction of the bias (e.g., according to opportunity cost and concern for the outcome of the survey).</p>
</div>
<p>This is perhaps a “double hurdle”: the individual may or may not observe the survey, and then may or may not decide whether it is worth participating. Separating these two processes would seem to separate the “coverage” and “differential response/participation” issues.</p>
<p>As a result, we may have an unobserved “propensity to participate” for each participant, in each year, which may vary by referrer and by demographic.</p>
<p>For now, we will simplify things; We will not model the two hurdles separately, but simply refer to a “latent propensity to (observe and) participate” for each subgroup:</p>
<p><span class="math inline">\(\alpha_{r,k,t}\)</span>: The true probability that an individual belonging to referrer <span class="math inline">\(r\)</span>, who is in demographic <span class="math inline">\(k\)</span> (in year <span class="math inline">\(t\)</span>) participates (returns the survey) in year <span class="math inline">\(t\)</span>.</p>
<p>If we had an (unbiased?) estimate of <span class="math inline">\(\alpha_{r,k,t} \forall r,k,t\)</span> (and if we observe responses from each combination of these, and if the some share of each group belongs to each referrer) we should be able to use inverse probability weighting to gain an unbiased estimate of the true demographic population shares as well as the overall true population size (?).</p>
<p><br />
</p>
<p><em>The parameters and the priors:</em></p>
<p>Specify some informative but still somewhat diffuse prior distribution over these parameters; Including true population shares and probabilities of response within each group.</p>
<p>Update and iterate towards ‘our best posteriors?’</p>
<!--chapter:end:survey_design_weighting/survey_design_weighting.Rmd-->
</div>
</div>
</div>
<div id="why_experiment_design" class="section level1" number="12">
<h1 number="12"><span class="header-section-number">12</span> Experimental design: Identifying meaningful and useful (causal) relationships and parameters</h1>
<div id="why-run-an-experiment-or-study" class="section level2" number="12.1">
<h2 number="12.1"><span class="header-section-number">12.1</span> Why run an experiment or study?</h2>
<p>I claim an experiment should:</p>
<ol style="list-style-type: decimal">
<li><p>Have a reasonable chance of an outcome that would not have been predicted in advance.</p></li>
<li><p>The realized outcome should meaningfully inform our understanding of the world in other words. In other words, if the outcome comes out one way it should cause us to update our beliefs about a particular hypothesis about the world in one direction (and if it comes out the other way we should update in the other direction.)</p></li>
</ol>
<blockquote>
<p>Experimenter should always ask: “What uncertainty (about real-world preferences, decision-making etc.) is ‘entangled’ (ala Eliezer Yudkowsky) with the results of this experiment?”… i.e., ‘how might my beliefs change depending on the results?’</p>
</blockquote>
<ul>
<li>givingtools on twitter</li>
</ul>
<p><br />
</p>

<div class="note">
<p><strong>Example:</strong> <a href="https://www.sciencedirect.com/science/article/abs/pii/S2214804318303896">“Giving to charity to signal smarts: evidence from a lab experiment”</a></p>
<p>Highlights: “We propose individuals give to charity to signal smarts. We designed a laboratory experiment to test this hypothesis. We randomize the publicity of a donation and the degree of meritocracy. We find suggestive evidence that donations are used to signal smarts.”</p>
<p>But “what is the thing in the real world that needs testing in the lab?”</p>
<p>It is not ‘do people want to signal smarts in general’ (or if it is, there is no need for the experiment to link it to charity).</p>
<p>It is more like:</p>
<ol style="list-style-type: decimal">
<li><p>‘do people think that donating is a signal of intelligence?’ and/or</p></li>
<li><p>‘do people even consider using donations in this way?’</p></li>
</ol>
<p>But in the lab experiment both (1) and (2) are guaranteed just by the setup of the treatment. So they don’t offer a way to test these, unless I’m missing something.</p>
<p>This is the “Sugden and Sitzia” critique.</p>
</div>
<div id="sitzia-and-sugden-on-what-theoretically-driven-experiments-can-and-should-do" class="section level3" number="12.1.1">
<h3 number="12.1.1"><span class="header-section-number">12.1.1</span> Sitzia and Sugden on what theoretically driven experiments can and should do</h3>
<p><strong>“Sitzia, Stefania, and Robert Sugden.”Implementing theoretical models in the laboratory, and what this can and cannot achieve." Journal of Economic Methodology 18.4 (2011): 323-343.</strong></p>
<p>This paper is a critique of how models are claimed to be “tested,” through a literal implementation, in the laboratory. They argue this misinterprets the intention of a model, and use of economic modelling in general. Ultimately, such experiments (they say) don’t really tell us one way or another about the truth or usefulness of the model for the real-world domain that was intended. Some key quotes..</p>
<blockquote>
<p>My reductio ad absurdum on this is an experimenter who ‘tests mechanism-design’ by asking subjects “do you want to choose this optimal mechanism and earn £20, or this inefficient mechanism and earn £10?” – givingtools on twitter</p>
</blockquote>
<p>They single-out two examples of well-published experiments for criticism: "an investigation of price dispersion by John Morgan,</p>
<p>Henrik Orzen and Martin Sefton (2006), and an investigation of information cascades by Lisa Anderson and Charles Holt (1997)"…</p>
<blockquote>
<p>In each case, the experimenters create a laboratory environment that closely resembles the model itself. The only important difference between the experiment and the model is that, whereas the model world contains imaginary agents who act according to certain principles of rational choice, the laboratory contains real human beings who are free to act as they wish. The decision problems that the human subjects face are exactly the problems specified by the model. We argue that such an experiment is not, in any useful sense, a test of what the model purports to say about the target domain. Instead, it is a test of those principles of rational choice that the modeller has attributed to the model world. Those principles are not specific to that model; they are generic theoretical components that are used in many economic models across a wide range of applications.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>Surprisingly, these doubts are not expressed in terms of the applicability of MSNE [mixed strategy Nash Equilibrium] to the model’s target domain, pricing decisions by retail firms. The doubts are about whether experimental subjects will act according to MSNE when placed in a laboratory environment that reproduces the main features of the model.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>If one takes the viewpoint of the subjects themselves, there seems to be very little resemblance between the decision problems they face and those by which retail firms set their prices. The connection between the two is given by the model: the subjects’ decision problems are like those of the firms in the model, and the firms in the model are supposed to represent firms in the world.</p>
</blockquote>
<blockquote>
<p>However, MOS are no more concrete than Varian in explaining how the comparative-static properties of the model relate to the real world of retail pricing.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>The suggestion in these passages is that the clearinghouse model’s claim to be informative about the world is strengthened if its results are confirmed in the laboratory. In this sense the experiment is informative about the world. But the experiment itself is a test of the model, not of what the model says about the world.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>The procedure of random and anonymous rematching of subjects is explained as a means of eliminating ‘unintended repeated game effects,’ such as tacit collusion among sellers (pp. 142–3). This argument illustrates how tightly the laboratory environment is being configured to match the model. In a test of MSNE, repeated game effects are indeed a source of contamination; and MSNE is a property of Varian’s model. But in the target domain of retail trade, the same firms interact repeatedly in the same markets, with opportunities for tacit collusion.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>Clearly, if an experiment implemented a model in its entirety, all that it could test would be the mathematical validity of the model’s results. Provided one were confident in the modeller’s mathematics, experimental testing would be pointless. Thus, when an experiment implements almost every feature of a model, all it can test in addition to mathematical validity are those features that have not been implemented.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>Thus, the experiment is a test of MSNE in a specific class of games. [emphasis added]</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>MSNE is what we will call a generic component of economic models – a piece of ready-to-use theory which economists insert into models with disparate target domain</p>
</blockquote>
<ul>
<li>ibid</li>
</ul>
<p>Relating back to the discussion of the different conceptions of theory:</p>
<blockquote>
<p>Is it informative at all to run experimental tests of theoretical principles such as MSNE and Bayesian rationality, viewed as generic components of economic models? … A strict instrumentalist (taking a position that is often attributed to Friedman) might answer ‘No’ to the first question, on the grounds that tests should be directed only at the predictions of theories and not at their assumptions.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>Such an experimental design should not be appraised in terms of what the model purports to say about its target domain. It should be appraised in terms of what it can tell us about the relevant generic component, considered generically. When (as in the cases of MSNE and Bayesian rationality) the same theoretical component appears in many different models, an experimenter can afford to be selective in looking for a suitable design for a test</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>Considered simply as a test of MSNE, MOS’s experiment uses extraordinarily complicated games. Many of the canonical experiments in game theory use 2×2 games. Depending on the treatment, MOS’s games are either 101×101 (for two players) or 101×101×101×101 (for four players). Payoffs to combinations of strategies are determined by a formula which, although perhaps intuitive to an economist (it replicates the demand conditions of the clearinghouse model), might not be easy for a typical subject to grasp.</p>
</blockquote>
<p><br />
</p>
</div>
</div>
<div id="causal-channels-and-identification" class="section level2" number="12.2">
<h2 number="12.2"><span class="header-section-number">12.2</span> Causal channels and identification</h2>
<ul>
<li>Ruling out alternative hypotheses, etc</li>
</ul>
</div>
<div id="artifacts" class="section level2" number="12.3">
<h2 number="12.3"><span class="header-section-number">12.3</span> Types of experiments, ‘demand effects’ and more artifacts of artificial setups</h2>
</div>
<div id="ws-bs" class="section level2" number="12.4">
<h2 number="12.4"><span class="header-section-number">12.4</span> Within vs between-subject designs</h2>
</div>
<div id="generalizability-and-heterogeneity" class="section level2" number="12.5">
<h2 number="12.5"><span class="header-section-number">12.5</span> Generalizability (and heterogeneity)</h2>

<div class="note">
<p><strong>“But all the other papers do it!”</strong></p>
<p>A common response to critiques (particularly critiques of the generalizability of experimental work) is that “all the other papers have the same problem” and that excepting this critique would require rejecting all previous work too. In politics this has been referred to as “what-about-ism.”</p>
<p>You can guess that I’m not a fan of this. I think one always needs to defend the paper and approach on its own merits. Generalisability is an important issue. Each of the other published papers that also suffers from such issues has a specific response and justification for that particular case, and if it doesn’t this is sorely lacking.</p>
<p>I think we <em>should</em> be reading and publishing papers that consider, discuss, and acknowledge their own limitations, and future work can test and build on this. This should promote to robust, reproducable science.</p>
<p><br />
</p>
<p>Just because I say “this is something we should be concerned with” doesn’t mean I’m saying “this paper has no value’. I just mean”let’s discuss reasons why this may or may not threaten internal or external validity/generalisability, and how we can design the study and analysis minimise these potential problems""</p>
<p><br />
</p>
<p>In writing a paper, I find it important that <em>we the authors</em> feel the results are credible and not overstated. So I feel like the best approach is “let’s write the best paper we can and consider every issue seriously, and then hopefully the good publication/peer-review outcome will follow.” That’s also the most motivating and least stressful way for me to work. (Rather than thinking ‘how can I sneak this paper into the best journal?’)</p>
<div class="marginnote">
<p>In fact I consider peer review and rating as the important outcome, not the publication itself. We live in a world where anyone can publish their work immediately on the WWW. The journals themselves are providing little or no service: it is the reviewers and editors offering feedback and evaluation that matters.</p>
</div>
</div>
<p><br />
</p>
<p><a href="https://twitter.com/GivingTools/status/1258704179306147841?s=20">A thought: Replace reviews (accept/reject/R&amp;R) with ratings (0-10)?</a></p>
<!--chapter:end:experiments_and_study_design/why_experiment_design.Rmd-->
</div>
</div>
<div id="quant_design_power" class="section level1" number="13">
<h1 number="13"><span class="header-section-number">13</span> Robust experimental design: pre-registration and efficient assignment of treatments</h1>
<div id="pre-reg-pap" class="section level2" number="13.1">
<h2 number="13.1"><span class="header-section-number">13.1</span> Pre-registration and Pre-analysis plans</h2>
<div id="the-benefits-and-costs-of-pre-registration-a-typical-discussion" class="section level3" number="13.1.1">
<h3 number="13.1.1"><span class="header-section-number">13.1.1</span> The benefits and costs of pre-registration: a typical discussion</h3>

<div class="fold">
<blockquote>
<p>BB: That said, I would be interested to think about the benefits – and more importantly limitations to – pre-registration. I think it could solve some of the p-hacking problems but not much else. How to not relegate exploratory analyses too far is also unclear to me.</p>
</blockquote>
<blockquote>
<p>DR: I’m much more on the ‘pro’ side pre-registration and PaPs. It also helps deal with publication bias and file drawers. And p-hacking is a huge issue IMHO. But it is also good to have some consideration of the pros and cons, so this would be great.</p>
</blockquote>
<blockquote>
<p>BB: RE pre-reg: yes I think it is enough that it prevents p-hacking (there could be very little cost associated with pre-reg) but I fear that it could prevent other advancements if it relegates exploratory analyses too far.</p>
</blockquote>
<blockquote>
<p>DR: I don’t think it should be binary. Systems need to be worked out for <em>adjustments</em> to the meaning of reported estimates depending on whether they were or were not preregistered, and how many were preregistered. While reported significance levels could be adjusted in the frequentist framework, this will all presumably based on measures of the likelihood that such a result would have been estimated/reported. Thus I think this could most easily be incorporated into a Bayesian framework but I’m not saying it would be easy. Still, they have done some good work on adjustments for ‘sequential designs.’</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>BB: I think that it could also stifle students a bit – it may reduce further the number of students who have access to funding that allows for experiments that will be able to be published if all experiments have to be high-powered.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>DR: Statistical power is an important issue. I was skeptical at first about the ‘dangers of underpowered studies’ but maybe I’m coming around a bit.</p>
</blockquote>
<div class="marginnote">
<p>My thinking was that ‘we can simply make downward adjustments to the estimates reported in underpowered studies.’ See the discussion under <a href="#power">power calculations</a>.</p>
</div>
<blockquote>
<p>Anyways, we don’t want to put the cart before the horse: as Gelman said at a conference we should be supporting science not the careers of scientists. I tend to think there are strong arguments for more centralization in social science.</p>
</blockquote>
<blockquote>
<p>And my impression is that we actually have too many different studies and distinct research programs being run, and too many papers being published and not carefully brought together into a framework. Going through the studies on the <a href="https://www.replicationmarkets.com/" class="uri">https://www.replicationmarkets.com/</a> reinforces this impression for me.</p>
</blockquote>
<blockquote>
<p>Still, I think there are ways around this to enable early career people. ‘Underpowered’ experiments could be registered as part of a longer/sequential research program, perhaps collaborative and enabling meta-analysis.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>BB: I also don’t think it gets at publication bias very much unless pre-reg’ed studies are followed up on. Only then do you know why the study didn’t come out – and quite a lot of the time I think it will be attrition/inability to gather the necessary data. Someone could launch that journal though – the Journal of Failed Studies – to have a place for a record that they have been run and what happened to be kept. So I am pro pre-reg, I just think the system needs a bit of work.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>DR: If preregistration is made public and well-organize, then the ‘failed’ exercises willtbe integrated into future meta-analyses; so that’s at least a partial solution here.</p>
</blockquote>
<blockquote>
<p>Agreed, we need to build better systems for incentivising pre-registration and careful data sharing. We need to give career credit to people for planning designing and reporting credible experiments and projects, even if they ‘fail.’ Part this is publishing/rewarding tight null results, which actually do add a lot of value.</p>
</blockquote>
<blockquote>
<p>We might also consider offering some reward careerwise to experiments that fail – in terms of being deeply inconclusive– for some arbitrary or random reason even though they were well-planned and executed. But I think it is hard to get the incentives right for the latter.</p>
</blockquote>
</div>
</div>
<div id="the-hazards-of-specification-searching" class="section level3" number="13.1.2">
<h3 number="13.1.2"><span class="header-section-number">13.1.2</span> The hazards of specification-searching</h3>
</div>
</div>
<div id="designs-for-decision-making" class="section level2" number="13.2">
<h2 number="13.2"><span class="header-section-number">13.2</span> Designs for <em>decision-making</em></h2>
<p>See ‘reinforcement learning,’ ‘lift,’ and multi-armed bandit explore/exploit tradeoffs (with emphasis on the explore part)</p>
<div id="notes-on-bandit-vs-exploration-problemsthompson-vs-exploration-sampling" class="section level3" number="13.2.1">
<h3 number="13.2.1"><span class="header-section-number">13.2.1</span> Notes on Bandit vs Exploration problems/Thompson vs Exploration sampling</h3>
<p>From a conversation</p>

<div class="fold">
<p>: “Ah interesting, my understanding is that Thompson sampling is used to balance out statistical power with treatment effectiveness?”</p>
<p>Sort of. None of this is really isometric to ‘statistical power’ imho.</p>
<p>My impression is that</p>
<ul>
<li><p>Thompson sampling is used to optimise in a case where we simultaneously explore (learn what’s best) and exploit (use what’s best)</p></li>
<li><p>Kasy’s “exploration sampling” is used to optimise in a case where we have a defined <em>exploration period</em> (testing period) during which exploitation is not important</p></li>
</ul>
<p>Thompson’s sampling converges to a single treatment to optimise exploitative benefit. Exploration sampling converges to two treatments to maximise ‘learning for future benefit.’</p>
</div>
<p>Q
## Sequential and adaptive designs {#sequential}</p>
</div>
<div id="sequential" class="section level3 unnumbered">
<h3 class="unnumbered">Sequential</h3>
<p>Needs to adjust significance tests for augmenting data/sequential analysis/peeking Statistics/econometrics new-statistics sagarin_2014 <a href="http://www.paugmented.com/" class="uri">http://www.paugmented.com/</a> resubmit_letterJpube.tex, <a href="http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/" class="uri">http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/</a></p>
<p>Yet …</p>
<p><span class="math inline">\(P_{augmented}\)</span> may <em>overstate</em> type-1 error rate Statistics/econometrics response to referees, new-statistics "</p>
<p>A process involving stopping “whenever the nominal <span class="math inline">\(p &lt; 0.05\)</span>” and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5%. Even if the subsequent data suggested a “one in a million chance of arising under the null” the overall process yields a 5%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the “likelihood of the null hypothesis” given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate.</p>
<p>Considering the calculations in , it is clear that <span class="math inline">\(p_{augmented}\)</span> should  the type-1 error of the process if there is a positive probability that after an initial experiment attains p<span class="math inline">\(&lt;0.05\)</span>, more data is collected. A headline <span class="math inline">\(p&lt;0.05\)</span> does  imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded <span class="math inline">\(p&gt;0.05\)</span>, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though <span class="math inline">\(p&lt;0.05\)</span>. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine’s experiment). Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p&lt;0.05, more data is collected, implying that <span class="math inline">\(p_{augmented}\)</span> as calculated above overstates the type I error rate that would arise from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below <span class="math inline">\(p=0.05\)</span>, in order to attain a type-1 error rate of 5% or less in our published corpus."</p>
</div>
<div id="adaptive" class="section level3" number="13.2.2">
<h3 number="13.2.2"><span class="header-section-number">13.2.2</span> Adaptive</h3>
<p>See Max Kasy’s slides and articles on <a href="https://maxkasy.github.io/home/files/slides/adaptive_field_slides_kasy.pdf">adaptive field experiments</a>, particularly considering ‘exploration sampling.’</p>
<p>This also relates to ‘reinforcement learning.’</p>
</div>
</div>
<div id="efficient-assignment-of-treatments" class="section level2" number="13.3">
<h2 number="13.3"><span class="header-section-number">13.3</span> Efficient assignment of treatments</h2>
<p>(Links back to <a href="#power">power analyses</a>)</p>
<div id="see-also-multiple-hypothesis-testing" class="section level3" number="13.3.1">
<h3 number="13.3.1"><span class="header-section-number">13.3.1</span> See also <a href="#mht">multiple hypothesis testing</a></h3>
</div>
<div id="how-many-treatment-arms-can-you-afford" class="section level3" number="13.3.2">
<h3 number="13.3.2"><span class="header-section-number">13.3.2</span> How many treatment arms can you ‘afford?’</h3>
<!--
A guiding principle might be:
"Will we have statistical power to identify a small true effect from this pairing? If not, we drop the pairing."

A caveat to this is that we may be able to pool some of the pairings to answer certain questions, but then it is only worth having the distinct variations that are being pooled if that doing so gives us power to answer some other question.
-->
<div id="a-side-conversation-who-runs-more-treatments-academics-or-practicioners" class="section level4 unnumbered">
<h4 class="unnumbered">A side conversation: who runs more treatments, academics or practicioners?</h4>

<div class="fold">
<blockquote>
<p>R: In a “policy” context, where we want to decide which message to use, and we are not necessarily trying to establish a very robust general result for scientific purposes, I would err on the side of more messages perhaps</p>
</blockquote>
<blockquote>
<p>M: That’s very interesting. I may just defer to what you think, but my intuitions would have been the reverse in terms of the practical policy vs academic/generalisation</p>
</blockquote>
<p>OK it really depends on the context and the cost/benefit of having confidence in a particular result.</p>
<p>E.g., p&lt;0.01 vs profitable Bayesian updating (‘lift’ etc)
In academia (Economics anyway) there is still the old-line insistence that ‘the result you present must be strongly statistically significant in a frequentist test or it is not publishable.’
In business practice a result may be highly valuable even if it is something like “there is an 80% chance that message A works better than message B, and the mean additional ‘lift’ of message A is +$50,000, with an 80% credible interval of (0,$50,000) and a 95% CI of (-$20k, +$70K)</p>
<p>But academia also cares about ‘deep’ and ‘fancy’ mechanisms stuff: academia asks ’is this question interesting, can you rule out alternative hypothesis, etc … motivating one to have more treatments/outcomes as long as you can ‘get p&lt;0.01’ for rejecting a (perhaps trivial null). So academia compares A1,A2,A3, B1,B2,B3, C1, C2,… etc.</p>
<p>While business might find it more valuable to gain greater insight into ‘whether A outperforms B’ even if we don’t know why. It may not care about testing A1 vs A2 if there is little practical difference between the two.</p>
<p>And ‘academic publication incentives’ doesn’t care much about precision after we ‘get p&lt;0.01’
E.g., if my paper shows that the some ‘early donation seed’ raises (p&lt;0.01 significantly) more funds than no such seed, ‘publication-wise’ I may not care much about bounding the size of this effect, nor about bounding a measure the size of the optimal seed.</p>
<p>But as a business (or EA org etc) I know these seeds are costly, and I may only want to do it if I have a certain level of confidence that it will be substantially above the cost of the seed, perhaps considering the risk/return tradeoffs. I may also find it valuable to have an extremely precise measure of the optimal seed.</p>
</div>
</div>
</div>
<div id="other-notes-and-resources" class="section level3" number="13.3.3">
<h3 number="13.3.3"><span class="header-section-number">13.3.3</span> Other notes and resources</h3>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
ICYMI:<br>Recording of my talk on experimental design in the Chamberlain seminar, with discussions by <a href="https://twitter.com/dmckenzie001?ref_src=twsrc%5Etfw">dmckenzie001</a> and Max Tabord-Meehan:<a href="https://t.co/xKtTrH8X1U">https://t.co/xKtTrH8X1U</a> <a href="https://t.co/e7Cq90D6Sl">https://t.co/e7Cq90D6Sl</a>
</p>
— Maximilian Kasy (maxkasy) <a href="https://twitter.com/maxkasy/status/1264966489595162627?ref_src=twsrc%5Etfw">May 25, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<!--chapter:end:experiments_and_study_design/quant_design_power.Rmd-->
</div>
</div>
</div>
<div id="power" class="section level1" number="14">
<h1 number="14"><span class="header-section-number">14</span> (Ex-ante) Power calculations for (Experimental) study design</h1>
<blockquote>
<p>Power is the ability to distinguish signal from noise.
- <a href="https://egap.org/resource/10-things-to-know-about-statistical-power/">Coppock</a></p>
</blockquote>
<p>The ‘statistical power’ of an analysis is the probability that this analysis diagnoses that ‘an effect is present’ (or a parameter is nonzero). The power of an analysis can only considered in light of (as a function of)</p>
<ul>
<li>a particular <em>true effect size</em> (parameter magnitude),</li>
<li>or an effect size stated as relative to the underlying dispersion,</li>
<li>or as ‘the probability of achieving a certain desired precision.’*</li>
</ul>
<p>Thus one often sees the power of an analysis ‘plotted’ against particular effect sizes (sometimes ‘alternative hypotheses’).</p>
<div class="marginnote">
<p>* E.g., we may see power calculated as a function of a measure of ‘effect relative to variation’ … like Cohen’s <span class="math inline">\(d\)</span> of … effect size/SD.</p>
</div>
<p>Considering standard frequentist null hypothesis testing, ‘the power of a test’ (or analysis) represents one minus the probability of a type-II error (approximately ‘one minus the false-negative rate’).</p>
<div class="marginnote">
<p>Todo: an aside here about power calculations in a Bayesian context.</p>
</div>
<p><br />
</p>
<p><em>A basic primer:</em> <a href="https://egap.org/resource/10-things-to-know-about-statistical-power/">Egap - 10 things to know about statistical power</a></p>
<div id="what-is-the-point-of-doing-a-power-analysis-or-power-calculations" class="section level2" number="14.1">
<h2 number="14.1"><span class="header-section-number">14.1</span> What is the point of doing a ‘power analysis’ or ‘power calculations?’</h2>
<p>There are several reasons to consider power and to do ‘power calculations’ in advance of running an experiment or doing an analysis.</p>
<p>An ‘underpowered study’ is one with a low likelihood of diagnosing a ‘substantial’ effect is present when it <em>is</em> present. Such a study is also likely to be an <em>uninformative</em> study. Furthermore, if ‘rejecting a particular null hypothesis’ is important**, an underpowered test is unlikely to reject this null hypothesis even when it is ‘substantially and meaningfully false.’</p>
<div class="marginnote">
<p>** But please see McElreath and other’s discussion about the follies of the ways NHST is used in science on this point.</p>
</div>
<p>An underpowered study is likely to yield wide ‘confidence intervals’ (or wide ‘posterior Bayesian credible intervals’). Simply put, after an underpowered study (at least in isolation), we still won’t have a good sense of what the true value (of the parameter or effect of interest) is.</p>
<p><br />
</p>
<p>All else equal, you want to run a study that is <em>as high-powered as possible</em> (or a study that is part of a larger project that collectively yields substantial power) because:</p>
<ol style="list-style-type: decimal">
<li><p>you want your study to be informative and to contribute to science (empirical analysis) and</p></li>
<li><p>‘low powered studies’ (at least in certain publication contexts) can potentially <a href="underpowered">harm the accuracy of our scientific consensus</a>.</p></li>
</ol>
<div class="marginnote">
<p>Other than perhaps being ‘too costly,’ is it bad for a study to be ‘overpowered?’ <a href="https://twitter.com/GivingTools/status/1350259972786057216">I argue that this is <em>not</em> a problem.</a></p>
</div>
<div id="practical-power" class="section level3" number="14.1.1">
<h3 number="14.1.1"><span class="header-section-number">14.1.1</span> What are the practical benefits of doing a power analysis</h3>
<p>A power analysis may allow you to:</p>
<ul>
<li><p>consider the <strong>cost/benefits of ‘more data’</strong>, to help you determine ‘how much to collect,’</p></li>
<li><p>consider and <strong>optimise over the tradeoffs in design choices</strong> (e.g., introducing more treatments usually involves a loss of power), and</p></li>
<li><p>understand whether you ‘<strong>have enough funds</strong> to gather enough data to make it worth doing a study?’</p></li>
<li><p>(maybe) be more credible in making ex-post statements about null effects.*</p></li>
</ul>
Furthermore, if you are trying to do a replication exercise to diagnose the credibility of previous work you want to be able to claim ‘I have power to do this credibly.’*
<div class="marginnote">
<p>* I’m not sure about these latter point, this needs to be stated more carefully</p>
</div>
<p><br />
</p>
<p>In general, power analyses and ensuring sufficient power is good for science, avoiding the harm from ‘underpowered studies.’ There are arguments that individual <a href="#underpowered">‘underpowered’ studies may undermine science</a>, particular in conjunction with ‘publication bias.’</p>
</div>
</div>
<div id="power-ingredients" class="section level2" number="14.2">
<h2 number="14.2"><span class="header-section-number">14.2</span> Key ingredients for doing a power analysis (and designing an experimental study in light of this)</h2>
<ol style="list-style-type: decimal">
<li>Our assumptions (or existing prior data used that we can use for simulations) over the data-generating-process. In particular, ‘what do we expect the distribution of the outcomes to look like’ (e.g., ‘normally distributed’), and ‘with what dispersion?’*</li>
</ol>
<div class="marginnote">
<p>However, a measure of dispersion is not necessary for <em>all</em> power calculations. As noted throughout, we can calculate the power to detect an effect of a particular size <em>relative to the dispersion</em>, or to attain a particular confidence interval over such an effect.</p>
<p>Also note that for binary outcomes the choice of a ‘distribution function’ is obvious and ‘dispersion’ only depends on the share of units with each outcome.</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>The nature of the sampling or assignment to experimental treatments *, **</li>
</ol>
<div class="marginnote">
<p>* E.g., <a href="https://cran.r-project.org/web/packages/randomizr/vignettes/randomizr_vignette.html">"complete random assignment</a> to two a single treatments and a single control, each with probability 1/2".)</p>
<p>** Remember, here we are focusing on power calculations in <em>experimental</em> contexts. However, power calculations are also relevant in other empirical contexts, particularly where data-collection is costly.</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>The specific proposed statistical test (or procedure) to be used (e.g., a t-test or a rank-sum test)</li>
</ol>
<p><br />
</p>
<p><em>and perhaps the most important and debated ingredient:</em></p>
<p>*4. Which ‘metrics of power and effect size’ are we considering, and what are our targets?**</p>
<ul>
<li><p>Are we simply seeking ‘precise estimates,’ estimates with small confidence/credible intervals? If so, how precise, and how do we measure this precision?</p></li>
<li><p>Do we seek power to detect some ‘minimal effect size of interest?’</p></li>
</ul>
<p>If so, <em>what is this ‘MESOI?’</em></p>
<div class="marginnote">
<p>Note that there is (often? always?) a mathematical equivalency between the confidence interval and the standard ‘power to detect X’ criteria.</p>
<p>Another criterion might be “power to do an ‘equivalency test’”… but I need to learn more about this.</p>
</div>
<p><br />
</p>
<div id="considering-should-i-use-a-function-of-previous-estimated-effect-sizes-to-determine-the-mesoi" class="section level4 unnumbered">
<h4 class="unnumbered">Considering: ‘should I use a function of previous estimated effect sizes to determine the MESOI?’</h4>
<p>From David Moss (unfold)</p>

<div class="fold">
<p>Moss:</p>
<blockquote>
<p>… not basing power calculations on previously observed effect sizes?</p>
</blockquote>
<blockquote>
<p>Lakens uses the SESOI approach, which we often do, but SESOI can be specified based on the effect sizes previous found in the literature <a href="https://journals.sagepub.com/doi/10.1177/2515245918770963">though obviously there are a bunch of ways to do it</a></p>
</blockquote>
</div>
<p><br />
</p>
<p>Moss, citing Lakens: … use earlier work to decide which effect sizes are deemed to be ‘meaningful,’ with particular specific recommendations:</p>

<div class="fold">
<blockquote>
<p>Subjective justification of a smallest effect size of interest … Second, the SESOI can be based on related studies in the literature. Ideally, researchers who publish novel research would always specify their SESOI, but this is not yet common practice. It is thus up to researchers who build on earlier work to decide which effect size is too small to be meaningful when they examine the same hypothesis. Simonsohn (2015) recently proposed setting the SESOI as the effect size that an earlier study would have had 33% power to detect.</p>
</blockquote>
<blockquote>
<p>With this small-telescopes approach, the equivalence bounds are thus primarily based on the sample size in the original study. For example, consider a study in which 100 participants answered a question, and the results were analyzed with a one-sample t test. A two-sided test with an alpha of .05 would have had 33% power to detect an effect of d = 0.15. Another example of how previous research can be used to determine the SESOI can be found in Kordsmeyer and Penke (2017), who based the SESOI on the mean of effect sizes reported in the literature. Thus, in their replication study, they tested whether they could reject effects at least as extreme as the average reported in the literature. Given random variation and bias in the literature, a more conservative approach could be to use the lower end of a confidence interval around the meta-analytic estimate of the effect size (cf. Perugini, Gallucci, &amp; Costantini, 2014).</p>
</blockquote>
Another justifiable option when choosing the SESOI on the basis of earlier work is to use the smallest observed effect size that could have been statistically significant in a previous study. In other words, the researcher decides that effects that could not have yielded a p less than <span class="math inline">\(\alpha\)</span> in an original study will not be considered meaningful in the replication study either, even if those effects are found to be statistically significant in the replication study. The assumption here is that the original authors were interested in observing a significant effect, and thus were not interested in observed effect sizes that could not have yielded a significant result. It might be likely that the original authors did not consider which effect sizes their study had good statistical power to detect, or that they were interested in smaller effects but gambled on observing an especially large effect in the sample purely as a result of random variation. Even then, when building on earlier research that does not specify a SESOI, a justifiable starting point might be to set the SESOI to the largest effect size that, when observed in the original study, would not have been statistically significant.
</div>
<p><br />
</p>
<p>DR response: Why do we assume previous authors considered MESOI?</p>

<div class="fold">
<p>I’m missing the logic in the quotes above as to “why the previously detected affects, or some bounds on these should represent the minimum effect size of interest?”</p>
<p>Perhaps there is some justification in “assuming that previous authors have powered their study correctly to detect such a minimum affects,” But to me this just seems like kicking the can down the road and I do not assume this in general. We know that people run under powered studies all the time (see the previous discussion on the harm to science)</p>
</div>
<p>DR: A second reason why one might see that as the “minimum effect size of interest” simply has to do with being able to publish a paper that can in some way “refute” previous claimed findings.</p>

<div class="fold">
<p>But that is flawed, in my view, as a way of doing science. We should power the study that is most informative, either by itself, or when made part of a meta analysis. I don’t see the value of this adversarial back-and-forth approach.</p>
</div>
<p><br />
</p>
<p>DR preferred approach - power a study based on policy concerns, also considering it’s use in meta-analysis.</p>

<div class="fold">
<p>One basic argument is: I want to power a study as a practical goal based on my policy concerns. Typically the value of the study will depend on how precisely you are able to estimate and “bound” a parameter. (This may be expressed as a conference interval or a credible interval if we are thinking of a Bayesian posterior).</p>
<p>So, in determining how much power I wish to achieve, I need to weigh the benefits of this precision against the cost of a larger sample size.</p>
<p>This is a very different considerations from “what power do I have to detect that a previously estimated effect size, if true, is ‘statistically significant’” (By the standard definition).</p>
</div>
<p>Of course “We should power” is subject to constraints and cost concerns.</p>
<div class="marginnote">
<p>So, when I am designing the study I am almost never able to have the power I want for all possible tests/hypotheses. Where these considerations come into play, is whether to decide to run the study now or wait to get more funding, and in considering which hypotheses to test and which treatments to put in the study, et cetera</p>
</div>
</div>
</div>
<div id="underpowered" class="section level2" number="14.3">
<h2 number="14.3"><span class="header-section-number">14.3</span> The ‘harm to science’ from running underpowered studies</h2>
<blockquote>
<p>"One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.
- From an anonymous referee report</p>
</blockquote>
<p>Perhaps most of us consider power largely in thinking about</p>
<ol style="list-style-type: decimal">
<li>“Is our analysis going to be fruitful for ourselves as researchers?”</li>
</ol>
<p>and perhaps also, where we find a null result…</p>
<ol start="2" style="list-style-type: decimal">
<li>“Is the analysis powerful enough to plausibly rule out an effect of a meaningful size?”</li>
</ol>
<p>The conventional wisdom has been that, at least for papers reporting non-null effects, running a low-power study is mostly done at the authors’ <em>own</em> peril. We might think “if I am lucky enough to observe a strong effect in an low-powered study then I have managed to mine a vein of truth on a relatively unproductive plot, and have thus earned my reward.”</p>
<p>However <span class="citation">(<a href="#ref-buttonPowerFailureWhy2013" role="doc-biblioref">Button et al. 2013</a>)</span> point out that running lower-powered studies reduces the positive predicted value—the probability that a “positive” research finding reflects a true effect—of a typical study reported to find a statistically significant result.</p>
<p>In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.</p>
<div class="marginnote">
<p>DR: However, I speculate that this idea might be less clear-cut than it seems. E.g., if we consider a (“non-sparse”) world where every factor indeed has an effect, lower powered studies are more likely to detect effects that are truly larger, which are arguably more policy-relevant; moreover, overstated effect sizes might be adjusted with a standard correction.</p>
</div>

<div class="note">
<p><a href="https://www.pauljferraro.com/publications/2020/2/1/is-there-a-replicability-crisis-on-the-horizon-for-environmental-and-resource-economics">Ferraro discussion on magnitude error due to underpowered studies:</a> {-}</p>
<p>… if you are looking at an under-powered design then sure, you might pick up a significant result which is actually spurious. But on top of that even if there is a genuine effect there, the effect that you actually pick up as being significant will (likely) be overestimated. The intuition behind that result is (I think) that for an effect to be picked up in a study then it has to be large enough to overcome the issue that you face with power. Low-powered studies can only detect really large effects, and so the large effect you pick up in such a study could be genuine, but it equally could be a poorly-estimated coefficient. By using a low powered study you sift through for these kind of effects.</p>
</div>
</div>
<div id="power-calculations-without-real-data" class="section level2" number="14.4">
<h2 number="14.4"><span class="header-section-number">14.4</span> Power calculations without real data</h2>
<p><a href="https://cran.r-project.org/web/packages/paramtest/vignettes/Simulating-Power.html">R ‘Paramtest’ package vignette</a> is helpful here.</p>
</div>
<div id="power-calculations-using-prior-data" class="section level2" number="14.5">
<h2 number="14.5"><span class="header-section-number">14.5</span> Power calculations using prior data</h2>
<p>Adapt example in ‘scopingwork.Rmd’ to this</p>
<div id="from-reinstein-upcoming-experiment-preregistration" class="section level3" number="14.5.1">
<h3 number="14.5.1"><span class="header-section-number">14.5.1</span> From Reinstein upcoming experiment preregistration</h3>
<blockquote>
<p>We are searching for a design and sample size that has sufficient power to detect (or ‘statistically rule out’) an effect of ‘minimal interest’ size, given our somewhat-limited budget. The ‘design parameters’ we can play with are given above.</p>
</blockquote>
<blockquote>
<p>While conventional practice seems to involve completely simulated data based on parametric assumptions (normality, etc) we prefer to draw from comparable ‘untreated’ real-world data (see (Barrios 2014) for a related discussion). Assuming the general distribution of outcomes (and covariates) is in some sense constant or predictable over time (perhaps stationarity?), this should give us more accurate estimates of power.</p>
</blockquote>
<blockquote>
<p>We will consider each design’s power to detect particular ‘treatment effects’ (of a minimum relevant size) on particular outcomes, which may be linear, proportional, or otherwise. Our calculations do not depend on any assumptions over the ‘true treatment effect.’</p>
</blockquote>
<div id="assign-notes" class="section level4 unnumbered">
<h4 class="unnumbered">Assignment procedures to consider</h4>
<p>We consider three categories of possible assignment criteria: (We are coding these <a href="#assign-functions">below</a>.)</p>
<p><strong>1. Simple data-based</strong></p>
<p>(Coded <a href="#power-simple">here</a>)</p>
<p>Here we imagine a very simple dynamic assignment to treatments with alternation (or repeated from an urn with one ball per treatment, refilling the urn once empty). This procedure will essentially guarantee an equal share of observations in each treatment.</p>
<p>We will also consider an unbalanced design, both in this and in other categories, which may achieve greater power, especially considering the differential costs of our treatments.</p>
<div class="marginnote">
<p>Although as we have no evidence on the treatments and thus no reason to anticipate a differential variance between treatments and control, an unbalanced design may allow greater power for the same <em>cost</em>, as observing controls is costless, and the ‘low-donation’ treatment is lower cost.</p>
</div>
<p><br />
</p>
<p><strong>2. Ad-hoc (Reinstein adapts Barrios), using prediction.Rmd quantiles</strong></p>
<p><em>General summary:</em> Fit a predictive model of the outcome (total donations) based on pre-treatment observables, using set-aside training data. Generate quantiles of ‘predicted donation’ (tuning parameter=number of quantiles?). Power-test block randomisation with these blocks as quantiles, using set-aside testing data.</p>
<p><em>Caveats:</em> If we test the power with multiple models on the test data (e.g., ‘tuning’ the number of quantiles) we will be overly optimistic and maybe overfitting.</p>
<p>Also, this assignment procedure is not necessarily robust to TE heterogeneity. By luck, it may assign substantial <em>imbalance</em> across any particular dimension.</p>
<p><br />
</p>
Prediction algorithm (folded)

<div class="fold">
<ul>
<li>Adapt from code in CharitySubstitutionExperiment repo, in assignments_power.Rmd and analysis_subst.Rmd; examples of Elastic net etc</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Define and organize the set of variables available at intervention</p></li>
<li><p>Define and calculate the outcome variables (total amount raised)</p></li>
<li><p>Split and set-aside validation and simulation data (?within prediction also)</p></li>
<li><p>Model the outcome, using a Ridge regression with all features. The regularization/penalty parameter could be optimized for best fit. (Cross-fold).</p></li>
</ol>
</div>
Blocking process (folded)

<div class="fold">
<ol start="5" style="list-style-type: decimal">
<li>We can test block randomization by the predicted quantile of this model with a bootstrapped procedure using set-aside ‘testing’ data not used in the above regression. We want to run the simulations until we find the optimal “block width” or quantile to use for blocking the randomization. Ideally, this procedure should take into account the fact that if we stopped at a random time we may have uneven cell sizes.</li>
</ol>
<p>(Caveat – this resampling may need to be done based a randomised ‘start time’ to address random time-specific effects. ) <em>Note</em>: Because of this and for feasibility, we may abridge step 5, and just try out a few reasonable large block widths (e.g., quartiles)</p>
<p>Probably the way to do this is, for each proposed block width (e.g., quartiles, deciles, 15 bins, etc) we draw random samples from the set-aside data according to this procedure, and estimate an “effect size” for each sample. We then consider the 99% bounds of these simulated effect sizes. The block width (and regularization parameter?) that consistently gives us the tightest bounds should be the one that allows us the greatest power to rule out an effect of a certain size, given the null hypothesis of no treatment effect.</p>
<p>The intuition: The smaller the largest difference in mean total donations (between treatment and control) that occurs by chance in 99% of draws… the smaller the <em>actual</em> effect that we will be powered to detect (able to judged as statistically significant a reasonable share of the time). [Note: these latter notes may bot go with the procedure proposed below; these are older.]</p>
</div>
<p><strong>Kasy method?</strong></p>
<p>We may or may not get to considering the method proposed in <span class="citation"><a href="#ref-kasyWhyExperimentersMight2016" role="doc-biblioref">Kasy</a> (<a href="#ref-kasyWhyExperimentersMight2016" role="doc-biblioref">2016</a>)</span>. It is ideal for Bayesian approaches to policy, but it may make frequentist inference difficult (?). (See steps/notes folded below.)</p>
<div class="fold">
<p>
See kasy_2016_dont_randomise.md; kasy_dynamic.md may also be relevant.
</p>
<ul>
<li>
Prepare concise data set (csv?) to throw into his app, choose baseline covariates
</li>
<li>
Estimator: Difference of means or Bayes
</li>
<li>
Prior: Squared exponential or Linear?
</li>
<li>
Re-randomization draws (default=1000); Expected R-sq (default=0.7)
</li>
</ul>
<p>
Notes: - Stratify on ‘discrete strata’ - Conservative: difference in means without controls or interactions - More reasonable, fully general: Make estimator in Power calc regression with strata dummies and interactions with treatment, usual Robust standard errors - But Kasy’s technique makes frequentist inference difficult (Bayesian OK)
</p>
</div>
<p><strong>See Guidance/code:</strong></p>
<ul>
<li>Simple; from <a href="https://egap.org/content/power-analysis-simulations-r" class="uri">https://egap.org/content/power-analysis-simulations-r</a></li>
</ul>
<p><a href="https://egap.org/resource/script-power-analysis-simulations-in-r/" class="uri">https://egap.org/resource/script-power-analysis-simulations-in-r/</a></p>
<p><a href="https://egap.org/resource/10-things-to-know-about-statistical-power/" class="uri">https://egap.org/resource/10-things-to-know-about-statistical-power/</a></p>
</div>
<div id="assign-functions" class="section level4 unnumbered">
<h4 class="unnumbered">Treatment assignment functions, used in power calculations</h4>
<ol style="list-style-type: decimal">
<li>simple_assign: assign treatment dummy to first t-share of n rows</li>
</ol>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>simple_assign <span class="ot">&lt;-</span> <span class="cf">function</span>(df, <span class="at">tshare=</span><span class="fl">0.5</span>, <span class="at">blockvar=</span><span class="st">&quot;NA&quot;</span>) { <span class="co">#note: no blocking here, this is just to get a homogenous code</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(df, <span class="at">d_t =</span> <span class="fu">row_number</span>() <span class="sc">&lt;=</span> (<span class="fu">n</span>() <span class="sc">*</span> tshare)) <span class="co">#note: the data must be randomised first!</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><br />
</p>
<p><strong>Data-based power calculation: create simulation function</strong> {#power-calc-sim-func}</p>
<!-- partly from https://stats.stackexchange.com/questions/37796/calculating-necessary-sample-size-using-bootstrap -->
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>power_data <span class="ot">&lt;-</span> <span class="cf">function</span>(ds, reps, yvar, N, <span class="at">tshare=</span><span class="fl">0.5</span>, <span class="at">linTE=</span><span class="dv">0</span>, <span class="at">propTE=</span><span class="dv">0</span>, <span class="at">alpha=</span><span class="fl">0.05</span>, <span class="at">test_nm=</span> wilcox.test, <span class="at">f_assign=</span>simple_assign, bv) {</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    results  <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>reps, <span class="cf">function</span>(r) { <span class="co">#TODO - replace with purr::map (Toby)</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co">#1. Sample size N from data for each iteration</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    exp_sample <span class="ot">&lt;-</span> <span class="fu">sample_n</span>(ds, <span class="at">size =</span> N, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">#2. Selection of control and treatment group using function `f_assign&#39;</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    exp_sample <span class="ot">&lt;-</span> exp_sample <span class="sc">%&gt;%</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">f_assign</span>(<span class="at">tshare=</span>tshare, <span class="at">blockvar=</span>bv) <span class="sc">%&gt;%</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">#3. Add treatment effects (`propTE` and `linTE`) to treatgroup</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>({{yvar}} <span class="sc">:</span><span class="er">=</span>   <span class="fu">ifelse</span>(</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        d_t <span class="sc">==</span> <span class="st">&quot;TRUE&quot;</span>, {{yvar}} <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> propTE) <span class="sc">+</span> linTE, {{yvar}})) <span class="sc">%&gt;%</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co">#TODO: Do this for several y-variables in each run; &#39;map&#39; these?</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">yv :=</span> {{yvar}}) <span class="sc">%&gt;%</span>  <span class="co">#reassign variable because I couldn&#39;t figure out how to get the unquoted argument to work in tests below #TODO-fix</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">#4. Run chosen test `test_nm`, output p-value</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">select</span>(yv, d_t)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    test <span class="ot">&lt;-</span> exp_sample <span class="sc">%&gt;%</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>      <span class="fu">do</span>(<span class="fu">tidy</span>(<span class="fu">test_nm</span>( yv <span class="sc">~</span> d_t, <span class="at">data =</span> ., <span class="at">paired =</span> <span class="cn">FALSE</span> )))</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    test<span class="sc">$</span>p.value</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="co">#5. Output share of p-values below alpha</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sum</span>(results <span class="sc">&lt;</span> alpha) <span class="sc">/</span> reps</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>block_1d_assign: assign treatment dummy to first t-share within each (pre-calculated) one-dimensional block group (<code>blockvar</code>)</li>
</ol>
<p><em>Note: I am using ‘randomizr’ here to assign blocks</em></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>block_1d_assign <span class="ot">&lt;-</span> <span class="cf">function</span>(df, <span class="at">tshare=</span><span class="fl">0.5</span>, blockvar) {</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>      block_rand <span class="ot">&lt;-</span> <span class="fu">as.tibble</span>(</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>        randomizr<span class="sc">::</span><span class="fu">block_ra</span>(</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">blocks =</span>  df[[<span class="fu">glue</span>(<span class="st">&quot;{blockvar}&quot;</span>)]], <span class="at">conditions =</span> <span class="fu">c</span>(<span class="st">&quot;control&quot;</span>,<span class="st">&quot;treat&quot;</span>), <span class="at">prob_each=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span>tshare, tshare)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>          )</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>      df <span class="ot">&lt;-</span> <span class="fu">as.tibble</span>(<span class="fu">bind_cols</span>(df, block_rand)) <span class="sc">%&gt;%</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">rename</span>(<span class="at">d_t=</span>value) <span class="sc">%&gt;%</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="fu">mutate</span>(</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>          <span class="at">d_t=</span>(d_t<span class="sc">==</span><span class="st">&quot;treat&quot;</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="power-calculation-just-simple-examples-adapt-to-built-in-data" class="section level4 unnumbered">
<h4 class="unnumbered">Power calculation (just simple examples): adapt to built-in data</h4>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">#pwr_n400_L50_p15 &lt;- power_data(ds=df,reps=100,yvar=sum_don,N=400,tshare=0.5,linTE=50,propTE=0.15,alpha=0.05, f_assign=simple_assign)</span></span></code></pre></div>
</div>
<div id="loop-and-plot-over" class="section level4" number="14.5.1.1">
<h4 number="14.5.1.1"><span class="header-section-number">14.5.1.1</span> Loop and plot over…</h4>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>linTE.try <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">50</span>,<span class="dv">100</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>propTE.try <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="fl">0.05</span>, <span class="at">to =</span> <span class="fl">0.2</span>, <span class="at">by =</span> <span class="fl">0.05</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>outcomes.try <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;sum_don&quot;</span>,<span class="st">&quot;count_don&quot;</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>tests.try <span class="ot">&lt;-</span> <span class="fu">c</span>(t.test, wilcox.test)</span></code></pre></div>
<p>Testing equicost parings, determine necessary cost</p>
<p>With only control and treatment we have</p>
<p><span class="math display">\[cost = multiplier \times avgcost \times N \times tshare\]</span>
<span class="math display">\[\rightarrow N = cost/\big(multiplier \times avgcost \times tshare\big)\]</span></p>
<p>Setting a 2x multiplier (for the ‘large’) treatment, avgcost=£30 (hard coded) for now, and imagining a £6000 initial budget yields</p>
<p><span class="math display">\[\rightarrow N = 6000/\big(60 \times tshare\big) \]</span></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>cost <span class="ot">&lt;-</span> <span class="dv">6000</span> <span class="co">#make this an entry in the &#39;design_params&#39; list</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>multip <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>avdon <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>tshare.try <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="fl">0.1</span>, <span class="at">to=</span><span class="fl">0.5</span>, <span class="at">by=</span><span class="fl">0.1</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>sample.try <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="dv">400</span>, <span class="at">to=</span><span class="dv">1200</span>, <span class="at">by=</span><span class="dv">200</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>equi_sample <span class="ot">&lt;-</span> cost<span class="sc">/</span>(avdon<span class="sc">*</span>multip<span class="sc">*</span>tshare.try)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># now maybe make a vector of tshare.try and equi_sample, for iterating over</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># I assume we can consider the &#39;optimal tradeoff&#39; and this will be invariant to the cost; am I right?</span></span></code></pre></div>
</div>
<div id="for-total-x-outcome-variable" class="section level4 unnumbered">
<h4 class="unnumbered">For ‘total x’ outcome variable</h4>
<p>(Some sample code below, needs discussion)</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Unvarying parameters up here:</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>linTE <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co">#power_vals: tibble to collect parameters and results #TODO: faster to generate a list?</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>power_vals <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_try=</span><span class="cn">NA</span>,</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">prop_te =</span> <span class="cn">NA</span>,</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">lin_te=</span><span class="cn">NA</span>,</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">test=</span><span class="cn">NA</span>,</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">power_est=</span><span class="cn">NA</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co">#tic() #timer</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="co">#for (o in seq_along(outcomes.try)) { #TODO: map instead of loops. See R4ds 21.7 &#39;mapping over multiple arguments&#39;</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>  <span class="co">#OUTCOME &lt;- outcomes.try[[o]] #TODO: may be faster to generate all outcomes for each sample ; also, I haven&#39;t got the syntax to work</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>  df_x <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(outcomes.try)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">#select(OUTCOME) #Minimal data set to speed it up; (seems to save about 50% of the time)</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>  <span class="co">#loop over tests</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (t <span class="cf">in</span> <span class="fu">seq_along</span>(tests.try)) {</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>    TEST <span class="ot">&lt;-</span> tests.try[t]</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Loop over proportional TE</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (p <span class="cf">in</span> <span class="fu">seq_along</span>(propTE.try)) {</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>      PT <span class="ot">&lt;-</span> propTE.try[[p]]</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Loop over sample sizes</span></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (s <span class="cf">in</span> <span class="fu">seq_along</span>(sample.try)) {</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>        N <span class="ot">&lt;-</span> sample.try[s]</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>        PW <span class="ot">&lt;-</span> <span class="fu">power_data</span>(</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>            <span class="at">ds =</span> dfX, <span class="at">reps =</span> <span class="dv">80</span>, <span class="at">yvar =</span> sum_don, <span class="at">N =</span> N, <span class="at">tshare =</span> <span class="fl">0.5</span>, <span class="at">linTE =</span> linTE, <span class="at">propTE =</span> PT, <span class="at">alpha =</span> <span class="fl">0.05</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>          )</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>        power_vals <span class="ot">&lt;-</span> <span class="fu">add_row</span>(</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>            power_vals, <span class="at">n_try =</span> N, <span class="at">prop_te =</span> PT, <span class="at">lin_te =</span> linTE, <span class="at">test =</span> <span class="fu">as.character</span>(TEST), <span class="at">power_est =</span> PW</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>          ) <span class="do">##TODO - more efficient to save the results in a list and combine it into a single vector or dataframe at end (see r4ds 21.3.3)</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>  <span class="co">#}</span></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a><span class="co">#toc()</span></span></code></pre></div>
<p><br />
</p>
</div>
</div>
</div>
<div id="lift-test" class="section level2" number="14.6">
<h2 number="14.6"><span class="header-section-number">14.6</span> Digression: Power calculations/optimal sample size for ‘lift’ in a ranking case</h2>
<p>We want to know what the ‘best title for our new movie’ is. Twenty titles have been suggested. We have funds to do a survey of a relevant representative audience.</p>
<p>We need to decide on a general experimental design, a statistical analysis, and on sample sizes considering power (or perhaps ‘lift’).</p>
<p>Note that although we are mainly framing this in terms of statistical inference, it might also/instead be considered a ‘reinforcement learning’ problem.*</p>
<div class="marginnote">
<p>* See Max Kasy’s slides and articles on <a href="https://maxkasy.github.io/home/files/slides/adaptive_field_slides_kasy.pdf">adaptive field experiments</a>, particularly considering ‘exploration sampling.’</p>
</div>
<p>A long debate on this in the folds below:</p>
<p><strong>Is classical statistical inference the right framing here?</strong></p>

<div class="fold">
<p>David: I think classical statistical inference is the wrong framing here* o think about ‘what sample size (or number of arms) to maximise value’ … even if we don’t do adaptive sampling</p>
<blockquote>
<p>Matt: I’m (currently) not sure there is any meaningful alternative (or, therefore, a better alternative)</p>
</blockquote>
<p>David: The reinforcement-learning and value-maximization (lift) framework used in industry (I believe). The challenge here would be to map between the survey responses given and the actual outcomes of interest… e.g., if a person responds “I would definitely watch a movie with this title,” what is the probability they actually will, relative to someone who responds “I might watch…” (also, what is the ‘population size’ we are sampling from, to understand the scale)</p>
<blockquote>
<p>Matt (“The challenge here would be to map between the survey responses given and the actual outcomes of interest”) This is the thing I’m just assuming we cannot do. Given that, it’s just not clear how reinforcement learning framework helps us?</p>
</blockquote>
<p>David: why not make reasonable assumptions about the above, for example? But even if we framed it strictly in terms of ‘what we learn about how the population would respond to similar questions asked in the survey,’ I still don’t see why (e.g.) our goal should be to be “strongly powered to reject an H0 that ‘all titles rank just as highly on average”. Instead we want to ‘learn as much as possible about whatever we think, in the survey, is likely to be the most valuable response’</p>
</div>
<p><br />
</p>
<p><strong>Is there a better approach to determining sample size?</strong></p>

<div class="fold">
<blockquote>
<p>Matt: <strong>What is the better approach to determining sample size?</strong></p>
</blockquote>
<p>David: It’s a cost/benefit calculation. If we are cost-constrained, this question becomes ‘how many arms (titles)’ and ‘is this worth doing.’ NHST framework logic may say ‘given your budget, only try 2 titles, and even if you have to choose them at random’ and/or “don’t bother, the chance of statistical significance is too low.”</p>
<p>In contrast (my loosely-informed guess is that) a RL approach says that (if we cannot do adaptive learning, if the ‘client’ finds it equally likely that all 20 titles are optimal, the ‘most value-increasing learning’ (highest ‘lift’) will simply come from dividing their budget equally across all 20 titles and choosing the one that ‘performs best’ (obviously ‘performs best’ is another can of worms) … even if this yields little power for strong statistical inference.</p>
<blockquote>
<p>Matt: The approach I describe/propose (using classical approach to power analysis/hypothesis testing) also involves cost/benefit calculation. So I think this misdescribes the contrast.</p>
</blockquote>
<p>David: You write “can detect Y size effects for X dollars while testing 20 titles vs can detect Z size effects for X dollars while testing 10 titles.” But I don’t think ‘can detect Y effect sizes’ is entirely accurate/descriptive.</p>
<p>CLASSICAL (NHST): At a certain cost you can achieve a certain probability (power) that you will conclude that a particular difference is ‘less than 5% likely to occur by chance under H0.’ However: 1. Even if you don’t ‘reject the null,’ the evidence (if interpreted using Bayesian methods) may already suggest that the null is very unlikely and the center of your belief distribution should be substantially shifted. 2. How does the Classical approach weigh the benefit of learning more about a few titles vs less about many titles?</p>
<blockquote>
<p>Matt: (setting aside number of titles question): NHST approach doesn’t tell us what sample size is best given the cost/benefit. NHST just tells us what sample size we need to have power to do X. Our overall judgement about what sample size to use (and what design etc.) is based on our independent judgements about what amount of cost/power is optimal..</p>
</blockquote>
<blockquote>
<p>Matt: I think may explain some of the apparent disagreement where you propose some approach to cost/benefit as some alternative to NHST, whereas I think NHST and the cost/benefit calculation of what we should do are separate questions On my approach we also judge cost/benefit of having a given level of power to detect an effect of a given size (and a given number of titles). So what’s the alternative approach to establishing sample size required for [a certain level of precision/ability to detect effect.. construed in whatever terms you like], and separately judging what level is optimal given the cost?</p>
</blockquote>
<p>DR: I think these are linked questions, and it is not meaningful to simply state ‘power to detect an effect’ … as there are something like 20 x 20 possible ‘effects’ we could be looking for here.</p>
</div>
<p><br />
<strong>Related discussion: should “Achieving a minimum statistical significance for a particular comparison” be our criterion?</strong></p>

<div class="fold">
<p>David: “Achieving a minimum statistical significance for a particular comparison” should probably not be our criterion … we can generate a lot of value even without being able to make a strong statistical inference (‘rejecting the null… meh’)</p>
<blockquote>
<p>Matt: This seems a separate question.
The fundamental Q here just seems to be how much confidence we want/need to think this valuable to our decision-making. Statistical significance is an arbitrary threshold: but point remains that &lt;significant results give less value Concretely, I would update little on noisy, less significant differences results</p>
</blockquote>
<p>David: My impression is that ‘optimal’ Bayesian updating, even starting with diffuse priors, actually does move a lot in response to data that would not yield strongly significant results. Yes, it does update less with less data and more noisy data, but it still updates a lot in ways that substantially change decisions and add value even in cases where NHST yawns and says ‘nothing to see here, move on.’ If “p=0.25” in a NHST statistical test, that (very loosely and probably not strictly correct here) might suggest that ‘there is a 25% chance that one option is substantially better.’ (I do think there is a place for NHST in scientific inquiry and ‘establishing results without giving the benefit of the doubt. But where you ‘must make a decision’ a different approach is justified IMHO).</p>
<blockquote>
<p>Matt: I agree that non-sig results can be worth updating on and potentially useful for practical purposes. We could have a meaningful discussion about how confident/precise we want our estimates to be / how small an effect is worth measuring without thereby ditching NHST: and you could argue that we should shoot for a lower level of power. It’s not clear there’s a simply better alternative framework for approaching the question. (Even translating these discussions into Bayesian terms seems to just leave us in exactly same position)</p>
</blockquote>
<p>David: Not sure if it’s the same position; see above (previous fold, “it’s a cost/benefit calculation”)</p>
<p>Mostly I agree with what you are saying but you may be missing some of the complications here. The problem is not uni-dimensional, as we are asked to consider a large set of (20) titles.
I am claiming that you might gain more value, and indeed substantial value, from learning “a little about 20 titles” instead of “a lot about 2 randomly selected titles”</p>
<blockquote>
<p>Matt: Question of the value of 20 titles vs 2 titles seems distinct from the question of how much we can learn from noisy/sub-significant results. (Links other post…)</p>
</blockquote>
<blockquote>
<p>… informed guess about what would be worth measuring.. In the simple case, this just involves comparing $X vs can detect Y size effects In the slightly more complex case this also involves comparing: ‘can detect Y size effects for X dollars while testing 20 titles’ vs ‘can detect Z size effects for X dollars while testing 10 titles.’</p>
</blockquote>
<blockquote>
<p>Concretely I have been informing my judgement of how small an effect we might want to detect based on our prior XXX message testing which found (by convention) small effects, and which seems as reasonable a proxy for what we might expect to find here. As noted, the key ingredient to better knowing how big an effect we want to be able to detect would be knowing how well these ratings correspond to real world differences, but we do not know that… Of course there is also a hard ceiling on [costs] i.e. our client won’t/can’t pay more than X dollars …</p>
</blockquote>
<blockquote>
<p>I agree that focusing on 2 random titles out of the 20 would probably provide little practical value as to which out of the proposed titles is best (due to providing no data about 18/20 titles).</p>
</blockquote>
<p>David: :)</p>
<blockquote>
<p>At the other extreme: if a test with 100 titles would only be able to detect effects that were enormous (given resource constraints on our sample size)- then it’s likely we should change test/test fewer titles, because a test that won’t be able to detect reasonable effect sizes with reasonable power, won’t be able to offer practically significant evidence.</p>
</blockquote>
<p>David: I was thinking of the same reductio that I think you are getting at here, and I think the answer may be ‘strictly speaking yes, if our prior is that all titles have identical distributions on average, better to test each title 1x than to test some titles 2x or more. For decision-making purposes, you learn the most and update the most from the first piece of evidence. So I think it actually would be practically significant to the decision problem. Highly recommended: read/listen to that Bayes Rule chapter of Algorithms to live by.</p>
</div>
<p><br />
</p>
<div id="design-which-questions-to-ask-the-audience-about-the-proposed-titles-and-in-what-order" class="section level3" number="14.6.1">
<h3 number="14.6.1"><span class="header-section-number">14.6.1</span> Design: Which questions to ask the audience about the proposed titles, and in what order?</h3>
<p>This is an ‘experimental design for internal identification and external generalisability’ question.
(See <a href="#why_experiment_design">’Identifying meaningful and useful (causal) relationships and parameters</a>)</p>
<p><br />
</p>
<p>Some possibilities:</p>
<ul>
<li>Subjects asked to rank (or rate) all <span class="math inline">\(K=20\)</span> titles (titles <span class="math inline">\(k=1,2,...,K=20\)</span>)</li>
<li>Subjects asked to identify ‘top <span class="math inline">\(C\)</span>’ and ‘bottom <span class="math inline">\(C\)</span>’ (e.g., top and bottom 3) titles</li>
<li>Subjects presented a series of pairwise comparisons</li>
<li>Subjects asked to rate (or say whether they would attend) a single title, with between-subject variation</li>
</ul>
</div>
<div id="which-statistical-testsanalyses-to-run-if-any-and-what-measures-to-report" class="section level3 unnumbered">
<h3 class="unnumbered">Which statistical test(s)/analyses to run (if any) and what measures to report?</h3>
<p>Suppose we asked each subject to rank all <span class="math inline">\(K=20\)</span> titles.</p>
<p>How could we <strong>test if there were any ‘substantial difference in the title rankings’</strong> and what would be a meaningful measure of the ‘extent’ of this difference? We might want to consider some ‘minimum effect size of interest’ and ensure that we have a large enough sample to diagnose such an effect with (e.g.) 80% probability (while maintaining a false-positive type-1 error rate of less than 5%).*</p>
<div class="marginnote">
<p>* However, it is not clear why this is the most relevant question. Simply determining ‘there is a difference of some minimum size’ doesn’t tell us how confident we are about the best title, nor how much value is gained by choosing that title. This suggests a reinforcement learning approach.</p>
</div>
<p><a href="https://en.wikipedia.org/wiki/Friedman_test#:~:text=The%20Friedman%20test%20is%20used,by%20many%20statistical%20software%20packages">Friedman’s Q</a>, is a measure of whether ‘any (at least one?) items are systematically ranked higher or lower.’ <span class="math inline">\(Q\)</span> can be normalized into, <a href="https://en.wikipedia.org/wiki/Kendall%27s_W">Kendall’s W</a>, a measure of ‘inter-rater agreement’ going from 0 to 1. There is a significant test for W “against a null hypothesis of no agreement (i.e. random rankings).”</p>
<p><a href="https://rdrr.io/cran/rstatix/man/friedman_effsize.html">Kendalls uses the Cohen’s interpretation guidelines of 0.1 to 0.3 being a ‘small effect’</a></p>
<p>“A significant Friedman test can be followed up by pairwise Wilcoxon signed-rank tests for identifying which groups are different,” with multiple testing corrections. <a href="https://www.datanovia.com/en/lessons/friedman-test-in-r/">datanovia website</a></p>
<p><br />
</p>
</div>
<div id="how-to-assign-the-treatments-and-how-large-a-sample-is-optimal-considering-power-or-lift" class="section level3 unnumbered">
<h3 class="unnumbered">How to assign the ‘treatments,’ and how large a sample is optimal, considering ‘power’ (or ‘lift’)?</h3>
<div id="simple-assignment" class="section level4 unnumbered">
<h4 class="unnumbered">Simple assignment</h4>
<p>Suppose we are restricted to a single allocation of treatments across the 20 titles. Suppose we asked all subjects to rank all of the <span class="math inline">\(K=20\)</span> titles, or perhaps only to focus on the ‘best’ and ‘worst’ <span class="math inline">\(C\)</span> titles.</p>
<p>The true population has a large number of subjects (individuals). In our survey we are sampling some number <span class="math inline">\(N\)</span> of individuals. Let each subject be indexed by <span class="math inline">\(i\)</span>, so a particular sample will contain subjects <span class="math inline">\(i=1,...,N\)</span>.</p>
<p><br />
</p>
</div>
<div id="calc.-1-detect-minimally-important-effect-with-simple-assignment" class="section level4 unnumbered">
<h4 class="unnumbered">Calc. 1: Detect ‘minimally important effect?’ (with simple assignment)</h4>
<p><em>We might frame our test and power calculation as the following:</em></p>
<p>Suppose the ‘Minimal effect of interest’ that we want to be able to detect is (sort of the ‘alternative hypothesis HA’)…</p>
<blockquote>
<p>HA: “One title is ranked by a share of the population that is one and a half times as high as any other title.”</p>
</blockquote>
<p>If all the other titles have the same (lower) ranking on average, this should offer the greatest chance of detecting such a difference. Thus, if we assume all other titles share the same average ranking, the computations (below) should <em>underestimate</em> the necessary sample size.</p>
<p>I.e., defining <span class="math inline">\(r_{k,i}\)</span> as the rank given to title <span class="math inline">\(k\)</span> by subject <span class="math inline">\(i\)</span>, and letting <span class="math inline">\(\bar{R^1_i}=\frac{1}{N}\sum I(r_{k,i}=1)\)</span> be the share of the sampled population (subjects) ranking title <span class="math inline">\(k\)</span> as first…</p>
<p>we may consider a case where <span class="math inline">\(\bar{R^1_j} &gt; \frac{3}{2}\frac{1}{19}\sum_{k\neq j}\bar{R^1_k}\)</span> for some title <span class="math inline">\(j\)</span> relative to all other titles <span class="math inline">\(k\)</span>.*</p>
<div class="marginnote">
<p>I think the latter term may simply be <span class="math inline">\(\frac{3}{2}\frac{19}{2}\)</span>, or something similar, the ‘average rank.’</p>
</div>
<p><br />
</p>
<p><strong>Perhaps, we want to power our test so, for the “HA” described above, we have an 80% chance that we ‘find an effect.’</strong> I.e., an 80% chance that our test statistic (whatever it is) tells us that “it is less than 5% likely that this title would have performed as well in our sample by chance if (H0) all titles been perceived as equally good and thus randomly ranked in the population.”**</p>
<div class="marginnote">
<p>** However, I don’t really think that that is <em>really</em> what we are looking for, as we are facing a <em>decision</em> problem.</p>
</div>
<p><strong>To test for this</strong> we would follow a certain procedure, e.g.,**</p>
<div class="marginnote">
<p>*** I am not sure this is an appropriate procedure.</p>
</div>
<ol style="list-style-type: decimal">
<li><p>Find the title ‘j’ that has the most people in our <em>sample</em> who rank it first; call this share <span class="math inline">\(\bar{R^1_j}\)</span></p></li>
<li><p>Compute (perhaps through simulation) the probability that, if all titles were randomly ranked by the population, in a sample of size <span class="math inline">\(N\)</span> (our actual sample size), the average rank of the highest-ranked title would be as high as <span class="math inline">\(\bar{R^1_j}\)</span>.****</p>
<div class="marginnote">
<p>**** Ideally there is an analytical formula for this; worth looking up.</p>
</div></li>
<li><p>If this computed ‘probability of such an extreme result,’ given our sample size <span class="math inline">\(N\)</span>—<span class="math inline">\(P(N, \bar{R^1_j})\)</span>— is below our threshold <span class="math inline">\(\alpha=0.05\)</span> we ‘reject the null.’</p></li>
</ol>
<p><br />
</p>
<p>We have defined a (design and) testing procedure. We can now <strong>simulate</strong> (or perhaps analytically calculate) <strong>the power (of our design and test) to detect the above HA</strong>, as follows.</p>
<p>Run <span class="math inline">\(T\)</span> simulations <span class="math inline">\((t=1,...,T)\)</span>. For each simulated sample <span class="math inline">\(t\)</span>:</p>
<ul>
<li>Draw <span class="math inline">\(N\)</span> observations from an imagined ‘true population,’ i.e.,
<ul>
<li>for each of the <span class="math inline">\(N\)</span> subjects drawn, say, ‘subject <span class="math inline">\(i\)</span>,’ for each of their ‘ranks <span class="math inline">\(r=1,2,..,20\)</span>,’ draw a title (by random) to be assigned this rank (<span class="math inline">\(r_{k,i}=r\)</span>),</li>
<li>with one title (the same one always) having a <span class="math inline">\(\frac{2}{21}\)</span> probability of being drawn for the first rank, and the other 19 titles each having a <span class="math inline">\(\frac{1}{21}\)</span> probability of being drawn for the first rank</li>
</ul></li>
<li>Compute <span class="math inline">\([\bar{R^1_j}]^t\)</span>: the average rank for each title in simulation <span class="math inline">\(t\)</span>,
<ul>
<li>and then compute (or simulate) <span class="math inline">\(P(N, [\bar{R^1_j}]^t) \equiv [P]^t\)</span>, the test statistic as defined above, the ‘likelihood of such an extreme result under the null hypothesis of random ranks’</li>
</ul></li>
</ul>
<p>Over a sufficient number of simulations, determine the average probability of ‘rejecting the null’ in favor of the above HA (specifically for the ‘correct’ title <span class="math inline">\(j\)</span>).* This is the estimated power of the test.</p>
<div class="marginnote">
<p>* This last point is a wrinkle I’ve not seen in previous work involving power calculations, so I hope I am not missing something here. Perhaps we should say something like “power of the test in the right direction.”</p>
</div>
<p><br />
</p>
</div>
<div id="calc.-2-find-a-title-in-the-top-eta-e.g.-third-of-the-value-of-title-distribution-with-simple-assignment" class="section level4 unnumbered">
<h4 class="unnumbered">Calc. 2: Find a title in the top <span class="math inline">\(\eta\)</span> (e.g., third) of the ‘value of title’ distribution (with simple assignment)</h4>
<p>For any target outcome (movie box office, general acclaim, etc.), each of the (<span class="math inline">\(K=20\)</span>) titles will have some <em>true population parameter</em>.</p>
Let us call the parameter of interest <span class="math inline">\(\theta_k\)</span> for title <span class="math inline">\(k\)</span>, which we will call the ‘value of title <span class="math inline">\(k\)</span>.’ For now, let us assume that this ‘population parameter’ represents the mean (or some other function?) of something that we can observe in our survey from each sampled subject.*
<div class="marginnote">
<p>* Of course, for this example, and in general, there are many reasons why we may not be able to observe exactly the thing that informs our parameter from a particular survey. Our survey/experiment design may not be able to get subjects to tell us exactly what we want to know about their impressions of the title. (E.g., they may not themselves know which title would be most likely to get them to attend the film. Even if we ask them something related to this like, “would you like us to email you a discount coupon for a movie with his title?” this may not perfectly track with movie-attendance choices in other contexts.</p>
</div>
Consider a ‘(prior) distribution over parameter <span class="math inline">\(\theta_k\)</span> for each title <span class="math inline">\(k\)</span>.’**
<div class="marginnote">
<p>** I won’t get into the details here about what this distribution means in a philosophical sense. There are various types of Bayesian conceptions of this. Perhaps another way of thinking about it is that each title was as if randomly drawn from some distribution of titles.</p>
</div>
<p>Without further information, we might let (our belief about) the distribution of this parameter for each title be the same. We might consider that, e.g., the “expected revenue from each title” is Normally distributed <span class="math inline">\(\theta_k \sim N(\mu,\sigma^2) \forall k\)</span>.</p>
<p>Perhaps each subject’s ranking of titles is monotonic (i.e., rank-preserving) in the true probability that they would attend a movie with each of these titles.</p>
<p><br />
</p>
<p>Perhaps, we want to choose a sample size such that the title we claim has the “best overall ranking in our sample” has a <span class="math inline">\(B=0.8\)</span> or greater chance of being in the top-third of the true most-profitable titles.</p>
Note that choosing a title that is “most likely to be in the top third” is not necessarily the same as “choosing the title with the greatest expected profit.” Our ranks are a multi-dimensional measure; thus one title need not ‘dominate’ the other title. E.g., in a sample of <span class="math inline">\(N=100\)</span> title <span class="math inline">\(A\)</span> may be ranked first by 10 people and last by 90 people, while title <span class="math inline">\(B\)</span> may be ranked second by all 100 people. Which is more profitable on average will depend on the relationship between ranking and probability of attendance.*
<div class="marginnote">
<p>* E.g., perhaps this implies a 90% probability that <span class="math inline">\(B\)</span> will be more profitable than <span class="math inline">\(A\)</span>, but for the 10% of the states-of-the world when is more profitable than <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span> is 100 times more profitable. This would imply that title <span class="math inline">\(A\)</span> has a greater expected value profit, but also involves more risk.</p>
</div>
<p><br />
</p>
<p><strong>Nonetheless</strong>,</p>
<p>Consider whether title <span class="math inline">\(k\)</span> is among the titles that is ‘ranked in the top-third’ by the largest share of the population. More specifically, consider <span class="math inline">\(\bar{R}^{7+}_k \equiv E[r_{k}\leq7]\)</span> as the ‘share of the population ranking title <span class="math inline">\(k\)</span> top 7 out of 20 or better. Let <span class="math inline">\(D_k=1\)</span> if <span class="math inline">\(\sum_{k,j}1\big(\bar{R}^{7+}_k&gt;\bar{R}^{7+}_j\big)\geq7\)</span>; i.e., if <span class="math inline">\(k\)</span> is one of the top-7 titles in terms of the measure ’ranked in the top third.’</p>
<p>We might want to…</p>
<ol style="list-style-type: decimal">
<li><p>based on our sample (and some assumptions?) choose the title with the highest chance of being ranked in the top-third by the population as a whole (<span class="math inline">\(k^\ast \equiv argmax_k\big(P(D_k=1)\big)\)</span>), and</p></li>
<li><p>sample <span class="math inline">\(N\)</span> large enough so that this ‘chosen title’ has a <span class="math inline">\(\beta=0.8\)</span> chance of indeed being ranked in the top-third by the population as a whole, i.e., <span class="math inline">\(P(D_{k^\ast}=1)&gt;\beta=0.8\)</span>.</p></li>
</ol>
<p><br />
</p>
<p>I imagine that under certain (overly restrictive?) assumptions about the distribution of rankings, we would be able to calculate:</p>
<ol style="list-style-type: decimal">
<li><p>The appropriate procedure for selecting the title that is best by the above metric (<span class="math inline">\(k^\ast\)</span>)</p></li>
<li><p>Determine the minimum neccesary <span class="math inline">\(N\)</span> to achieve this <span class="math inline">\(\beta=0.8\)</span> chance of …</p></li>
</ol>
<p><em>But do we want to do this?!</em></p>
</div>
<div id="sequentialadaptive-designs-multi-armed-bandits" class="section level4 unnumbered">
<h4 class="unnumbered">Sequential/adaptive designs, multi-armed bandits</h4>
<div class="marginnote">
<p>More generally, see @ref(sequential).</p>
</div>
</div>
</div>
</div>
<div id="survey-power-likert" class="section level2" number="14.7">
<h2 number="14.7"><span class="header-section-number">14.7</span> Survey design digression: sample size for a “precise estimate of a ‘population parameter’” (focus: mean of a Likert scale response)</h2>
<div id="how-to-measure-and-consider-the-precision-of-likert-item-responses" class="section level3" number="14.7.1">
<h3 number="14.7.1"><span class="header-section-number">14.7.1</span> How to measure and consider the precision of Likert-item responses</h3>
<p>Considering ‘precision of Likert-item responses’ and sample-size calculations:</p>
<p>What are commonly used/justifiable measures of central tendency and dispersion for Likert-items?</p>
<p>How can we think about ‘precision of estimated Likert-item responses?’ and attaining sufficient precision, and a metric for this?</p>
<p>“How precise is precise, and by what metric?”</p>
<p><br />
</p>
<div id="a-simple-naive-approach" class="section level4 unnumbered">
<h4 class="unnumbered">A simple naive approach?</h4>
<p>Interval coding: <span class="math inline">\(y=[1,2,3,4,5]\)</span> for a 5-item</p>
<p>Outcome: <span class="math inline">\(\bar{y} :=\)</span> Sample mean of numeric-coded responses,</p>
<p>Measure of dispersion: <span class="math inline">\(\hat{s}\)</span> := Sample standard deviation of y *</p>
<div class="marginnote">
<p>Perhaps with the <span class="math inline">\(n-1\)</span> correction, but who cares/</p>
</div>
<p>Measure of (inverse of) precision: estimated standard error of the mean <span class="math inline">\(\hat{SE_m} = s/\sqrt(n)\)</span></p>
<p>If we assume <span class="math inline">\(y\)</span> is normally distributed (which obviously can’t be precisely the case)…**</p>
<div class="marginnote">
<p>* * but Wiki (Derrick and White 2017?) claim “responses often show a quasi-normal distribution.”</p>
</div>
<p>… then a 95% confidence interval for <span class="math inline">\(\bar{y}\)</span> would be</p>
<p><span class="math display">\[\bar{y} \pm  1.96 \: \hat{SE_m}\]</span></p>
<p><br />
</p>
<p><strong>A. ‘Absolute’ metric?:</strong> Target a 95% CI range less than (e.g.) 1 ‘Likert scale unit,’
i.e.,</p>
<p><span class="math display">\[2 * 1.96 \: \hat{SE_m} &lt; 1\]</span> *</p>
<div class="marginnote">
<p>* Or perhaps considering the actual rather than estimated 95% CI this should be “<span class="math inline">\(2 * 1.96 \: SE_m &lt; 1\)</span>.”</p>
</div>
<p>Recall that <span class="math inline">\(\hat{SE_m} = s/\sqrt(n)\)</span>.</p>
<p>Thus, to choose a sample size to achieve these bounds we need to have a measure/guess/estimate of <span class="math inline">\(s\)</span>, the standard deviation of <span class="math inline">\(y\)</span>, perhaps based on previous data.**</p>
<div class="marginnote">
<p>To have (e.g.) an 80% probability of getting these bounds for the actual confidence intervals we would also need a measure of the dispersion of our estimate of this sd. (Hmm, it’s getting complicated).</p>
</div>
<p><br />
</p>
<p><strong>B. ‘Relative’ metric?</strong>: Target a 95% CI range below <span class="math inline">\(B\)</span> sd of the Likert-item-integer-response <span class="math inline">\(y\)</span>:
i.e.,</p>
<p><span class="math display">\[2*1.96 SE_m &lt; B*sd(y)\]</span></p>
<p>i.e., <span class="math inline">\(2*1.96 * s/\sqrt(n) &lt; B*s\)</span>,
i.e., <span class="math inline">\(2*1.96/\sqrt(n) &lt; B\)</span>
i.e., <span class="math inline">\(\sqrt(n) &gt; 2*1.96/B\)</span>
i.e.,</p>
<p><span class="math display">\[n &gt;  15.3664/(B^2)\]</span></p>
<p>… where <span class="math inline">\(sd(y)\)</span> is the true standard deviation of the outcome.</p>
<div class="marginnote">
<p>Note: This <span class="math inline">\(n\)</span> gives should give us an estimated CI with a range of B standard deviations of the outcome. I’m not sure if it implies that, after collecting the sample, our estimates of the CI will always have a range equal to the estimated sd. Need to think about this more.</p>
</div>
<p>As you can see (caveat: calculations need doublechecking), if we assume the Likert-integer-thing is normally distributed, the calculation of ‘how big a sample size (n) we need in order to get, on average, a CI of 1 sd or smaller’ is straightforward.</p>
</div>
</div>
<div id="computing-sample-size-to-achieve-this-precision" class="section level3" number="14.7.2">
<h3 number="14.7.2"><span class="header-section-number">14.7.2</span> Computing sample size to achieve this precision</h3>
<p>Initial thoughts (unfold):</p>

<div class="fold">
<p>If we assume normality, there should be a simple analytical formula for</p>
<p>’Minimum sample size….
… for (e.g.) 80% likelihood …
… of achieving an (e.g.) 95% “confidence interval (CI) over the mean” of a variable…
… that is within (e.g.) 1 standard deviation of the variable on either side</p>
<p><em>Notes:</em></p>
<ul>
<li><p>This is to get CI bounds on the means stated relative to the SD of the variable. If we wanted to bounds in ‘units of the variable’ we would need to know, guess, or estimate the SD of the variable.</p></li>
<li><p>For a Likert variable normality is not a great assumption. We should probably make another assumption over the distribution (or even draw from past data), and then we can either do a similar analytical computation or a simulation based computation (which should be fairly easy)</p></li>
<li><p>I put ‘CI over the mean’ in scare quotes because these are frequentist confidence intervals which are hard to interpret. A Bayesian approach might be more appropriate… worth thinking about</p></li>
<li><p>Not sure whether ‘mean of a Likert-item response’ is important anyway. I’ll read more on Likert scales.</p></li>
</ul>
</div>
<!--chapter:end:experiments_and_study_design/power_calc.Rmd-->
</div>
</div>
</div>
<div id="experimetrics_te" class="section level1" number="15">
<h1 number="15"><span class="header-section-number">15</span> ‘Experimetrics’ and measurement of treatment effects from RCTs</h1>
<div id="which-error-structure-random-effects" class="section level2" number="15.1">
<h2 number="15.1"><span class="header-section-number">15.1</span> Which error structure? Random effects?</h2>
</div>
<div id="randomization-inference" class="section level2" number="15.2">
<h2 number="15.2"><span class="header-section-number">15.2</span> Randomization inference?</h2>
</div>
<div id="parametric-and-nonparametric-tests-of-simple-hypotheses" class="section level2" number="15.3">
<h2 number="15.3"><span class="header-section-number">15.3</span> Parametric and nonparametric tests of simple hypotheses</h2>
<div id="parametric-tests" class="section level3" number="15.3.1">
<h3 number="15.3.1"><span class="header-section-number">15.3.1</span> Parametric tests</h3>
</div>
<div id="non-parametric-tests" class="section level3" number="15.3.2">
<h3 number="15.3.2"><span class="header-section-number">15.3.2</span> Non-parametric tests</h3>
<div id="fligner-policello-test" class="section level4" number="15.3.2.1">
<h4 number="15.3.2.1"><span class="header-section-number">15.3.2.1</span> Fligner-Policello test</h4>
<p><span class="citation"><a href="#ref-randlesAsymptoticallyDistributionfreeTest1980" role="doc-biblioref">Randles et al.</a> (<a href="#ref-randlesAsymptoticallyDistributionfreeTest1980" role="doc-biblioref">1980</a>)</span></p>
<p>Null-hypothesis that the medians in the two groups (samples) are the same.</p>
<blockquote>
<p>Note on assumptions: Under the null, it assumes that the the distributions are symmetric It does not assume that the shape of the distribution is similar in two groups, contrary to the Mann-Whitney-Wilcoxon test.</p>
</blockquote>
</div>
</div>
</div>
<div id="adjustments-for-exogenous-but-non-random-treatment-assignment" class="section level2" number="15.4">
<h2 number="15.4"><span class="header-section-number">15.4</span> Adjustments for exogenous (but non-random) treatment assignment</h2>
</div>
<div id="iv-in-an-experimental-context-to-get-at-mediators" class="section level2" number="15.5">
<h2 number="15.5"><span class="header-section-number">15.5</span> IV in an experimental context to get at ‘mediators?’</h2>
</div>
<div id="heterogeneity-in-an-experimental-context" class="section level2" number="15.6">
<h2 number="15.6"><span class="header-section-number">15.6</span> Heterogeneity in an experimental context</h2>
</div>
<div id="incorporate-above-notes-on-the-econometrics-of-randomised-experiments-athey-and-imbens" class="section level2" number="15.7">
<h2 number="15.7"><span class="header-section-number">15.7</span> Incorporate above: Notes on “The econometrics of randomised experiments” (Athey and Imbens)</h2>
<p>(with an eye towards giving experiments_</p>
<p>Page 7</p>
<blockquote>
<p>Fundamentally, most concerns with external validity are related to treatment effect heterogeneity …
[ considering extrapolation between settings A and B] Units in the two settings may differ in observed or unobserved characteristics, or treatments may differ in some aspect.
to assess these issues it is helpful to have … randomized experiments, in multiple settings [varying] in the distribution of characteristics of the units, and possibly … the nature of the treatments or the treatment rate, in order to assess the credibility of generalizing to other settings</p>
</blockquote>
<ul>
<li><p>Shall we do Fisher’s test based on computing the distribution of differences in means randomly reassigning the “treatment?”</p></li>
<li><p>F-tests to consider multiple outcomes for any cases?</p></li>
<li><p>(When) shall we use covariates (esp those for interactions) in the ‘deviations from mean’ form?</p></li>
<li><p>Considering when to use controls (and interactions?)</p></li>
</ul>
<blockquote>
<p>the asymptotic variance for <span class="math inline">\(\hat{\tau}}\)</span> is less than that of the simple difference estimator by a factor equal to <span class="math inline">\(1-R^2\)</span> from including the covariates relative to not including the covariates
If… the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial</p>
</blockquote>
<ul>
<li><p>DR: Could there not ever be a loss from doing interactions dividing up the sample too fine in doing this interactive estimation? This should depend on the true <span class="math inline">\(R^2\)</span> I think. Try to remember what is the real tradeoff?</p></li>
<li><p>Statistics adjusted for stratification:</p></li>
</ul>
<blockquote>
<p>One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance</p>
</blockquote>
<div class="marginnote">
DR: Is it valid to simply say “we choose the lower value of the estimated variances?” Are they advocating this? Such a procedure seems like it would have a bias.
</div>
<p>** Things to potential incorporate in NL HE lottery paper(s) **</p>
<ul>
<li>(When) shall we use covariates (esp those for interactions) in the ‘deviations from mean’ form?</li>
</ul>
<p>“randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received”</p>
<ul>
<li><p>Consider a “partial identification or bounds analysis” to deal with noncompliance at each margin</p></li>
<li><p>Look up “randomization-based approach to IV” (Imbens and Rosenbaum, 2005)</p></li>
</ul>
<div id="abstract-and-intro" class="section level3" number="15.7.1">
<h3 number="15.7.1"><span class="header-section-number">15.7.1</span> Abstract and intro</h3>
<ol style="list-style-type: decimal">
<li>randomisation-based inference as opposed to sampling based inference</li>
</ol>
<ul>
<li>DR: I Disagree as the object of interest is ultimately not the experimental sample, particularly not in the lab</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>efficiency gains from stratification into small strata, adjust se to capture these gains (We have only done this to a limited extent)</li>
<li>(Non-compliance, intention-to-treat, and IV)</li>
<li>Estimation and inference for <strong>heterogeneous treatment with covariates… </strong> subpopulations… Maintaining the ability to construct valid confidence (mht etc). “Conditional average treatment effects”</li>
<li>(interaction between units)</li>
</ol>
<p>Why careful statistics are important even for randomised experiments</p>
<p>“Randomisation approach”: potential outcomes fixed, Assignment to treatments random</p>
<p>Example of why the randomisation-based inference approach matters:</p>
<p>“in the conventional sampling paradigm… Controlling for observable heterogeneity using a regression model” required for the assumptions to be justified with this approach. With randomisation approach it makes more sense to put data into strata by covariates, analyse within-group experiments and average results.</p>
<p>Recommend small strata but not too small as variances cannot be estimated within pairs</p>
<ul>
<li><p>DR: Section 10 on heterogeneity is particularly relevant for us</p></li>
<li><p>Other experimetrics methodology surveys mentioned (Duflo et al ’06, Glennerster and T ’13, Glennerster ’16); present one is more theoretical</p></li>
</ul>
</div>
<div id="randomised-experiments-and-validity" class="section level3" number="15.7.2">
<h3 number="15.7.2"><span class="header-section-number">15.7.2</span> Randomised experiments and validity</h3>
<p>Defined as settings “where the assignment mechanism does not depend on characteristics of the units”
- That seems to be “pure randomisation”</p>
<p>… Debate about the supremacy of randomised experiments</p>
<p>Definition of internal validity (DR:: it is a bit imprecise here).</p>
<p><br />
</p>
<p>Typical argument about how external validity is no more guaranteed in observational studies then and randomised experiments “there is nothing in non-experimental methods which made some superior randomised experiments with the same population and sample size in this regard.”</p>
<p>-DR: I think this is a bit of a strawman and a weak argument here
- GR: This is the Deaton argument, very strange</p>
<p>They argue for experiments in multiple settings varying in the characteristics of the units and perhaps the treatments to assess the credibility of generalizing to other settings.</p>
<p><br />
</p>
<p>(?Graphical methods to deal with external validity issues?)</p>
<p><br />
</p>
<p><strong>Finite population versus random sample from super-population</strong></p>
<p>We can interpret the uncertainty as unobserved potential outcomes rather than sampling uncertainty.</p>
<ul>
<li>DR: I don’t see why these are mutually exclusive.</li>
<li>GR: agreed</li>
<li>DR: Viewing the sample of the full population of interest may not even work I’m considering some experiments such as those using within subject treatments</li>
</ul>
<p><br />
</p>
<p>The differences in these approaches matter in some settings but not others.
Sometimes “…conventional sampling-based standard errors will be unnecessarily conservative”</p>
<ul>
<li>DR: This could be helpful</li>
</ul>
</div>
<div id="potential-outcomes-rubin-causal-model-framework-covered-earlier" class="section level3" number="15.7.3">
<h3 number="15.7.3"><span class="header-section-number">15.7.3</span> Potential outcomes/ Rubin causal model framework (covered earlier)</h3>
<p>(This is somewhat familiar by now)</p>
<p><br />
</p>
<p><strong>Potential outcomes</strong></p>
<p>If we do not impose limitations on interactions between units (like SUTVA) there will be a dimensionality problem</p>
<p><span class="math display">\[p\:\{0,1\}^N \times Y^{2N}\times X^N \rightarrow [0,1]\]</span></p>
<ul>
<li>DR: I am not sure I understand this notation, particularly the <span class="math inline">\(\{0,1\}\)</span> bit</li>
</ul>
<p><br />
</p>
<p>" For randomized experiments we disallow dependence on the potential outcomes, and we assume that the functional form of the assignment mechanism is known"
(this goes further than “completely randomized”)</p>
</div>
<div id="classification-of-assignment-mechanisms" class="section level3" number="15.7.4">
<h3 number="15.7.4"><span class="header-section-number">15.7.4</span> 3.2 Classification of assignment mechanisms</h3>
<p>(Formula for probabilities of each assignment combination of control and treatment given for each one)</p>
<ol style="list-style-type: decimal">
<li><p>Completely Randomized: <span class="math inline">\(N_t\)</span> Units drawn at random from a population of <span class="math inline">\(N\)</span> to receive the treatment, remaining <span class="math inline">\(N_c\)</span> get control.</p></li>
<li><p>Stratified randomized: Partition into <span class="math inline">\(G\)</span> strata based on covariates values, “Disallowing assignments that are likely to be uninformative about the treatment effects of interest”</p></li>
<li><p>Paired randomized (Extreme stratification)</p></li>
<li><p>Cluster randomized: Treatments assigned randomly to entire clusters. Maybe cheaper to implement and more valid in the presence of interactions between units within but not across clusters.</p></li>
</ol>
</div>
<div id="the-analysis-of-completely-randomized-experiments" class="section level3" number="15.7.5">
<h3 number="15.7.5"><span class="header-section-number">15.7.5</span> The analysis of Completely randomized experiments</h3>

<div class="note">
Aside: see the limitations of the Fisher test when not considering a lady tasting tea (known shares of outcomes). One response: <a href="https://statmodeling.stat.columbia.edu/2009/10/13/what_is_the_bay/">a Bayesian approach</a>
</div>
<p><strong>Exact p-values for sharp null hypotheses (Fisher etc)</strong></p>
<p>“Sharp”: “Under which we can an for all the missing potential outcomes from the observed data” … So we can infer the distribution of any statistics under the Null.</p>
<p>E.g., <span class="math inline">\(H0\)</span>, The treatment has no effect <span class="math inline">\(Y_i(0)=Y_i(1)\forall i\)</span>, vs <span class="math inline">\(Ha\)</span>, At least one unit i has <span class="math inline">\(Y_i(0)\neq Y_i(1)\)</span></p>
<p><br />
</p>
<p>Difference in means by treatment status: Calculate the probability over the randomization distribution of a value with as large an absolute value as the one observed given the actual assignments.</p>
<p>This is done by reassigning what we call the “treatments” to all possible combinations (keeping the number of treated units constant) and calculating the “placebo” treatment effect. Calculate the fraction of assignment vectors with statistic at least as large (in absolute value) as the observed one.</p>
<ul>
<li><p>DR: What is the statistic called and is there preprogrammed code? Is it the Fisher’s exact test?</p></li>
<li><p>Can do for means or means of the ranks by treatment status (rank sum?) or any stat.</p>
<ul>
<li>Latter is less sensitive to outliers and thick-tailed distributions</li>
</ul></li>
</ul>
<p><br />
</p>
<p>With multiple outcomes, multiple comparisons issues</p>
<ul>
<li><p>Use statistics it takes into account all the outcomes (e.g., F-stat, calculate exact P value using the ‘Fisher randomization distribution’ as in Young, ’16)</p></li>
<li><p>Or use adjustments to P values e.g., Bonferroni or tighter bounds (Which are still more conservative than the Fisher thing); Romano ea survey (2010)</p></li>
<li><p>Rosenbaum ’92 on estimating treatment effects based on rank statistics (DR: I don’t get this at all)</p></li>
</ul>
</div>
<div id="randomization-inference-for-average-treatment-effects" class="section level3" number="15.7.6">
<h3 number="15.7.6"><span class="header-section-number">15.7.6</span> Randomization inference for Average treatment effects</h3>
<p>Neyman wanted to estimate the ATE for the sample at hand</p>
<p><span class="math display">\[\tau=\frac{1}{N}\sum_{i=1..N}{(Y_i(1)-Y_i(0)i)}=\bar{Y}(1)-\bar{Y}(0)\]</span></p>
<ul>
<li><p>DR: Again, this is really not what we care about particularly not in a small-scale experiment.</p>
<p><br />
</p></li>
</ul>
<p>Proposed the estimator “Difference in average outcomes by treatment status” (DR: Same as in last section)</p>
<p><br />
</p>
<p>Defining <span class="math inline">\(D_i\)</span>, a term representing “assignment minus the average assignment”</p>
<p>Allows a restatement of the estimator which makes it clear that this is unbiased for the average treatment effect <span class="math inline">\(\tau\)</span></p>
<p><span class="math display">\[\hat{i\tau}=\tau+\frac{1}{N}\sum_{i=1..N}{(D_i(\frac{N}{N_t}Y_i(1)+\frac{N}{N_c}Y_i(0)}\]</span></p>
<p>Sampling variance of <span class="math inline">\(\hat{\tau}\)</span> over the randomization distribution decomposed as</p>
<p><span class="math display">\[V({\hat{\tau})=\frac{S_c^2}{N_c}+\frac{S_t^2}{N_t}-\frac{S_{tc}^2}{N}\]</span></p>
<p>Where <span class="math inline">\(S_c^2\)</span> and <span class="math inline">\(S_t^2\)</span> Are the variances of the control and treated outcomes,
and <span class="math inline">\(S_{tc}^2\)</span> is the variance of the unit level treatment effect (DR: This must be related to the covariance)</p>
<p>We can estimate rhe first two terms but not the latter term as we have no observations with both a control and a treatment.</p>
<p><br />
</p>
<p>In practice researchers ignore the third term, which leads to an upward bias for the <em>sample</em> treatment effect but an unbiased estimator of the population ATE</p>
<ul>
<li>DR: Any intuition for this?</li>
</ul>
<p><br />
</p>
<p>We still need to make large sample approximations to construct confidence intervals for the ATE.</p>
<ul>
<li>DR: Does this yield any practical strategy for us to use?</li>
</ul>
</div>
<div id="quantile-treatment-effect-infinite-population-context" class="section level3" number="15.7.7">
<h3 number="15.7.7"><span class="header-section-number">15.7.7</span> Quantile treatment effect (Infinite population context)</h3>
<p>Usefulness:</p>
<ol style="list-style-type: decimal">
<li><p>Uncover “Treatment effects in the tails”</p></li>
<li><p>Results robust to thick tails</p></li>
</ol>
<p>S-th quantile treatment effect defined as the difference in quantiles between the <span class="math inline">\(Y_i(1)\)</span> and<span class="math inline">\(Y_i(0)\)</span> distributions:</p>
<p><span class="math display">\[\tau_s=q_{Y(1)}(s)-q_{Y(0)}(s)\]</span> …</p>
<ul>
<li>this is distinct from the “quantile of the differences”: <span class="math inline">\(q_{Y(1)-Y(0)}(s)\)</span>, which is in general not identified
<ul>
<li>DR: the letter is truly more interesting; we care about the distribution of the <em>impact of the treatment</em> and not so much about the impact of the treatment on the distribution of outcomes.</li>
</ul></li>
<li>the two are equal if there is “perfect rank correlation between the two potential outcomes” (DR: I think this simply means that the unit ranked n’th if not treated would also be the unit ranked n’th if treated … no crossing over).</li>
</ul>
<p><br />
</p>
<p>Making lemonade: they argue here that the (identifiable) difference in quantiles would be more interesting to a policymaker considering exposing all units to the treatment … (DR: presumably because she should not care <em>who</em> get the particular outcome but only about the distribution of outcomes, a common axiom for social
welfare functions).</p>
<p><br />
</p>
<p>Estimates and tests: use the difference in quantiles as a statistic in and exact P value computation … results for such exact tests are quite different than those based on estimated effects and standard errors because “Quantile estimates are far from normally distributed.”</p>
</div>
<div id="covariates-if-not-stratified-in-completely-randomized-experiments" class="section level3" number="15.7.8">
<h3 number="15.7.8"><span class="header-section-number">15.7.8</span> Covariates (if not stratified) in completely randomized experiments</h3>
<p>(They strongly recommend stratifying instead of ex post controls.)</p>
<p>Why use controls if a simple difference in means is unbiased for the ATE?</p>
<ol style="list-style-type: decimal">
<li>“incorporating covariates may make analyses more informative” (greater precision)</li>
</ol>
<ul>
<li><p>Can incorporate covariates in exact P value analysis, or estimate average treatment effects within subpopulations and average these up appropriately</p></li>
<li><p>DR: How to do these things in practice?</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>correcting for compromised randomization … which may occur because of missing data and selective attrition</li>
</ol>
<p><br />
They give an example with the data from Lalonde where they estimate the average treatment effect for two groups those with and without prior earnings. They then add these up weighted by the estimated probability of being in each group.</p>
<ul>
<li>DR: I understand this correctly, as they do not define all variables
<ul>
<li>also, how do they compute the standard error of the combined estimator here?!</li>
<li>this seems more like an interaction than a standard control here, which would allow a different intercept (control outcome) but not a different treatment effect.</li>
</ul></li>
</ul>
<p>? se of
<span class="math display">\[\hat{p}(\bar{Y}_t|Y^{t-1}=0-\bar{Y}_c|Y^{t-1}=0)+(1-\hat{p})(\bar{Y}_t|Y^{t-1}=1-\bar{Y}_c|Y^{t-1}=1)\]</span></p>
<ul>
<li><p>DR: I think there is a simple formula for difference in means that could be applied to this</p>
<p><br />
</p></li>
</ul>
</div>
<div id="randomization-inference-and-regression-estimators" class="section level3" number="15.7.9">
<h3 number="15.7.9"><span class="header-section-number">15.7.9</span> Randomization inference and regression estimators</h3>
<p>They urge caution in using reg. “Since randomization does not justify the models, almost anything can happen” (Freedman 08)</p>
<p>But using only “indicator variables based on partitioning the covariate space” preserves many of the finite simple properties of simple comparisons of means.<br />
</p>
<p><strong>Regression estimators for average treatment effects</strong></p>
<p>With a single variable, the least-squares estimate of <span class="math inline">\(\tau\)</span> is identical to the simple difference in means:</p>
<p><span class="math display">\[\hat{tau}_{ols} = \bar{Y^o}_t - \bar{Y}^o_c\]</span></p>
<p>The intercept is the control value of course: <span class="math inline">\(\hat{\alpha}_{ols}=\bar{Y}^o-\hat{\tau_{ols}}\bar{W}=\bar{Y^0}_c\)</span>.</p>
<p><em>Conceptually important:</em></p>
<blockquote>
<p>the unbiasedness claim in the Neyman analysis is conceptually different from the one in conventional regression analysis: in the first case the repeated sampling paradigm keeps the potential outcomes fixed and varies the assignments, whereas in the latter the realized outcomes and assignments are fixed but different units with different residuals, but the same treatment status, are sampled.</p>
</blockquote>
<p><br />
</p>
<p>Redefining the residual in randomisation-based inference terms</p>
<blockquote>
<p>Now the error term has a clear meaning as the difference between potential outcomes and their population expectation
[DR: I think they mean the expectation conditional on treatment]</p>
</blockquote>
<blockquote>
<p>The randomization implies that the average residuals for treated and control units are zero …</p>
</blockquote>
<p>DR: They mean it implies mean independence (?) but not full independence, heteroskedasticity still likely</p>
<blockquote>
<p>Because the general robust variance estimator has no natural degrees-of-freedom adjustment [DR: ??], these standard [Randomisation-based?] robust variance estimators differs slightly from the Neyman unbiased variance estimator <span class="math inline">\(\hat{V}_{neyman}\)</span></p>
</blockquote>
<p><br />
</p>
<p><span class="math inline">\(\hat{V}_{robust} =\frac{s^2_c}{N_c}\frac{N_c-1}{N_c}+\frac{s^2_t}{N_t}\frac{N_t-1}{N_t}\)</span></p>
<p>Compared to the previously stated estimator for the TE variance for the sample (which we argued overstates the true sample TE variance)</p>
<p><br />
</p>
<p><span class="math inline">\(\hat{V}_{neyman} =\frac{s^2_c}{N_c}+\frac{s^2_t}{N_t}\)</span></p>
<p><br />
</p>
<blockquote>
<p>The Eicker-Huber-White variance estimator is not unbiased, and in settings where one of the treatment arms is rare, the difference may matter</p>
</blockquote>
<p>They give an example where it does not matter.</p>
<ul>
<li>DR: This point seems ignorable for most of our designs, as we intentionally avoid such rare arms (but in NL lottery maybe)</li>
</ul>
</div>
<div id="regression-estimators-with-additional-covariates-dr-seems-important" class="section level3" number="15.7.10">
<h3 number="15.7.10"><span class="header-section-number">15.7.10</span> Regression Estimators with Additional Covariates [DR: seems important]</h3>
<p>For now they continue to focus on ‘pure randomisation,’ not stratified nor merely exogenous conditional on observables</p>
<p><br />
</p>
<ol style="list-style-type: decimal">
<li>Can include these additively:</li>
</ol>
<p><span class="math display">\[Y^{obs}_i=\alpha+\tau W_i + \beta&#39;\dot{X}_i +  \epsilon_i\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Can allow a ‘full set of interactions’</li>
</ol>
<p><br />
</p>
<p><span class="math inline">\(Y^{obs}_i=\alpha+\tau W_i + \beta&#39;\dot{X}_i + \gamma&#39;\dot{X}_i W_i + \epsilon_i\)</span></p>
<p><br />
</p>
<ul>
<li>DR: They do not do much discussion here of whether to do additive or full interactions; maybe it comes later (causal trees etc)</li>
</ul>
<p><br />
</p>
<blockquote>
<p>In general the least squares estimates based on these regression functions are not unbiased for the average treatment effects over the randomization distribution given the finite population.</p>
</blockquote>
<ul>
<li><p>DR: Why not? Intuition? Which regression function of the ones above is referred to here?</p></li>
<li><p>DR: The discussion below suggests it will <em>still</em> be consistent (asymptotically unbiased)</p></li>
</ul>
<p><br />
</p>
<blockquote>
<p>There is one exception. If the covariates are all indicators and they partition the population, and we estimate the model with a full set of interactions, Equation (5.4), then the least squares estimate of <span class="math inline">\(\tau\)</span> is unbiased for the average treatment effect</p>
</blockquote>
<p><br />
</p>
<p>If <span class="math inline">\(\bar{X}\)</span> is the average value of <span class="math inline">\(X_i\)</span> in the sample, then =_1{X}+_0(1−{X})$, and <span class="math inline">\(\hat{\gamma}=\hat{\tau}_1-\hat{\tau}_0\)</span></p>
<p><br />
</p>
<p>With large sample approximations we can ‘say something about the case with multivalued covariates’ … “<span class="math inline">\(\tau\)</span> [DR: estimated how?] is asymptotically unbiased for the average treatment effect …”</p>
<blockquote>
<p>the asymptotic variance for <span class="math inline">\(\hat{\tau}}\)</span> is less than that of the simple difference estimator by a factor equal to <span class="math inline">\(1-R^2\)</span> from including the covariates relative to not including the covariates</p>
</blockquote>
<ul>
<li>DR: This motivates the use of covariates even in a randomized design, and even if we don’t take the ‘model of the covariates’ seriously.
<ul>
<li>“results do not rely on the regression model being true in the sense that the conditional expectation of Y obs i is actually linear in the covariates and the treatment indicator in the population”</li>
</ul></li>
<li>DR: Is this for the linear controls model or for the full interactions model?</li>
</ul>
<p>However, …</p>
<blockquote>
<p>If… the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial</p>
</blockquote>
<ul>
<li>DR: Intuition?</li>
</ul>
<p>The presence of non-zero values for γ imply treatment effect heterogeneity.</p>
<p><em>Best argument for using only binary/categorical interactions: interpretation</em></p>
<p>“Only if the covariates partition the population do these <span class="math inline">\(\gamma\)</span> have a clear interpretation as differences in average treatment effects.”</p>
<p><br />
</p>
<ul>
<li>DR: Could there not ever be a loss from including interactions and dividing up the sample too fine in doing this interactive estimation? This should depend on the true <span class="math inline">\(R^2\)</span> I think. Try to remember what is the real tradeoff.</li>
</ul>
<p>** 6 The Analysis of Stratified and paired randomized experiments **</p>
</div>
<div id="stratified-randomized-experiments-analysis" class="section level3" number="15.7.11">
<h3 number="15.7.11"><span class="header-section-number">15.7.11</span> Stratified randomized experiments: analysis</h3>
<p><em>Case for stratification</em></p>
<blockquote>
<p>capture the gains from ex post regression adjustment without the potential costs of linear regression, and the potential costs of linear regression, and therefore stratification is generally preferable over regression adjustment</p>
</blockquote>
<blockquote>
<p>Within this stratum we can estimate the average effect as the difference in average outcomes for treated and control units: <span class="math inline">\(\tauˆg = \bar{Y}^{obs}_{t,g} − \bar{Y}^{obs}_{c,g}\)</span>,</p>
</blockquote>
<blockquote>
<p>and we can estimate the within-stratum variance, using the Neyman results, as</p>
</blockquote>
<p><span class="math inline">\(\hat{V}(\hat{\tau}) =\frac{s^2_{t,g}}{N_{t,g}} + \frac{s^2_{c,g}}{N_{c,g}}\)</span></p>
<blockquote>
<p>where the g-subscript indexes the stratum [They wrote ‘j’ but I think its a typo]</p>
</blockquote>
<p>Next just average weighted by stratum shares:</p>
<p><span class="math inline">\(\hat{\tau} = \sum_{g=1..G}{\hat{tau}_g \frac{N_g}{N}\)</span></p>
<p>with estimated variance <span class="math inline">\(\sum_{g=1..G}\)</span>(_g)()^2$</p>
<ul>
<li>DR: Presumably they mean the above mentioned Neyman variance
<ul>
<li>Also note the squared term in the variance estimation, this may be how they computed the variance in the above empirical example</li>
</ul></li>
</ul>
<p>“Special case”: proportion treat units the same in all strata <span class="math inline">\(\rightarrow\)</span> ATE estimator equals difference in means by treatment status:</p>
<p><span class="math display">\[\hat{\tau} = \sum_{g=1..G}{\hat{tau}_g \frac{N_g}{N}}=\bar{Y}^{obs}_t-\bar{Y}^{obs}_c\]</span></p>
<p>… same as estimator for completely randomized experiment</p>
<p>But the estimated variance for the latter will be overly conservative.</p>
<ul>
<li>DR: But I thought stratifying sometime ends up yielding a larger <em>estimated</em> variance?</li>
</ul>
<p>##Paired randomized experiments: analysis</p>
<p>(Skipping note-taking for now)</p>
</div>
<div id="the-design-of-randomised-experiments-and-the-benefits-of-stratification" class="section level3" number="15.7.12">
<h3 number="15.7.12"><span class="header-section-number">15.7.12</span> 7 The Design of randomised experiments and the benefits of stratification</h3>
<blockquote>
<p>… Our recommendation is that one should always stratify as much as possible, up to the point that each stratum contains at least two treated and two control units</p>
</blockquote>
</div>
<div id="power-calculations" class="section level3" number="15.7.13">
<h3 number="15.7.13"><span class="header-section-number">15.7.13</span> 7.1 Power calculations</h3>
<ul>
<li>DR: This section is fairly basic and trivial, largely what we already know</li>
</ul>
<blockquote>
<p>we largely focus on the formulation where the output is the minimum sample size required to find treatment effects of a pre-specified size with a pre-specified probability</p>
</blockquote>
<ul>
<li><p>DR: My usual formulation</p></li>
<li><p>DR: Why are they doing these calculations based on the t-statistic, when they recommend using other measures?</p></li>
<li><p>DR: They claim equal sample sizes is “typically close to optimal” in cases without homoskedasticity. I think this is pure speculation.</p></li>
</ul>
</div>
<div id="stratified-randomized-experiments-benefits" class="section level3" number="15.7.14">
<h3 number="15.7.14"><span class="header-section-number">15.7.14</span> Stratified randomized experiments: Benefits</h3>
<blockquote>
<p>Stratifying does not remove any bias, it simply leads more precise inferences than complete randomization</p>
</blockquote>
<blockquote>
<p>confusion in the literature concerning the benefits of stratification in small samples if this correlation is weak [between the stratifying variables and the outcome]</p>
</blockquote>
<blockquote>
<p>in fact there is no tradeoff. We present formal results that show that in terms of expected-squared-error, stratification (with the same treatment probabilities in each stratum) cannot be worse than complete randomization.</p>
</blockquote>
<blockquote>
<p>if one stratifies on a covariate that is independent of all other variables, then stratification is obviously equivalent to complete randomization.</p>
</blockquote>
<blockquote>
<p>Ex ante, committing to stratification can only improve precision, not lower it</p>
</blockquote>
<p><em>Qualifications to this:</em></p>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<blockquote>
<p>Ex-post, given the joint distribution of the covariates in the sample, a particular stratification may be inferior to complete randomization.</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<blockquote>
<p>… Second, the result requires that the sample can be viewed as a (stratified) random sample from an infinitely large population… guarantees that outcomes within strata cannot be negatively correlated.</p>
</blockquote>
<p>(Note)</p>
<blockquote>
<p>The lack of any finite sample cost … contrasts with … regression adjustment. [which] may increase the finite sample variance, and in fact it will strictly increase the variance for any sample size, if the covariates have no predictive power at all.</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li><blockquote>
<p>Although there is no cost to stratification in terms of the variance, there is a cost in terms of estimation of the variance.</p>
</blockquote></li>
</ol>
<p><em>Still</em></p>
<blockquote>
<p>One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance</p>
</blockquote>
<ul>
<li>DR: Is it valid to simply say “we choose the lower value of the estimated variances?” Are they advocating this? Such a procedure seems like it would have a bias.</li>
</ul>
<blockquote>
<p>exact variance for a completely randomized experiment can be written as … variance for the corresponding stratified randomized experiment is… the difference in the two variances is <span class="math inline">\(V_C − V_S =... \geq 0\)</span></p>
</blockquote>
<ul>
<li>DR: I am curious how these terms are derived and compared</li>
</ul>
<blockquote>
<p>if the strata we draw from are small, say litters of puppies, it may well be that the within-stratum correlation is negative, but that is not possible if all the strata are large: in that case the correlation has to be non-negative</p>
</blockquote>
<ul>
<li>DR: unless sutva violated perhaps (?)</li>
</ul>
<blockquote>
<p>consider two estimators for the variance [both unbiased]</p>
</blockquote>
<p><span class="math inline">\(\hat{V}_C=\frac{s^2_{t,g}}{N_{t,g}} + \frac{s^2_{c,g}}{N_{c,g}}\)</span>
&gt; the natural estimator for the variance under the completely randomized experiment is: <span class="math inline">\(\hat{V}_c=\frac{s^2_{t}}{N_{t}} + \frac{s^2_{c,g}}{N_{c,g}}\)</span></p>
<blockquote>
<p>or a stratified randomized experiment the natural variance estimator, taking into account the stratification, is:
<span class="math inline">\(\hat{V}_S=\frac{N_f}{N_f+N_m}\Big(\frac{s^2_{fc}}{N_{fc}}\frac{s^2_{ft}}{N_{ft}}\Big)+\frac{N_f}{N_f+N_m}\Big(\frac{s^2_{mc}}{N_{mc}}\frac{s^2_{mt}}{N_{mt}}\Big)\)</span></p>
</blockquote>
<blockquote>
<p>Hence, <span class="math inline">\(E\hat{V}_S\leq\hat{V}_C\)</span>.</p>
</blockquote>
<ul>
<li>DR: Because we know both are unbiased and we know the true variance of <span class="math inline">\(\hat{V}_C\)</span> is larger.</li>
</ul>
<p>Nevertheless, the reverse may hold in a particular sample</p>
<blockquote>
<p>where the stratification is not related to the potential outcomes … the two variances are identical in expectation</p>
</blockquote>
<p>but the <span class="math inline">\(var\Big(hat{V}_S\Big) &lt; var\Big(hat{V}_C\Big)\)</span></p>
<ul>
<li>DR: This seems contradictory at first but I think it’s correct. The expectation of the estimated variance can be smaller or identical, while the <em>variance of the estimated variance</em> can still be larger.</li>
</ul>
</div>
<div id="re-randomization" class="section level3" number="15.7.15">
<h3 number="15.7.15"><span class="header-section-number">15.7.15</span> Re-randomization</h3>
<p>Basically, they argue that if the first pre-implementation experiment comes out very unbalanced, you can randomize again – this will be an indirect method of stratifying.</p>
<p>P-values could/should be adjusted to take into account that you are basically stratifying imprecisely.</p>
</div>
<div id="analysis-of-clustered-randomised-experiments" class="section level3" number="15.7.16">
<h3 number="15.7.16"><span class="header-section-number">15.7.16</span> Analysis of Clustered Randomised Experiments</h3>
<blockquote>
<p>our main recommendation is to include analyses that are based on the cluster as the unit of analysis. Although more sophisticated analyses may be more informative than simple analyses using the clusters as units, it is rare that these differences in precision are substantial, and a cluster-based analysis has the virtue of great transparency</p>
</blockquote>
<p><em>DR: skipping most of this section for now</em></p>
</div>
<div id="noncompliance-in-randomized-experiments-dr-relevant-to-nl-lottery-not-to-charity-experiments" class="section level3" number="15.7.17">
<h3 number="15.7.17"><span class="header-section-number">15.7.17</span> Noncompliance in randomized experiments (DR: Relevant to NL lottery, not to charity experiments)</h3>
<blockquote>
<p>randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received.</p>
</blockquote>
<ul>
<li>DR: good quote for Nlmed</li>
</ul>
<p>Responses to noncompliance:</p>
<ol style="list-style-type: decimal">
<li>ITT</li>
<li>LATE</li>
<li>Partial identification or bounds analysis</li>
</ol>
<ul>
<li>Latter: “to obtain the range of values for the average causal effect of the receipt of treatment for the full population.”</li>
</ul>
<blockquote>
<p>Another approach, not further discussed here, is the randomization-based approach to instrumental variables developed in Imbens and Rosenbaum (2005).
[check into that]</p>
</blockquote>
<p>They recommend against:
&gt; The first of these is an as-treated analysis, where units are compared by the treatment received; this relies on an unconfoundedness or selectionon-observables assumption. A second type of analysis is a per protocol analysis, where units are dropped who do not receive the treatment they were assigned to. We need some additional notation in this section.</p>
<ul>
<li>DR: Skipping full note-taking on this for now but <em>COME BACK TO IT</em> as it is very relevant to NL Med; the bounds analysis could be particularly interesting</li>
</ul>
</div>
<div id="heterogenous-treatment-effects-and-pretreatment-variables" class="section level3" number="15.7.18">
<h3 number="15.7.18"><span class="header-section-number">15.7.18</span> Heterogenous Treatment Effects and Pretreatment Variables</h3>
<ul>
<li>Crump et al setup (?)</li>
</ul>
<p>Multiple splits and tests may lead to overstated statistical significance for differences in TE’s.</p>
<ul>
<li>Bonferroni “overly conservative in an environment where many covariates are correlated with one another”
<ul>
<li>List, Shaikh, and Xu (2016) propose an approach accounting for this; it uses bootstrapping, and requires pre-specifying list of tests to conduct</li>
</ul></li>
</ul>
<p>** 10.3 Estimating Treatment Effect Heterogeneity **</p>
<ul>
<li>Parametric estimators, ‘all interactions’ (presumably with a correction as noted above)</li>
<li>Nonparametric estimator of <span class="math inline">\(\tau(x)\)</span></li>
</ul>
<blockquote>
<p>The approach of List, Shaikh, and Xu (2016) works for an arbitrary set of null hypotheses, so the researcher could generate a long list of hypotheses using the causal tree approach restricted to different subsets of covariates, and then test them with a correction for multiple testing. Since in datasets with many covariates, there are often many ways to describe what are essentially the same sub-groups, we expect a lot of correlation in test statistics, reducing the magnitude of the correction for multiple hypothesis testing.</p>
</blockquote>
</div>
<div id="data-driven-subgroup-analysis-recursive-partitioning-for-treatment-effects" class="section level3" number="15.7.19">
<h3 number="15.7.19"><span class="header-section-number">15.7.19</span> Data-driven Subgroup Analysis: Recursive Partitioning for Treatment Effects</h3>
<ul>
<li>Partition sample by “region of covariate space”</li>
<li>Determine which partition produces subgroups that differ the most in terms of treatment effects.</li>
<li>The method avoids introducing biases in the estimated average treatment effects and allows for valid confidence intervals using “sample splitting,” or “honest” estimation</li>
<li>Output of the method … is a set of subgroups, selected to optimize for treatment effect heterogeneity (to minimize expected mean-squared error of treatment effects), together with treatment effect estimates and standard errors for each subgroup.</li>
</ul>
<p><br />
</p>
<p>If instead…
&gt; we estimate the average treatment effect on the two subsamples using the same sample, the fact that this particular split led to a high value of the criterion would often imply that the average treatment effect estimate is biased.</p>
<p><br />
</p>
<p>But here ,,,
&gt; The treatment effect estimates are unbiased on the two subsamples, and the corresponding confidence intervals are valid, even in settings with a large number of pretreatment variables or covariates.</p>
<p><br />
</p>
<p>Because unit level TE is not observed, it is difficult to use standard protocols<br />
</p>
<p>… suggest transforming outcome from Y_i^{obs} to <span class="math inline">\(Y_i^\ast=Y_i^{obs}\frac{W_i−p}{p(1−p)}\)</span></p>
<p>… “so that standard methods for recursive partitioning based on prediction apply”</p>
<p>Which implies <span class="math inline">\(E[Y_i^\ast|X_i=x]=\tau(x)=E[Y_i(1)−Y_i(0)|X_i = x]\)</span></p>
<ul>
<li>DR: Are these p’s conditional on the x’s? Probably it doesn’t matter here as they are assuming pure randomisation.</li>
</ul>
<p>AI criterion</p>
<blockquote>
<p>focuses directly on the expected squared error of the treatment effect estimator … which turns out to depend both on the t-statistic and on the fit measures.
… further modified to anticipate … that the treatment effects will be re-estimated on an independent sample after the subgroups are selected</p>
</blockquote>
<ul>
<li>This penalises too small groups and too much variance,</li>
<li>(in general) rewards explain outcomes but not treatment effect heterogeneity…enables a lower-variance estimate of the treatment effect.</li>
</ul>
<p>Wager and W argue for inflating SE’s rather than partitioning
- DR: I see an advantage there, as the AI approach throws away data</p>
</div>
<div id="non-parametric-estimation-of-treatment-effect-heterogeneity" class="section level3" number="15.7.20">
<h3 number="15.7.20"><span class="header-section-number">15.7.20</span> 10.3.2 Non-Parametric Estimation of Treatment Effect Heterogeneity</h3>
<ul>
<li><p>Many allow descriptive evidence and prediction, but few methods available that allow for confidence intervals</p></li>
<li><p>K-nearest neighbors, hurdle methods</p>
<ul>
<li>Do not prioritise ‘more important’ covariates</li>
</ul></li>
</ul>
<blockquote>
<p>… can work well and provide satisfactory coverage of confidence intervals with one or two covariates, but performance deteriorates quickly after that.
The output of the nonparametric estimator is a treatment effect for an arbitrary x. The estimates generally must be further summarized or visualized since the model produces a distinct prediction for each x.</p>
</blockquote>
<blockquote>
<p>A key problem with kernels and nearest neighbor matching is that all covariates are treated symmetrically; if one unit is close to another in 20 dimensions, the units are probably not particularly similar in any given dimension. We would ideally like to prioritize dimensions that are most important for heterogeneous treatment effects, as is done in many machine learning methods, including the highly successful random forest algorithm.</p>
</blockquote>
<p>But these are often “bias-dominated asymptotically” … except the ones proposed by Wager and Athey (2015) :)</p>
<blockquote>
<p>asymptotically normal and centered on the true value of the treatment effect,… consistent estimator for the asymptotic variance.</p>
</blockquote>
<blockquote>
<p>Averages over the many “trees” of the form developed in Athey and Imbens (2016)</p>
</blockquote>
<blockquote>
<p>… different subsamples are used for each tree [plus some randomness]
Each tree is “honest,” in that one subsample is used to determine a partition and [another] to estimate treatment effects within the leaves.
Unlike the case of a single tree, no data is “wasted” because each observation is used to determine the partition in some trees and used to estimate treatment effects in other trees, and subsampling is already an inherent part of the method.</p>
</blockquote>
<p>What does this mean?:</p>
<blockquote>
<p>can obtain nominal coverage with more covariates than K-Nearest Neighbour matching or kernel methods,</p>
</blockquote>
<p>(but still “eventually becomes bias-dominated when the number of covariates grows” … but “much more robust to irrelevant covariates than kernels or nearest neighbor matching.”)</p>
<ul>
<li><p>Also, approaches fitting separately for treatment and control</p></li>
<li><p>Also, Bayesian perspectives on this: Green and Kern (2011), Hill (2012), others … but unknown asymptotic properties (DR: do we care?)</p></li>
</ul>
</div>
<div id="treatment-effect-heterogeneity-using-regularized-regression" class="section level3" number="15.7.21">
<h3 number="15.7.21"><span class="header-section-number">15.7.21</span> 10.3.3 Treatment Effect Heterogeneity Using Regularized Regression</h3>
<ul>
<li><p>Lasso-like (Imai and Ratkovic (2013), etc.)</p></li>
<li><p>With few important covariates (a ‘sparse’ model), can derive valid CI’s w/o sample-splitting</p></li>
<li><p>Some proposed modeling heterogeneity separately for treatment and control;… can be inefficient if the covariates that affect the level of outcomes are distinct from those that affect treatment effect heterogeneity.</p>
<ul>
<li>alternative … incorporate interactions … as covariates, and then allow LASSO to select which covariates are important.</li>
</ul></li>
</ul>
</div>
<div id="comparison-of-methods" class="section level3" number="15.7.22">
<h3 number="15.7.22"><span class="header-section-number">15.7.22</span> 10.3.4 Comparison of Methods</h3>
<ul>
<li>Lasso: more sparsity restrictions, better handle linear or polynomial relationships between covariates and outcomes;
<ul>
<li>outputs a regression; but CI’s justified only under strict conditions</li>
</ul></li>
<li>Random forest methods … are more localized, … capture complex, multi-dimensional interactions among covariates, or highly nonlinear interactions.
<ul>
<li>Less sensitive to sparsity, CI’s do not ‘deteriorate’ as covariates grow (but MSE of predictions suffer)</li>
<li>Inference more justifiable by random assignment (Lasso requires stronger assumptions)</li>
</ul></li>
</ul>
<!--chapter:end:experiments_and_study_design/experimetrics_te.Rmd-->
</div>
</div>
</div>
<div id="other_approaches" class="section level1 unnumbered">
<h1 class="unnumbered"><strong>OTHER APPROACHES, TECHNIQUES, AND APPLICATIONS</strong></h1>
</div>
<div id="psychometrics" class="section level1" number="16">
<h1 number="16"><span class="header-section-number">16</span> Boiling down: Construct validation/reliability, dimension reduction, factor analysis, and Psychometrics</h1>
<div id="constructs-and-construct-validation-and-reliability" class="section level2" number="16.1">
<h2 number="16.1"><span class="header-section-number">16.1</span> Constructs and construct validation and reliability</h2>
<p><em>Reliability vs validity:</em></p>
<p>Reliability <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)">wiki</a>: “similar[ity] of results under similar conditions”</p>
<p>Validity of a tool [wiki]: – “the degree to which the tool measures what it claims to measure”</p>
<div id="validity-general-discussion" class="section level3" number="16.1.1">
<h3 number="16.1.1"><span class="header-section-number">16.1.1</span> Validity: general discussion</h3>
<blockquote>
<p>In psychometrics, validity has a particular application known as test validity: “the degree to which evidence and theory support the interpretations of test scores” (“as entailed by proposed uses of tests”).[3]</p>
</blockquote>
<blockquote>
<p>Construct validity refers to the extent to which operationalizations of a construct (e.g., practical tests developed from a theory) measure a construct as defined by a theory. It subsumes all other types of validity.</p>
</blockquote>
<p>There are a number of other measures and types of validity defined, some of which seem to involve subjective judgement, others of which are formalizable</p>
<p><br />
</p>
</div>
<div id="reliability-general-discussion" class="section level3" number="16.1.2">
<h3 number="16.1.2"><span class="header-section-number">16.1.2</span> Reliability: general discussion</h3>
<p>Reliability of measurements and multicomponent measuring instruments:</p>
<ul>
<li>Inter-rater reliability</li>
<li>Test-retest reliability</li>
<li>Inter-method reliability</li>
<li>Internal consistency reliability (“across items within a test.”[6]")</li>
</ul>
<p><br />
“Classical test theory”</p>
<blockquote>
<p>measurement errors are essentially random. … they are not correlated with true scores or with errors on other tests.</p>
</blockquote>
<blockquote>
<p>variance of obtained scores is simply the sum of the variance of true scores plus the variance of errors of measurement.[7]</p>
</blockquote>
<p><span class="math display">\[\sigma_{X}^{2}=\sigma_{T}^{2}+\sigma_{E}^{2}}\sigma_{X}^{2}=\sigma_{T}^{2}+\sigma_{E}^{2}\]</span></p>
<blockquote>
<p>The reliability coefficient is defined as the ratio of true score variance to the total variance of test scores.</p>
</blockquote>
<div class="marginnote">
<p>Why is a larger reliability coefficient a good thing? I guess because it indicates that made the measurement error part as small as possible given the true score variance. But wouldn’t it be better to choose a measure with a lower ‘true score variance?’</p>
</div>
</div>
<div id="raykovmetaanalysisscalereliability2013" class="section level3" number="16.1.3">
<h3 number="16.1.3"><span class="header-section-number">16.1.3</span> <span class="citation"><a href="#ref-raykovMetaanalysisScaleReliability2013" role="doc-biblioref">Raykov and Marcoulides</a> (<a href="#ref-raykovMetaanalysisScaleReliability2013" role="doc-biblioref">2013</a>)</span></h3>
<p><span class="citation"><a href="#ref-raykovMetaanalysisScaleReliability2013" role="doc-biblioref">Raykov and Marcoulides</a> (<a href="#ref-raykovMetaanalysisScaleReliability2013" role="doc-biblioref">2013</a>)</span></p>
<p><strong>The basic problem</strong> (?)</p>
<p>Paraphrasing…</p>
<blockquote>
<p>frequently … in empirical social and behavioral research, we consider a measuring instrument consisting of a prespecified set of <span class="math inline">\(p\)</span> congeneric [related?] components denoted <span class="math inline">\(X_1, ..., X_p\)</span>. …</p>
</blockquote>
<blockquote>
<p>Let <span class="math inline">\(T_1, ..., T_p\)</span> and <span class="math inline">\(E_1, ..., E_p\)</span> be their corresponding true scores and error scores, respectively, with the latter assumed uncorrelated [to each other]</p>
</blockquote>
<p>… Like ‘measurement error’ in Econometrics</p>
<blockquote>
<p>The scale components thus measure the same underlying latent dimension, designated <span class="math inline">\(\xi\)</span>, with possibly different units and origins of measurement as well as error variances; that is,</p>
</blockquote>
<p><span class="math display">\[X_i= T_i + E_i = \alpha_i + \beta_i \xi + E_i\]</span></p>
<p>Often used, of concern: the “composite (scale) score” <span class="math inline">\(Z = X_1 + X_2 + ... + X_k\)</span> (e.g., Likert scale index)</p>
<p><br />
</p>
<ul>
<li><p>Convergent and discriminant validity: Things meant to measure similar things should correlate, things meant to not be related are not</p></li>
<li><p>“analysis of composite reliability”</p></li>
</ul>
<p>Latent factors/latent variables</p>
</div>
</div>
<div id="factor-analysis-and-principal-component-analysis" class="section level2" number="16.2">
<h2 number="16.2"><span class="header-section-number">16.2</span> Factor analysis and principal-component analysis</h2>
</div>
<div id="other" class="section level2" number="16.3">
<h2 number="16.3"><span class="header-section-number">16.3</span> Other</h2>
<p>“Common methods bias”</p>
<!--chapter:end:psychometrics/psychometrics.Rmd-->
</div>
</div>
<div id="metaanalysis" class="section level1" number="17">
<h1 number="17"><span class="header-section-number">17</span> Meta-analysis and combining studies: Making inferences from previous work</h1>
<div id="introduction-4" class="section level2" number="17.1">
<h2 number="17.1"><span class="header-section-number">17.1</span> Introduction</h2>
<p>My notes go over on</p>
<ol style="list-style-type: decimal">
<li>Christensen et al’s open-social-science guide for an overview of meta-analysis and what it foes</li>
<li>Harrer’s <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/">’Doing Meta-Analysis in R</a>’ to go through the process and highlight key implementation issues and statistical choices</li>
</ol>
<p><u><strong>My opinions on why meta-analysis and data pooling is so important (unfold):</strong></u></p>
<ol style="list-style-type: decimal">
<li>Stronger inference and better organized science, clearer direction of progress</li>
</ol>

<div class="fold">
<p>It is lame how often I see ‘new experiments’ and ‘new studies’ that tread most of the ground as old studies, spend lots of money, get a publication and … ignore or pay lip service to the previous findings. There is tons of data out there that can inform new questions and bring better through re-analysis and combination with other data. Otherwise we are not actually building progress. This is why I became involved with a project I called ‘ExpArchive,’ later working with projects such as GESIS’ X-Econ to try to encourage and facilitate data sharing in experimental economics, as well as the innovationsinfundraising.org project, which is now collaborating with the Lily Institute’s “revolutionizing philanthropy research” (RPR) project.</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>‘Stimulus sampling’ and robust evidence</li>
</ol>
</div>
<div id="christensen-meta" class="section level2" number="17.2">
<h2 number="17.2"><span class="header-section-number">17.2</span> An overview of meta-analysis, from Christensen et al 2019, ch 5, ’Using all evidence, registration and meta-analysis</h2>
<blockquote>
<p>how the research community can systematically collect, organize, and analyze a body of research work</p>
</blockquote>
<ul>
<li>Limitations to the ‘narrative literature review’: subjectivity, too much info to narrate</li>
</ul>
<div id="the-origins-and-importance-of-study-pre-registration" class="section level3" number="17.2.1">
<h3 number="17.2.1"><span class="header-section-number">17.2.1</span> The origins [and importance] of study [pre-]registration</h3>
<p>… Make details of planned and ongoing studies available to the community …. including those not (yet) published</p>
<ul>
<li><p>Required by FDA in 1997, many players in medical community followed soon after</p></li>
<li><p>Turner ea (08) and others documented massive publication bias and misrepresentation</p></li>
</ul>
<p>… but registration far from fully enforced (Mathieu ea ’09) found 46% clealy registered, and discrepancies between registered and published outcomes !</p>
</div>
<div id="social-science-study-registries" class="section level3" number="17.2.2">
<h3 number="17.2.2"><span class="header-section-number">17.2.2</span> Social science study registries</h3>
<ul>
<li>Jameel 2009</li>
<li>AEA 2013, 2100 registrations to date</li>
<li>RIDIE, EGAP, AsPredicted</li>
<li>OSF allowing a DOI (25,000+)</li>
</ul>
</div>
<div id="meta-analysis" class="section level3" number="17.2.3">
<h3 number="17.2.3"><span class="header-section-number">17.2.3</span> Meta-analysis</h3>
<p>Key references: Borenstein ea ’09, Cooper, Hedges, and V ’09</p>
<div id="selecting-studies" class="section level4" number="17.2.3.1">
<h4 number="17.2.3.1"><span class="header-section-number">17.2.3.1</span> Selecting studies</h4>
<p>"some scholarly discretion regarding which measures are ‘close enough’ to be included… contemperanous meta-analyses on the same topic finding opposit e conclusions</p>
<p>‘asses the robustness… to different inclusion conditions’… see Doucouliagos ea ’17 on inclusion options</p>
<div class="marginnote">
<p>My opinion: this is the key barrier to meta-analysis in social science! How do we weight studies using different methodologies and in different contexts? The Bayesian Random Effects approach seems to offer some help here (this not to be confused with the random effects panel-data models discussed in standard Econometrics texts).</p>
</div>
</div>
<div id="assembling-estimates" class="section level4" number="17.2.3.2">
<h4 number="17.2.3.2"><span class="header-section-number">17.2.3.2</span> Assembling estimates</h4>
<ul>
<li>Which statistic to collect?</li>
</ul>
<p><br />
</p>
<p>Studies <span class="math inline">\(j \in J, j= 1..N_j\)</span></p>
<p>Relevant estimate of stat from each study is <span class="math inline">\(\hat{ \beta_j}\)</span> with SE <span class="math inline">\(\hat{\sigma_j}\)</span></p>
<ul>
<li>Papers report several estimates (e.g., in robustness checks): which to choose, esp if author’s preferred approach differs from other scholars.</li>
</ul>
<p><br />
</p>
<p><em>Ex from Hsiang, B, Miguel, ’13</em>: links between extreme climate and violence</p>
<ul>
<li><p>how to classify outcomes… interpersonal and intergroup… normalised as pct changes wrt the meanoutcome in that dataset</p></li>
<li><p>how to standardice climate varn measures… chose SD from local area mean (DR: this choice implicitly reflects a behavioural assumption)</p></li>
</ul>
<p><span class="math inline">\(\rightarrow\)</span> ‘pct change in a conflict outcome as a fncn of a 1 SD schock to local climate’</p>
</div>
</div>
<div id="combining-estimates" class="section level3" number="17.2.4">
<h3 number="17.2.4"><span class="header-section-number">17.2.4</span> Combining estimates</h3>
<p>‘Fixed-effect meta-analysis approach’: assumes a single true effect’</p>
<div class="marginnote">
<p>DR: I’m not sure I agree on this assesment of <em>why</em> this is unlikely to be true in practice… ‘differences in measures’ (etc) seem to be a different issue</p>
</div>
<p><em>Equal weight approach</em>: (Simply the average across studies… ugh)</p>
<p><br />
</p>
<p><em>Precision-weighted approach</em>:</p>
<p><span class="math display">\[\hat{\beta}_{PW}=
\sum_{j}p_j\hat{\beta}_j/
\sum_{j}p_j\]</span></p>
<p>where <span class="math inline">\(p_j\)</span> is the estimated precision for study <span class="math inline">\(j\)</span>: <span class="math inline">\(\frac{1}{\hat{\sigma_i}^2}\)</span></p>
<p>Thus the weight <span class="math inline">\(\omega_j\)</span> placed on study <span class="math inline">\(j\)</span> is proportional to it’s precision.</p>
<div class="marginnote">
<p>‘implies weight in proportion to sample size?’ I think that’s loosely worded, it must be nonlinear.</p>
</div>
<p><span class="math inline">\(\rightarrow\)</span> This minimises the variance in the resulting meta-analytical estimate:</p>
<p><span class="math display">\[var(\hat{\beta}_{PW}) =\sum_j \omega_j\hat{\sigma_j}^2 = \frac{1}{\sum_j(p_j)}\]</span></p>
<p>‘inclusion of additional estimates always reduces the SE of <span class="math inline">\(\hat{\beta_{PW}}\)</span> [in expectation].’ … so more estimtes can’t hurt as long as you know their precision.</p>
<p>(they give a numerical example here with 3 estimates)</p>
<!-- Todo: add R code explicitly doing these calculations -->
</div>
<div id="heterogeneous-estimates" class="section level3" number="17.2.5">
<h3 number="17.2.5"><span class="header-section-number">17.2.5</span> Heterogeneous estimates…</h3>
<div id="wls-estimate" class="section level4" number="17.2.5.1">
<h4 number="17.2.5.1"><span class="header-section-number">17.2.5.1</span> WLS estimate</h4>
<p>(Stanley and Doucouliagos ’15)</p>
<p>Interpreted as ‘an estimate of the average of potentially heterogenous estimates’</p>
<p>This may feel like a more familiar to Economists but it is also seems to be far less useful than the Bayesian approach.</p>
<p><br />
</p>
</div>
<div id="random-effects-more-common" class="section level4" number="17.2.5.2">
<h4 number="17.2.5.2"><span class="header-section-number">17.2.5.2</span> Random-effects (more common)</h4>
<p><em>Focus here on hierarchical Bayesian approach</em> (Gelman and Hill ’06; Gelman ea ’13)</p>
<blockquote>
<p>The magnitude and precision of the common component represents the generalizable conclusions we might draw from a literature’</p>
</blockquote>
<p>… continuing from above notation</p>
<blockquote>
<p>cross-study differences we observe might not be driven solely by sampling variability… [even with] infinite data, they would not converge to the exact same [estimate]</p>
</blockquote>
<p>True Treatment Effect (TE) <span class="math inline">\(\beta_j\)</span> for study j drawn from a normal distribution…</p>
<p><span class="math display">\[\beta_j \sim N(\mu, \tau^2)\]</span></p>
<p>‘Hyper-parameters’ <span class="math inline">\(\mu\)</span> determines central tendency of findings… <span class="math inline">\(\tau\)</span> the extent of heterogeneity across contexts.</p>
<p>Considering <span class="math inline">\(\tau\)</span> vs <span class="math inline">\(\mu\)</span> is informative in itself. And a large <span class="math inline">\(\mu\)</span> may suggest looking into sample splits for hety on obsl lines.</p>
<p><br />
</p>
<p>Uniform prior for <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> conditional posterior:</p>
<p><span class="math display">\[\mu|\tau,y \sim N(\hat{\mu}, V_{\mu})\]</span> where the estimated common effect <span class="math inline">\(\hat{\mu}\)</span> is</p>
<p><span class="math display">\[\hat{\mu}=
\frac{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))\hat{\beta}}
{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))}\]</span></p>
<p>(Similar to precision-weighted approach but now the between-study dispersion is incorporated into the weights)</p>
<p>and where the estimated variance of the generalizable component <span class="math inline">\(V_\mu\)</span> is:</p>
<p><span class="math display">\[Var(\hat{\mu})= \frac{1}{\sum_j\big(1/(\hat{\sigma_i}^2 + \hat{tau}^2)}\]</span></p>
<div class="marginnote">
<p>Confusion/correction? Is this the estimated variance or the variance of the estimate?</p>
</div>
<ul>
<li>and how do we estimate some of the components of these, like <span class="math inline">\(\hat{\tau}\)</span>?</li>
</ul>
<blockquote>
<p>Intuitively, if estimated [TE] in all studies are near one another and have relatively wide and overlapping [CI’s], then most of the difference in estimates is likely the result of sampling variation [and <span class="math inline">\(\tau\)</span>] is likely to be close to zero.</p>
</blockquote>
<div class="marginnote">
<p>DR: But if the TE have wide CI’s, do we have power to idfy btwn-study hety? … I guess that’s what the ‘estimated TE are all near each other’ gives us?</p>
</div>
<p>… Alternatively, if there is extensive variation in the estimated ATEs but each is precise… <span class="math inline">\(\tau\)</span> is likely to be relatively large.</p>

<div class="note">
<p>Coding meta-analyses in R</p>
<p>“A Review of Meta-Analysis Packages in R” offers a helpful guide to the various packages, such as <code>metafor</code>.</p>
<p><a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/">Doing Meta-Analysis in R: A Hands-on Guide</a> appears extremely helpful; see, e.g., their chapter <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-meta-analysis-in-r-using-the-brms-package.html">Bayesian Meta-Analysis in R using the brms package</a></p>
</div>
<!-- TODO: some code exercises should be put or linked here? Perhaps drawn from the above references? -->
<p> The <span class="math inline">\(I^2\)</span> stat is a measure of the proportion of total variation attributed to cross-study variation; if <span class="math inline">\(\hat{\sigma}_j\)</span> is the same across all studies we have: <span class="math inline">\(I^2(.) = \hat{\tau}^2/(\hat{\tau}^2 + \hat{\sigma}^2)\)</span></p>
<!-- *DR: more detail would be welcome here. Material from [this syllabus]() may be helpful.

https://docs.google.com/document/d/1oImg-ojUFqak5KyZ-ETD2qGvkvUgx8Ym6b8gG4GwfM8/edit?usp=drivesdk

-->
</div>
</div>
</div>
<div id="doing-meta" class="section level2" number="17.3">
<h2 number="17.3"><span class="header-section-number">17.3</span> Excerpts and notes from <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/">‘Doing Meta-Analysis in R: A Hands-on Guide’</a> (Harrer et al)</h2>
<div class="marginnote">
<p>Note that installation of the required packages can be tricky here. For Mac Catalina with R 4.0 I followed the instructions <a href="https://discourse.mc-stan.org/t/dealing-with-catalina-iii/12731/30">HERE</a></p>
</div>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">#devtools::install_github(&quot;MathiasHarrer/dmetar&quot;)</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co">#...I did not &#39;update new packages&#39;</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&quot;extraDistr&quot;)</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pacman)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="fu">p_load</span>(tidyverse, meta, brms, dmetar, extraDistr, ggplot2, tidybayes, dplyr, ggplot2, ggridges, glue, stringr, forcats, meta, metafor, here)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">here</span>(<span class="st">&quot;meta_anal_and_open_science&quot;</span>, <span class="st">&quot;Doing-Meta-Analysis-in-R-master&quot;</span>, <span class="st">&quot;_data&quot;</span>, <span class="st">&quot;Meta_Analysis_Data.RData&quot;</span>))</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a> madata <span class="ot">&lt;-</span> Meta_Analysis_Data</span></code></pre></div>
<div id="pooling-effect-sizes" class="section level3" number="17.3.1">
<h3 number="17.3.1"><span class="header-section-number">17.3.1</span> Pooling effect sizes</h3>
<div id="fixed-effects-model" class="section level4 unnumbered">
<h4 class="unnumbered">Fixed effects model</h4>
<!-- SD:
Fixed the equations according to https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/fixed.html
-->
<p><span class="math display">\[\hat{\theta_F} = \frac{\sum\limits_{k=1}^K\hat{\theta_k}/\hat{\sigma}^2_k}{\sum\limits_{k=1}^K1/\hat{\sigma}^2_k} \]</span> <span class="math display">\[\hat{\sigma^2_k}=\sum\limits_{k=1}^K \frac{1}{K}\hat{\sigma}^2_k \]</span></p>
<!--
And (it looks like) the simple average of the variabces for the pooled effect variance:

$$\hat{\sigma^2_k}=\sum\limits_{k=1}^K \frac{1}{K}\hat{\sigma}^2_k $$

-->
<ul>
<li>note that this process does not ‘dig in’ to the raw data, it just needs the summary statistics, neither does the “RE model” they refer to:</li>
</ul>
<blockquote>
<p>Both of these models only require an effect size, and a dispersion (variance) estimate for each study, of which the inverse is taken. This is why the methods are often called generic inverse-variance methods.</p>
</blockquote>
<p>Nor do the Bayesian models, apparently (they use the same datasets in his examples I think)</p>
<p>The sample data:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(madata)</span></code></pre></div>
<pre><code>## tibble[,17] [18 × 17] (S3: tbl_df/tbl/data.frame)
##  $ Author               : chr [1:18] &quot;Call et al.&quot; &quot;Cavanagh et al.&quot; &quot;DanitzOrsillo&quot; &quot;de Vibe et al.&quot; ...
##  $ TE                   : num [1:18] 0.709 0.355 1.791 0.182 0.422 ...
##  $ seTE                 : num [1:18] 0.261 0.196 0.346 0.118 0.145 ...
##  $ RoB                  : chr [1:18] &quot;low&quot; &quot;low&quot; &quot;high&quot; &quot;low&quot; ...
##  $ Control              : chr [1:18] &quot;WLC&quot; &quot;WLC&quot; &quot;WLC&quot; &quot;no intervention&quot; ...
##  $ intervention duration: chr [1:18] &quot;short&quot; &quot;short&quot; &quot;short&quot; &quot;short&quot; ...
##  $ intervention type    : chr [1:18] &quot;mindfulness&quot; &quot;mindfulness&quot; &quot;ACT&quot; &quot;mindfulness&quot; ...
##  $ population           : chr [1:18] &quot;undergraduate students&quot; &quot;students&quot; &quot;undergraduate students&quot; &quot;undergraduate students&quot; ...
##  $ type of students     : chr [1:18] &quot;psychology&quot; &quot;general&quot; &quot;general&quot; &quot;general&quot; ...
##  $ prevention type      : chr [1:18] &quot;selective&quot; &quot;universal&quot; &quot;universal&quot; &quot;universal&quot; ...
##  $ gender               : chr [1:18] &quot;female&quot; &quot;mixed&quot; &quot;mixed&quot; &quot;mixed&quot; ...
##  $ mode of delivery     : chr [1:18] &quot;group&quot; &quot;online&quot; &quot;group&quot; &quot;group&quot; ...
##  $ ROB streng           : chr [1:18] &quot;high&quot; &quot;low&quot; &quot;high&quot; &quot;low&quot; ...
##  $ ROB superstreng      : chr [1:18] &quot;high&quot; &quot;high&quot; &quot;high&quot; &quot;low&quot; ...
##  $ compensation         : chr [1:18] &quot;none&quot; &quot;none&quot; &quot;voucher/money&quot; &quot;voucher/money&quot; ...
##  $ instruments          : chr [1:18] &quot;DASS&quot; &quot;PSS&quot; &quot;DASS&quot; &quot;other&quot; ...
##  $ guidance             : chr [1:18] &quot;f2f&quot; &quot;self-guided&quot; &quot;f2f&quot; &quot;f2f&quot; ...</code></pre>
<blockquote>
<p>As our effect sizes are already calculated, we can use the <code>meta::metagen</code> function.</p>
</blockquote>
<p><br />
</p>
<p>A particularly relevant parameter to specify:</p>
<p><code>sm=</code>: The summary measure we want to calculate. We can either calculate the mean difference (MD) or Hedges’ g/Cohen’s d (SMD)".^*^</p>
<div class="marginnote">
<p>Recall, Hedges’ and Cohen’s measures are the mean difference between the groups divided by the pooled standard deviation. Below these seem to yield the same result, perhaps because these results are already normalised.</p>
</div>
<p><br />
</p>
<p>Our first fixed-effects model meta-analysis:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> madata <span class="sc">%&gt;%</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>            <span class="fu">metagen</span>(</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>              TE, <span class="co"># the treatment effect variable</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>             seTE, <span class="co"># the SE of treatment effect variable</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">data=</span>.,</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">studlab=</span><span class="fu">paste</span>(Author), <span class="co">#labels for each study</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">comb.fixed =</span> <span class="cn">TRUE</span>, <span class="co">#yes fixed effects estimation</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">comb.random =</span> <span class="cn">FALSE</span>, <span class="co">#no RE estimation (we could do both)</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">prediction=</span><span class="cn">TRUE</span>, <span class="co">#&quot;print a prediction interval for the effect of future studies based on present evidence&quot;</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">sm=</span><span class="st">&quot;SMD&quot;</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>##                           SMD            95%-CI %W(fixed)
## Call et al.            0.7091 [ 0.1979; 1.2203]       3.6
## Cavanagh et al.        0.3549 [-0.0300; 0.7397]       6.3
## DanitzOrsillo          1.7912 [ 1.1139; 2.4685]       2.0
## de Vibe et al.         0.1825 [-0.0484; 0.4133]      17.5
## Frazier et al.         0.4219 [ 0.1380; 0.7057]      11.6
## Frogeli et al.         0.6300 [ 0.2458; 1.0142]       6.3
## Gallego et al.         0.7249 [ 0.2846; 1.1652]       4.8
## Hazlett-Stevens &amp; Oren 0.5287 [ 0.1162; 0.9412]       5.5
## Hintz et al.           0.2840 [-0.0453; 0.6133]       8.6
## Kang et al.            1.2751 [ 0.6142; 1.9360]       2.1
## Kuhlmann et al.        0.1036 [-0.2781; 0.4853]       6.4
## Lever Taylor et al.    0.3884 [-0.0639; 0.8407]       4.6
## Phang et al.           0.5407 [ 0.0619; 1.0196]       4.1
## Rasanen et al.         0.4262 [-0.0794; 0.9317]       3.6
## Ratanasiripong         0.5154 [-0.1731; 1.2039]       2.0
## Shapiro et al.         1.4797 [ 0.8618; 2.0977]       2.4
## SongLindquist          0.6126 [ 0.1683; 1.0569]       4.7
## Warnecke et al.        0.6000 [ 0.1120; 1.0880]       3.9
## 
## Number of studies combined: k = 18
## 
##                        SMD            95%-CI    z  p-value
## Fixed effect model  0.4805 [ 0.3840; 0.5771] 9.75 &lt; 0.0001
## Prediction interval        [-0.0344; 1.1826]              
## 
## Quantifying heterogeneity:
##  tau^2 = 0.0752 [0.0357; 0.3046]; tau = 0.2743 [0.1891; 0.5519]
##  I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]
## 
## Test of heterogeneity:
##      Q d.f. p-value
##  45.50   17  0.0002
## 
## Details on meta-analytical method:
## - Inverse variance method
## - DerSimonian-Laird estimator for tau^2
## - Jackson method for confidence interval of tau^2 and tau</code></pre>
<p>Results include:</p>
<ul>
<li>The <strong>individual effect sizes</strong> for each study (SMD), and their weight (%W(fixed))</li>
<li>The <strong>overall effect</strong> (in our case, <span class="math inline">\(g\)</span> = 0.48) and its confidence interval and <span class="math inline">\(p\)</span>-value</li>
<li>Measures of <strong>between-study heterogeneity</strong>, such as <span class="math inline">\(tau^2\)</span> or <span class="math inline">\(I^2\)</span> and a <span class="math inline">\(Q\)</span>-test of heterogeneity</li>
</ul>
<div class="marginnote">
<p><span class="math inline">\(tau^2\)</span> - “The square root of this number (i.e. <span class="math inline">\(\tau\)</span>) is the estimated standard deviation of underlying effects across studies” - Cochrane handbook</p>
<p><span class="math inline">\(I^2\)</span> is a transformation of the ch-sq statistic… ‘This describes the percentage of the variability in effect estimates that is due to heterogeneity rather than sampling error (chance).’</p>
</div>
<p>You can output these results as a text file using the ‘sink’ command, but I don’t see the point.</p>
<p><br />
</p>
<p><strong>You can do a similar calculation with raw data</strong> (rather than aggregated data) using the <code>meta::metacont</code> command, as he shows. I think it just does all the aggregation within the function but it’s otherwise the same.</p>
<p><br />
</p>
<p>Harrer’s notes on whether we should use RE or FE (unfold)</p>

<div class="fold">
<p>“The random-effects-model pays more attention to small studies when pooling the overall effect in a meta-analysis (Schwarzer, Carpenter, and Rücker 2015). Yet, small studies in particular are often fraught with bias (see Chapter 8.1). This is why some have argued that the fixed-effects-model should be nearly always preferred (Poole and Greenland 1999; Furukawa, McGuire, and Barbui 2003).”</p>
DR: I’m not sure the “pays more attention to small studies” point is correct, although it is certainly empirically the case below.
</div>
</div>
<div id="random-effects-model" class="section level4 unnumbered">
<h4 class="unnumbered">Random effects model</h4>
<p>There are a variety of estimators for the variance of the true effect size <span class="math inline">\(\tau^2\)</span>. You must choose one. He recommends <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4950030/">Veroniki et al’s</a> discussion of this.</p>
<ul>
<li><p>DerSimonian-Laird (1986) estimator is the most common</p></li>
<li><p>"Maximum-Likelihood, Sidik-Jonkman, and Empirical Bayes estimators have better properties in estimating the between-study variance (Sidik and Jonkman 2007; Viechtbauer 2005).</p></li>
<li><p>DerSimonian-Laird is also seen to be prone to producing false positives (for the mean effect I think?)</p>
<ul>
<li>HKSJ is more conservative, but has ‘residual concerns’</li>
</ul></li>
</ul>
<p><strong>Running an RE meta-analysis:</strong></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>m.hksj <span class="ot">&lt;-</span> <span class="fu">metagen</span>(TE,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                  seTE,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> madata,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">studlab =</span> <span class="fu">paste</span>(Author),</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">comb.fixed =</span> <span class="cn">FALSE</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">comb.random =</span> <span class="cn">TRUE</span>,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>                  <span class="at">method.tau =</span> <span class="st">&quot;SJ&quot;</span>, <span class="co">#Sidik-Jonkman method</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>                  <span class="at">hakn =</span> <span class="cn">TRUE</span>, <span class="co">#&quot;use the Knapp-Hartung method&quot; or &#39;adjustment&#39;... seems to effect only the CI for the pooled effect</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>                  <span class="at">prediction =</span> <span class="cn">TRUE</span>,</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>                  <span class="at">sm =</span> <span class="st">&quot;SMD&quot;</span>)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>##                           SMD            95%-CI %W(random)
## Call et al.            0.7091 [ 0.1979; 1.2203]        5.2
## Cavanagh et al.        0.3549 [-0.0300; 0.7397]        6.1
## DanitzOrsillo          1.7912 [ 1.1139; 2.4685]        4.2
## de Vibe et al.         0.1825 [-0.0484; 0.4133]        7.1
## Frazier et al.         0.4219 [ 0.1380; 0.7057]        6.8
## Frogeli et al.         0.6300 [ 0.2458; 1.0142]        6.1
## Gallego et al.         0.7249 [ 0.2846; 1.1652]        5.7
## Hazlett-Stevens &amp; Oren 0.5287 [ 0.1162; 0.9412]        5.9
## Hintz et al.           0.2840 [-0.0453; 0.6133]        6.5
## Kang et al.            1.2751 [ 0.6142; 1.9360]        4.3
## Kuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.1
## Lever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6
## Phang et al.           0.5407 [ 0.0619; 1.0196]        5.4
## Rasanen et al.         0.4262 [-0.0794; 0.9317]        5.3
## Ratanasiripong         0.5154 [-0.1731; 1.2039]        4.1
## Shapiro et al.         1.4797 [ 0.8618; 2.0977]        4.5
## SongLindquist          0.6126 [ 0.1683; 1.0569]        5.7
## Warnecke et al.        0.6000 [ 0.1120; 1.0880]        5.4
## 
## Number of studies combined: k = 18
## 
##                         SMD            95%-CI    t  p-value
## Random effects model 0.5935 [ 0.3891; 0.7979] 6.13 &lt; 0.0001
## Prediction interval         [-0.2084; 1.3954]              
## 
## Quantifying heterogeneity:
##  tau^2 = 0.1337 [0.0295; 0.3533]; tau = 0.3657 [0.1717; 0.5944]
##  I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]
## 
## Test of heterogeneity:
##      Q d.f. p-value
##  45.50   17  0.0002
## 
## Details on meta-analytical method:
## - Inverse variance method
## - Sidik-Jonkman estimator for tau^2
## - Q-profile method for confidence interval of tau^2 and tau
## - Hartung-Knapp adjustment for random effects model</code></pre>
</div>
<div id="binary-outcomes" class="section level4 unnumbered">
<h4 class="unnumbered">Binary outcomes</h4>
<p>Outcomes like</p>
<ul>
<li><p>pooled Odds Ratio: relative incidence in treatment vs control</p></li>
<li><p>the Relative Risk: share of total incidence in treatment, scaled by sample size</p></li>
<li><p>Incidence Rate Ratio: similar to OR but scaled by duration, I think</p></li>
</ul>
<p>Packages:</p>
<ul>
<li>Again we use <code>metagen</code> for pooled data.</li>
<li>with raw outcome data we’ve <code>meta::metabin()</code> or <code>meta::metainc()</code></li>
</ul>
<blockquote>
<p>[standard inverse variance weighting] is suboptimal for binary outcome data … [especially when] dealing with <strong>sparse</strong> data, meaning that the number of events or the total sample size of a study is small,</p>
</blockquote>
</div>
</div>
<div id="doing-bayes-meta" class="section level3" number="17.3.2">
<h3 number="17.3.2"><span class="header-section-number">17.3.2</span> Bayesian Meta-analysis</h3>

<div class="note">
<p>“The model we apply in Bayesian Meta-Analysis is a so-called Bayesian Hierarchical Model…
every meta-analytical model inherently possesses a multilevel, and thus ‘hierarchical,’ structure.”</p>
</div>
<p>A Bayesian hierarchical model has three layers:</p>
<p>- A data layer (the likelihood)</p>
<p>- A process layer (the parameters describing the underlying process)</p>
<p>- A prior layer (priors on hyperparameters)</p>
<div id="the-setup" class="section level4 unnumbered">
<h4 class="unnumbered">The setup</h4>
<p>Underlying RE model (as before)</p>
<p>Study-specific estimates <span class="math inline">\(\hat{\theta}_k\)</span> are distributed :</p>
<p><span class="math display">\[ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) \]</span></p>
<p>True study-specific effects <span class="math inline">\(\theta_k\)</span> are distributed:</p>
<p><span class="math display">\[ \theta_k \sim \mathcal{N}(\mu,\tau^2) \]</span></p>
<p>… simplified to the ‘marginal’ form, this is</p>
<p><span class="math display">\[ (\hat\theta_k | \mu, \tau, \sigma_k) \sim \mathcal{N}(\mu,\sigma_k^2 + \tau^2)\]</span></p>
<p><br />
</p>
<p>And now we specify priors for these parameters, ‘making it Bayesian’</p>
<p><span class="math display">\[(\mu, \tau^2) \sim p(.)\]</span> <span class="math display">\[ \tau^2 &gt; 0 \]</span><br />
</p>
<p>Estimation will…</p>
<blockquote>
<p>[involve] Markov Chain Monte Carlo based sampling procedures, such as the Gibbs Sampler. In the brms package we will be using in this chapter, the No-U-Turn Sampler, or NUTS (Hoffman and Gelman 2014), is used.<br />
</p>
</blockquote>
<p>MCMC is used to sample from the posterior distribution. Without MCMC, to sample from the posterior the normalising constant would have to be found. But with most prior/likelihood combinations, the integral to find it is intractable.</p>

<div class="note">
<p><strong>Why use Bayesian?</strong></p>
<ul>
<li><p>to “directly model the uncertainty when estimating [the between-study variance] <span class="math inline">\(\tau^2\)</span>”</p></li>
<li><p>“have been found to be superior in estimating the between-study heterogeneity and pooled effect, particularly when the number of included studies is small”</p></li>
<li><p>“produce full posterior distributions for both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>” … so we can make legitimate statements about the probabilities of true parameters</p></li>
<li><p>“allow us to integrate prior knowledge and assumptions when calculating meta-analyses” (including methodological uncertainty perhaps)</p></li>
</ul>
</div>
<p><br />
</p>
</div>
<div id="setting-weakly-informative-priors-for-the-mean-and-cross-study-variance-of-the-te-sizes" class="section level4 unnumbered">
<h4 class="unnumbered">Setting weakly informative’ priors for the mean and cross-study variance of the TE sizes</h4>
<blockquote>
<p>It has been argued that a good approach is to use weakly informative priors (Williams, Rast, and Bürkner 2018) [rather than ‘non-informative priors!’].<br />
</p>
</blockquote>
<p>With a non-informative prior, we are essentially saying that we do not have any prior information on the effect size, whereas often we do have some information, and would like to include it in our analyis. Make sure to research priors before using them, as some be be very informative, with an example being the uniform distibution saying that extreme values are as likely as moderate values.</p>
<p><strong>For</strong> <span class="math inline">\(\mu\)</span>:</p>
<blockquote>
<p>include distributions which represent that we do indeed have <strong>some confidence that some values are more credible than others</strong>, while still not making any overly specific statements about the exact true value of the parameter. … In most applied cases, it seems reasonable to assume that the true effect size we want to estimate must lie somewhere between, for example, Cohen’s <span class="math inline">\(d=-2.0\)</span> and <span class="math inline">\(d=2.0\)</span>, but will unlikely be hovering around <span class="math inline">\(d=50\)</span>. A good starting point for our <span class="math inline">\(\mu\)</span> prior may therefore be a normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. This means that we grant a 95% prior probability that the true pooled effect size <span class="math inline">\(\mu\)</span> lies between <span class="math inline">\(d=-2.0\)</span> and <span class="math inline">\(d=2.0\)</span>.</p>
</blockquote>
<p><span class="math display">\[ \mu \sim \mathcal{N}(0,1)\]</span></p>
<p>Discussion (unfold)</p>

<div class="fold">
<p>SD: There has to be very good reason for setting the prior mean to be different from zero, as that would mean holding a prior belief of a (potentially) strong, positive or negative effect. Normally it is better to use a prior with a neutral effect. With regards to the variance of <span class="math inline">\(\mu\)</span>, consider the order of magnitude of the response, and how much each effect will have. A variance of one is large enough to explore the parameter space, without being too large to allow for extreme values.</p>
<p>… [A prior distinct from 0] only if we genuinely believe there to be a positive effect, otherwise we are not being “consistent” (a Bayesian term which links to our probability for an event to being our best guess estimate). Because if we are unsure if there is a positive or negative effect, then we would be ‘incoherent’ by having a positive prior effect - it would be better for our best guess estimate to have a neutral effect.</p>
</div>
<p><br />
</p>
<p>A further discussion on ‘coherence’ in the Bayesian framework versus ‘bias’ in the frequentist world (unfold).</p>

<div class="fold">
<p>SD:
I think a big part of the discussion here comes from the subjective-objective debate in probability theory, and the way that statistics is studied. Classical statistics postulates that a variable theta is a true, objective, physical value, and that data y is randomly drawn from a distribution with those parameters. Expectation is defined from probability.</p>
<p>Bayesians on the other hand start with the data; y is not random, it is known to us. The parameters theta are random, in that they are unknown to us (and subjectively unknown to each of us). Probability is defined from expectation, and our probability for an event is our subjective best guess estimate for that event. We are not being coherent if our probability is not our best guess estimate, that is, if we can make a better estimate for sure. If we’re not not coherent, we’re not being consistent with our beliefs.</p>
<p>So because of this, the concept of bias doesn’t really exist for subjectivists; there is no long run frequentist true parameter that our best guess is being distant from. Coherence is absolutely key to the concept, as it is about my belief and my best guess. That’s where the subjectivity comes into the analysis, because you may believe one prior (for instance one with a mean of 1), whereas I may believe another prior (with a mean of 0). There isn’t any correct answer, and as long as we are being coherent, because we’re sampling from our posteriors for this parameter given our own beliefs and our own subjective uncertainty. Often on Bayesian projects there is a prior elicitation, where people come together and discuss what their subjective priors are and how they will proceed.</p>
<p>I think there is some merit in this line of thought though, because clearly if you have a prior with a mean of, say, 1,000,000 then you’re really influencing your effect. But then arguably if you do so, you’re not being coherent. It’s quite a deep, long, philosophical discussion for a meta analysis, but it is crucial to the frequentist vs Bayesian debate.</p>
<p>More here: <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bayesian_view" class="uri">https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bayesian_view</a></p>
</div>
<p><strong>For</strong> <span class="math inline">\(\tau^2\)</span></p>
<ul>
<li><p>must be non-negative, but might be very close to zero.</p></li>
<li><p>Recommended distribution for this case (for variances in general): <em>Half-Cauchy prior</em> (a censored Cauchy)</p></li>
</ul>
<p><span class="math inline">\(\mathcal{HC}(x_0,s)\)</span></p>
<ul>
<li>with <em>location parameter</em> <span class="math inline">\(x_0\)</span> (peak on x-axis)</li>
<li>and <span class="math inline">\(s\)</span>, scaling parameter ‘how heavy-tailed’</li>
</ul>
<p><br />
</p>
<p>Half-Cauchy distribution for varying <span class="math inline">\(s\)</span>, with <span class="math inline">\(x_0=0\)</span>:</p>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-61-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>HC is ’heavy-tailed;… gives some probability to very high values but low values are still more likely.</p>
<p>One might consider <span class="math inline">\(s=0.3\)</span></p>
<p>Note that the Half-Cauchy distribution has an undefined mean, variance, skewness, and kurtosis. It does have a mode, at <span class="math inline">\(x_0\)</span>.</p>
<p>One might consider <span class="math inline">\(s=0.3\)</span></p>
<div class="marginnote">
<p>DR: <span class="math inline">\(s\)</span> corresponds to the std deviation here? … so an SD of the effect size about 1/3 of it’s mean size? SD: Since the Cauchy distribution has infinite mean (hence undefined first moment) and variance, it doesn’t make sense to discuss <span class="math inline">\(s\)</span> in this way. <span class="math inline">\(s\)</span> is a way of capturing the dispersion of the distribution.</p>
</div>
<p>Checking the share of this distribution below 0.3…</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">phcauchy</span>(<span class="fl">0.3</span>, <span class="at">sigma =</span> <span class="fl">0.3</span>) <span class="co">#cumulative share of distribution below 0.3 ... is 1/2 ... with sigma=0.3</span></span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<p><br />
</p>
<p>… But they go for the ‘more conservative’ <span class="math inline">\(s=0.5\)</span>.</p>
<blockquote>
<p>In general, it is advised to always conduct sensitivity analyses with different prior specifications to check if they affect the results substantially.<br />
</p>
</blockquote>
<p>With enough data, the effect of the prior should shrink. Some priors never allow the data to overload them, so we should be careful using them.</p>
<div class="marginnote">
<p>DR: Todo - insert a formal definition</p>
<p>“Smaller and smaller” could be measured in terms of some metrics over the shift in the distribution, I presume. Perhaps there are several ways of measuring this (average shifts, weighted average, maximum shift for any segment, etc).</p>
</div>
<p>Complete model:</p>
<p><span class="math display">\[ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) \]</span> <span class="math display">\[ \theta_k \sim \mathcal{N}(\mu,\tau^2) \]</span> <span class="math display">\[ \mu \sim \mathcal{N}(0,1)\]</span> <span class="math display">\[ \tau \sim \mathcal{HC}(0,0.5)\]</span></p>
</div>
<div id="bayesian-meta-analysis-in-r-using-the-brms-package" class="section level4 unnumbered">
<h4 class="unnumbered">Bayesian Meta-Analysis in R using the <code>brms</code> package</h4>
<p>You specify the priors as a vector of elements, each of which invokes the ‘prior’ function, which makes some sort of data frame. The priors function takes a distribution function with parameters, and a ‘class.’</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>priors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">class =</span> Intercept), <span class="fu">prior</span>(<span class="fu">cauchy</span>(<span class="dv">0</span>,<span class="fl">0.5</span>), <span class="at">class =</span> sd))</span></code></pre></div>
<p>A quick look at the data we’re using here:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(madata[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>])</span></code></pre></div>
<pre><code>## tibble[,3] [18 × 3] (S3: tbl_df/tbl/data.frame)
##  $ Author: chr [1:18] &quot;Call et al.&quot; &quot;Cavanagh et al.&quot; &quot;DanitzOrsillo&quot; &quot;de Vibe et al.&quot; ...
##  $ TE    : num [1:18] 0.709 0.355 1.791 0.182 0.422 ...
##  $ seTE  : num [1:18] 0.261 0.196 0.346 0.118 0.145 ...</code></pre>
<p><code>TE</code>: calculated effect size of each study, expressed as the Standardized Mean Difference (SMD)</p>
<p><code>seTE</code>: the standard error corresponding to each effect size</p>
<p><code>Author</code>: a unique identifier for each study/effect size.</p>
<p><br />
</p>
<p>To actually run the model, he uses the following code:</p>
<div class="marginnote">
<p>This requires careful installation of packages. See <a href="https://discourse.mc-stan.org/t/dealing-with-catalina-iii/12731/30">here</a> for Mac OS Catalina, R 4.9 instructions.</p>
</div>
<div class="marginnote">
<p>DR: I find it surprising how long this procedure takes to run this simulation, given that the actual data used (estimates and SE’s) is rather small. It seems to be that the C++ model takes long to compile; we sort this out below.</p>
</div>
<div class="marginnote">
<p>2000 iterations seems to be a ‘norm.’ [source?]</p>
</div>
<div class="marginnote">
<p>The issue here seems to be that it takes a long time to compile the C++ model each time; it might be something we can write to avoid this step.</p>
<p>SD: It will take some time to compile the model, and this is inevitable (it takes a long time with every model, this one works pretty quickly at a few minutes), but once it has compiled, it should not take a long time to sample. Below is some code to change the number of samples and show the speed once the model has compiled.</p>
</div>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>m.brm <span class="ot">&lt;-</span> <span class="fu">brm</span>(TE<span class="sc">|</span><span class="fu">se</span>(seTE) <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>Author),</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> madata,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">prior =</span> priors,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">iter =</span> <span class="dv">2000</span>)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Compiling Stan program...</code></pre>
<pre><code>## Trying to compile a simple C file</code></pre>
<pre><code>## Start sampling</code></pre>
<p><br />
</p>
<p><strong>Coding tip: use ‘update’</strong></p>
<p>The brms package has a way of running the model again once it has compiled, without compiling it again. The following code increases the number of iterations from 2000 to 10000. Notice the difference in time it takes to run the code blocks.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">update</span>(m.brm, <span class="at">iter=</span><span class="dv">10000</span>)</span></code></pre></div>
<pre><code>## Start sampling</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;dc600f25ec166987c6302293dcc00ca8&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.442731 seconds (Warm-up)
## Chain 1:                0.517813 seconds (Sampling)
## Chain 1:                0.960544 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;dc600f25ec166987c6302293dcc00ca8&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 2: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.503471 seconds (Warm-up)
## Chain 2:                0.486556 seconds (Sampling)
## Chain 2:                0.990027 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;dc600f25ec166987c6302293dcc00ca8&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.4e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 3: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.492362 seconds (Warm-up)
## Chain 3:                0.475184 seconds (Sampling)
## Chain 3:                0.967546 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;dc600f25ec166987c6302293dcc00ca8&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.4e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 4: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.451087 seconds (Warm-up)
## Chain 4:                0.489763 seconds (Sampling)
## Chain 4:                0.94085 seconds (Total)
## Chain 4:</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>model2</span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: TE | se(seTE) ~ 1 + (1 | Author) 
##    Data: madata (Number of observations: 18) 
## Samples: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;
##          total post-warmup samples = 20000
## 
## Group-Level Effects: 
## ~Author (Number of levels: 18) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.29      0.10     0.11     0.51 1.00     5030     6840
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.57      0.09     0.40     0.76 1.00     9874     9924
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00    20000    20000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>If you would want to change something with the model, such as the priors, then it would have to compile again in C++.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>priors2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">class =</span> Intercept), <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">class =</span> sd))</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">update</span>(m.brm, <span class="at">prior =</span> priors2)</span></code></pre></div>
<pre><code>## The desired updates require recompiling the model</code></pre>
<pre><code>## Compiling Stan program...</code></pre>
<pre><code>## Trying to compile a simple C file</code></pre>
<pre><code>## Start sampling</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>model3</span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: TE | se(seTE) ~ 1 + (1 | Author) 
##    Data: madata (Number of observations: 18) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~Author (Number of levels: 18) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.31      0.11     0.12     0.54 1.00     1136     1418
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.57      0.09     0.39     0.76 1.00     1858     2352
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<div class="marginnote">
<p>The number of iterations can be any ‘’high number’’. A commonly used amount is 2000 iterations and four chains. The reason the number should be high is that the MCMC sampler needs to converge in distribution to the target distribution.</p>
<p>SD: “High” may depend on several factors (such as what distribution we’re trying to sample from, the amount of time it takes to sample, how accurate we want it to be, etc.). But broadly we want to make sure we have more samples than fewer as we want to be confident that we have converged to the target distribution. I’d usually stick to something in the thousands to be sure.</p>
<p>There is a good discussion about it on the answers here: <a href="https://stats.stackexchange.com/questions/203281/number-of-markov-chain-monte-carlo-samples" class="uri">https://stats.stackexchange.com/questions/203281/number-of-markov-chain-monte-carlo-samples</a></p>
</div>
<p>The <em>formula for the model</em> is specified using ‘regression formula notation’ (unfold)…</p>

<div class="note">
<ul>
<li><p>As there is no ‘predictor variable’ in such analyses (unless it’s meta-regression), <code>x</code> is replaced with <code>1</code>.</p></li>
<li><p>But we want to give studies that more precisely estimate the effect size (perhaps because they have a larger sample) a greater weight.</p>
<ul>
<li>Coded using the <code>y|se(se_y)</code> element</li>
</ul></li>
<li><p>For the <em>random effects terms</em> he adds <code>(1|study)</code> to the predictor part (or here <code>(1|author)</code>.</p></li>
<li><p><code>prior</code>: Plugs in the priors created above plug in the <code>priors</code> object we created previously here.</p></li>
<li><p><code>iter</code>: Number of iterations of MCMC algorithm… the more complex your model, the higher this number should be. [DR: but what’s a rule of thumb here – see links in margein note?]</p></li>
</ul>
</div>
</div>
<div id="assesing-convergence-has-mcmc-algo-found-an-optimum" class="section level4 unnumbered">
<h4 class="unnumbered">Assesing convergence (has MCMC algo found an optimum?)</h4>
<ul>
<li><p>If it hasn’t converged, don’t trust it!</p>
<ul>
<li>You may need to boost the number of iterations</li>
</ul></li>
</ul>
<p>“Posterior predictive checks”: If it’s converged, then the density of the replications should resemble the original data.</p>
<div class="marginnote">
<p>DRL Not sure I understand this; may need to revise and give a clear explanation. I guess it’s something like ‘the estimated parameter densities can then be drawn and run to simulate a dgp, which should yield data looking like the original data set’…</p>
</div>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pp_check</span>(m.brm)</span></code></pre></div>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-69-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>A commonly used method of assessing convergence is to look at the traceplots. There should be no distinctive patterns, and they should not have large spikes. Good traceplots look “well mixed,” suggesting that the parameter space is being explored.*</p>
<p><em>Is ‘assessing convergence’ a subjective judgement?</em> (Unfold a discussion.)</p>

<div class="fold">
<p>SD: We will never know if the sampler has converged to the target distribution, but with more and more samples and well mixed traceplots, along with R-hats close to 1, we can be very confident that it has.</p>
<p>I think that it is the same for other subjective methods such as GAMs, where the number of splines to include is very subjective. There are some tests used to help inform the statistician about the suitability of more/less splines, but ultimately it is down to the statistician to make the judgement about it.</p>
<p>DR: Rather than ‘subjective’ I would suspect that: 1. There are numerical measures for assessing this convergence (as there are in optimisation problems)
2. Perhaps there remains debate over which is the best method.</p>
<p><br />
</p>
<p>SD: I think that the R hat is one of those numerical methods, I’m not completely sure about that though. There is a paper discussing them and it comes up on <a href="https://arxiv.org/pdf/1909.11827.pdf">Arxiv</a>. I think effective sample size is another one of those numerical measures.</p>
<p>There is some discussion on which methods are best here: <a href="https://stats.stackexchange.com/questions/507/what-is-the-best-method-for-checking-convergence-in-mcmc" class="uri">https://stats.stackexchange.com/questions/507/what-is-the-best-method-for-checking-convergence-in-mcmc</a></p>
<p>It seems that most people use a combination of both numerical methods and visual checking. When I studied it, we checked the Rhat, the effective sample size, and checked for patterns in the trace plots (eg obvious autocorrelation, large spikes, etc)</p>
<p>There are some good slides on visually assessing convergence here: <a href="https://astrostatistics.psu.edu/RLectures/diagnosticsMCMC.pdf" class="uri">https://astrostatistics.psu.edu/RLectures/diagnosticsMCMC.pdf</a></p>
</div>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bayesplot)</span></code></pre></div>
<pre><code>## This is bayesplot version 1.8.0</code></pre>
<pre><code>## - Online documentation and vignettes at mc-stan.org/bayesplot</code></pre>
<pre><code>## - bayesplot theme set to bayesplot::theme_default()</code></pre>
<pre><code>##    * Does _not_ affect other ggplot2 plots</code></pre>
<pre><code>##    * See ?bayesplot_theme_set for details on theme setting</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mcmc_trace</span>(m.brm)</span></code></pre></div>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-71-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p><br />
</p>
<p>Also check for a Potential Scale Reduction Factor (PSRF), or <span class="math inline">\(\hat{R}\)</span> below 1.01.</p>
<div class="marginnote">
<p>SD: Rhat is a Bayesian measure used to determine convergence - I don’t think that Harrer will go into detail. It has a complicated formula, but all we should focus on is that Rhat converges to 1 as n tends to infinity. Read more here: <a href="https://mc-stan.org/rstan/reference/Rhat.html" class="uri">https://mc-stan.org/rstan/reference/Rhat.html</a></p>
</div>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.brm)</span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: TE | se(seTE) ~ 1 + (1 | Author) 
##    Data: madata (Number of observations: 18) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~Author (Number of levels: 18) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.29      0.10     0.12     0.51 1.00     1218     2172
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.57      0.09     0.40     0.77 1.00     1800     1621
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.00      0.00     0.00     0.00 1.00     4000     4000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="interpreting-the-results" class="section level4 unnumbered">
<h4 class="unnumbered">Interpreting the Results</h4>
<p>Above we see the estimated ‘sd of the mean effect’ and the ‘mean effect,’ and ‘est. error’ and CI’s for each.</p>
<div class="marginnote">
<p>Is ‘est error’ this like a measure of the standard deviation of the estimated coefficient?</p>
<p>Here CI’s are ‘credible intervals.’</p>
</div>
<p><br />
</p>
<p>We can also extract the estimated deviation of each study’s “true” effect size from the pooled effect:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ranef</span>(m.brm)</span></code></pre></div>
<pre><code>## $Author
## , , Intercept
## 
##                        Estimate Est.Error    Q2.5  Q97.5
## Call et al.               0.072      0.20 -0.3215  0.469
## Cavanagh et al.          -0.143      0.18 -0.5274  0.187
## DanitzOrsillo             0.491      0.29  0.0019  1.130
## de Vibe et al.           -0.326      0.15 -0.6287 -0.054
## Frazier et al.           -0.118      0.15 -0.4369  0.163
## Frogeli et al.            0.035      0.17 -0.3224  0.371
## Gallego et al.            0.086      0.18 -0.2727  0.462
## Hazlett-Stevens &amp; Oren   -0.029      0.18 -0.3717  0.316
## Hintz et al.             -0.211      0.17 -0.5510  0.097
## Kang et al.               0.287      0.24 -0.1278  0.811
## Kuhlmann et al.          -0.312      0.19 -0.7155  0.043
## Lever Taylor et al.      -0.113      0.19 -0.5094  0.241
## Phang et al.             -0.024      0.19 -0.4110  0.362
## Rasanen et al.           -0.079      0.20 -0.5053  0.309
## Ratanasiripong           -0.025      0.22 -0.4682  0.402
## Shapiro et al.            0.398      0.26 -0.0374  0.952
## SongLindquist             0.024      0.18 -0.3398  0.380
## Warnecke et al.           0.010      0.20 -0.3934  0.393</code></pre>
<div class="marginnote">
<p>These are measures of <em>deviations</em>. But they don’t exactly equal the difference between the input effect size and the estimated pooled effect size. In fact this is coming from an estimate of the true effect for each study which ‘averages towards the mean’ following some criteria (this is mentioned later).</p>
</div>
<ul>
<li>No p-values listed because this is Bayesian.</li>
</ul>
<div class="marginnote">
<p>SD: With a classical analysis, we would ask if there is enough evidence of an effect to reject the null hypothesis of no impact at a (subjective) significance level. With a Bayesian approach, the entire distribution of the parameter is calcuated, is it is better to observe the distribution of the parameter. Frequentist analyses give binary answers to continuous questions, whereas with Bayesian analyses, continuous questions have continuous answers. (Paraphrasing Daniel XXX).</p>
</div>
<p>Instead he states:</p>
<blockquote>
<p>the Estimate of the pooled effect size is SMD = 0.57, with the 95% credibility interval (not confidence interval!) ranging from 95% CrI: 0.40 − 0.77. This indicates that there is in fact a moderate-sized overall effect of the interventions studied in this meta-analysis.</p>
</blockquote>
<p>But now we can model the parameters we want to estimate probabilistically…</p>
<p>Taking samples from the (?) simulated posterior density of the population intercept (mean effect size) and sd of the effect…</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>post.samples <span class="ot">&lt;-</span> <span class="fu">posterior_samples</span>(m.brm, <span class="fu">c</span>(<span class="st">&quot;^b&quot;</span>, <span class="st">&quot;^sd&quot;</span>))</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(post.samples)</span></code></pre></div>
<pre><code>## [1] &quot;b_Intercept&quot;          &quot;sd_Author__Intercept&quot;</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(post.samples) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;smd&quot;</span>, <span class="st">&quot;tau&quot;</span>)</span></code></pre></div>
<p>“… make a <strong>density plot</strong> of the posterior distributions”</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot for SMD</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>smd_density  <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> smd), <span class="at">data =</span> post.samples) <span class="sc">+</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">color =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">y =</span> <span class="dv">0</span>, <span class="at">x =</span> <span class="fu">mean</span>(post.samples<span class="sc">$</span>smd)) <span class="sc">+</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">italic</span>(SMD)),</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Standardized mean difference&quot;</span>, <span class="at">subtitle =</span> <span class="st">&quot;Posterior density plot&quot;</span>)</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot for tau</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>tau_density <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> tau), <span class="at">data =</span> post.samples) <span class="sc">+</span></span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;lightgreen&quot;</span>, <span class="at">color =</span> <span class="st">&quot;lightgreen&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">y =</span> <span class="dv">0</span>, <span class="at">x =</span> <span class="fu">mean</span>(post.samples<span class="sc">$</span>tau)) <span class="sc">+</span></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(tau),</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Between-study variation (SD = tau)&quot;</span>, <span class="at">subtitle =</span> <span class="st">&quot;Posterior density plot&quot;</span>)</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a><span class="co">#Display plots together</span></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(gridExtra)</span></code></pre></div>
<pre><code>## Loading required package: gridExtra</code></pre>
<pre><code>## 
## Attaching package: &#39;gridExtra&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(smd_density, tau_density, <span class="at">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-75-1.png" width="80%" style="display: block; margin: auto;" /></p>
<ul>
<li>posterior distributions unimodal, roughly normal distribution</li>
<li>… “peaking around the values for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> we saw in the output”</li>
</ul>
<div class="marginnote">
<p>Consider: why are the peaks not exactly these values? Mean versus mode, I guess.</p>
</div>
<p><br />
Maybe we want to know (e.g.) “the probability that the pooled effect is <span class="math inline">\(SMD=0.30\)</span> or smaller, based on our model.”</p>
<div class="marginnote">
<p>Because maybe an effect of 0.30 or smaller means it’s not worth using this drug or something</p>
</div>
<p>Consider the <em>Empirical Cumulative Distribution Function</em> (ECDF) “of the posterior distribution for the pooled effect size”…</p>
<blockquote>
<p>Use the <code>ecdf</code> function to implement the ECDF… then check…</p>
</blockquote>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>smd.ecdf <span class="ot">&lt;-</span> <span class="fu">ecdf</span>(post.samples<span class="sc">$</span>smd)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="fu">smd.ecdf</span>(<span class="fl">0.3</span>)</span></code></pre></div>
<pre><code>## [1] 0.0018</code></pre>
<blockquote>
<p>We see that the probability of our pooled effect being smaller than <span class="math inline">\(SMD = 0.30\)</span> is <strong>very, very low</strong>, so the effects of the interventions we find in this meta-analysis are very likely to be meaningful.</p>
</blockquote>
<p><br />
</p>
<p>Plotting the ECDF below:</p>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-77-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="forest-plots" class="section level3" number="17.3.3">
<h3 number="17.3.3"><span class="header-section-number">17.3.3</span> Forest plots</h3>
<p>Forest plots are great, esp. with Bayesian, where we’ve ‘sampled posterior distributions’… but there’s no prepackaged tool yet. So we’ve to build it with help from the <code>tidybayes</code> package.</p>
<p>First we prepare the data, extracting the posterior distribution for each study individually.</p>
<div class="marginnote">
<p>Use the <code>spread_draws</code> function … takes, as input 1. the fitted <code>brms</code> model, 2. the random-effects index factor and the parameter to extract (here <code>b_Intercept</code>). …calculate actual effect sizes for each by adding the pooled effect size <code>b_Intercept</code> to the estimated deviation for each study.</p>
</div>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>study.draws <span class="ot">&lt;-</span> <span class="fu">spread_draws</span>(m.brm, r_Author[Author,], b_Intercept) <span class="sc">%&gt;%</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">b_Intercept =</span> r_Author <span class="sc">+</span> b_Intercept)</span></code></pre></div>
<p><br />
</p>
<p>Next, generate the distribution of the pooled effect (usually out in the last row of forest plots).</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>pooled.effect.draws <span class="ot">&lt;-</span> <span class="fu">spread_draws</span>(m.brm, b_Intercept) <span class="sc">%&gt;%</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Author =</span> <span class="st">&quot;Pooled Effect&quot;</span>)</span></code></pre></div>
<p>Bind this together, clean labels and reorder:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>forest.data <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(study.draws, pooled.effect.draws) <span class="sc">%&gt;%</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mutate</span>(<span class="at">Author =</span> <span class="fu">str_replace_all</span>(Author, <span class="st">&quot;[.]&quot;</span>, <span class="st">&quot; &quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mutate</span>(<span class="at">Author =</span> <span class="fu">reorder</span>(Author, b_Intercept))</span></code></pre></div>
<p>Generate summarized data (the mean and credibility interval) of each study. Group the above by author, use the <code>mean_qi</code> function (generates 95pct intervals) to calculate these.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>forest.data.summary <span class="ot">&lt;-</span> <span class="fu">group_by</span>(forest.data, Author) <span class="sc">%&gt;%</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean_qi</span>(b_Intercept)</span></code></pre></div>
<p><br />
</p>
<p>Now generate the forest plot:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(b_Intercept, <span class="fu">relevel</span>(Author, <span class="st">&quot;Pooled Effect&quot;</span>, <span class="at">after =</span> <span class="cn">Inf</span>)),</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">data =</span> forest.data) <span class="sc">+</span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">fixef</span>(m.brm)[<span class="dv">1</span>, <span class="dv">1</span>], <span class="at">color =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">fixef</span>(m.brm)[<span class="dv">1</span>, <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">color =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density_ridges</span>(<span class="at">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">rel_min_height =</span> <span class="fl">0.01</span>, <span class="at">col =</span> <span class="cn">NA</span>, <span class="at">scale =</span> <span class="dv">1</span>,</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_pointintervalh</span>(<span class="at">data =</span> forest.data.summary, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> <span class="fu">mutate_if</span>(forest.data.summary, is.numeric, round, <span class="dv">2</span>),</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">glue</span>(<span class="st">&quot;{b_Intercept} [{.lower}, {.upper}]&quot;</span>), <span class="at">x =</span> <span class="cn">Inf</span>), <span class="at">hjust =</span> <span class="st">&quot;inward&quot;</span>) <span class="sc">+</span></span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Standardized Mean Difference&quot;</span>,</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-82-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="marginnote">
<p>Remember, these are not the effect sizes from the original studies. There has been some bayesian updating of each of these, considering all the others.</p>
</div>
</div>
</div>
<div id="pubbias" class="section level2" number="17.4">
<h2 number="17.4"><span class="header-section-number">17.4</span> Dealing with publication bias</h2>
<ul>
<li>Exacerbated by multiple-hypothesis-testing?</li>
</ul>
<blockquote>
<p>But i think it also talks somewhat to the Steffano DellaVigna paper that was presented at the VAFE - that the publication bias really creeps in when we are publishing studies that have just one or two p values around 0.05.</p>
</blockquote>
<div id="diagnosis-and-responses-p-curves-funnel-plots-adjustments" class="section level3" number="17.4.1">
<h3 number="17.4.1"><span class="header-section-number">17.4.1</span> Diagnosis and responses: P-curves, funnel plots, adjustments</h3>
</div>
</div>
<div id="other-notes-links-and-commentary" class="section level2" number="17.5">
<h2 number="17.5"><span class="header-section-number">17.5</span> Other notes, links, and commentary</h2>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
<p>A high quality meta-analysis should:<br><br>- Have a pre-registered protocol<br>- Appropriately deal with dependent effect sizes<br>- Explore effect size heterogeneity <br>- Have a clear methods description<br>- Report COIs<br>- Publish data and code <a href="https://t.co/cHj11wv5vm">https://t.co/cHj11wv5vm</a></p>
</p>
<p>— Dan Quintana (dsquintana) <a href="https://twitter.com/dsquintana/status/1196551674132914176?ref_src=twsrc%5Etfw">November 18, 2019</a></p>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div id="other-resources-and-tools" class="section level2" number="17.6">
<h2 number="17.6"><span class="header-section-number">17.6</span> Other resources and tools</h2>
<p>Feldman: <a href="https://docs.google.com/document/d/1z3QBDYr86S9FxGjptZP94jJnZeeN4aQaBQP3VVT89Ec/edit#heading=h.gjdgxs">Experimental Studies Meta-Analysis Registered Report template: Main manuscript</a></p>
<p><br />
</p>
<div id="institutional-and-systematic-guidelines" class="section level3" number="17.6.1">
<h3 number="17.6.1"><span class="header-section-number">17.6.1</span> Institutional and systematic guidelines</h3>
<p><a href="https://training.cochrane.org/handbook/current">Cochrane Handbook for Systematic Reviews of Interventions online</a></p>
</div>
</div>
<div id="example-discussion-of-meta-analyses-of-the-paleolithic-diet-below" class="section level2" number="17.7">
<h2 number="17.7"><span class="header-section-number">17.7</span> Example: discussion of meta-analyses of the Paleolithic diet <a href="#paleo-example">BELOW</a></h2>
<!--chapter:end:meta_anal_and_open_science/metaanalysis.Rmd-->
</div>
</div>
<div id="bayes" class="section level1" number="18">
<h1 number="18"><span class="header-section-number">18</span> Bayesian approaches</h1>

<div class="note">
I take notes on several different resources/texts below. Ultimately I’ll try to integrate these into a single set of notes.
</div>
<div id="my-david-reinsteins-uses-for-bayesian-approaches-brainstorm" class="section level2" number="18.1">
<h2 number="18.1"><span class="header-section-number">18.1</span> My (David Reinstein’s) uses for Bayesian approaches (brainstorm)</h2>
<div id="meta-analysis-of-previous-evidence" class="section level3" number="18.1.1">
<h3 number="18.1.1"><span class="header-section-number">18.1.1</span> Meta-analysis of previous evidence</h3>
<ul>
<li><p>Of prior work, especially on motivators of (effective) charitable giving and responses to effectiveness information</p></li>
<li><p>Of my own series’ of experiments (potentially joint with prior work)</p></li>
</ul>
</div>
<div id="inference-particularly-about-null-effects" class="section level3" number="18.1.2">
<h3 number="18.1.2"><span class="header-section-number">18.1.2</span> Inference, particularly about ‘null effects’</h3>
<p>When/what can we say about the ‘absence of an effect’</p>
<p>How to integrate into inferences from diagnostic testing (e.g., common-trend assumption)?</p>
</div>
<div id="policy-and-business-implications-and-recommendations" class="section level3" number="18.1.3">
<h3 number="18.1.3"><span class="header-section-number">18.1.3</span> ‘Policy’ and business implications and recommendations</h3>
<p>In particular, in a charitable giving social-media fundraising context, we might consider whether it is worth offering ‘seed contributions’ to encourage giving on existing pages. If so, ‘which pages should we seed and how much?’</p>
<p><br />
</p>
</div>
<div id="theory-driven-inference-about-optimizing-agents-esp.-in-strategic-settings" class="section level3" number="18.1.4">
<h3 number="18.1.4"><span class="header-section-number">18.1.4</span> Theory-driven inference about optimizing agents, esp. in strategic settings</h3>
<ul>
<li>Especially in the context od ‘predicted contributions to public goods… and 2nd order beliefs’</li>
</ul>
</div>
<div id="experimental-design" class="section level3" number="18.1.5">
<h3 number="18.1.5"><span class="header-section-number">18.1.5</span> Experimental design</h3>
<ul>
<li><p>Optimal treatment assignment, with previous observables and a track record</p></li>
<li><p>Sequential designs</p></li>
<li><p>Bayesian Power calculation<br />
</p></li>
</ul>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co">#sessionInfo()</span></span></code></pre></div>
<p>Package loadings from Kurtz:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>pacman<span class="sc">::</span><span class="fu">p_unload</span>(pacman<span class="sc">::</span><span class="fu">p_loaded</span>(), <span class="at">character.only =</span> <span class="cn">TRUE</span>)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">theme_set</span>(ggplot2<span class="sc">::</span><span class="fu">theme_grey</span>())</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>bayesplot<span class="sc">::</span><span class="fu">color_scheme_set</span>(<span class="st">&quot;blue&quot;</span>)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co">#adds in next chapter</span></span></code></pre></div>
</div>
</div>
<div id="statistical-thinking-mcelreath-and-aj-kurtz-recoded-bookdown-highlights-and-notes" class="section level2" number="18.2">
<h2 number="18.2"><span class="header-section-number">18.2</span> ‘Statistical thinking’ (McElreath) and <a href="https://bookdown.org/ajkurz/Statistical_Rethinking_recoded">AJ Kurtz ‘recoded’ (bookdown)</a>: highlights and notes</h2>

<div class="note">
<p>McElreath’s course and text looks great. I’m taking selective notes here; I’ll try to incorporate content from both text and <a href="https://www.youtube.com/watch?v=4WVelCswXo4&amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI">youtube video lectures</a>.</p>
<p><a href="https://bookdown.org/ajkurz/Statistical_Rethinking_recoded">AJ Kurtz has re-written the code</a> using the <code>brms</code> package, which he finds superior. More crucially for me, he redoes the code using ggplot and tidyverse?</p>
I’m planning to through this here, adding my own notes, questions, and considerations and (hopefully) incorporating some of my own work.
</div>
<div class="marginnote">
<p>I’ve also forked Kurtz’s repo <a href="https://github.com/daaronr/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse">here</a>, which I may play with.</p>
</div>
<div id="the-golem-of-prague-map-ant-the-territory" class="section level3" number="18.2.1">
<h3 number="18.2.1"><span class="header-section-number">18.2.1</span> 1. The Golem of Prague (map ant the territory)</h3>
<p>Don’t let your model or approach turn into a Golem you can’t control. Don’t ‘believe the model’; continuously validate it. The map is not the territory.</p>
<p>‘Statistical decision trees’ lend a false sense of security… and almost never fit the actual case we are dealing with. (fig 1.1)<br />
</p>
<p>Statistical models are non-unique maps to ‘process models’ which are non-unique maps to <em>hypotheses</em>. (He offers the example of neutral evolutionary selection’ example.)</p>
<p>This makes strict falsification impossible: How can you falsify a hypothesis/theory if it corresponds to a wide set of process models and statistical models, many of which overlap other hypotheses?<br />
</p>
<p>But this warning is at least as relevant for Bayesian analyses, which must be based on specifically defined (term) models of the DGP etc. Thus he recommends caution and continuous (?) interplay between the model and the data. (See next chapter … ‘small worlds and large worlds.’)<br />
</p>
<p>He also suggests we refer not to ‘Confidence intervals’ or even ‘Credible intervals,’ but to ‘Consistent intervals’ … as in ‘these intervals are consistent with the model and data.’</p>
<p><br />
</p>
<p>And…</p>
<blockquote>
<p>[so you should] ‘…Explicitly compare predictions of more than one model’</p>
</blockquote>
<div id="rethinking-is-nhst-falsificationist" class="section level4 unnumbered">
<h4 class="unnumbered">Rethinking: Is NHST falsificationist?</h4>
<div class="figure" style="text-align: center">
<img src="images/failure_of_falsification.png" alt="From McElreath video lecture 1" width="70%" />
<p class="caption">
(#fig:failure_of_falsification.png)From McElreath video lecture 1
</p>
</div>
<blockquote>
<p>Null hypothesis significance testing, NHST, is often identified with the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify a null hypothesis, not the actual research hypothesis. So the falsification is being done to something other than the explanatory model. This seems the reverse from Karl Popper’s philosophy.</p>
</blockquote>
<div class="marginnote">
<p>I.e., scientists have turned things upside down; originally the idea was that you had substitute of hypotheses that you would want to falsify and now we try to falsify silly null hypotheses that “nothing is going on.” You should try to really build a hypothesis and test it not just reject that nothing is going on.</p>
</div>
</div>
<div id="books-foci" class="section level4" number="18.2.1.1">
<h4 number="18.2.1.1"><span class="header-section-number">18.2.1.1</span> Book’s foci</h4>
<ol style="list-style-type: decimal">
<li>Bayesian data analysis</li>
<li>Multilevel modeling</li>
<li>Model comparison using information criteria</li>
</ol>
<p><br />
</p>
</div>
</div>
<div id="small-worlds-and-large-worlds" class="section level3" number="18.2.2">
<h3 number="18.2.2"><span class="header-section-number">18.2.2</span> 2. Small Worlds and Large Worlds</h3>
<blockquote>
<p>… The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20)</p>
</blockquote>
<p><br />
</p>
<p>We imagine a bag filled with four marbles, each of which is blue or white.</p>
<p>“So, if we’re willing to code the marbles as 0 =”white" 1 = “blue,” we can arrange the possibility data in a tibble as follows." I.e., we can consider the five possible worlds, in each of which the bag has a different number of white and blue marbles, and represent each of these worlds as a column vector:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">p_1 =</span> <span class="dv">0</span>,</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">p_2 =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">0</span>, <span class="at">times =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)),</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">p_3 =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">0</span>, <span class="at">times =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)),</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">p_4 =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">0</span>, <span class="at">times =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">1</span>)),</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">p_5 =</span> <span class="dv">1</span>)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>d</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-87">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-87) </caption><col><col><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p_1</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p_2</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p_3</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p_4</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p_5</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td></tr>
</table>

<p><br />
</p>
<p>We visualize this in the plot below, where each column is one ‘world’:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() <span class="sc">%&gt;%</span> <span class="co">#make it long, with an ket variable for the possibility &#39;world&#39;</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">x =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">times =</span> <span class="dv">5</span>), <span class="co">#an index for &#39;which ball&#39;</span></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">possibility =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">each =</span> <span class="dv">4</span>)) <span class="sc">%&gt;%</span> <span class="co">#distributing the &#39;which world&#39; index</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> possibility,</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">fill =</span> value <span class="sc">%&gt;%</span> <span class="fu">as.character</span>())) <span class="sc">+</span></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;white&quot;</span>, <span class="st">&quot;navy&quot;</span>)) <span class="sc">+</span></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(.<span class="dv">75</span>, <span class="fl">4.25</span>),</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>                  <span class="at">ylim =</span> <span class="fu">c</span>(.<span class="dv">75</span>, <span class="fl">5.25</span>)) <span class="sc">+</span></span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-88-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p><br />
</p>
<p>Simple combinatorics (permutations rule) tells us how many ‘ways’ we can draw 1, 2, and 3 marbles… Here we think about ‘which’ marble is drawn, and not just ‘which color’ it is. We can draw marble 1-4, the first time, then 1-4 the second time, and then 1-4 the third time… so <code>possibilities=marbles ^ draw</code>.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">draw    =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">marbles =</span> <span class="dv">4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">possibilities =</span> marbles <span class="sc">^</span> draw) <span class="sc">%&gt;%</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:right;">
draw
</th>
<th style="text-align:right;">
marbles
</th>
<th style="text-align:right;">
possibilities
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
64
</td>
</tr>
</tbody>
</table>

<div class="note">
<p>Next, there is a huge amount of code explaining how to make the ‘garden of forking paths’ diagrams. I’m basically going to skip all that code, and paste in a few images. You can find all the code <a href="https://bookdown.org/content/3890/small-worlds-and-large-worlds.html#the-garden-of-forking-data">HERE</a></p>
</div>
<p><br />
</p>
<p>Suppose there is only one blue ball and three white balls, possibility ‘2’ above. For this world, we see the full ‘garden of forking paths’ — the number of ways to select 1, 2, and 3 balls (with replacement) — below.</p>
<p>Every path starting from the center is a possible (sequence of) draws.</p>
<p>[CUT A BUNCH HERE]</p>
</div>
</div>
<div id="title-introduction-to-bayesian-analysis-in-r-and-stata---katz-qstep" class="section level2" number="18.3">
<h2 number="18.3"><span class="header-section-number">18.3</span> Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep”</h2>
<p><em>Content from notes from this lecture</em></p>
<div id="why-and-when-use-bayesian-mcmc-methods" class="section level3" number="18.3.1">
<h3 number="18.3.1"><span class="header-section-number">18.3.1</span> Why and when use Bayesian (MCMC) methods?</h3>
<div id="pros" class="section level4" number="18.3.1.1">
<h4 number="18.3.1.1"><span class="header-section-number">18.3.1.1</span> Pros</h4>
<ol style="list-style-type: decimal">
<li><p>No need for asymptotics … good when sample sizes are small</p></li>
<li><p>Incorporate previous information</p></li>
</ol>
<p>You can consider the ‘robustness to other priors’</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Fit complex nonstandard models … e.g., with difficult functional forms or likelihood settings (more computation, less thinking)</p></li>
<li><p>Easy to make predictions (e.g., simulate scenarios) after estimation</p></li>
<li><p>Incorporate evidence, results, expert judgement</p></li>
</ol>
<p>(‘restrictions’ with some lee-way?)</p>
<p>(ISn’t this the same as number 2?)</p>
<ol start="6" style="list-style-type: decimal">
<li>Cleaner treatment/imputation of missing values … these are just parameters</li>
</ol>
</div>
<div id="cons" class="section level4" number="18.3.1.2">
<h4 number="18.3.1.2"><span class="header-section-number">18.3.1.2</span> Cons</h4>
<ol style="list-style-type: decimal">
<li><p>Must specify prior distributions … allows subjective judgement</p></li>
<li><p>Different way of thinking about stats and inference; probability distributions and simulations, not much about p-values, point estimates and standard errors … path dependence</p></li>
<li><p>Computational cost</p></li>
</ol>
<p>This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>.</p>
<p>When you click the <strong>Knit</strong> button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:</p>
</div>
<div id="why-more-popular-today" class="section level4" number="18.3.1.3">
<h4 number="18.3.1.3"><span class="header-section-number">18.3.1.3</span> Why more popular today?</h4>
<ul>
<li>Starting from around 2005 in Political Science and Sociology</li>
</ul>
<p>Computational revolution comes from Markov chain Monte Carlo (MCMC) methods … don’t need analytical solutions</p>
<p>Software implementations – many in R, specialised software like EWinBugs, JAGS, STAN; also increasingly in Stata</p>
</div>
</div>
<div id="theory" class="section level3" number="18.3.2">
<h3 number="18.3.2"><span class="header-section-number">18.3.2</span> Theory</h3>
<p>Bayes theorem … inverting conditional probability thing … ‘inversion’ to make inferences about the parameters</p>
<ul>
<li>In Bayesian stats the <em>parameters</em> (and sometimes missing values) are random variables, we make probability statements about them</li>
</ul>
<p><span class="math display">\[P(A|B)=P(B|A)P(A)/P(B)\]</span></p>
<p>Frequentist: Point estimates, unknown fixed parameters, data from a hyol repeataable random sample</p>
<p>Bayesian: Fixed data (from the experiment), parameters are random variables … results based on probability distributions about rthese</p>
<p>Classical statistics: likelihood of data given parameter: <span class="math inline">\(p(y|\theta)\)</span></p>
<p>Bayes we want, <span class="math inline">\(p(\theta|y) = p(y|\theta)p(\theta)/p(y)\)</span></p>
<p><span class="math inline">\(p(y)\)</span> is a ‘constant’ in our estimation … the data is fixed.</p>
<p>So it’s proportional to <span class="math inline">\(p(\theta|y) = p(y|\theta)\times p(\theta)\)</span></p>
<p><span class="math inline">\(p(y|\theta)\)</span> is what we max when we do ML</p>
<p>$ p()$: prior distribution capturing beliefs about <span class="math inline">\(\theta\)</span></p>
<div id="so-how-do-we-estimate-it" class="section level4" number="18.3.2.1">
<h4 number="18.3.2.1"><span class="header-section-number">18.3.2.1</span> So how do we estimate it?</h4>
<ol style="list-style-type: decimal">
<li><p>Specify a probability model, a distribution for Y (likelihood function) and the priors for <span class="math inline">\(\theta\)</span></p></li>
<li><p>Solve (find) the posterior distribution <span class="math inline">\(p(\theta|Y)\)</span> and summarise the parameters of interest</p></li>
</ol>
<p>In practice, step 2 is usually done via MCMC simulation rather than analytically.</p>
<p>… via simulations, I approach the ‘true’ value on <span class="math inline">\(\theta\)</span></p>
<p>(Given ‘regularity conditions’)</p>
</div>
<div id="linear-regression-model-example" class="section level4" number="18.3.2.2">
<h4 number="18.3.2.2"><span class="header-section-number">18.3.2.2</span> Linear regression model example</h4>
<p><span class="math display">\[Y = x&#39;\beta+\epsilon\]</span> with n obs</p>
<p>only random term is epsilon … natural candidate is a normal distribution, so <span class="math inline">\(Y \sim N(x&#39;\beta,\sigma^2_e)\)</span></p>
<p>So we want to find <span class="math inline">\(p(\beta, \sigma^2_\epsilon|Y,X)\)</span>. This depends on the choices of <span class="math inline">\(p(\beta)\)</span> and <span class="math inline">\(p(\epsilon)\)</span>. Could choose conjugate priors, leading to a particular joint posterior, you can solve it analytically.</p>
<p>Can yield a joint posterior.</p>
<p>Instead, let’s assume that the latter (variance) parameter is known, you can show that the posterior for <span class="math inline">\(\beta\)</span> is also normally distributed. (Conjugate)</p>
<p>Similarly, if we assume <span class="math inline">\(\beta\)</span> is known, if the variance term had an inverse gamma distribution (prior), so will the posterior.</p>
<p>In these conjugate priors, the posterior mean will be a weighted average of the priors and the data.</p>
</div>
<div id="gibbs" class="section level4" number="18.3.2.3">
<h4 number="18.3.2.3"><span class="header-section-number">18.3.2.3</span> Gibbs</h4>
<p>Needs closed form conditional posterior for every parameter.</p>
<p>What Gibbs sampler does is break the parameter space into sets of parameters</p>
<ol style="list-style-type: decimal">
<li><p>Choose starting values, <span class="math inline">\(\theta^0_1,...\theta^0_k\)</span></p></li>
<li><p>sample from the first parameter’s distribution given the others … the second one, … the k’th one .</p></li>
<li><p>Repeat step 2 … thousands of times (starting with the parameters from the previous iteration) Eventually ‘we obtain samples of <span class="math inline">\(p(\theta|y)\)</span>’</p></li>
</ol>
<p><em>But if we don’t have a closed form, we cannot simply sample from known distributions in each step</em></p>
<p>E.g., in case of Logit distribution.</p>
</div>
<div id="metropolis-hastings" class="section level4" number="18.3.2.4">
<h4 number="18.3.2.4"><span class="header-section-number">18.3.2.4</span> Metropolis Hastings</h4>
<ol style="list-style-type: decimal">
<li>Choose ‘proposal distribution’ to sample parameter values (a candidate like normal, uniform)</li>
<li>Start w a prelim guess for parameter values <span class="math inline">\(\theta_0\)</span></li>
<li>At iteration t sample a proposal <span class="math inline">\(\theta_t\)</span> from <span class="math inline">\(p(\theta_t|\theta_{t-1})\)</span> ?? what does this come from?</li>
<li>If <span class="math inline">\(p(\theta_t|y)&gt;p(\theta_{t-1}|y)\)</span> accept it as the new value of <span class="math inline">\(\theta\)</span>. ??? how is this computed if we don’t have conjugate closed-form posteriors?</li>
<li>Otherwise flip a coin with probability r = (ratio of those probabilities)</li>
</ol>
<ul>
<li>if coin tosses heads, accept as new theta, otherwise stay at previous theta</li>
<li>allows algorithm to avoid getting stuck at local maxima</li>
</ul>
<p>Commonly used proposal: random walk sample: <span class="math inline">\(\theta_t=\theta_{t-1}+z_t\)</span>, <span class="math inline">\(z_t \sim f\)</span></p>
<p>?? I do this because there is no analytical way to derive this, unlike in the conjugate case, where we might use the Gibbs</p>
<ul>
<li>can combine Gibbs with Metropolis steps; relevant to some problems</li>
</ul>
</div>
<div id="assessing-convergence" class="section level4" number="18.3.2.5">
<h4 number="18.3.2.5"><span class="header-section-number">18.3.2.5</span> Assessing convergence</h4>
<ul>
<li><p>previous … ‘eyeballing’</p></li>
<li><p>formal:</p>
<ul>
<li>single-chain tests (Geweke/Heidel) … is the last part of the chain stable (stationary)… compare simulation at middle and end, is there much variation?</li>
<li>multiple-chain test… (starting from different values), do they end similar … Gelman-Rubin diagnosting <span class="math inline">\(\hat{R}\)</span></li>
<li>typically either a very long chain and use GH convergence, or multiple shorter chains and use <span class="math inline">\(\hat{R}\)</span></li>
</ul></li>
</ul>
<p>Gabriel: Gelman-Rubin is probably preferred; more conservative</p>
<p>?? What am I iterating towards? Converging on what?</p>
</div>
<div id="assesing-fit-in-bayesian" class="section level4" number="18.3.2.6">
<h4 number="18.3.2.6"><span class="header-section-number">18.3.2.6</span> Assesing ‘fit’ in Bayesian</h4>
<ul>
<li>No r-squared</li>
<li>Typical measure is ‘posterior predictive comparisons’</li>
</ul>
<p><span class="math inline">\(p(y_{replicated}|y_{observed}= ...\)</span></p>
<ol style="list-style-type: decimal">
<li>Simulate data from estimated parameters</li>
<li>Compare to observed data</li>
<li>Use an overall fit measure to assess model fit</li>
</ol>
<p>E.g., percent correct predictions (binary), whether the true data is within the 95% CI of the replicates, deviance</p>
<p>For each replicate Choose statistic D, compare the replicated</p>
<p><span class="math inline">\(D(y^s_{replicated})\)</span> against <span class="math inline">\(D(y^s_{observed})\)</span></p>
<p>Quantify the discrepancy … percent of correct predictions, proportion of times replicated y is below true y … compute ‘bayesian p-value’s’</p>
<p>Systematic differences between replicate and actual data indicate model limitations</p>
<p>(?? what are reasonable values here??)</p>
</div>
</div>
<div id="comparing-models-equivalent-of-likelihood" class="section level3" number="18.3.3">
<h3 number="18.3.3"><span class="header-section-number">18.3.3</span> Comparing models … Equivalent of ‘likelihood’</h3>
<p>‘Deviance Information Criterion’ (most used); specific for MCMC simulations: compares expected LL of the model (of the data given the estimated parameters; average here across much of the later points in the chain) against the llhd at the posterior parameter mean. Always select model with lowest DIC.</p>
<p>Bayes Factor (less used): Ratio of llhd of the models; higher BF means model is more supported; BF&gt;10 seen to provide strong evidence for model w higher value</p>
</div>
<div id="on-choosing-priors" class="section level3" number="18.3.4">
<h3 number="18.3.4"><span class="header-section-number">18.3.4</span> On choosing priors</h3>
<p>Most social scientists use non-informative or vague priors; i.e., large variance… e.g., <span class="math inline">\(\beta \sim N(0,1000)\)</span></p>
<p>But its often useful to incorporate information into your priors</p>
<p>Small pilot to test, <span class="math inline">\(\rightarrow\)</span> data <span class="math inline">\(Y_1\)</span>, another study gives data <span class="math inline">\(Y_2\)</span>; repeated application of Bayes theorem gives the posterior.</p>
<p>Same result whether you obtained these together, or whether you did one and then updated (e.g., via an MCMC, starting with the first one as a prior)</p>
<p>Conjugate priors (mentioned before)</p>
<ul>
<li>Jeffrey’s priors (??)</li>
</ul>
</div>
<div id="implementation" class="section level3" number="18.3.5">
<h3 number="18.3.5"><span class="header-section-number">18.3.5</span> Implementation</h3>
<p>If you don’t need to do fancy things, and don’t want to (?) generate the full posterior distribution (or something)</p>
<p>Some Stata/R commands that make Bayesian look frequentist.</p>
<p>In Jags and Winbugs, we only have to specify the prior… rest is done for us</p>
<p>Jags is great … you only need to do self-coding with lots of data and super complicated models as it can freeze up</p>
<p>We went through it the fancy way in Probit.R</p>
<p>Then the easy way with ‘script probit Jags.R’</p>
</div>
<div id="generate-predictions-from-a-winbugs-model" class="section level3" number="18.3.6">
<h3 number="18.3.6"><span class="header-section-number">18.3.6</span> Generate predictions from a WinBUGS model</h3>
<p>You can just generate these outcomes …</p>
<p>Prediction: generate a new observation #note, he is doing one per iteration, but since these are convergent it would be basically the same if you just chose a random iteration and did all the draws from that one</p>
</div>
<div id="missing-data-case" class="section level3" number="18.3.7">
<h3 number="18.3.7"><span class="header-section-number">18.3.7</span> Missing data case</h3>
<p>One solution – multiple imputation</p>
<ul>
<li>choose imputation model to predict missings,</li>
<li>generate many copies of orig data set, imputing missibg value for each</li>
<li>2 more steps here</li>
</ul>
<p>Need a model for X|alpha, because missing variables are random variables</p>
</div>
<div id="stata" class="section level3" number="18.3.8">
<h3 number="18.3.8"><span class="header-section-number">18.3.8</span> Stata</h3>
<p>Has some rather simple implementations; e.g., just using commands like <code>bayes: regress y x</code></p>
</div>
<div id="r-mcmc-pac" class="section level3" number="18.3.9">
<h3 number="18.3.9"><span class="header-section-number">18.3.9</span> R mcmc pac</h3>
<p>Also simple code; great for standard use</p>
<p>Speedup with parallelization; see “script for parallel probit.R” and “parallelprobit.R”</p>
<p>More advanced: C++; can integrate it with Rcpp, or even use Exeter’s ISCA cluster</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cars)</span></code></pre></div>
<pre><code>##      speed           dist    
##  Min.   : 4.0   Min.   :  2  
##  1st Qu.:12.0   1st Qu.: 26  
##  Median :15.0   Median : 36  
##  Mean   :15.4   Mean   : 43  
##  3rd Qu.:19.0   3rd Qu.: 56  
##  Max.   :25.0   Max.   :120</code></pre>
</div>
</div>
<div id="other-resources-and-notes-to-integrate" class="section level2" number="18.4">
<h2 number="18.4"><span class="header-section-number">18.4</span> Other resources and notes to integrate</h2>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
<p>Hey stats twitter: got a very sharp psych UG student wanting to dive into Bayes. Many resources are too technical (i.e., not good teaching texts for UG level, but useful references). Where should I point her?</p>
</p>
<p>— Tom Carpenter (@tcarpenter216) <a href="https://twitter.com/tcarpenter216/status/1223406990325534720?ref_src=twsrc%5Etfw">February 1, 2020</a></p>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<!--chapter:end:bayesian/bayes_notes.Rmd-->
</div>
</div>
<div id="n_ds4bs" class="section level1" number="19">
<h1 number="19"><span class="header-section-number">19</span> Notes on Data Science for Business by Foster Provost and Tom Fawcett (2013)</h1>
<ul>
<li>Notes by David Reinstein and others (Oska Fentem in parts)</li>
</ul>
<div id="evaluation-of-this-resource" class="section level2" number="19.1">
<h2 number="19.1"><span class="header-section-number">19.1</span> Evaluation of this resource</h2>
<p>This (2013) book seems to have some slightly outdated focus (terms like ‘data-mining’) but it is nonetheless very useful, both conceptually and in many specifics. It does a good job of giving actual insight into mathematical, statistical, and data-science techniques, and uses a reasonable amount of actual maths and some literal code. There is ‘meat in this sandwich.’</p>
<p>Terms like ‘lift’</p>
<blockquote>
<p>As another example, in evaluating the utility of a pattern, we see a notion of lift— how much more prevalent a pattern is than would be expected by chance—recurring broadly across data science. It is used to evaluate very different sorts of patterns in different contexts. Algorithms for targeting advertisements are evaluated by computing the lift one gets for the targeted population. Lift is used to judge the weight of evidence for or against a conclusion. Lift helps determine whether a co-occurrence (an association) in data is interesting, as opposed to simply being a natural consequence of popularity.</p>
</blockquote>
</div>
<div id="ch-1-introduction-data-analytic-thinking" class="section level2 unnumbered">
<h2 class="unnumbered">Ch 1 Introduction: Data-Analytic Thinking</h2>

<div class="note">
<p>A range of examples illustrating applications, techniques, and principles.</p>
</div>
<div id="example-during-hurricane-frances-predicting-demand-to-gear-inventory-and-avoid-shortages-lead-to-huge-profit-for-wal-mart" class="section level3 unnumbered">
<h3 class="unnumbered">Example: During Hurricane Frances… predicting demand to gear inventory and avoid shortages … lead to huge profit for Wal-Mart</h3>
<p>Digging into the day to do some surprising findings:*</p>
<blockquote>
<p>’We didn’t know in the past that strawberry PopTarts increase in sales, like seven times their normal sales rate, ahead of a hurricane"</p>
</blockquote>
<div class="marginnote">
<p>* Strange results like these might seem like examples of overfitting, but it is also seems reasonable that demand patterns may be predictable (using careful data analysis) even if we don’t always have a straightforward intuitive explanation for a ‘mechanism.’</p>
</div>
<blockquote>
<ol style="list-style-type: decimal">
<li>Classification and class probability estimation attempt to predict, for each individual in a population, which of a (small) set of classes this individual belongs to. Usually the classes are mutually exclusive. An example classification question would be: “Among all the customers of MegaTelCo, which are likely to respond to a given offer?” In this example the two classes could be called will respond and will not respond.</li>
</ol>
</blockquote>
</div>
<div id="example-predicting-customer-churn" class="section level3 unnumbered">
<h3 class="unnumbered">Example: Predicting Customer Churn</h3>
<blockquote>
<p>Customers switching from one company to another is called churn,</p>
</blockquote>
<blockquote>
<p>Your task is to devise a precise, step-by-step plan for how the data science team should use MegaTelCo’s vast data resources to decide which customers should be offered the special retention deal prior to the expiration of their contracts</p>
</blockquote>
</div>
<div id="data-science-engineering-and-data-driven-decision-making" class="section level3" number="19.1.1">
<h3 number="19.1.1"><span class="header-section-number">19.1.1</span> Data Science, Engineering, and Data-Driven Decision Making</h3>
<blockquote>
<p>They show that statistically, the more data-driven a firm is, the more productive it is—even controlling for a wide range of possible confounding factors. And the differences are not small. One standard deviation higher on the DDD scale is associated with a 4%–6% increase in productivity. DDD also is correlated with higher return on assets, return on equity, asset utilization, and market value, and the relationship seems to be causal.*</p>
</blockquote>
<div class="marginnote">
<p>* DR: I am still somewhat skeptical of the causality here!</p>
</div>
<p><br />
</p>
<p><strong>Two types of decisions that data analysis can benefit:</strong></p>
<blockquote>
<ol style="list-style-type: decimal">
<li>decisions for which “discoveries” need to be made within data, and (2) decisions that repeat, especially at massive scale, and so decision-making can benefit from even small increases in decision-making accuracy based on data analysis</li>
</ol>
</blockquote>

<div class="note">
<p>Getting the jump on the competition …</p>
<blockquote>
<p>Target wanted to get a jump on their competition. They were interested in whether they could predict that people are expecting a baby. If they could, they would gain an advantage by making offers before their competitors. Using techniques of data science, Target analyzed historical data on customers who later were revealed to have been pregnant, and were able to extract information that could predict which consumers were pregnant. For example, pregnant mothers often change their diets, their wardrobes, their vitamin regimens, and so on. These indicators could be extracted from historical data, assembled into predictive models, and then deployed in marketing campaigns.</p>
</blockquote>
</div>
</div>
<div id="data-processing-and-big-data" class="section level3" number="19.1.2">
<h3 number="19.1.2"><span class="header-section-number">19.1.2</span> Data Processing and “Big Data”</h3>
</div>
<div id="data-asset" class="section level3" number="19.1.3">
<h3 number="19.1.3"><span class="header-section-number">19.1.3</span> Data and Data Science Capability as a <strong>Strategic Asset</strong></h3>
<blockquote>
<p>data, and the capability to extract useful knowledge from data, should be regarded as key strategic assets.</p>
</blockquote>
<p><br />
</p>
<div id="signet-bank-from-the-1990s-a-key-example-of-data-as-a-strategic-asset" class="section level4 unnumbered">
<h4 class="unnumbered">Signet Bank from the 1990s … a key example of ‘data as a strategic asset’</h4>
<blockquote>
<p>, but at the time, credit cards essentially had uniform pricing, for two reasons: (1) … [lack of] information systems to deal with differential pricing at massive scale, and (2) bank management believed customers would not stand for price discrimination.</p>
</blockquote>
<p><br />
</p>
<p>A realization: with their data assets,</p>
<blockquote>
<p>[they] could do more sophisticated predictive modeling… and offer different terms (nowadays: pricing, credit limits, low-initial-rate bal‐ ance transfers, cash back, loyalty points, and so on)</p>
</blockquote>
<p><br />
</p>
<p>Economic insight: model <em>profitability</em>, not just default probability</p>
<blockquote>
<p>They knew that a small proportion of customers actually account for more than 100% of a bank’s profit from credit card operations (because the rest are break-even or money-losing). If they could model profitability, they could make better offers to the best customers and “skim the cream” of the big banks’ clientele</p>
</blockquote>
<p><br />
</p>
<p><strong>“Fundamental strategy of data science: acquire the necessary data at a cost”</strong></p>
<blockquote>
<p>Different terms were offered at random to different customers. This may seem foolish outside the context of data-analytic thinking: you’re likely to lose money! This is true. In this case, losses are the cost of data acquisition. Losses continued for a few years while the data scientists worked to build predictive models from the data,*</p>
</blockquote>
<div class="marginnote">
<p>* This falls under the category of ‘experimentation,’ ‘A/B testing,’ or ‘split testing.’ In general, more efficient experimentation will be ‘adaptive, involving optimal dynamic allocation into different “treatments.” Algorithm-wise, this is a ’multi-armed bandit’ problem. Popular discussion of this: <span class="citation">(<a href="#ref-christianAlgorithmsLiveComputer2016" role="doc-biblioref">Christian and Griffiths 2016</a>)</span>, chapter 2 “Explore/Exploit.”</p>
</div>
<p><br />
</p>
<p><strong>Evidence for this?</strong></p>
<blockquote>
<p>Studies giving clear quantitative demonstrations of the value of a data asset are hard to find, primarily because firms are hesitant to divulge results of strategic value.</p>
</blockquote>
<blockquote>
<p>The huge valuation of Facebook has been credited to its vast and unique data assets (Sengupta, 2012), including both information about individuals and their likes, as well as information about the structure of the social network. Information about network structure has been shown to be important to predicting and has been shown to be remarkably helpful in building models of who will buy certain products (Hill, Provost, &amp; Volinsky, 2006).</p>
</blockquote>
<p><br />
</p>
</div>
</div>
<div id="da-thinking" class="section level3" number="19.1.4">
<h3 number="19.1.4"><span class="header-section-number">19.1.4</span> Data-Analytic Thinking</h3>
<p><strong>Why business people need to understand data science</strong></p>
<p>E.g., in making valuations <em>based on data assets</em>:</p>
<blockquote>
<p>venture capitalists must be able to invest wisely in businesses with substantial data assets, and business strategists must be able to devise plans that exploit data.</p>
</blockquote>
<p>More details and examples (unfold)</p>

<div class="fold">
<blockquote>
<p>As a few examples, if a consultant presents a proposal to mine a data asset to improve your business, you should be able to assess whether the proposal makes sense. If a competitor announces a new data partnership, you should recognize when it may put you at a strategic disadvantage. Or, let’s say you take a position with a venture firm and your first project is to assess the potential for investing in an advertising company. The founders present a convincing argument that they will realize significant value from a unique body of data they will collect, and on that basis are arguing for a substantially higher valuation. Is this reasonable? With an understanding of the fundamentals of data science you should be able to devise a few probing questions to determine whether their valuation arguments are plausible.</p>
</blockquote>
</div>
<p><br />
</p>
<p>And employees interact with these issues:</p>
<blockquote>
<p>Data analytics projects reach into all business units. Employees throughout these units must interact with the data science team. If these employees do not have a fundamental grounding in the principles of data analytic thinking, they will not really understand what is happening in the business</p>
</blockquote>
<p><br />
</p>
</div>
<div id="data-mining-and-data-science-revisited" class="section level3" number="19.1.5">
<h3 number="19.1.5"><span class="header-section-number">19.1.5</span> Data Mining and Data Science, Revisited</h3>
<blockquote>
<p>… <strong>Extraction of useful (nontrivial, hopefully actionable) patterns or models from large bodies of data</strong></p>
</blockquote>
<blockquote>
<p>Fundamental concept: Extracting useful knowledge from data to solve business problems can be treated systematically by following a process with reasonably well-defined stages.*</p>
</blockquote>
<div class="marginnote">
<p>* “The Cross Industry Standard Process for Data Mining, abbreviated CRISP-DM (CRISPDM Project, 2000), is one codification of this process”</p>
</div>
<p><strong>Key principle: Overfitting</strong></p>
<blockquote>
<p>The concept of overfitting and its avoidance permeates data science processes, algorithms, and evaluation methods</p>
</blockquote>
<p><a href="#ds4bs-overfitting">Chapter 5</a> is devoted to this.**</p>
<div class="marginnote">
<p>** <span class="citation">(<a href="#ref-christianAlgorithmsLiveComputer2016" role="doc-biblioref">Christian and Griffiths 2016</a>)</span> also have a chapter on overfitting, but in parts they confound overfitting with the complexity of models. Even very simple models, e.g. involving only one feature and one outcome of interest, can be substantially overfit, while complicated models with many features and parameters can be “penalized” (using techniques such as ridge regression) and validated to detect and avoid overfitting.</p>
</div>
<p><strong>Quantify the benefits of using data</strong></p>
<blockquote>
<p>For our churn-management example, how exactly are we going to use the patterns extracted from historical data? Should the value of the customer be taken into account in addition to the likelihood of leaving? More generally, does the pattern lead to better decisions than some reasonable alternative? How well would one have done by chance? How well would one do with a smart “default” alternative?</p>
</blockquote>
<p><br />
</p>
<p><em>The point:</em> it is costly/time-consuming to maintain and analyze data. Thus it is important to continually assess whether this process is yielding value for money.</p>
<p><br />
</p>
</div>
</div>
<div id="ds4bs-ch2" class="section level2" number="19.2">
<h2 number="19.2"><span class="header-section-number">19.2</span> Ch 2 Business Problems and Data Science Solutions</h2>
<blockquote>
<p>A critical skill in data science is the ability to decompose a data analytics problem into pieces such that each piece matches a known task for which tools are available.</p>
</blockquote>
<p><br />
</p>

<div class="note">
Examples of some techniques and applications below…
</div>
<div id="types-of-problems-and-approaches" class="section level3" number="19.2.1">
<h3 number="19.2.1"><span class="header-section-number">19.2.1</span> Types of problems and approaches</h3>
<div id="classification-and-class-probability-estimation" class="section level4 unnumbered">
<h4 class="unnumbered">Classification and class probability estimation</h4>
<p><em>Example: “Among all the customers of MegaTelCo, which are likely to respond to a given offer?”</em></p>

<div class="fold">
<blockquote>
<p>… for each individual in a population, which of a (small) set of classes this individual belongs to. Usually the classes are mutually exclusive. An example classification question would be: “Among all the customers of MegaTelCo, which are likely to respond to a given offer?” In this example the two classes could be called will respond and will not respond.</p>
</blockquote>
</div>
<div class="marginnote">
<p>Some specific techniques: Logit, Multinomial logit, etc.</p>
</div>
<p><br />
</p>
<p>A <em>score</em> or <em>class probability</em> is a more informative measure, that can also be used for classification:</p>
<blockquote>
<p>A scoring model applied to an individual produces, instead of a class prediction, a score representing the probability (or some other quantification of likelihood) that that individual belongs to each class. In our customer response scenario, a scoring model would be able to evaluate each individual customer and produce a score of how likely each is to respond to the offer.</p>
</blockquote>
<p><br />
</p>
</div>
<div id="regression-value-estimation" class="section level4 unnumbered">
<h4 class="unnumbered">Regression (“value estimation”) *</h4>
<blockquote>
<p>Regression (“value estimation”) attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. An example regression question would be:</p>
</blockquote>
<p><strong>“How much will a given customer use the service?”</strong><br />
</p>
<div class="marginnote">
<p>* (Linear) regression is probably the tool that academics are most familiar with. The idea of a ‘line’ (or plane) of best fit, is ubiquitous, and one of the major concepts taught in basic statistics and econometrics. Of course it has a million flavors and issues.</p>
</div>
<p><br />
</p>
<blockquote>
<p>Here we are less interested in explaining a particular dataset as we are in extracting patterns that will generalize to other data, and for the purpose of improving some business process.</p>
</blockquote>
<div class="marginnote">
<p><strong>DR: The above seems wrong</strong> – this is not what regression analysis <em>does</em>, without many further assumptions!</p>
<p><strong>OF:</strong> This section notes that regression analysis is used for prediction rather than inference. Hence the use of models to <em>predict</em> new cases rather than provide inference.<br />
</p>
</div>
</div>
<div id="sim-matching" class="section level4 unnumbered">
<h4 class="unnumbered">Similarity matching</h4>
<blockquote>
<p>Similarity matching attempts to identify similar individuals based on data known about them. Similarity matching can be used directly to find similar entities. For example, IBM is interested in finding companies similar to their best business customers, in order to focus their sales force on the best opportunities</p>
</blockquote>
<blockquote>
<p>one of the most popular methods for making product recommendations (finding people who are similar to you in terms of the products they have liked or have purchased)</p>
</blockquote>
<div class="marginnote">
<p>DR: To me this seems very close to the classification of class probability problem. If I put two people in the same category, I am suggesting that they are similar. However, I can see how this can be done more flexibly; I can be “similar” to someone in many ways and consideringmany dimensions.</p>
</div>
<p><br />
</p>
</div>
<div id="ds4bs-clustering-intro" class="section level4 unnumbered">
<h4 class="unnumbered">Clustering</h4>
<blockquote>
<p>… attempts to group individuals in a population together by their similarity, but not driven by any specific purpose. An example clustering question would be: “Do our customers form natural groups or segments?” Clustering is useful in preliminary domain exploration to see which natural groups exist because these groups in turn may suggest other data mining tasks or approaches. *</p>
</blockquote>
<div class="marginnote">
<p>* DR: This seems very similar and related to similarity matching as well as classification. Perhaps the clusterning technique can be used in these problems.</p>
</div>
<p><br />
</p>
</div>
<div id="co-occurance" class="section level4 unnumbered">
<h4 class="unnumbered">Co-occurrence grouping</h4>
<p><em>What items are commonly purchased together?</em></p>
<blockquote>
<p>Co-occurrence grouping (also known as frequent itemset mining, association rule discovery, and market-basket analysis) attempts to find associations between entities based on transactions involving them.</p>
</blockquote>
<blockquote>
<p>What items are commonly purchased together? While clustering looks at similarity between objects based on the objects’ attributes, co-occurrence grouping considers similarity of objects based on their appearing together in transactions.</p>
</blockquote>
<div class="marginnote">
<p>So, is this indeed a form of clustering or similarity matching?</p>
</div>
</div>
<div id="ds4bs-profiling" class="section level4 unnumbered">
<h4 class="unnumbered">Profiling</h4>
<p><em>“What is the typical cell phone usage of this customer segment?”</em></p>
<blockquote>
<p>Profiling (also known as behavior description) attempts to characterize the typical behavior of an individual, group, or population.</p>
</blockquote>
<blockquote>
<p>Profiling is often used to establish behavioral norms for anomaly detection applications such as fraud detection …</p>
</blockquote>
<p><br />
</p>
<p>This seems like a ‘descriptive and soft’ measure, but it can also be formalized.</p>
</div>
<div id="link-predict" class="section level4 unnumbered">
<h4 class="unnumbered">Link prediction</h4>
<p><em>"Since you and Karen share 10 friends, maybe you’d like to be Karen’s friend?</em></p>
<blockquote>
<p>attempts to predict connections between data items, usually by suggesting that a link should exist,</p>
</blockquote>
<p><br />
</p>
</div>
<div id="data-reduction" class="section level4 unnumbered">
<h4 class="unnumbered">Data reduction</h4>
<blockquote>
<p>attempts to take a large set of data and replace it with a smaller set of data that contains much of the important information in the larger set.</p>
</blockquote>
<p><br />
</p>
</div>
<div id="causal-modeling" class="section level4 unnumbered">
<h4 class="unnumbered">Causal modeling</h4>
<blockquote>
<p>Techniques for causal modeling include those involving a substantial investment in data, such as randomized controlled experiments (e.g., so-called “A/B tests”), as well as sophisticated methods for drawing causal conclusions from observational data</p>
</blockquote>
<blockquote>
<p>In all cases, a careful data scientist should always include with a causal conclusion the exact assumptions that must be made in order for the causal conclusion to hold (there always are such assumptions—always ask)</p>
</blockquote>
<p><br />
</p>

<div class="note">
<p>Establishing and justifying “causal inference” (also redundantly called “causal effects”) and the ‘identification strategy’ for this, is at the core of modern applied Economics and Econometrics. In fact, economists are often surprised by the emphasis on “prediction without causality” in machine learning and business data science work.</p>
<p><br />
</p>
<p><em>Econometric approaches to inferring causality include:</em></p>
<ul>
<li><p>Experimentation and exogenous random assignment (arguably the ‘gold standard’)</p></li>
<li><p>‘Control strategies’ (to try to ‘hold all observables constant’; sometimes drawing on Machine Learning)</p></li>
<li><p>Natural experiments and ‘instrumental variables’ approaches (e.g., using the variation driven by seemingly ‘exogenous’ factors such as weather)</p></li>
<li><p>"Restrictive’ assumptions about time and the continuity of relationships <span class="math inline">\(\rightarrow\)</span> ‘difference in differences,’ ‘fixed effects,’ and ‘regression discontinuity’ approaches</p></li>
</ul>
<p><br />
</p>
<p>Causality is discussed in popular and non-technical work including by Economists (‘Freakonomics,’ ‘the Why Axis’…) and more directly, in ‘The Book of Why’ (by Judea Pearl, a computer scientist). For more formal and mathematical treatments, see Pearl’s other work, as well as, in Econometrics, <span class="citation">(<a href="#ref-angrist2008mostly" role="doc-biblioref">Joshua D. Angrist and Pischke 2008</a>)</span>,<span class="citation">(<a href="#ref-cunninghamCausalInferenceMixtape2018" role="doc-biblioref">Cunningham 2018</a>; <a href="#ref-hernanCausalInference2010" role="doc-biblioref">Hernán and Robins 2010</a>)</span>.</p>
</div>
<p><br />
</p>
</div>
<div id="sup-vs-unsup" class="section level4" number="19.2.1.1">
<h4 number="19.2.1.1"><span class="header-section-number">19.2.1.1</span> Supervised Versus Unsupervised Methods</h4>
<blockquote>
<p>Metaphorically, a teacher “supervises” the learner by carefully providing target information along with a set of examples.</p>
</blockquote>
<p><br />
</p>
<p>The term ‘label’ is important here:*</p>
<div class="marginnote">
<p>* A term from Data Science that I’ve not seen in statistics/Econometrics, where we typically refer to an ‘outcome variable’ or ‘dependent variable.’</p>
</div>
<blockquote>
<p>Technically, another condition must be met for supervised data mining: there must be data on the target.</p>
</blockquote>
<blockquote>
<p>The value for the target variable for an individual is often called the individual’s label, emphasizing that often (not always) one must incur expense to actively label the data.</p>
</blockquote>
<p><br />
</p>
<p>DR: I always tell me dissertation and project students to ‘ask a question with a question mark,’ and to consider <a href="https://daaronr.github.io/writing_econ_research/getting-started.html#ask_qn">whether the question could even <em>have</em> a meaningful answer</a>. This more or less characterises a <em>supervised</em> problem.</p>
<!--todo: link Hitchiker's video here -->
<blockquote>
<p>A vital part in the early stages of the data mining process is (i) to decide whether the line of attack will be supervised or unsupervised, and (ii) if supervised, to produce a precise definition of a target variable.</p>
</blockquote>
</div>
</div>
<div id="data-mining-process" class="section level3" number="19.2.2">
<h3 number="19.2.2"><span class="header-section-number">19.2.2</span> The Data Mining Process</h3>
<p>“The CRISP data mining process”</p>
<div class="figure">
<img src="../picsfigs/crisp.png" alt="" />
<p class="caption">From DS for Business</p>
</div>
<p>Iterative… questions, data, answers, lather, rinse repeat</p>
<div id="business-understanding" class="section level4 unnumbered">
<h4 class="unnumbered">Business Understanding</h4>
<blockquote>
<p>often the key to a great success is a creative problem formulation by some analyst regarding how to cast the business problem as one or more data science problems.</p>
</blockquote>
</div>
<div id="data-understanding" class="section level4 unnumbered">
<h4 class="unnumbered">Data Understanding</h4>
<blockquote>
<p>Those who commit fraud are a subset of the legitimate users; there is no separate disinterested party who will declare exactly what the “correct” charges should be. Consequently the Medicare billing data have no reliable target variable indicating fraud, and a supervised learning approach that could work for credit card fraud is not applicable. Such a problem usually requires unsupervised approaches such as profiling, clustering, anomaly detection, and co-occurrence grouping.</p>
</blockquote>
</div>
<div id="data-preparation" class="section level4 unnumbered">
<h4 class="unnumbered">Data Preparation</h4>
<p>“Leaks” – variables used in building the model that you can’t actually use in decision-making</p>
<blockquote>
<p>One very general and important concern during data preparation is to beware of “leaks” (Kaufman et al. 2012). A leak is a situation where a variable collected in historical data gives information on the target variable—information that appears in historical data but is not actually available when the decision has to be made. As an example, when predicting whether at a particular point in time a website visitor would end her session or continue surfing to another page, the variable “total number of webpages visited in the session” is predictive. However, the total number of webpages visited in the session would not be known until after the session was over (Kohavi et al., 2000)—at which point one would know the value for the target variable! As another illustrative example, consider predicting whether a customer will be a “big spender”; knowing the categories of the items purchased (or worse, the amount of tax paid) are very predictive, but are not known at decision-making time (Kohavi &amp; Parekh, 2003).</p>
</blockquote>
</div>
<div id="evaluation" class="section level4 unnumbered">
<h4 class="unnumbered">Evaluation</h4>
<blockquote>
<p>assess the data mining results rigorously and to gain confidence that they are valid and reliable before moving</p>
</blockquote>
<blockquote>
<p>test a model first in a controlled laboratory setting. Equally important, the evaluation stage also serves to help ensure that the model satisfies the original business goal</p>
</blockquote>
<blockquote>
<p>A model may be extremely accurate (&gt; 99%) by laboratory standards, but evaluation in the actual business context may reveal that it still produces too many false alarms to be economically feasible. (How much would it cost to provide the staff to deal with all those false alarms?</p>
</blockquote>
<blockquote>
<p>Think about the comprehensibility of the model to stakeholders (not just to the data scientists).</p>
</blockquote>
<p><br />
</p>
<p>Use an experiment to test the model:</p>
<blockquote>
<p>in some cases we may want to extend evaluation into the development environment, for example by instrumenting a live system to be able to conduct random‐ ized experiments. In our churn example, if we have decided from laboratory tests that a data mined model will give us better churn reduction, we may want to move on to an “in vivo” evaluation, in which a live system randomly applies the model to some cus‐ tomers while keeping other customers as a control group (recall our discussion of causal modeling from Chapter 1).</p>
</blockquote>
<p>… noting</p>
<blockquote>
<p>behavior can change—in some cases, like fraud or spam, in direct response to the deployment of models.</p>
</blockquote>
<p><br />
</p>
</div>
<div id="deployment" class="section level4 unnumbered">
<h4 class="unnumbered">Deployment</h4>
<blockquote>
<p>In deployment the results of data mining—and increasingly the data mining techniques themselves—are put into real use in order to realize some return on investment. The</p>
</blockquote>
<p>You may need to ‘deploy the whole data mining system’ (i.e., rebuild the model using newer data?)</p>
<blockquote>
<p>Two main reasons for deploying the data mining system itself rather than the models produced by a data mining system are (i) the world may change faster than the data science team can adapt, as with fraud and intrusion detection, and (ii) a business has too many modeling tasks for their data science team to manually curate each model individually.</p>
</blockquote>
<p><br />
</p>
<p>Difficulties with ‘over the wall’ transfers:</p>
<blockquote>
<p>“Your model is not what the data scientists design, it’s what the engineers build.” From a management perspective, it is advisable to have members of the development team involved early on in the data science project.</p>
</blockquote>
</div>
<div id="implications-for-managing-the-data-science-team" class="section level4 unnumbered">
<h4 class="unnumbered">Implications for Managing the Data Science Team</h4>
<blockquote>
<p>data mining is an exploratory undertaking closer to research and development than it is to engineering</p>
</blockquote>
</div>
<div id="other-analytics-techniques-and-methods" class="section level4 unnumbered">
<h4 class="unnumbered">Other analytics techniques and methods</h4>
<p>Data Warehousing</p>
</div>
<div id="database-querying" class="section level4 unnumbered">
<h4 class="unnumbered">Database Querying</h4>
<blockquote>
<p>s are available to answer one-off or repeating queries about data posed by an analyst. These tools are usually frontends to database systems, based on Structured Query Language (SQL) or a tool with a graphical user interface (GUI) to help formulate queries (e.g., query-by-example, or QBE).</p>
</blockquote>
<blockquote>
<p>On-line Analytical Processing (OLAP) provides an easy-to-use GUI to query large data collections, for the purpose of facilitating data exploration.</p>
</blockquote>
</div>
<div id="machine-learning-and-data-mining" class="section level4 unnumbered">
<h4 class="unnumbered">Machine Learning and Data Mining</h4>
<blockquote>
<p>because Machine Learning is concerned with many types of performance improvement, it includes subfields such as robotics and computer vision that are not part of KDD. It also is concerned with issues of agency and cognition—how will an intelligent agent use learned knowledge to reason and act in its environment—which are not concerns of Data Mining.</p>
</blockquote>
</div>
<div id="answering-business-questions-with-these-techniques" class="section level4 unnumbered">
<h4 class="unnumbered">Answering Business Questions with These Techniques</h4>
<blockquote>
<p>consider a set of questions that may arise and the technologies that would be appropriate for answering them. These questions are all related but each is subtly different.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>Who are the most profitable customers?</li>
<li>Is there really a difference between the profitable customers and the average customer?</li>
<li>But who really are these customers? Can I characterize them?</li>
<li>Will some particular new customer be profitable? How much revenue should I expect this customer to generate?</li>
</ol>
</blockquote>
</div>
</div>
</div>
<div id="ch-3-introduction-to-predictive-modeling-from-correlation-to-supervised-segmentation" class="section level2" number="19.3">
<h2 number="19.3"><span class="header-section-number">19.3</span> Ch 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation</h2>
<blockquote>
<p>we will begin by thinking of predictive modeling as supervised segmentation—how can we segment the population into groups that differ from each other with respect to some quantity of interest. In particular, how can we segment the population with respect to something that we would like to predict or estimate.</p>
</blockquote>
<blockquote>
<p>Tree induction incorporates the idea of supervised segmentation in an elegant manner, repeatedly selecting informative attributes</p>
</blockquote>
<blockquote>
<p>A descriptive model must be judged in part on its intelligibility, and a less accurate model may be preferred if it is easier to understand. A predictive model may be judged solely on its predictive performance, although we will discuss why intelligibility is nonetheless important</p>
</blockquote>
<div id="models-induction-and-prediction" class="section level3" number="19.3.1">
<h3 number="19.3.1"><span class="header-section-number">19.3.1</span> Models, Induction, and Prediction</h3>
<blockquote>
<p>The creation of models from data is known as model induction. Induction is a term from philosophy that refers to generalizing from specific cases to general rules (or laws, or truths).</p>
</blockquote>
<blockquote>
<p>Probability Estimation may be overly optimistic about the probability of class membership for segments with very small numbers of instances. At the extreme, if a leaf happens to have only a single instance, should we be willing to say that there is a 100% probability that members of that segment will have the class that this one instance happens to have?</p>
</blockquote>
<blockquote>
<p>Example: Addressing the Churn Problem with Tree InductionLaplace correction, the purpose of which is to moderate the influence of leaves with only a few instances</p>
</blockquote>
<blockquote>
<p>However, the order in which features are chosen for the tree doesn-t exactly correspond to their ranking in Figure 3-17. Why is this? The answer is that the table ranks each feature by how good it is independently, evaluated separately on the entire population of instances. Nodes in a classification tree depend on the instances above them in the tree. Therefore, except for the root node, features in a classification tree are not evaluated on the entire set of instances</p>
</blockquote>
</div>
<div id="supervised-segmentation" class="section level3" number="19.3.2">
<h3 number="19.3.2"><span class="header-section-number">19.3.2</span> Supervised Segmentation</h3>
<blockquote>
<p>If the segmentation is done using values of variables that will be known when the target is not, then these segments can be used to predict the value of the target variable</p>
</blockquote>
<blockquote>
<p>a formula that evaluates how well each attribute splits a set of examples into segments, with respect to a chosen target variable. Such a formula is based on a purity measure. The most common splitting criterion is called information gain, and it is based on a purity measure called entropy.</p>
</blockquote>
<blockquote>
<p>Disorder corresponds to how mixed (impure) the segment is with respect to these properties of interest. So, for example, a mixed up segment with lots of write-offs and lots of non-write-offs would have high entropy. More technically, entropy is defined as:</p>
</blockquote>
<p><span class="math display">\[entropy = - p_1 log (p_1) - p_2 log (p_2) - ... \]</span></p>
<p><br />
</p>
<blockquote>
<p>Each <span class="math inline">\(p_i\)</span> is the probability (the relative percentage) of property i within the set. the logarithm is generally taken as base 2.</p>
</blockquote>
<p><img src="../picsfigs/entropy_plot.png" /></p>
<blockquote>
<p>Strictly speaking, information gain measures the change in entropy due to any amount of new information being added; here, in the context of supervised segmentation, we consider the information gained by splitting the set on all values of a single attribute.</p>
</blockquote>
<p><br />
</p>
<blockquote>
<p>Information gain resulting from some partitioning of the parent set—how much information has this attribute provided? That depends on how much purer the children are than the parent</p>
</blockquote>
<blockquote>
<p>the entropy for each child <span class="math inline">\((c_i)\)</span> is weighted by the proportion of instances be‐ longing to that child, <span class="math inline">\(p(c_i)\)</span></p>
</blockquote>
<p><img src="../picsfigs/infogain.png" /></p>
</div>
<div id="summary-1" class="section level3" number="19.3.3">
<h3 number="19.3.3"><span class="header-section-number">19.3.3</span> Summary</h3>
<blockquote>
<p>Tree induction recursively finds informative attributes for subsets of the data. In so doing it segments the space of instances into similar regions</p>
</blockquote>
<blockquote>
<p>The resulting tree-structured model partitions the space of all possible instances into a set of segments with different predicted values for the target</p>
</blockquote>
</div>
<div id="note-check-if-there-is-a-gap-here" class="section level3" number="19.3.4">
<h3 number="19.3.4"><span class="header-section-number">19.3.4</span> NOTE – check if there is a gap here</h3>
</div>
</div>
<div id="ds4bs-model-to-data" class="section level2" number="19.4">
<h2 number="19.4"><span class="header-section-number">19.4</span> Ch. 4: Fitting a Model to Data</h2>
<blockquote>
<p>The data miner specifies the form of the model and the attributes; the goal of the data mining is to tune the parameters so that the model fits the data as well as possible. This general approach is called parameter learning or parametric modeling</p>
</blockquote>
<blockquote>
<p>What exactly do we mean when we say a model fits the data well ?</p>
</blockquote>
<blockquote>
<p>The methods here can all be generalized to work with multiple (nonbinary) classes, but the generalization com- plicates the description unnecessarily</p>
</blockquote>
<div id="classification-via-mathematical-functions" class="section level3" number="19.4.1">
<h3 number="19.4.1"><span class="header-section-number">19.4.1</span> Classification via Mathematical Functions</h3>
<blockquote>
<p>because if we take away the axis-parallel boundaries (see Figure 4-2) we can see that there clearly are other, possibly better, ways to partition</p>
</blockquote>
<p>example, we can separate the instances almost perfectly (by class) if we are allowed to introduce a boundary that is still a straight line, but is not perpendicular to the axes (</p>
<p>dataset of Figure 4-2 with a single linear split. This is called a linear classifier and is essentially a weighted sum of the values for the various attributes, as we will describe next.</p>
<p>Linear Discriminant Functions</p>
<blockquote>
<p>Equation 4-1. Classification function class(-) = { + if 1.0 - Age - 1.5 × Balance + 60 &gt; 0 - if 1.0 × Age - 1.5 × Balance + 60 ≤ 0 This is called a linear discriminant because it discriminates between the classes, and the function of the decision boundary is a linear combination-a weighted sum—of the attributes</p>
</blockquote>
<p><span class="math inline">\(f(\cdot) = 60 + 1.0 \times Age - 1.5 \times Balance\)</span></p>
<p>To use this model as a linear discriminant, for a given instance represented by a feature vector x, we check whether f(x) is positive or negative. As discussed above, in the twodimensional case, this corresponds to seeing whether the instance x falls above or below the line</p>
<p>We now have a parameterized model: the weights of the linear function (<span class="math inline">\(w_i\)</span>) are the parameters.</p>
<p>Roughly, the larger the magnitude of a feature’s weight, the more important that feature is for classifying the target with standardization</p>
<blockquote>
<p>Optimizing an Objective Function. In fact, there are infinitely many lines (models) that classify this training set perfectly. Which should we pick? underdetermined</p>
</blockquote>
<p>for Scoring and Ranking InstancesIn other applications, we do not need a precise probability estimate. We simply need a score that will rank cases by the likelihood of belonging to one class or the other. For example, for targeted marketing we may have a limited budget for targeting prospective customers. We would like to have a list of consumers ranked by their predicted likeli- hood of responding positively to our offer</p>
<p>for Scoring and Ranking InstancesThus f(x) itself—the output of the linear discriminant function—gives an intuitively satisfying ranking of the instances by their (estimated) likelihood of belong- ing to the class of interest.</p>
<p>Support Vector Machines, Briefly</p>
<blockquote>
<p>In short, support vector machines are linear discriminants</p>
</blockquote>
<blockquote>
<p>SVMs choose based on a simple, elegant idea: instead of thinking about separating with a line, first fit the fattest bar between the classes</p>
</blockquote>
<blockquote>
<p>Then once the widest bar is found, the linear discriminant will be the center line through the bar</p>
</blockquote>
<blockquote>
<p>distance between the dashed parallel lines is called the margin around the linear discriminant, and thus the objective is to maximize the margin.</p>
</blockquote>
<blockquote>
<p>Hopefully they will be distributed similarly to the training data, but they will in fact be different points. In particular, some of the positive examples will likely fall closer to the discriminant boundary than any positive example we have yet seen. All else being equal, the same applies to the negative examples. In other words, they may fall in the margin. The margin-maximizing boundary gives the maximal lee- way for classifying such points</p>
</blockquote>
<blockquote>
<p>original example of Figure 4-2 shows a situa‐ tion in which a single line cannot perfectly separate the data into classes. This is true of most data from complex real-world applications</p>
</blockquote>
<blockquote>
<p>In the case where the data indeed are linearly separable, we incur no penalty and simply maximize the margin. If the data are not linearly separable, the best fit is some balance between a fat margin and a low total error penalty. The penalty for a misclassified point is proportional to the distance from the decision boundary, so if possible the SVM will make only -small" errors</p>
</blockquote>
<blockquote>
<p>known as hinge loss</p>
</blockquote>
</div>
<div id="regression-via-mathematical-functions" class="section level3" number="19.4.2">
<h3 number="19.4.2"><span class="header-section-number">19.4.2</span> Regression via Mathematical Functions</h3>
<blockquote>
<p>Errors have positive distances from the separator in Figure 4-9, while correct classifications have negative distances</p>
</blockquote>
<blockquote>
<p>The hinge loss only becomes positive when an example is on the wrong side of the boundary and beyond the margin. Loss then increases linearly with the example-s distance from the margin, thereby penalizing points more the farther they are from the separating boundary. Zero-one loss, as its name implies, assigns a loss of zero for a correct decision and one for an incorrect decision</p>
</blockquote>
<ul>
<li>not sure I get what the margin means here</li>
</ul>
<blockquote>
<p>For classification, this would apply large penalties to points far over on the “wrong side” of the separating boundary. Unfortunately, using squared error for classification also penalizes points far on the correct side of the decision boundary</p>
</blockquote>
<blockquote>
<p>principle of thinking carefully about whether the loss function is aligned with the business goal</p>
</blockquote>
</div>
<div id="class-probability-estimation-and-logistic-regression" class="section level3" number="19.4.3">
<h3 number="19.4.3"><span class="header-section-number">19.4.3</span> Class Probability Estimation and Logistic Regression</h3>
<p>"More pragmatically, analysts often claim to prefer squared error because it strongly penalizes very large errors. Whether the quadratic penalty is actually appro- priate is specific to each application</p>
<blockquote>
<p>For least squares regression a serious drawback is that it is very sensitive to the data: erroneous or otherwise outlying data points can severely skew the resultant linear func- tion. For some business applications, we may not have the resources to spend as muchfor systems that build and apply models totally automatically, the modeling needs to be much more robust than when doing a detailed regression analysis -by hand." Therefore, for the former application we may want to use a more robust modeling procedure (e.g., use as the objective function absolute error instead of squared error</p>
</blockquote>
<blockquote>
<p>The director of the fraud control operation may want the analysts to focus not simply on the cases most likely to be fraud, but on the cases where the most money is at stake-that is, accounts where the company’s monetary loss is expected to be the highest. For this we need to estimate the actual probability of fraud</p>
</blockquote>
<blockquote>
<p>model designed to give accurate esti‐ mates of class probability. The most common procedure by which we do this is called logistic regression</p>
</blockquote>
<blockquote>
<p>What exactly is an accurate estimate of class membership probability is a subject of debate beyond the scope of this book. Roughly, we would like (i) the probability estimates to be well calibrated, meaning that if you take 100 cases whose class membership probability is estimated to be 0.2, then about 20 of them will actually belong to the class. We would also like (ii) the probability estimates to be discriminative, in that if possible they give meaningfully different probability estimates to different examples.</p>
</blockquote>
<blockquote>
<p>there another representation of the likelihood of an event that we use in everyday life? If we could come up with one that ranges from -∞ to ∞, then we might model this other notion of likelihood with our linear equation</p>
</blockquote>
<blockquote>
<p>The odds of an event is the ratio of the probability of the event occurring to the probability of the event not occurring</p>
</blockquote>
<blockquote>
<p>Again, the distance from the boundary is between -∞ and ∞, but as we can see from the example, the odds range from 0 to -. Nonetheless, we can solve our garden-path problem simply by taking the logarithm of the odds (called the -log-odds"), since for any number in the range 0 to - its log will be between –∞ to ∞. These</p>
</blockquote>
<blockquote>
<p>The output of the logistic regression model is interpreted as the log-odds of class membership</p>
</blockquote>
</div>
<div id="logistic-regression-some-technical-details" class="section level3" number="19.4.4">
<h3 number="19.4.4"><span class="header-section-number">19.4.4</span> Logistic Regression: Some Technical Details</h3>
<blockquote>
<p>Recall that the distinction between classification and regression is whether the value for the target variable is categorical or numeric</p>
</blockquote>
<blockquote>
<p>Equation 4-4. The logistic function p+ (-) = 1</p>
</blockquote>
<blockquote>
<p>The model can be applied to the training data to produce estimates that each of the training data points belongs to the target class. What would we want? Ideally, any positive example x+ would have p+ (-+) = 1 and any negative example x•</p>
</blockquote>
<blockquote>
<p>model (set of weights) that gives the highest sum is the model that gives the highest -likelihood" to the data—the “maximum likeli‐ hood- model. The maximum likelihood model”on average" gives the highest proba‐ bilities to the positive examples and the lowest probabilities to the negative examples</p>
</blockquote>
</div>
<div id="example-logistic-regression-versus-tree-induction" class="section level3" number="19.4.5">
<h3 number="19.4.5"><span class="header-section-number">19.4.5</span> Example: Logistic Regression versus Tree Induction</h3>
<blockquote>
<p>Though classification trees and linear classifiers both use linear decision boundaries, there are two important differences between them: 1. A classification tree uses decision boundaries that are perpendicular to the instancespace axes (see Figure 4-1), whereas the linear classifier can use decision boundaries of any direction or orientation (see Figure 4-3). This is a direct consequence of the fact that classification trees select a single attribute at a time whereas linear classifiers use a weighted combination of all attributes</p>
</blockquote>
<p>classification tree is a “piecewise” classifier that segments the instance space re‐ cursively when it has</p>
<blockquote>
<p>linear classifier places a single decision surface through the entire space. It has great freedom in the orien- tation of the surface, but it is limited to a single division into two segments</p>
</blockquote>
<p>practically speaking, what are the consequences of these differences?</p>
<p>A de‐ cision tree, if it is not too large, may be considerably more understandable to someone without a strong statistics or mathematics background. I’m not convinced. but I can imagine it’s easier to implement in production</p>
</div>
<div id="nonlinear-functions-support-vector-machines-and-neural-networksthe-two-most-common-families-of-techniques-that-are-based-on-fitting-the-parameters-of-complex-nonlinear-functions-are-nonlinear-supportvector-machines-and-neural-networks." class="section level3" number="19.4.6">
<h3 number="19.4.6"><span class="header-section-number">19.4.6</span> Nonlinear Functions, Support Vector Machines, and Neural NetworksThe two most common families of techniques that are based on fitting the parameters of complex, nonlinear functions are nonlinear supportvector machines and neural networks.</h3>
<p>Support vector machines have a so-called “kernel function” that maps the original features to some other feature space. Then a linear model is fit to this new feature space, just as</p>
<p>implement a nonlinear support vector machine with a “polynomial ker‐ nel,- which essentially means it would consider”higher-order" combinations</p>
<p>One can think of a neural network as a -stack" of models. On the bottom of the stack are the original features. From these features are learned a variety of relatively simple models. Let-s say these are logistic regressions. Then, each subsequent layer in the stack applies a simple model (let-s say, another logistic regression) to the outputs of the next layer down</p>
<p>We could think of this very roughly as first creating a set of -experts" in different facets of the problem (the first-layer models), and then learning how to weight the opinions of these different experts (the second-layer model).</p>
<p>more generally with neural networks target labels for training are provided only for the final layer (the actual target variable).</p>
<p>The stack of models can be represented by one big parameterized numeric function. The so why consider it in many layers?</p>
<p>we can then apply an optimization procedure to find the best parameters to this very complex numeric function</p>
<p>When we’re done, we have the parameters to all the models, and thereby have learned the -best" set of lower-level experts and also the best way to com‐ bine them, all simultaneously "</p>
<p><strong>OF:</strong> Neural Networks are made up of layers of <em>perceptrons.</em> These models are given an input vector and assign weights to each input. The inputs are combined into a weighted sum which is passed through an “activation function” that decides on the outcome classification. This method is known as “forward propagation.” A useful analogy from (Melania, 2019):</p>
<blockquote>
<p>For example, you might get input from several friends on how much they liked a particular movie, but you trust some of those friends’ taste in movies more than others. If the total amount of “friend enthusiasm”—giving more weight to your more trusted friends—is high enough (that is, greater than some unconscious threshold), you decide to go to the movie. This is how a perceptron would decide about movies, if only it had friends.</p>
</blockquote>
<p><br />
It is the way in which these weights are learned that is both mentally and computationally strenuous. The “back-propagation” algorithm is key to the way that weights are learned. The procedure involves multiple stages. Essentially we initialize our model with a random state, feed our input data in and compute the error that our model has made. Then our model works backwards and updates the weights in a manner which provides a prediction consistent with the true data. This updating process can and often is done multiple times.</p>
</div>
</div>
<div id="ds4bs-overfitting" class="section level2" number="19.5">
<h2 number="19.5"><span class="header-section-number">19.5</span> Ch 5: Overfitting and its avoidance</h2>
<div id="generalization" class="section level3" number="19.5.1">
<h3 number="19.5.1"><span class="header-section-number">19.5.1</span> Generalization</h3>
<p>Generalization is the property of a model or modeling process, whereby the model applies to data that were not used to build the model</p>
</div>
<div id="holdout-data-and-fitting-graphs" class="section level3" number="19.5.2">
<h3 number="19.5.2"><span class="header-section-number">19.5.2</span> Holdout Data and Fitting Graphs</h3>
<blockquote>
<p>These are not the actual use data, for which we ultimately would like to predict the value of the target variable. Instead, creating holdout data is like creating a -lab test" of generalization performance. We will simulate the use scenario on these holdout data: we will hide from the model (and possibly the modelers) the actual values for the target on the holdout data. The</p>
</blockquote>
<blockquote>
<p>This is known as the base rate, and a classifier that always selects the majority class is called a base rate classifier. A corresponding baseline for a regression model is a simple model that always predicts the mean or median value of the target variable</p>
</blockquote>
<p><strong>OF:</strong> Holdout data focuses around the need to train models on data that is separate from the data a model is tested on. If we didn’t use holdout validation then our data would always seem to perform incredibly well in testing and would perform poorly on new data.</p>
</div>
<div id="example-overfitting-linear-functions" class="section level3" number="19.5.3">
<h3 number="19.5.3"><span class="header-section-number">19.5.3</span> Example: Overfitting Linear Functions</h3>
<blockquote>
<p>In many modern applications, where large numbers of models are built automatically, and/or where there are very large sets of attributes, manual selection may not be feasible. For example, companies that do data science-driven targeting of online display advertisements can build thousands of models each week, sometimes with mil- lions of possible features. In such cases there is no choice but to employ automatic feature selection (or to ignore feature selection all together).</p>
</blockquote>
</div>
<div id="example-why-is-overfitting-bad" class="section level3" number="19.5.4">
<h3 number="19.5.4"><span class="header-section-number">19.5.4</span> Example: Why Is Overfitting Bad?</h3>
<ul>
<li>It does not explain why overfitting often causes models to become worse I guess this is because the meaningless information that is incorporated causes less attention paid to more meaningful information</li>
</ul>
<blockquote>
<p>From Holdout Evaluation to Cross-ValidationWhile a holdout set will indeed give us an estimate of generalization performance, it is just a single estimate. Should we have any confidence in a single estimate of model accuracy? It might have just been a single particularly lucky (or unlucky) choice of training and test data</p>
</blockquote>
</div>
<div id="from-holdout-evaluation-to-cross-validation" class="section level3" number="19.5.5">
<h3 number="19.5.5"><span class="header-section-number">19.5.5</span> From Holdout Evaluation to Cross-Validation</h3>
<blockquote>
<p>Building the infrastructure for a modeling lab may be costly and time consuming, but after this investment many aspects of model performance can be evaluated quickly in a controlled environment</p>
</blockquote>
<blockquote>
<p>5-9. An illustration of cross-validation</p>
</blockquote>
</div>
<div id="learning-curves" class="section level3" number="19.5.6">
<h3 number="19.5.6"><span class="header-section-number">19.5.6</span> Learning Curves</h3>
<blockquote>
<p>plot of the generalization performance against the amount of training data is called a learning curve. The</p>
</blockquote>
<blockquote>
<p>asset. The learning curve may show that generalization perforance has leveled off so investing in more training data is probably not worthwhile</p>
</blockquote>
</div>
<div id="avoiding-overfitting-with-tree-induction" class="section level3" number="19.5.7">
<h3 number="19.5.7"><span class="header-section-number">19.5.7</span> Avoiding Overfitting with Tree Induction</h3>
<blockquote>
<p>Overfitting Avoidance and Complexity Control</p>
</blockquote>
<blockquote>
<p>To avoid overfitting, we control the complexity of the models induced from the data.</p>
</blockquote>
<blockquote>
<p>The main problem with tree induction is that it will keep growing the tree to fit the training data until it creates pure leaf nodes</p>
</blockquote>
<blockquote>
<ol style="list-style-type: lower-roman">
<li>to stop growing the tree before it gets too complex, and (ii) to grow the tree until it is too large, then “prune” it back <strong>OF:</strong> Pruning also has the advantage of improved model interpretation.</li>
</ol>
</blockquote>
<blockquote>
<p>specify a minimum number of instances that must be present in a leaf.</p>
</blockquote>
<blockquote>
<p>this value is below a threshold (often 5%, but problem specific), then the hypothesis test concludes that the difference is likely not due to chance somewhat imprecise</p>
</blockquote>
<blockquote>
<p>So, for stopping tree growth, an alternative to setting a fixed size for the leaves is to conduct a hypothesis test at every leaf to determine whether the observed difference in (say) information gain could have been due to chance. If the hypothesis test concludes that it was likely not due to chance, then the split is accepted and the tree growing continues. (See -Sidebar: Beware of -multiple comparisons"" on page 139.) Overfitting Avoidance and Complexity</p>
</blockquote>
</div>
<div id="a-general-method-for-avoiding-overfitting" class="section level3" number="19.5.8">
<h3 number="19.5.8"><span class="header-section-number">19.5.8</span> A General Method for Avoiding Overfitting</h3>
<blockquote>
<p>One general idea is to estimate whether replacing a set of leaves or a branch with a leaf would reduce accuracy</p>
</blockquote>
<blockquote>
<p>have a collection of models with different complexities, we could choose the best simply by estimating the generalization performance of each. But how could we estimate their generalization performance? On the (labeled) test data? There-s one big problem with that: test data should be strictly independent of model building so that we can get an independent estimate of model accuracy</p>
</blockquote>
<blockquote>
<p>The key is to realize that there was nothing special about the first training/test split we made. Let-s say we are saving the test set for a final assessment. We can take the training set and split it again into a training subset and a testing subset. Then we can build models on this training subset and pick the best model based on this testing subset. Let-s call the former the subtraining set and the latter the validation set for clarity. The validation set is separate from the final test set, on which we are never going to make any modeling decisions. This procedure is often called nested holdout testing. sorts out a key point of confusion</p>
</blockquote>
</div>
<div id="a-general-method-for-avoiding-overfitting-1" class="section level3" number="19.5.9">
<h3 number="19.5.9"><span class="header-section-number">19.5.9</span> A General Method for Avoiding Overfitting</h3>
<blockquote>
<p>we can induce trees of many complexities from the subtraining set, then we can estimate the generalization performance for each from the validation set</p>
</blockquote>
<blockquote>
<p>Then we could use this model as our best choice, possibly estimating the actual generalization performance on the final holdout</p>
</blockquote>
<blockquote>
<p>But once we-ve chosen the complexity, why not induce a new tree with 122 nodes from the whole, original training set? Then we might get the best of both worlds: using the subtraining/ validation split to pick the best complexity without tainting the test set, and building a model of this best complexity on the entire training set (subtraining plus validation)</p>
</blockquote>
<blockquote>
<p>nested holdout procedure</p>
</blockquote>
<blockquote>
<p>Nested cross-validation</p>
</blockquote>
<blockquote>
<p>example, sequential forward selection (SFS) of features uses a nested holdout pro‐ cedure to first pick the best individual feature, by looking at all models built using just one feature. After i don’t entirely get how the nesting works</p>
</blockquote>
</div>
<div id="avoiding-overfitting-for-parameter-optimization" class="section level3" number="19.5.10">
<h3 number="19.5.10"><span class="header-section-number">19.5.10</span> Avoiding Overfitting for Parameter Optimization</h3>
<blockquote>
<p>equations, such as logistic regression, that unlike trees do not automatically select what attributes to include, complexity can be controlled by choosing a -right" set of attributes</p>
</blockquote>
<blockquote>
<p>The general strategy is that instead of just opti‐ mizing the fit to the data, we optimize some combination of fit and simplicity. Models will be better if they fit the data better, but they also will be better if they are simpler. This general methodology is called regularization, a term that is heard often in data science discussions. The rest of</p>
</blockquote>
<blockquote>
<p>Complexity control via regularization works by adding to this objective function a pen‐ alty for complexity: arg max - weight that determines how much importance the optimization procedure should place on the penalty, compared to the data fit. At this point, the mod- eler has to choose - and the penalty function</p>
</blockquote>
<blockquote>
<p>To learn a “regularized” logistic regression model we would instead compute: arg max -</p>
</blockquote>
<blockquote>
<p>most commonly used penalty is the sum of the squares of the weights, sometimes called the -L2-norm" of w ridge like model</p>
</blockquote>
<blockquote>
<p>linear support vector machine learning is almost equivalent to the L2-regularized logistic re- gression just discussed; the only difference is that a support vector machine uses hinge loss instead of likelihood in its optimization. The support vector machine optimizes this equation: arg max - hinge loss term, is negated because lower hinge loss is better.</p>
</blockquote>
<blockquote>
<p>cross-validation would es‐ sentially conduct automated experiments on subsets of the training data and find a good - value. Then this λ would be used to learn a regularized model on all the training data</p>
</blockquote>
<blockquote>
<p>to optimizing the parameter values of a data mining procedure is known as grid search</p>
</blockquote>
<blockquote>
<p>Sidebar: Beware of “multiple comparisons”</p>
</blockquote>
<blockquote>
<p>index, some will be worse, and some will be better. The best one might be a lot better. Now, you liquidate all the funds but the best few, and you present these to the public. You can -honestly" claim that their 5-year return is substantially better than the return of the Russell 2000 index.</p>
</blockquote>
<blockquote>
<p>The underlying reasons for overfitting when building models from data are essentially problems of multiple comparisons (Jensen &amp; Cohen, 2000)</p>
</blockquote>
<blockquote>
<p>if the fitting graph truly has an inverted-U-shape, one can be much more confident that the top represents a -good" complexity than if the curve jumps around randomly</p>
</blockquote>
<blockquote>
<p>e model performance on the training and testing data as a function of model complexity</p>
</blockquote>
<blockquote>
<p>Summary learning curve shows model per‐ formance on testing data plotted against the amount of training data used</p>
</blockquote>
<p><strong>OF:</strong> Note the importance of the bias-variance trade-off. As we increase the flexibility of a model, by for example adding more tree nodes, we decrease the bias but increase the variability. This means that the model will tend to perform poorly on newer data. The relative change of bias and variance helps determine whether the mean-squared error increases or decreases. These will vary across models. Highly non-linear models tend to have higher variation.</p>
</div>
</div>
<div id="ds4bs-similarity" class="section level2" number="19.6">
<h2 number="19.6"><span class="header-section-number">19.6</span> Ch 6.: Similarity, Neighbors, and Clusters</h2>
<blockquote>
<p>Fundamental concepts: Calculating similarity of objects described by data; Using simi‐ larity for prediction; Clustering as similarity-based segmentation. Exemplary techniques: Searching for similar entities; Nearest neighbor methods; Clus- tering methods; Distance metrics for calculating similarity.</p>
</blockquote>
<blockquote>
<p>example, IBM wants to find companies that are similar to their best business customers, in order to have the sales staff look at them as prospects</p>
</blockquote>
<blockquote>
<p>Hewlett-Packard maintains many highperformance servers for clients; this maintenance is aided by a tool that, given a server configuration, retrieves information on other similarly configured servers. Advertisers often want to serve online ads to consumers who are similar to their current good customers. production function example</p>
</blockquote>
<div id="similarity-and-distance" class="section level3" number="19.6.1">
<h3 number="19.6.1"><span class="header-section-number">19.6.1</span> Similarity and Distance</h3>
<p>unsupervised segmentation</p>
</div>
<div id="similarity-and-distance-1" class="section level3" number="19.6.2">
<h3 number="19.6.2"><span class="header-section-number">19.6.2</span> Similarity and Distance</h3>
<blockquote>
<p>similarity to provide recommen‐ dations of similar products or from similar people</p>
</blockquote>
<blockquote>
<p>The field of Artificial Intelligence has a long history of building systems to help doctors and lawyers with such case-based reasoning. Sim- ilarity judgments are a key component</p>
</blockquote>
<blockquote>
<p>Euclidean distance</p>
</blockquote>
<p><strong>OF:</strong> Because of the mechanics of distance measures we <strong>should</strong> range-normalize data before using methods such as KNN. This is because the distance measure is affected by the distance between variables, if variables are all on a different scale then this is equivalent to them having different variances. Data doesn’t need to be normalized for methods such as decision trees as information gain is calculated for each individual variable, thus not taking into account the relationship with other variables.</p>
<p>The Euclidean and Manhattan distance are special cases of the <strong>Minkowski</strong> distance. Where the higher the value of <span class="math inline">\(p\)</span> the more emphasis is placed on features with a large difference in the values due to these differences being raised to the <span class="math inline">\(p^{th}\)</span> power</p>
<p><span class="math display">\[
\text{Minkowski}(\textbf{a}, \textbf{b}) = (\sum_{i=1}^m abs(\textbf{a}[i]-\textbf{b}[i])^p)^{1/p}
\]</span>
Source: <span class="citation">(<a href="#ref-kelleherFundamentalsMachineLearning2015" role="doc-biblioref">Kelleher, Mac Namee, and D’Arcy 2015</a>)</span></p>
</div>
<div id="example-whiskey-analytics" class="section level3" number="19.6.3">
<h3 number="19.6.3"><span class="header-section-number">19.6.3</span> Example: Whiskey Analytics</h3>
<blockquote>
<ol start="19" style="list-style-type: decimal">
<li>This distance is just a number—it has no units, and no meaningful interpretation this needs more explanation… at least discuss standardising these features</li>
</ol>
</blockquote>
<p>How can we describe single malt Scotch whiskeys as feature vectors, in such a way that we think similar whiskeys will have similar taste</p>
</div>
<div id="nearest-neighbors-for-predictive-modeling" class="section level3" number="19.6.4">
<h3 number="19.6.4"><span class="header-section-number">19.6.4</span> Nearest Neighbors for Predictive Modeling</h3>
<blockquote>
<p>Legendre’s representation of whiskeys with Euclidean distance to find similar ones for him. how to do with categorical variables?</p>
</blockquote>
<blockquote>
<p>Figure 6-2. Nearest neighbor classification. The point to be classified, labeled with a question mark, would be classified + because the majority of its nearest (three) neigh- bors are +</p>
</blockquote>
<blockquote>
<p>have some combining function (like voting or averaging) operating on the neighbors- known target values</p>
</blockquote>
<blockquote>
<p>His nearest neighbors (Rachael, John, and Norah) have classes of No, Yes, and Yes, respectively. If we score for the Yes class, so that Yes=1 and No=0, we can average these into a score of 2/3 for David. If we were to do this in practice, we might want to use more than just three nearest neighbors</p>
</blockquote>
</div>
<div id="how-many-neighbors-and-how-much-influence" class="section level3" number="19.6.5">
<h3 number="19.6.5"><span class="header-section-number">19.6.5</span> How Many Neighbors and How Much Influence?</h3>
<blockquote>
<p>assume that David’s three nearest neighbors were again Rachael, John, and Norah. Their respective incomes are 50, 35, and 40 (in thousands). We then use these values to generate a prediction for David-s income. We could use the average (about 42) or the median (40). nn for ’regression,,. kernel models would be an extension of this</p>
</blockquote>
<blockquote>
<p>important to note that in retrieving neighbors we do not use the target variable because we-re trying to predict it</p>
</blockquote>
<blockquote>
<p>Nearest neighbor algorithms are often referred to by the shorthand k-NN, where the k refers to the number of neighbors used, such as 3-NN.</p>
</blockquote>
<blockquote>
<p>nearest-neighbor methods often use weighted voting or similarity moderated voting such that each neighbor-s contribution is scaled by its sim‐ ilarity</p>
</blockquote>
<blockquote>
<p>other sorts of prediction tasks, for example regression and class probability estimation. Generally, we can think of the procedure as weighted scor- ing</p>
</blockquote>
<p><strong>OF:</strong> For classification purposes it is a common rule of thumb to use an odd number as the value for <span class="math inline">\(k\)</span>. This is done in order to avoid ties between two label classes.</p>
</div>
<div id="geometric-interpretation-overfitting-and-complexity-control" class="section level3" number="19.6.6">
<h3 number="19.6.6"><span class="header-section-number">19.6.6</span> Geometric Interpretation, Overfitting, and Complexity Control</h3>
<blockquote>
<p>related technique in artificial intelligence is Case-Based Reasoning (Kolodner, 1993; Aamodt &amp; Plaza, 1994), abbreviated CBR relevant to meta analysis?!!</p>
</blockquote>
<blockquote>
<p>Although no explicit boundary is created, there are implicit regions created by instance neighborhoods</p>
</blockquote>
<blockquote>
<p>Figure 6-3. Boundaries created by a 1-NN classifier</p>
</blockquote>
<blockquote>
<p>Note also the one negative instance isolated inside the positive in‐ stances creates a -negative island" around itself. This point might be considered noise or an outlier, and another model type might smooth over it.</p>
</blockquote>
<blockquote>
<p>generally, irregular concept bound‐ aries are characteristic of all nearest-neighbor classifiers, because they do not impose any particular geometric form on the classifier</p>
</blockquote>
<blockquote>
<p>The 1-NN classifier predicts perfectly for training examples, but it also can make an often reason- able prediction on other examples: it uses the most similar training example.</p>
</blockquote>
<blockquote>
<p>k in a k-NN classifier is a complexity parameter. At one extreme, we can set k = n and we do not allow much complexity at all in our</p>
</blockquote>
</div>
<div id="issues-with-nearest-neighbor-methods" class="section level3" number="19.6.7">
<h3 number="19.6.7"><span class="header-section-number">19.6.7</span> Issues with Nearest-Neighbor Methods</h3>
<blockquote>
<p>in some fields such as medicine and law, reasoning about similar historical cases is a natural way of coming to a decision about a new case</p>
</blockquote>
<blockquote>
<p>other areas, the lack of an explicit, interpretable model may pose a problem. There are really two aspects to this issue of intelligibility: the justification of a specific decision and the intelligibility of an entire model.</p>
</blockquote>
<blockquote>
<p>With k-NN, it usually is easy to describe how a single instance is decided: the set of neighbors participating in the decision can be presented, along with their contributions.</p>
</blockquote>
<blockquote>
<p>The movie Billy Elliot was recommended based on your interest in Amadeus, The Con‐ stant Gardener and Little Miss Sunshine-</p>
</blockquote>
<blockquote>
<p>e got a recommendation. On the other hand, a mortgage applicant may not be satisfied with the explanation, -We declined your mortgage application because you remind us of the Smiths and the Mitchells, who both defaulted.- Indeed, some legal regulations restrict the sorts of mod‐ els that can be used for credit scoring to models for which very simple explanations can be given based on specific, important variables. For example, with a linear model, one may be able to say: -all else being equal, if your income had been $20,000 higher you would have been granted this particular mortgage.</p>
</blockquote>
<blockquote>
<p>What is difficult is to explain more deeply what “knowledge” has been mined from the data. If a stakeholder asks -What did your system learn from the data about my cus‐ tomers? On what basis does it make its decisions?- there may be no easy answer because there is no explicit model</p>
</blockquote>
<blockquote>
<p>numeric attributes may have vastly different rang‐ es, and unless they are scaled appropriately the effect of one attribute with a wide range can swamp the effect of another with a much smaller range. But scaling</p>
</blockquote>
<blockquote>
<p>example, in the credit card offer domain, a customer database could contain much incidental information such as number of children, length of time at job, house size, median income, make and model of car, average education level, and so on. Conceivably</p>
</blockquote>
<blockquote>
<p>probably most would be irrelevant</p>
</blockquote>
<blockquote>
<p>curse of dimensionality—and this poses problems for nearest neighbor methods</p>
</blockquote>
<blockquote>
<p>feature selection</p>
</blockquote>
<blockquote>
<p>domain knowledge</p>
</blockquote>
<blockquote>
<p>is to tune the similarity/distance function manually. We may know, for example, that the attribute Number of Credit Cards should</p>
</blockquote>
<blockquote>
<p>training is very fast because it usually involves only storing the instances. No effort is expended in creating a model</p>
</blockquote>
<blockquote>
<p>The main computational cost of a nearest neighbor method is borne by the prediction/classifica- tion step, when the database</p>
</blockquote>
<blockquote>
<p>Heterogeneous AttributesThere are techniques for speeding up neighbor retrievals. Specialized data structures like kd-trees and hashing methods (Shakhnarovich, Darrell, &amp; Indyk, 2005; Papadopoulos &amp; Manolopoulos, 2005) are employed in some commerical database and data mining systems to make nearest neighbor queries more efficient</p>
</blockquote>
</div>
<div id="other-distance-functions" class="section level3" number="19.6.8">
<h3 number="19.6.8"><span class="header-section-number">19.6.8</span> Other Distance Functions</h3>
<blockquote>
<p>Euclidean distance</p>
</blockquote>
<blockquote>
<p>general, intuitive and computationally very fast</p>
</blockquote>
<blockquote>
<p>Euclidean(ᅚ, ᅛ) = ֫ ᅚ - ᅛ ֫ 2 = (x1 - y1) 2 + (x2 - y2) -ٴ + 2 Though</p>
</blockquote>
<blockquote>
<p>Manhattan distance or L1-norm is the sum of the (unsquared) pairwise distances,</p>
</blockquote>
<blockquote>
<p>or taxicab</p>
</blockquote>
<blockquote>
<p>Jaccard distance treats the two objects as sets of characteristics</p>
</blockquote>
<blockquote>
<p>Jaccard distance is the proportion of all the characteristics (that either has) that are shared by the two</p>
</blockquote>
<blockquote>
<p>appropriate for problems where the possession of a common characteristic between two items is important, but the common absence of a characteristic is not</p>
</blockquote>
<blockquote>
<p>Cosine distance is often used in text classification to measure the similarity of two docu‐ ments how different from L2 with normalization?</p>
</blockquote>
<blockquote>
<p>three occurrences of transition, and two occurrences of monetary. Docu‐ ment B contains two occurrences of performance, three occurrences of transition, and no occurrences of monetary. The two documents would be represented as vectors of counts of these three words: A = &lt;7,3,2&gt; and B = &lt;2,3,0&gt;. The cosine distance of the two documents is: dcosine(A, B) = 1 - 7, 3, 2 - 2, 3, 0 - 7, 3, 2 ֫ 2 - ֫ 2, 3, 0 ֫ 2 = 1 - 7 - 2 + 3 · 3 + 2 · 0 49 + 9 + 4 - 4 + 9 = 1 - 23 28.4 - 0.19</p>
</blockquote>
<blockquote>
<p>is particularly useful when you want to ignore differences in scale across instances-technically, when you want to ignore the magnitude of the vectors. As a concrete example, in text classification you may want to ignore whether one document is much longer than another, and just concentrate on the textual content</p>
</blockquote>
<blockquote>
<ul>
<li>Other Distance Functions distance or the Levenshtein metric. This metric counts the minimum number of edit operations required to convert one string into the other</li>
</ul>
</blockquote>
</div>
<div id="stepping-back-solving-a-business-problem-versus-data-exploration" class="section level3" number="19.6.9">
<h3 number="19.6.9"><span class="header-section-number">19.6.9</span> Stepping Back: Solving a Business Problem Versus Data Exploration</h3>
<blockquote>
<p>Recall the CRISP data mining process, replicated in Figure 6-15. We should spend as much time as we can in the business understanding/ data understanding mini-cycle, until we have a concrete, specific definition of the prob- lem we are trying to solve. In predictive modeling applications, we are aided by our need to define the target variable precisely, and</p>
</blockquote>
<blockquote>
<p>In our similarity-matching examples, again we had a very concrete notion of what exactly we were looking for: we want to find similar companies to optimize our efforts, and we will define specifically what it means to be similar.</p>
</blockquote>
<blockquote>
<p>What do we do when in the business understanding phase we conclude: we would like to explore our data, possibly with only a vague notion of the exact problem we are solving? The problems to which we apply clustering often fall into this category. We want to perform unsupervised segmentation</p>
</blockquote>
<blockquote>
<p>igure 6-15. The CRISP data mining process</p>
</blockquote>
<blockquote>
<p>finding groups that “naturally” occur (subject, of course, to how we define our similarity measures).</p>
</blockquote>
</div>
<div id="summary-2" class="section level3" number="19.6.10">
<h3 number="19.6.10"><span class="header-section-number">19.6.10</span> Summary</h3>
<p>that for problems where we did not achieve a precise formulation of the problem in the early stages of the data mining process, we have to spend more time later in the process-in the Evaluation stage</p>
<blockquote>
<p>Therefore, for clustering, additional creativity and business knowledge must be applied in the Evaluation stage of the data mining process</p>
</blockquote>
<blockquote>
<p>they settled on five clusters that represented very different consumer credit behavior (e.g., those who spend a lot but pay off their cards in full each month versus those who spend a lot and keep their balance near their credit limit). These different sorts of customers can tolerate very different credit lines (in the two examples, extra care must be taken with the latter to avoid default)</p>
</blockquote>
<blockquote>
<p>They used the knowledge to define a precise predictive mod‐ eling problem: using data that are available at the time of credit approval, predict the probability that a customer will fall into each of these clusters</p>
</blockquote>
</div>
</div>
<div id="ds4bs-decision-thinking" class="section level2" number="19.7">
<h2 number="19.7"><span class="header-section-number">19.7</span> Ch. 7. Decision Analytic Thinking I: What Is a Good Model?</h2>
<blockquote>
<p>Fundamental concepts: Careful consideration of what is desired from data science results; Expected value as a key evaluation framework; Consideration of appropriate comparative baselines. Exemplary techniques: Various evaluation metrics; Estimating costs and benefits; Cal- culating expected profit; Creating baseline methods for comparison economics connection</p>
</blockquote>
<blockquote>
<p>Often it is not possible to measure perfectly one-s ultimate goal, for example because the systems are inadequate, or because it is too costly to gather the right data, or because it is difficult to assess causality. So, we might conclude that we need to measure some surrogate for what we’d really like to measure. It is nonetheless crucial to think carefully about what we’d really like to meas‐ ure. If we have to choose a surrogate, we should do it via careful, data-analytic thinking.</p>
</blockquote>
<div id="evaluating-classifier" class="section level3" number="19.7.1">
<h3 number="19.7.1"><span class="header-section-number">19.7.1</span> Evaluating Classifier</h3>
<blockquote>
<p>we discussed how for evaluation we should use a holdout test set to assess the generalization performance of the model. But how should we measure generalization performance?</p>
</blockquote>
</div>
<div id="the-confusion-matrix" class="section level3" number="19.7.2">
<h3 number="19.7.2"><span class="header-section-number">19.7.2</span> The Confusion Matrix</h3>
<blockquote>
<p>Here we will reserve accuracy for its specific technical meaning as the proportion of correct decisions: accuracy = Number of correct decisions made Total number of decisions made This is equal to 1-error rate</p>
</blockquote>
<blockquote>
<p>class con‐ fusion and the confusion matrix, which is one sort of contingency table. A confusion matrix for a problem involving n classes is an n - n matrix with the columns labeled with actual classes and the rows labeled with predicted classes. Each example in a test set has an actual class label as well as the class predicted by the classifier (the predicted class), whose combination determines which matrix cell the instance counts into. For simplicity we will deal with two-class problems having 2 - 2 confusion matrices.</p>
</blockquote>
<blockquote>
<p>making explicit how one class is being confused for another</p>
</blockquote>
</div>
<div id="problems-with-unbalanced-classes" class="section level3" number="19.7.3">
<h3 number="19.7.3"><span class="header-section-number">19.7.3</span> Problems with Unbalanced Classes</h3>
<blockquote>
<p>Table 7-1. The layout of a 2 × 2 confusion matrix showing</p>
</blockquote>
<blockquote>
<p>Because the unusual or interesting class is rare among the general population, the class distribution is unbal- anced or skewed</p>
</blockquote>
<blockquote>
<p>Consider a domain where the classes appear in a 999:1 ratio. A simple rule-always choose the most prevalent class—gives 99.9% accuracy</p>
</blockquote>
<blockquote>
<p>Chap‐ ter 5 mentioned the -base rate" of a class, which corresponds to how well a classifier would perform by simply choosing that class for every instance</p>
</blockquote>
<blockquote>
<p>Consider again our cellular-churn example. Let-s say you are a manager at MegaTelCo and as an analyst I report that our churnprediction model generates 80% accuracy. This sounds good, but is it? My coworker reports that her model generates an accuracy of 37%. That-s pretty bad, isn’t it? You might say, wait-we need more information about the data</p>
</blockquote>
<blockquote>
<p>say you know that in these data the baseline churn rate is approximately 10% per month. Let-s consider a customer who churns to be a positive example, so within our population of customers we expect a positive to negative class ratio of 1:9. So if we simply classify everyone as negative we could achieve a base rate accuracy of 90%!</p>
</blockquote>
<blockquote>
<p>My coworker calculated the accuracy on a representative sample from the population, whereas I created artificially balanced datasets for training and testing (both common practices). Now my coworker-s model looks really bad—she could have achieved 90% accuracy, but only got 37%. However, when she applies her model to my balanced data set, she also sees an accuracy of 80% a good practice problem</p>
</blockquote>
<blockquote>
<p>7-1. Two churn models, A and B, can make an equal number of errors on a bal‐ anced population used for training (top) but a very different number of errors when tes- ted against the true population (bottom</p>
</blockquote>
<blockquote>
<p>accuracy: we don’t know how much we care about the different errors and correct decisions. This</p>
</blockquote>
</div>
<div id="generalizing-beyond-classification" class="section level3" number="19.7.4">
<h3 number="19.7.4"><span class="header-section-number">19.7.4</span> Generalizing Beyond Classification</h3>
<blockquote>
<p>Another problem with simple classification accuracy as a metric is that it makes no distinction between false positive and false negative errors</p>
</blockquote>
<blockquote>
<p>Compare this with the opposite error: a patient who has cancer but she is wrongly told she does not. This is a false negative. This second type of error would mean a person with cancer would miss early detection, which could have far more serious consequences</p>
</blockquote>
<blockquote>
<p>consider the cost of giving a customer a re‐ tention incentive which still results in departure (a false positive error). Compare this with the cost of losing a customer because no incentive was offered (a false negative). Whatever costs you might decide for each, it is unlikely they would be equal; and the errors should be counted separately regardless.</p>
</blockquote>
<blockquote>
<p>hard to imagine any domain in which a decision maker can safely be in‐ different to whether she makes a false positive or a false negative</p>
</blockquote>
<blockquote>
<p>Once aggregated, these will produce an expected profit (or expected benefit or expected cost) estimate for the classifier</p>
</blockquote>
<blockquote>
<p>vital to return to the question: what is important in the application? What is the goal? Are we assessing the results of data mining appropriately given the actual goal?</p>
</blockquote>
<blockquote>
<p>Generalizing Beyond Classification recommendation model predicts how many stars a</p>
</blockquote>
</div>
<div id="a-key-analytical-framework-expected-value" class="section level3" number="19.7.5">
<h3 number="19.7.5"><span class="header-section-number">19.7.5</span> A Key Analytical Framework: Expected Value</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>A course in decision theory would lead you into a thicket of interesting related issues ecoonomics tie in</li>
</ol>
</blockquote>
<blockquote>
<p>user will give an unseen</p>
</blockquote>
<blockquote>
<p>Why is the mean-squared-error on the predicted number of stars an appro‐ priate metric for our recommendation problem? Is it meaningful? Is there a better met- ric? Hopefully, the analyst has thought this through carefully. It is surprising how often one finds that an analyst has not, and is simply reporting some measure he learned about in a class in school.</p>
</blockquote>
<blockquote>
<p>expected value computation provides a framework that is extremely useful in organizing thinking about data-analytic problems. Specifically, it decomposes data-analytic thinking into (i) the structure of the problem, (ii) the elements of the analysis that can be extracted from the data, and (iii) the elements of the analysis that need to be acquired from other sources (e.g., business knowledge of subject matter experts)</p>
</blockquote>
<blockquote>
<p>In an expected value calculation the possible outcomes of a situation are enumerated. The expected value is then the weighted average of the values of the different possible outcomes, where the weight given to each value is its probability of occurrence. For example, if the outcomes represent different possible levels of profit, an expected profit calculation weights heavily the highly likely levels of profit, while unlikely levels of profit are given little weight. For this book, we will assume that we are considering repeated tasks (like targeting a large number of consumers, or diagnosing a large number of problems) and we are interested in maximizing expected profit.1</p>
</blockquote>
<blockquote>
<p>Equation 7-1. The general form of an expected value calculation EV = p(o1)- v(o1) + p(o2)· v(o2) + p(o3)· v(o3) … Each oi its probability and v(oi ) is its value</p>
</blockquote>
<blockquote>
<p>The probabilities often can be estimated from the data (ii), but the business values often need to be acquired from other sources (</p>
</blockquote>
</div>
<div id="using-expected-value-to-frame-classifier-use" class="section level3" number="19.7.6">
<h3 number="19.7.6"><span class="header-section-number">19.7.6</span> Using Expected Value to Frame Classifier Use</h3>
<p>targeted marketing often the probability of response for any individual consumer is very low-maybe one or two percent—so no consumer may seem like a likely responder. If we choose a -common sense" threshold of 50% for deciding what a likely responder is, we would probably not target anyone.</p>
<blockquote>
<p>targeted marketing scenario</p>
</blockquote>
<blockquote>
<p>If the offer is not made to a con‐ sumer, the consumer will not buy the product. We have a model, mined from historical data, that gives an estimated probability of response pR (-) for any consumer whose feature vector description x is given as input so it’s all lift</p>
</blockquote>
<blockquote>
<p>whether to target a particular consumer</p>
</blockquote>
<blockquote>
<p>provides a framework for carrying out the analysis. Specifically, let’s calculate the expected benefit (or cost) of targeting consumer x: Expected benefit of targeting = pR (-)· vR + 1 - pR (-) · vNR where vR is the value we get from a response and vNR is the value we get from no response</p>
</blockquote>
<blockquote>
<p>To be concrete, let’s say that a consumer buys the product for $200 and our productrelated costs are $100. To target the consumer with the offer, we also incur a cost. Let’s say that we mail some flashy marketing materials, and the overall cost including postage is $1, yielding a value (profit) of vR = $99 if the consumer responds (buys the product). Now, what about vNR, the value to us if the consumer does not respond? We still mailed the marketing materials, incurring a cost of $1 or equivalently a benefit of -$1. Now we are ready to say precisely whether we want to target this consumer: do we expect to make a profit? Technically, is the expected value (profit) of targeting greater than zero concrete example</p>
</blockquote>
</div>
<div id="using-expected-value-to-frame-classifier-evaluation" class="section level3" number="19.7.7">
<h3 number="19.7.7"><span class="header-section-number">19.7.7</span> Using Expected Value to Frame Classifier Evaluation</h3>
<blockquote>
<p>With these example values, we should target the consumer as long as the estimated probability of responding is greater than 1% is this not obvious to business people?</p>
</blockquote>
<blockquote>
<p>It is likely that each model will make some decisions better than the other model. What we care about is, in aggregate, how well does each model do: what is its expected value.</p>
</blockquote>
<blockquote>
<p>Figure 7-2. A diagram of the expected value calculation.</p>
</blockquote>
<blockquote>
<p>may be that data on customers’ prior usage can be helpful in this estimation. In many cases, average estimated costs and benefits are used rather than individual-specific costs and benefits, for simplicity of problem formulation and calculation but benefits may be correlated to observables and unobservable s</p>
</blockquote>
<blockquote>
<p>Figure 7-4. A cost-benefit matrix for the targeted marketing example</p>
</blockquote>
<blockquote>
<p>Given a matrix of costs and benefits, these are multiplied cell-wise against the matrix of probabilities, then summed into a final value representing the total expected profit</p>
</blockquote>
<blockquote>
<p>A common way of expressing expected profit is to factor out the probabilities of seeing each class, often referred to as the class priors</p>
</blockquote>
<blockquote>
<p>rule of basic probability is: p(x, y) = p(y)- p(x | y)</p>
</blockquote>
<blockquote>
<p>Equation 7-2. Expected profit equation with priors p(p) and p(n) factored. Expected profit</p>
</blockquote>
<p>[Formula here]</p>
<ul>
<li>how does better information and forecasting boost payoffs?</li>
<li>better to commit or wait to learn?</li>
</ul>
<blockquote>
<p>easy mistake in formulating cost-benefit matrices is to "dou‐ ble count- by putting a benefit in one cell and a negative cost for the same thing in another cell (or vice versa).</p>
</blockquote>
<blockquote>
<p>useful practical test is to compute the benefit improvement for changing the decision on an example test instance</p>
</blockquote>
<blockquote>
<p>improvement in benefit</p>
</blockquote>
<blockquote>
<p>True positive rate and False negative rate refer to the frequency of being correct and incorrect, respectively, when the instance is actually positive: TP/(TP + FN) and FN/(TP + FN).</p>
</blockquote>
<blockquote>
<p>Precision and Recall are often used, especially in text classification and information retrieval. Recall is the same as true positive rate, while precision is TP/(TP</p>
</blockquote>
</div>
<div id="evaluation-baseline-performance-and-implications-for-investments-in-data" class="section level3" number="19.7.8">
<h3 number="19.7.8"><span class="header-section-number">19.7.8</span> Evaluation, Baseline Performance, and Implications for Investments in Data</h3>
<blockquote>
<ul>
<li>FP), which is the accuracy over the cases predicted to be positive</li>
</ul>
</blockquote>
<blockquote>
<p>The F-measure is the harmonic mean of precision and recall at a given point, and is: F-measure = 2 - precision -recall precision + recall Practitioners in many fields such as statistics, pattern recognition, and epidemiology speak of the sensitivity and specificity of a classifier: Sensitivity = TN /(TN + FP) = True negative rate = 1 - False positive rate Specificity = TP /(TP + FN ) = True positive rate You may also hear about the positive predictive value, which is the same as precision.</p>
</blockquote>
<blockquote>
<p>Accuracy, as mentioned before, is simply the count of correct decisions divided by the total number of decisions, or: Accuracy = TP + TN</p>
</blockquote>
<blockquote>
<p>what would be a reasonable baseline against which to compare model performance</p>
</blockquote>
<blockquote>
<p>For classification models it is easy to simulate a completely random model and measure its performance</p>
</blockquote>
<blockquote>
<p>There are two basic tests that any weather forecast must pass to demonstrate its merit: It must do better than what meteorologists call persistence: the assumption that the weather will be the same tomorrow (and the next day) as it was today. It must also beat climatology, the long-term historical average of conditions on a particular date in a particular area.</p>
</blockquote>
<blockquote>
<p>For classification tasks, one good baseline is the majority classifier, a naive classifier that always chooses the majority class of the training dataset</p>
</blockquote>
<blockquote>
<p>For regression problems we have a directly analogous baseline: predict the average value over the population (usually the mean or median</p>
</blockquote>
<blockquote>
<p>. If we find the one variable that correlates best with the target, we can build a classification or regression model that uses just that variable, which gives another view of baseline performance: how well does a simple -conditional" model perform?</p>
</blockquote>
<blockquote>
<p>“decision stump” — a decision tree with only one internal node, the root node</p>
</blockquote>
<blockquote>
<p>Robert Holte (1993) showed that decision stumps often produce quite good baseline performance on many of the test datasets used in machine learning research.</p>
</blockquote>
<blockquote>
<p>data as an asset to be invested in. If you are considering building models that integrate data from various sources, you should compare the result to models built from the individual sources. Often</p>
</blockquote>
<blockquote>
<p>To be thorough, for each data source the data science team should compare a model that uses the source to one that does not</p>
</blockquote>
<blockquote>
<p>Beyond comparing simple models (and reduced-data models), it is often useful to implement simple, inexpensive models based on domain knowledge or -received wisdom" and evaluate their performance. For example, in one fraud detection application it was commonly believed that most defrauded accounts would experience a sudden increase in usage, and so checking accounts for sudden jumps in volume was sufficient for catching a large proportion of fraud.</p>
</blockquote>
</div>
<div id="summary-3" class="section level3" number="19.7.9">
<h3 number="19.7.9"><span class="header-section-number">19.7.9</span> Summary</h3>
<blockquote>
<p>They were able to demonstrate that their data mining added significant value beyond this simpler strategy</p>
</blockquote>
</div>
<div id="ranking-instead-of-classifying" class="section level3" number="19.7.10">
<h3 number="19.7.10"><span class="header-section-number">19.7.10</span> Ranking Instead of Classifying</h3>
<blockquote>
<p>discussed how the score assigned by a model can be used to compute a decision for each individual case based on its expected value. A different strategy for making decisions is to rank a set of cases by these scores, and then take actions on the cases at the top of the ranked list</p>
</blockquote>
<blockquote>
<p>some reason we may not be able to obtain accurate probability estimates from the classifier. This happens, for example, in targeted marketing applications when one cannot get a sufficiently representative training sample. The classifier scores may still be very useful for deciding which prospects are better than others</p>
</blockquote>
<blockquote>
<p>A common situation is where you have a budget for actions, such as a fixed marketing budget for a campaign, and so you want to target the most promising candidates</p>
</blockquote>
<blockquote>
<p>With a ranking classifier, a classifier plus threshold produces a single confusion matrix.</p>
</blockquote>
</div>
<div id="profit-curves" class="section level3" number="19.7.11">
<h3 number="19.7.11"><span class="header-section-number">19.7.11</span> Profit Curves</h3>
<blockquote>
<p>with a ranking classifier, we can produce a list of instances and their predicted scores, ranked by decreasing score, and then measure the expected profit that would result from choosing each successive cut-point in the list</p>
</blockquote>
<blockquote>
<p>Graphing these values gives us a profit curve</p>
</blockquote>
<blockquote>
<p>Among the classifiers tested here, the one labeled Clas‐ sifier 2 produces the maximum profit of $200 by targeting the top-ranked 50% of con- sumers. If your goal was simply to maximize profit and you had unlimited resources, you should choose Classifier 2, use it to score your population of customers, and target the top half (highest 50%) of customers on the list. If you believe the validation exercise and the cost benefit estimates</p>
</blockquote>
<blockquote>
<p>Now consider a slightly different but very common situation where you’re constrained by a budget.</p>
</blockquote>
<blockquote>
<p>ROC Graphs and Curves The best-performing model at this performance point is Classifier 1. You should use it to score the entire population, then send offers to the highest-ranked 8,000 cus- tomers. justifying the use of a curve</p>
</blockquote>
</div>
</div>
<div id="ds4bs-contents" class="section level2" number="19.8">
<h2 number="19.8"><span class="header-section-number">19.8</span> Contents and consideration</h2>
<div id="introduction-data-analytic-thinking" class="section level4 unnumbered">
<h4 class="unnumbered">1. Introduction: Data-Analytic Thinking</h4>
<div class="marginnote">
<p>This introduction is extremely relevant and nontechnical</p>
</div>
<p>The Ubiquity of Data Opportunities 1 Example: Hurricane Frances 3 Example: Predicting Customer Churn 4 Data Science, Engineering, and Data-Driven Decision Making 4 Data Processing and “Big Data” 7 From Big Data 1.0 to Big Data 2.0 8 Data and Data Science Capability as a Strategic Asset 9 Data-Analytic Thinking 12 This Book 14 Data Mining and Data Science, Revisited 14 Chemistry Is Not About Test Tubes: Data Science Versus the Work of the Data Scientist 15 Summary 16</p>
</div>
<div id="business-problems-and-data-science-solutions" class="section level4 unnumbered">
<h4 class="unnumbered">2. Business Problems and Data Science Solutions</h4>
<div class="marginnote">
<p>Very useful, not too technical)</p>
</div>
<p>Fundamental concepts: A set of canonical data mining tasks; The data mining process; Supervised versus unsupervised data mining. From Business Problems to Data Mining Tasks 19 Supervised Versus Unsupervised Methods 24 Data Mining and Its Results 25 The Data Mining Process 26 Business Understanding 27 Data Understanding 28 Data Preparation 29 Modeling 31 Evaluation 31 Implications for Managing the Data Science Team 34 Other Analytics Techniques and Technologies 35 Statistics 35 Database Querying 37 Data Warehousing 38 Regression Analysis 39 Machine Learning and Data Mining 39 Answering Business Questions with These Techniques 40 Summary 41</p>
</div>
<div id="introduction-to-predictive-modeling-from-correlation-to-supervised-segmentation" class="section level4 unnumbered">
<h4 class="unnumbered">3. Introduction to Predictive Modeling: From Correlation to Supervised Segmentation</h4>
<div class="marginnote">
<p>Gets somewhat technical; maybe too much so for C-Suite?</p>
</div>
<p>Fundamental concepts: Identifying informative attributes; Segmenting data by progressive attribute selection. Exemplary techniques: Finding correlations; Attribute/variable selection; Tree induction. Models, Induction, and Prediction 44 Supervised Segmentation 48 Selecting Informative Attributes 49 Example: Attribute Selection with Information Gain 56 Supervised Segmentation with Tree-Structured Models 62 Visualizing Segmentations 67 Trees as Sets of Rules 71 Probability Estimation 71 Example: Addressing the Churn Problem with Tree Induction 73 Summary 78</p>
</div>
<div id="fitting-a-model-to-data" class="section level4 unnumbered">
<h4 class="unnumbered">4. Fitting a Model to Data</h4>
<div class="marginnote">
<p>Mostly too technica for C-Suite?</p>
</div>
<p>Fundamental concepts: Finding “optimal” model parameters based on data; Choosing the goal for data mining; Objective functions; Loss functions. Exemplary techniques: Linear regression; Logistic regression; Support-vector machines. Classification via Mathematical Functions 83 Linear Discriminant Functions 85 Optimizing an Objective Function 87 An Example of Mining a Linear Discriminant from Data 88 Linear Discriminant Functions for Scoring and Ranking Instances 90 Support Vector Machines, Briefly 91 Regression via Mathematical Functions 94 Class Probability Estimation and Logistic “Regression” 96 * Logistic Regression: Some Technical Details 99 Example: Logistic Regression versus Tree Induction 102 Nonlinear Functions, Support Vector Machines, and Neural Networks 105</p>
</div>
<div id="overfitting-and-its-avoidance" class="section level4 unnumbered">
<h4 class="unnumbered">5. Overfitting and Its Avoidance</h4>
<div class="marginnote">
<p>Should be discussed only in part; largely too technical for C-Suite?</p>
</div>
<p>Fundamental concepts: Generalization; Fitting and overfitting; Complexity control. Exemplary techniques: Cross-validation; Attribute selection; Tree pruning; Regularization. Generalization 111 Overfitting 113 Overfitting Examined 113 Holdout Data and Fitting Graphs 113 Overfitting in Tree Induction 116 Overfitting in Mathematical Functions 118 Example: Overfitting Linear Functions 119 * Example: Why Is Overfitting Bad? 124 From Holdout Evaluation to Cross-Validation 126 The Churn Dataset Revisited 129 Learning Curves 130 Overfitting Avoidance and Complexity Control 133 Avoiding Overfitting with Tree Induction 133 A General Method for Avoiding Overfitting 134 * Avoiding Overfitting for Parameter Optimization 136 Summary 140</p>
</div>
<div id="similarity-neighbors-and-clusters" class="section level4 unnumbered">
<h4 class="unnumbered">6. Similarity, Neighbors, and Clusters</h4>
<div class="marginnote">
<p>Should be discussed only briefly; largely too technical for C-Suite?</p>
</div>
<p>Fundamental concepts: Calculating similarity of objects described by data; Using similarity for prediction; Clustering as similarity-based segmentation. Exemplary techniques: Searching for similar entities; Nearest neighbor methods; Clustering methods; Distance metrics for calculating similarity. Similarity and Distance 142 Nearest-Neighbor Reasoning 144 Example: Whiskey Analytics 144 Nearest Neighbors for Predictive Modeling 146 How Many Neighbors and How Much Influence? 149 Geometric Interpretation, Overfitting, and Complexity Control 151 Issues with Nearest-Neighbor Methods 154 Some Important Technical Details Relating to Similarities and Neighbors 157 Heterogeneous Attributes 157 * Other Distance Functions 158 * Combining Functions: Calculating Scores from Neighbors 161 Clustering 163 Example: Whiskey Analytics Revisited 163 Hierarchical Clustering 164 Example: Clustering Business News Stories 174 Understanding the Results of Clustering 177 * Using Supervised Learning to Generate Cluster Descriptions 179 Stepping Back: Solving a Business Problem Versus Data Exploration 182 Summary 184</p>
<p><br />
OF: <strong>What is clustering and why is it useful?</strong> Clustering analysis is a form of unsupervised learning which focuses around splitting data into groups. The aim of this is to identify key patterns in our data. For example: The supermarket Tesco may wish to identify which products are bought together by consumers. Using clustering may reveal that 80% customers who buy bread also buy milk. Insights such as these could be used in order to alter the layout of a supermarket in order to encourage higher spending.<br />
Whilst insights such as the one given in the example above are very simple, in larger datasets it can be much more difficult to draw insights such as these. Provided that the correct method is used, clustering is an incredibly powerful tool for exploring big datasets.<br />
Clustering uses distance measures in order to group similar observations into clusters.</p>
<p><strong>K-Means</strong>:
- Simple and easy to implement
- Efficient
- Difficult to know the value of K to specify
- Sensitive to the initial points chosen</p>
<p><strong>DBSCAN</strong>
- Very good at dealing with noise
- Can handle different shaped clusters
- Tends to perform worse when the density of data varies or when data is highly dimensional</p>
</div>
<div id="decision-analytic-thinking-i-what-is-a-good-model" class="section level4 unnumbered">
<h4 class="unnumbered">7. Decision Analytic Thinking I: What Is a Good Model?</h4>
<div class="marginnote">
<p>Very relevant for C-suite, if they don’t already know these concepts</p>
</div>
<p>Fundamental concepts: Careful consideration of what is desired from data science results; Expected value as a key evaluation framework; Consideration of appropriate comparative baselines. Exemplary techniques: Various evaluation metrics; Estimating costs and benefits; Calculating expected profit; Creating baseline methods for comparison. Evaluating Classifiers 188 Plain Accuracy and Its Problems 189 The Confusion Matrix 189 Problems with Unbalanced Classes 190 Problems with Unequal Costs and Benefits 193 Generalizing Beyond Classification 193 A Key Analytical Framework: Expected Value 194 Using Expected Value to Frame Classifier Use 195 Using Expected Value to Frame Classifier Evaluation 196 Evaluation, Baseline Performance, and Implications for Investments in Data 204 Summary 207</p>
<p><br />
</p>
</div>
<div id="visualizing-model-performance" class="section level4 unnumbered">
<h4 class="unnumbered">8. Visualizing Model Performance</h4>
<p>Fundamental concepts: Visualization of model performance under various kinds of uncertainty; Further consideration of what is desired from data mining results. Exemplary techniques: Profit curves; Cumulative response curves; Lift curves; ROC curves. Ranking Instead of Classifying 209 Profit Curves 212 ROC Graphs and Curves 214 The Area Under the ROC Curve (AUC) 219 Cumulative Response and Lift Curves 219 Example: Performance Analytics for Churn Modeling 223 Summary 231</p>
</div>
<div id="evidence-and-probabilities" class="section level4 unnumbered">
<h4 class="unnumbered">9. Evidence and Probabilities</h4>
<p>Fundamental concepts: Explicit evidence combination with Bayes’ Rule; Probabilistic reasoning via assumptions of conditional independence. Exemplary techniques: Naive Bayes classification; Evidence lift. Combining Evidence Probabilistically 235 Joint Probability and Independence 236 Bayes’ Rule 237 Applying Bayes’ Rule to Data Science 239 Conditional Independence and Naive Bayes 240 Advantages and Disadvantages of Naive Bayes 242 A Model of Evidence “Lift” 244 Example: Evidence Lifts from Facebook “Likes” 245 Evidence in Action: Targeting Consumers with Ads 247 Summary 247</p>
</div>
<div id="representing-and-mining-text" class="section level4 unnumbered">
<h4 class="unnumbered">10. Representing and Mining Text</h4>
<div class="marginnote">
<p>Probably less relevant.</p>
</div>
<p>Fundamental concepts: The importance of constructing mining-friendly data representations; Representation of text for data mining. Exemplary techniques: Bag of words representation; TFIDF calculation; N-grams; Stemming; Named entity extraction; Topic models. Why Text Is Important 250 Why Text Is Difficult 250 Representation 251 Bag of Words 252 Term Frequency 252 Measuring Sparseness: Inverse Document Frequency 254 Combining Them: TFIDF 256 Example: Jazz Musicians 256 * The Relationship of IDF to Entropy 261 Beyond Bag of Words 263 N-gram Sequences 263 Named Entity Extraction 264 Topic Models 264 Example: Mining News Stories to Predict Stock Price Movement 266 The Task 266 The Data 268 Data Preprocessing 270 Results 271 Summary 275</p>
</div>
<div id="decision-analytic-thinking-ii-toward-analytical-engineering" class="section level4 unnumbered">
<h4 class="unnumbered">11. Decision Analytic Thinking II: Toward Analytical Engineering</h4>
<div class="marginnote">
<p>Seems very relevant and not technical.</p>
</div>
<p>Fundamental concept: Solving business problems with data science starts with analytical engineering: designing an analytical solution, based on the data, tools, and techniques available. Exemplary technique: Expected value as a framework for data science solution design. The Expected Value Framework: Decomposing the Business Problem and Recomposing the Solution Pieces 278 A Brief Digression on Selection Bias 280 Our Churn Example Revisited with Even More Sophistication 281 The Expected Value Framework: Structuring a More Complicated Business Problem 281 Assessing the Influence of the Incentive 283 From an Expected Value Decomposition to a Data Science Solution 284 Summary 287</p>
</div>
<div id="other-data-science-tasks-and-techniques-." class="section level4 unnumbered">
<h4 class="unnumbered">12. Other Data Science Tasks and Techniques .</h4>
<div class="marginnote">
<p>Mostly relevant and not too technical.</p>
</div>
<p>Fundamental concepts: Our fundamental concepts as the basis of many common data science techniques; The importance of familiarity with the building blocks of data science. Exemplary techniques: Association and co-occurrences; Behavior profiling; Link prediction; Data reduction; Latent information mining; Movie recommendation; Biasvariance decomposition of error; Ensembles of models; Causal reasoning from data. Co-occurrences and Associations: Finding Items That Go Together 290 Measuring Surprise: Lift and Leverage 291 Example: Beer and Lottery Tickets 292 Associations Among Facebook Likes 293 Profiling: Finding Typical Behavior 296 Link Prediction and Social Recommendation 301 Data Reduction, Latent Information, and Movie Recommendation 302 Bias, Variance, and Ensemble Methods 306 Data-Driven Causal Explanation and a Viral Marketing Example 309 Summary 310</p>
</div>
<div id="data-science-and-business-strategy" class="section level4 unnumbered">
<h4 class="unnumbered">13. Data Science and Business Strategy</h4>
<div class="marginnote">
<p>Seems very relevant and not technical.</p>
</div>
<p>Fundamental concepts: Our principles as the basis of success for a data-driven business; Acquiring and sustaining competitive advantage via data science; The importance of careful curation of data science capability. Thinking Data-Analytically, Redux 313 Achieving Competitive Advantage with Data Science 315 Sustaining Competitive Advantage with Data Science 316 Formidable Historical Advantage 317 Unique Intellectual Property 317 Unique Intangible Collateral Assets 318 Superior Data Scientists 318 Superior Data Science Management 320 Attracting and Nurturing Data Scientists and Their Teams 321 Be Ready to Accept Creative Ideas from Any Source 324 Be Ready to Evaluate Proposals for Data Science Projects 324 Example Data Mining Proposal 325 Flaws in the Big Red Proposal 326 A Firm’s Data Science Maturity 327</p>
</div>
<div id="conclusion" class="section level4 unnumbered">
<h4 class="unnumbered">14. Conclusion</h4>
<div class="marginnote">
<p>Seems very relevant and not technical. Some references to tech tools may be outdated, however.</p>
</div>
<p>The Fundamental Concepts of Data Science 331 Applying Our Fundamental Concepts to a New Problem: Mining Mobile Device Data 334 Changing the Way We Think about Solutions to Business Problems 337 What Data Can’t Do: Humans in the Loop, Revisited 338 Privacy, Ethics, and Mining Data About Individuals 341 Is There More to Data Science? 342 Final Example: From Crowd-Sourcing to Cloud-Sourcing 343 Final Words 344</p>
</div>
<div id="proposal-review-guide" class="section level4 unnumbered">
<h4 class="unnumbered">Proposal Review Guide</h4>
</div>
<div id="another-sample-proposal" class="section level4 unnumbered">
<h4 class="unnumbered">Another Sample Proposal</h4>
<p>Glossary 355</p>
<!--chapter:end:data_sci/ds_for_bsns_notes_remote.Rmd-->
</div>
</div>
</div>
<div id="paleo-example" class="section level1 unnumbered">
<h1 class="unnumbered">Meta-analysis arbitrary example: the ‘Paleo diet’</h1>
<p>Introduction and discussion</p>
<blockquote>
<p>Summarize the evidence base for the effectiveness of the Paleo diet in reducing obesity (as measured by waist circumference). Please cover: a) the basics of the intervention being studied; b) the research methodology; c) the results.</p>
</blockquote>
<blockquote>
<p>What is your conclusion on how effective the Paleo diet is at reducing waist circumference? What do you regard as the key strengths and limitations of the evidence and why?</p>
</blockquote>
<blockquote>
<p>Briefly, what are the major uncertainties in your analysis? How could your conclusion be wrong? This will likely involve saying what you explicitly chose not to do and what you do not know.</p>
</blockquote>
<p>There is some evidence supporting the claim that the ‘Paleo diet’ reduces obesity as measured by waist circumference, at least for certain targeted groups relative to certain ’standard recommended diets. At least a small set of randomized trials have concluded that participants in the “Paleo diet treatment” groups reduced their waist circumference more than those in control groups.</p>
<p>This evidence is summarized in the meta-analysis of <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span> who report:</p>
<blockquote>
<p>The Paleolithic diet resulted in greater short-term improvements in metabolic syndrome components than did guideline-based control diets. The available data warrant additional evaluations of the health benefits of Paleolithic nutrition.</p>
</blockquote>
<p>More recent meta-analyses in reputable journals have also found favorable results (<span class="citation"><a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">Ghaedi et al.</a> (<a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">2019</a>)</span>, <span class="citation"><a href="#ref-demenezesInfluencePaleolithicDiet2019a" role="doc-biblioref">de Menezes et al.</a> (<a href="#ref-demenezesInfluencePaleolithicDiet2019a" role="doc-biblioref">2019</a>)</span>).</p>
<div class="marginnote">
<p>According to <a href="https://www.scimagojr.com/journalrank.php?category=2916">Scimago</a>, American Journal of Clinical Nutrition (<span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>) ranked second among Nutrition and Dietetics journals, Advances in Nutrition (<span class="citation"><a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">Ghaedi et al.</a> (<a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">2019</a>)</span>) is ranked fourth, and Nutrition Journal (<span class="citation"><a href="#ref-demenezesInfluencePaleolithicDiet2019a" role="doc-biblioref">de Menezes et al.</a> (<a href="#ref-demenezesInfluencePaleolithicDiet2019a" role="doc-biblioref">2019</a>)</span>) is ranked 35th.</p>
</div>
<p>However, the Paleo diet remains controversial; it is still referred to as a ‘fad diet’ on <a href="https://en.wikipedia.org/wiki/Paleolithic_diet">Wikipedia</a> (accessed 20 Dec 2020), which claims ‘there is no good evidence that following a paleolithic diet lessens the risk of cardiovascular disease or metabolic syndrome.’*</p>
<div class="marginnote">
<p>* However, this Wikipedia article references <span class="citation">(<a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">Ghaedi et al. 2019</a>; <a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al. 2015</a>)</span>; both of these meta-analyses would more accurately be characterized as stating that there is some evidence but the results are not yet definitive.</p>
</div>
<p><span class="citation">(<a href="#ref-fentonPaleoDietStill2016" role="doc-biblioref">Fenton and Fenton 2016</a>)</span> criticized the <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span> meta-analysis, arguing it overstated the findings and citing limitations of the included original studies. ***</p>
<div class="marginnote">
<p>*** While I agree with some of the conceptual issues <span class="citation">(<a href="#ref-fentonPaleoDietStill2016" role="doc-biblioref">Fenton and Fenton 2016</a>)</span> raise, I find their criticism of the statistics a bit imprecise as well as arbitrary in citing statistical norms without justification. I return to these issues below.</p>
</div>
<p><br />
</p>
<p>In this brief review I:</p>
<ul>
<li><p><a href="#conceptual">Consider conceptual issues in defining and measuring the ‘impact of a diet’ through standard study methodologies</a></p></li>
<li><p><span class="citation">(Focus (as suggested) on the meta-analysis of <a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al. 2015</a> considering its methods, findings, and limitations)</span>(#limitations-p),</p>
<ul>
<li>[considering key conceptual and statistical issuess]((#man-results),</li>
<li>considering the <span class="citation">(critiques of <a href="#ref-fentonPaleoDietStill2016" role="doc-biblioref">Fenton and Fenton 2016</a> and Manheimer’s responses)</span>(#critiques), as well as other evaluations of this meta-analysis,</li>
<li>weighing this evidence in light of a (shallow survey of) the consensus and other meta-analyses.</li>
</ul></li>
<li><p><a href="#boers">Assess a particularly promising study</a> <span class="citation">(<a href="#ref-boersFavourableEffectsConsuming2014" role="doc-biblioref">Boers et al. 2014</a>)</span> (incorporated into <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>)</p></li>
</ul>
<p><em>Disclaimers:</em> (See <a href="#limitations-p">‘limitations’</a> below).</p>
<div id="conceptual" class="section level2" number="19.9">
<h2 number="19.9"><span class="header-section-number">19.9</span> Conceptual: Thoughts on nutritional studies and meta-analysis issues</h2>
<div id="compliance" class="section level3" number="19.9.1">
<h3 number="19.9.1"><span class="header-section-number">19.9.1</span> Limited compliance; ‘what are we aiming to measure and why?’</h3>
Randomized controlled trials on nutrition and diet have important limitations not faced by many other medical RCTs, such as drug trials.
<div class="marginnote">
<p>Another important limitation: “RCTs of dietary interventions
cannot be controlled with true placebos, but rather with certain constraints on nutrient compositions, food groups, or dietary patterns” <span class="citation"><a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">Schwingshackl et al.</a> (<a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">2016</a>)</span>. They also cite
“lack of double blinding… crossover bias, and high dropout rates”; although dropout (attrition) is not an exclusive problem for dietary studies.</p>
</div>
<p><em>Limited compliance</em> is a particularly important issue: not all participants who are assigned to a particular diet will carefully follow this diet. There are ways of <em>tracking</em> compliance (e.g. “Diet-associated biomarkers” such as 24-hour urine checks, which are highly-rated by <span class="citation"><a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">Schwingshackl et al.</a> (<a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">2016</a>)</span>’s Nutrigrade). There are also ways of encouraging and incentivizing compliance, such as providing free food (as in <span class="citation"><a href="#ref-boersFavourableEffectsConsuming2014" role="doc-biblioref">Boers et al.</a> (<a href="#ref-boersFavourableEffectsConsuming2014" role="doc-biblioref">2014</a>)</span>).</p>
<p>However, our consideration of this issue depends on what exactly itis we want to measure, and for what purpose. For example may want to consider either, what I will call… *</p>
<div class="marginnote">
<p>* Note that my terminology below comes from the “treatment effect” literature in statistics and econometrics, with some slight abuses of terminology. See, e.g., <span class="citation"><a href="#ref-angristIdentificationEstimationLocal1995" role="doc-biblioref">Joshua D. Angrist and Imbens</a> (<a href="#ref-angristIdentificationEstimationLocal1995" role="doc-biblioref">1995</a>)</span>.</p>
</div>
<ol style="list-style-type: decimal">
<li>“Average Treatment Effect (ATE)”: The difference in some medical outcome, averaged over the population of interest… for the ‘Treatment diet’ if consumed exactly as proposed relative to a ‘Control diet.’**</li>
</ol>
<div class="marginnote">
<p>** Even here, there may be some ambiguities that need to be clarified. Should these diets specify exact amounts/calories consumed (‘isocaloric’), or only the types of foods, allowingconsumption as desired? Do we need to ensure that other patterns, such as exercise, are held constant across diets? (One might imagine that with random treatment exercise patterns will be identical on average, however, some diet may give people more energy to do exercise. We need to decide whether this is part of the effect we wish to consider.)</p>
</div>
<p><em>or the</em></p>
<ol start="2" style="list-style-type: decimal">
<li>“Intention to Treat (ITT)” effect: The difference in some medical outcome, averaged over the population of interest… for those <em>assigned</em> to the treatment diet relative to the control diet.</li>
</ol>
<p>The ATE measure would seem to be more relevant:</p>
<ul>
<li>if we want to get at basic biological mechanisms or</li>
<li>if we believe we should recommed a ‘best diet’ without adjusting for compliance. *
<div class="marginnote">
<p>E.g., perhaps we are making recommendations for a context where compliance is not particularly difficult (e.g. perhaps school or military meals). It may also be reasonable to assume that people reading our analysis (e.g., dieters), will already make these self-control considerations themselves, and simply wish to know which diet would ve best, if they could live up to it.</p>
</div></li>
</ul>
<p>On the other hand the ITT measure might be more relevant if we want to consider which diet to propose “for real people to achieve their best outcomes.” However:</p>
<ul>
<li><p>while the impact of the diet itself (the ATE) might be expected to be fairly uniform, as the basic biological mechanisms will be similar for similar groups of people,</p></li>
<li><p>I would expect that the ITT measure to have more heterogeneity and variation, as compliance with a particular diet might be expected to vary greatly for a variety of psychological and cultural/lifestyle reasons.</p></li>
</ul>
<p>Thus the ITT may be more sensitive to the representativeness of our sample of dieters. Given self selection and other issues, it may be extremely difficult to recruit a “nationally representative” sample of the targeted group.</p>
<p>In the discussion of <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>, it is not clear to me which measure of the studies are targeting. The discussion (“whether Paleolithic <em>nutrition</em> has any effect in improving metabolic risk factors,” emphasis added) seems to suggest that they are considering the ATE. However most of the designs seem to involve simply different <em>recommendations</em> for treatment and control groups; these would seem much more likely to pick up the ITT for their study sample.</p>
</div>
<div id="control-group-what-is-being-measured" class="section level3" number="19.9.2">
<h3 number="19.9.2"><span class="header-section-number">19.9.2</span> Control group: what is being measured?</h3>
<p>(Time-permitting, I would more carefully consider which ‘control group’ diets are being administered or recommended.)</p>
<p><br />
</p>
</div>
<div id="what-is-being-tested-and-how-broadly-should-we-interpret-the-results" class="section level3" number="19.9.3">
<h3 number="19.9.3"><span class="header-section-number">19.9.3</span> What is being tested and how broadly should we interpret the results?</h3>
<p>Broadly, the Paleo diet involves:</p>
<ol style="list-style-type: decimal">
<li><p>Multiple differences relative to more traditional diets</p></li>
<li><p>A series of connected claims about the <em>mechanism</em> by which it achieves benefits, and about nutrition and health in general</p></li>
</ol>
<p><br />
</p>
<p>It is important to note that even if there is clear evidence that (recommending or implementing) the ‘Paleo diet’ causes positive health benefits relative to an older diet:</p>
<ol style="list-style-type: decimal">
<li><p>There is some inconsistency and much heterogeneity in the definition of the Paleo diet, as noted by <span class="citation"><a href="#ref-zazpeScopingReviewPaleolithic2020" role="doc-biblioref">Zazpe et al.</a> (<a href="#ref-zazpeScopingReviewPaleolithic2020" role="doc-biblioref">2020</a>)</span>, who propose a particular ‘PaleoDiet’ score</p></li>
<li><p>We don’t know <em>which</em> of the components of the Paleo diet were beneficial (some may even have been harmful, but outweighed by others which were even more strongly beneficial.</p></li>
<li><p>We don’t have evidence that narrowly supports the claims made by Paleo advocates. Even if elements of the Paleo diet are beneficial, it may be through other mechanisms and channels than those proposed.</p></li>
</ol>
</div>
</div>
<div id="manheimer" class="section level2" number="19.10">
<h2 number="19.10"><span class="header-section-number">19.10</span> Manheimer et al</h2>
<div id="strengths-and-limitations" class="section level3" number="19.10.1">
<h3 number="19.10.1"><span class="header-section-number">19.10.1</span> Strengths and limitations</h3>
The authors follow standard protocols and recommendations. However, these systems themselves may merit some further examination.*
<div class="marginnote">
<p>* E.g., I would like to better understand how the GRADE system used to asses the quality of evidence has <em>determined</em> the weights for different categories of ‘quality’ chosen. <span class="citation"><a href="#ref-Quintana2015" role="doc-biblioref">Quintana</a> (<a href="#ref-Quintana2015" role="doc-biblioref">2015</a>)</span> note “there are more than 80 tools available to assess the quality and risk of bias in observational studies”… Perhaps there are a similar number of tools for RCT studies.</p>
</div>
<p>They have:</p>
<ol style="list-style-type: decimal">
<li>Prospectively (in advance) [registered their protocol with PROSPERO](<a href="http://www.crd.york.ac.uk/PROSPERO\*\" class="uri">http://www.crd.york.ac.uk/PROSPERO\*\</a>*
<div class="marginnote">
<p>**
This reduces the risk of “hypothesizing after the results are known” … ‘HARKing’ kerrHARKingHypothesizingResults1998a", as noted by <span class="citation"><a href="#ref-Quintana2015" role="doc-biblioref">Quintana</a> (<a href="#ref-Quintana2015" role="doc-biblioref">2015</a>)</span>.
However it is difficult to verify when a meta-analysis was actually started, even privately. Here pre-registration partially depends on an honor and trust system.</p>
</div></li>
<li>Searched a broad collection of appropriate databases:</li>
</ol>
<ul>
<li>both literature databases such as PubMed</li>
<li>and databases of clinical trials</li>
</ul>
<p>They also claim to have contacted “experts in the field” (these should have been named) to consider whether any trials had been overlooked. This seems to have been a well-planned and adequate search, not neglecting the “gray literature” (<span class="citation"><a href="#ref-paezGrayLiteratureImportant2017" role="doc-biblioref">Paez</a> (<a href="#ref-paezGrayLiteratureImportant2017" role="doc-biblioref">2017</a>)</span>) and limiting “file drawer” bias (<span class="citation"><a href="#ref-kennedyOldFiledrawerProblem2004" role="doc-biblioref">D. Kennedy</a> (<a href="#ref-kennedyOldFiledrawerProblem2004" role="doc-biblioref">2004</a>)</span>). They carefully explained their selection method, and illustrated it in a flow diagram.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Broadly assessed the quality of the evidence following the GRADE protocol, finding the overall evidence to be of ‘moderate’ quality (for waist circumference).</p></li>
<li><p>Analyzed the overall evidence using a frequentist Random-Effects model, estimating both a mean effect and a heterogeneity term. I find this approach to be reasonably appropriate, but I would prefer a <em>Bayesian</em> meta-analysis.</p></li>
<li><p>Plotting the evidence for each outcome with the standard recommended tree plots, clearly depicting the (frequentist) confidence intervals for each study and overall.</p></li>
</ol>
<p><strong>Presentation, transparency and characterization</strong>: My general impression is that they have clearly and cleanly stated their approach and methods, and reasonably characterized their findings, without overstatement.* E.g., they write:</p>
<blockquote>
<p>Although there is moderate quality evidence from randomized controlled intervention studies to suggest that the Paleolithic diet can improve metabolic syndrome components, we believe that more studies are required before Paleolithic nutrition can be recommended in future guidelines. (p. 928).</p>
</blockquote>
<div class="marginnote">
* However, I would have liked to see their code and data made available, or at least more prominently noted (I may have overlooked it).
</div>
<div id="limitations" class="section level4" number="19.10.1.1">
<h4 number="19.10.1.1"><span class="header-section-number">19.10.1.1</span> Limitations</h4>
<ol style="list-style-type: decimal">
<li>Small number of studies, small sample, wide confidence intervals</li>
</ol>
<blockquote>
<p>Four RCTs that involved 159 participants were included. The 4 control diets were based on distinct national nutrition guidelines but were broadly similar.</p>
</blockquote>
<p>As the authors acknowledge, this is a small number of studies and the sample as a whole is rather small. This leads to fairly wide confidence intervals by most measures.</p>
<p>As noted below, the evidence does not allow us to rule out heterogeneity across studies. With only a few studies presented, we do not have good sense of how robust these results would be to considering other populations, control diet comparisons, etc.</p>
<p><br />
</p>
<p><strong>Note</strong>: This suggests the limitations of the evidence, not a weakness of the authors’ analysis.</p>

<div class="fold">
<p>To me their inclusion criteria seem reasonable. I agree, e.g., with their decision that:</p>
<blockquote>
<p>Crossover RCTs were eligible for inclusion, but we used data from only the first phase before the crossover occurred because we considered the risk of carryover effects to be high</p>
</blockquote>
<p>However, particularly in line with the points made in <span class="citation"><a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">Schwingshackl et al.</a> (<a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">2016</a>)</span>, I would like to see a meta-analysis considering not only RCTs but also other reasonable approaches, such as cohort studies.</p>
</div>
<p><br />
</p>
<ol start="2" style="list-style-type: decimal">
<li>Possible signs of authors’ pre-judgement and funding pressure</li>
</ol>
<ul>
<li>The study was ‘Supported in part by the National Center for Complementary and Alternative Medicine of the US NIH (grant R24 AT001293; EWM),’ a group that I might suspect to be somewhat sympathetic to the anti-establishment linked ‘back to nature’ philosophy of the Paleo diet.</li>
</ul>
</div>
</div>
<div id="overall-results-interpretation-consideration-of-evidence-presented-in-manheimerpaleolithicnutritionmetabolic2015" class="section level3" number="19.10.2">
<h3 number="19.10.2"><span class="header-section-number">19.10.2</span> Overall results, interpretation, consideration of evidence presented in <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span></h3>
<p>I reproduce the most relevant part of <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>’s results below:</p>
<div class="figure" style="text-align: center">
<img src="images/waist_manheim.png" alt="Forest plot from Manheimer et al: Waist circumference" width="90%" />
<p class="caption">
(#fig:unnamed-chunk-98)Forest plot from Manheimer et al: Waist circumference
</p>
</div>
<p>In the individual studies, the Paleo diet participants reduced their waist circumference (‘WC’) by an average of 3-11cm, with some differences across studies. For control diet participants the reduction was between 3 and 6 cm on average. The ‘difference in loss’ ranged from</p>
<ul>
<li>a virtual zero with a 95% confidence interval of [-4cm, +4cm] in Boers et al (the study where food was provided)</li>
<li>to a 5.5 cm greater loss for Paleo diets (in Mellberg et al).</li>
</ul>
<p>The reported meta-analytic estimate is an average of 2.38 cm greater loss in Paleo relative to the control diet, with a 95% confidence interval [-4.73, -0.04], thus a ‘just significant’ difference at the standard NHST threshold of <span class="math inline">\(p&lt;0.05\)</span>.</p>
<p><br />
</p>
<p><em>Does this imply a large relative effect of Paleo?</em> <em>Does it bound the effect as ‘small at best’</em>?</p>
To consider the magnitude of an effect such as this, it is helpful to consider overall averages and spreads.
The average US male WC is roughly 102 cm (<span class="citation"><a href="#ref-fryarMeanBodyWeight2018" role="doc-biblioref">Fryar et al.</a> (<a href="#ref-fryarMeanBodyWeight2018" role="doc-biblioref">2018</a>)</span>) and 98 cm for women…*
<div class="marginnote">
<p>* I could not quickly find a reputable source for averages in Sweden and Netherlands, where most of the studies were conducted, but <a href="https://www.dailymail.co.uk/health/article-2441250/Average-BMI-Artist-compares-sizes-men-various-countries.html">secondhand reports</a> suggest it is somewhat smaller, perhaps 90 cm.</p>
</div>
… which (for men) is also approximately the <a href="https://www.heart.org/en/health-topics/metabolic-syndrome/about-metabolic-syndrome">threshold circumference considered a risk factor for metabolic syndrome</a> (accessed 29 Dec 2020). I quickly, (and perhaps incorrectly) imputed a standard-deviation of roughly 52 cm.**
<div class="marginnote">
<p>** Drawing from <span class="citation"><a href="#ref-fryarMeanBodyWeight2018" role="doc-biblioref">Fryar et al.</a> (<a href="#ref-fryarMeanBodyWeight2018" role="doc-biblioref">2018</a>)</span> statistics, I multiplied the square root of the reported sample size (<span class="math inline">\(\sqrt(43169 )\)</span>) by the reported standard error of 0.1 (for the table in inches, the figure in the centimeter table didn’t make sense), and converted back to cm. This calculation may be incorrect; I would like to check it, or better still, find some scientific estimates of dispersion measures for this (SD, quantile, IQR, etc.) I would also like to read more about what WC losses are considered normal, reasonable, and substantial in this context.</p>
</div>
<p><br />
</p>
<p><strong>Heterogeneity</strong></p>
The evidence also suggest between-study heterogeneity; at least we do not have strong evidence that would allow us to rule this out. Prima facie, we see strong differences between the studies. In particular, the one study with a strongly different methodology (Boers et al), finds a much smaller (near-zero effect). In their formal measures, we see an <span class="math inline">\(I^2\)</span> of 52%, conventionally classified as “moderate heterogeneity.” I believe <span class="math inline">\(\tau^2 of 2.81\)</span> can be interpreted as a ‘standard deviation in true effect size between studies’ of <span class="math inline">\(\tau =\)</span> 1.68, which seems, again, ‘moderate.’***
<div class="marginnote">
<p>*** I suspect that if we allowed a variety of other methods of estimating and reporting this (including Bayesian) and we reported the confidence/credible intervals on this between-study variance, the results might suggest the possibility of more substantial heterogeneity.</p>
</div>
Although the NHST for heterogeneity is not significant by standard measures (<span class="math inline">\(P=0.10\)</span>), a simple ‘failure to reject in a diagnostic test’ does not constitute substantial reason to ignore a potentially-important concern.*
<div class="marginnote">
<p>A ‘failure to reject the null’ is not in itself, strong evidence for the null hypothesis. Among other things, I suspect a lack of power to detect heterogeneity with only four studies.</p>
</div>
</div>
<div id="my-rough-conclusions-from-manheimer-et-al" class="section level3" number="19.10.3">
<h3 number="19.10.3"><span class="header-section-number">19.10.3</span> My rough conclusions from Manheimer et al</h3>
<ol style="list-style-type: decimal">
<li><p>The above evidence suggests some relative benefit of the Paleo diet, but not an extremely large benefit. To the extent</p></li>
<li><p>These (3) studies that find greater benefits are ones where compliance is particularly important, while the one study where food is provided (isocaloric for both diets) Does not seem to find a benefit. This casually suggests that ‘easier compliane’ may be one of the major benefits of the Paleo diet relative to a traditional diet, but obviously, more evidence would be needed to support this conclusion.</p></li>
</ol>
</div>
<div id="critiques" class="section level3" number="19.10.4">
<h3 number="19.10.4"><span class="header-section-number">19.10.4</span> External critiques and evaluations of Manheimer et al, (esp Fenton) authors’ response</h3>
<!--
### Ghaedi et al meta-analysis
-->
<span class="citation">(<a href="#ref-fentonPaleoDietStill2016" role="doc-biblioref">Fenton and Fenton 2016</a>)</span>’s critique of <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>’s focal statistical comparisons appears to result from a mis-reading.*
<div class="marginnote">
<p>* “We did not report separate tests against baseline within groups as Fenton and Fenton suggest but rather a comparison of the differences in change from baseline between groups.” <span class="citation">(<a href="#ref-manheimerReplyTRFenton2016" role="doc-biblioref">Manheimer 2016</a>)</span></p>
</div>
<span class="citation"><a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">Schwingshackl et al.</a> (<a href="#ref-schwingshacklPerspectiveNutriGradeScoring2016" role="doc-biblioref">2016</a>)</span> develops the ‘Nutrigrade’ scoring system for the strength of the evidence presented in a meta-analysis, specifically for nutrition research.*
<div class="marginnote">
<p>* Note that these scores are not a verdict on the quality of the meta-analysis itself but of the evidence strength presnted within.</p>
</div>
<p>Their system and relative points system was independently developed; a major point of contrast with GRADE (<span class="citation"><a href="#ref-guyattGRADEEmergingConsensus2008" role="doc-biblioref">Guyatt et al.</a> (<a href="#ref-guyattGRADEEmergingConsensus2008" role="doc-biblioref">2008</a>)</span>) seems to be Nutrigrade’s greater valuation of cohort studies relative to RCTs, reflecting some of the rationales mentioned above, such as <a href="#compliance">limited compliance</a>. They ‘Nutrigrade’ the evidence in <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span> as 5.75, or ‘Low,’ explained as “there is low confidence in the effect estimate; further research will provide important evidence on the confidence and likely change the effect estimate.” Of 10 randomly meta-analysis studies they rated, this was the fifth-highest ‘Nutrigrade.’</p>
<p>Overall, the ‘reviews’ of the strength of evidence meta-analysis are fair to middling. All, including the original authors, seem to agree that the “the current evidence on the paleo diet might be considered early or preliminary and that further research is critically needed” (<span class="citation"><a href="#ref-manheimerReplyTRFenton2016" role="doc-biblioref">Manheimer</a> (<a href="#ref-manheimerReplyTRFenton2016" role="doc-biblioref">2016</a>)</span>), although there seems to be some disagreement about how promising an avenue this is for further inquiry.</p>
</div>
</div>
<div id="other-meta-analyses-and-consideration-of-the-paleo-diet" class="section level2" number="19.11">
<h2 number="19.11"><span class="header-section-number">19.11</span> Other meta-analyses and consideration of the Paleo diet</h2>
<p>As noted, the Paleo diet is controversial and not widely accepted in the medical profession. As <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span> note</p>
<blockquote>
<p>a 2015 US News and World Report ranking of 35 diets with input from a panel of health experts placed the Paleolithic diet dead last, citing a lack of research evidence that showed clinical benefits.</p>
</blockquote>
<p>However, more work also seems to support a claim such as ‘recommending the Paleo diet is succesful in reducing waist circumference in the short-term, relative to recommending more traditional diets.’</p>
<p>In particular, <span class="citation"><a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">Ghaedi et al.</a> (<a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">2019</a>)</span> perform a meta-analysis of eight RCT studies (the four considered by Mannheimier et al, plus four more that were conducted since 2015) and find “a [Paleo diet] significantly reduced … waist circumference (WMD = -2.90 cm; 95% CI: -4.51, -1.28 cm).” These results are rather similar to those of <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>. Time-permitting, I would like to examine this meta-analyis in more detail.</p>
<div id="process-of-finding-relevant-work-informal" class="section level3" number="19.11.1">
<h3 number="19.11.1"><span class="header-section-number">19.11.1</span> Process of finding relevant work (informal)</h3>
<p><em>Notes</em>: In the limited time I had available, I did some keyword searches on Google scholar, such as <code>'paleo diet' AND 'meta-analysis</code>‘<code>and</code>paleo diet<code>AND</code>randomized controlled trial`. I also followed a ’snowball’ approach to consider which studies were cited, and which studies cited <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>, and the papers most prominently cited within. I did not do the systematically nor did I keep track of my process (as I would do if I were doing this more seriously, e.g., for my own meta-analysis).</p>
</div>
</div>
<div id="boers" class="section level2" number="19.12">
<h2 number="19.12"><span class="header-section-number">19.12</span> Focus: Boers et al</h2>
<p>Unlike the other trials considered in <span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>, <span class="citation"><a href="#ref-boersFavourableEffectsConsuming2014" role="doc-biblioref">Boers et al.</a> (<a href="#ref-boersFavourableEffectsConsuming2014" role="doc-biblioref">2014</a>)</span> “provided home-delivered Paleolithic and control diet meals to participants.” This permits a more direct examination of what I called the <a href="#compliance">ATE</a>. I expect this effect to be less variable and more generalizable across populations, because issues of differential compliance will be reduced.</p>
This study is also particular strong because it has (apparently) a zero attrition rate (‘34/34 analyzed’), while attrition in the other cited studies is substantial. Particular forms of non-random attrition can strongly bias estimated treatment effects, biasing both the ATE and ITT mentioned above.*
<div class="marginnote">
<p>* See 1 for a clear discussion of this, as well as an approach to ‘bounding this bias’ under a particular monotonicity assumption.</p>
</div>
</div>
<div id="overall-analysis" class="section level2" number="19.13">
<h2 number="19.13"><span class="header-section-number">19.13</span> Overall analysis</h2>
<p>My overall impression from the above is that the meta-analytic results do suggest some benefit of the Paleo diet in reducing waist circumference for groups that are vulnerable to the metabolic syndrome.</p>
<p>However:</p>
<ul>
<li><p>The evidence plausibly suggests that the benefit of this diet comes largely through greater adherence, rather than through a direct nutritional advantage. Perhaps, as is popular conventional wisdom “eating more proteinssimply makes it easier to feel full and thus not overeat.”</p></li>
<li><p>The studies I have seen compare the diet only to a traditional diet, rather than to another low carbohydrate diet. Thus, I do not see any strong evidence that the Paleo diet offers advantages (or disadvantages) over, e.g., the ‘South Beach’ or ‘Keto’ diets. Thus the evidence does not support (nor refute) the <em>mechanism and diet philopsophy</em> behind Paleo.<br />
</p></li>
</ul>
<p>Furthermore, there also appears to be <strong>limited evidence</strong>; I am not highly confident in even the above claim. I did not see aa wide array of high-quality studies evaluating the Paleo diet for the relevant large and diverse populations in natural settings.</p>
<div id="limitations-p" class="section level3" number="19.13.1">
<h3 number="19.13.1"><span class="header-section-number">19.13.1</span> Limitations and uncertainties to my own analysis; proposed future steps</h3>
<blockquote>
<p>Briefly, what are the major uncertainties in your analysis? How could your conclusion be wrong? This will likely involve saying what you explicitly chose not to do and what you do not know.</p>
</blockquote>
<p><strong>Caveats:</strong> I am an economist, not a nutritionist or a life scientist. While I am working to build up my expertise in this area I have only limited experience conducting and for evaluating meta-analyses. The evaluation here was only a limited and shallow exercise done in a 10 hour window for a specific purpose. I did not do a broad and systematic review of the literature.</p>
<p>I was only able to take a narrow focus for this exercise, mainly considering <a href="mailto:only@manheimerPaleolithicNutritionMetabolic2015" class="email">only@manheimerPaleolithicNutritionMetabolic2015</a> review, as well as some connected work. To gain a more reliable answer to this question, there are a number of further , involving gaining better understanding, digging deeper into the data and analysis, and consulting a range of experts.</p>
<p><br />
</p>
<p><strong>Some steps I would take</strong></p>
<ol style="list-style-type: decimal">
<li>Better understand: <em>What is the relevant question?</em></li>
</ol>
<p>As I suggested above, the nature of evidence that is appropriate will depend on what question is being asked, and what purpose this is being put to. Why do we care about the answer to this? Are we considering recommending this diet to a particular population? How strong incentive/food provision will we be able to provide? Are we deeply interested in the biological mechanisms or the behavioral ones? What do we intend to do with the results of this analysis, i.e., what policy choices will it inform?</p>
<p>If I had better answers to question like these, I would know which types of studies to focus on, and how to better consider their results.</p>
<ol start="2" style="list-style-type: decimal">
<li>Pursue a more systematic consideration of the literature and evidence</li>
</ol>
<ul>
<li>documenting my protocol</li>
<li>carefully considering which fields and subfields to focus on</li>
<li>discussing my approaches with experts in these fields, seeking their consideration of the points I have made in this essay</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Pursue a better understanding of</li>
</ol>
<ul>
<li><p>the reasoning behind standardized protocols for meta-analysis and ‘bias-assessment,’ consulting, e.g., the <a href="https://training.cochrane.org/handbook/current">‘Cochrane Handbook for Systematic Reviews of Interventions’</a></p></li>
<li><p>How nutritionists and dieticians differentiate (what I call above) the ITT and ATE</p></li>
<li><p>The argument for particular meta-analytic approaches in nutrition and health studies (in comparison to my area of greater expertise, economics and social science)</p></li>
</ul>
<p><br />
</p>
<ol start="4" style="list-style-type: decimal">
<li>More closely inspect and evaluate</li>
</ol>
<ul>
<li><p><span class="citation"><a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">Manheimer et al.</a> (<a href="#ref-manheimerPaleolithicNutritionMetabolic2015" role="doc-biblioref">2015</a>)</span>’s code used for doing the (frequentist) Random-effects meta-analysis, to understand the specific inclusion, variable-coding, and estimator choices, and the sensitivity of the results to this.*</p>
<div class="marginnote">
<p>* For example, as explained in the excellent tutorial <span class="citation"><a href="#ref-harrerDoingMetaanalysisHandson2019" role="doc-biblioref">Harrer et al.</a> (<a href="#ref-harrerDoingMetaanalysisHandson2019" role="doc-biblioref">2019</a>)</span>, <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/random.html#tau2">there are a range of popular estimators for <span class="math inline">\(\tau^2\)</span></a>, the variance of the distribution of true effect sizes. There is also some debate over whether random effects is appropriate, particularly because of its sensitivity to small studies.</p>
</div></li>
<li><p>The particularly promising <span class="citation"><a href="#ref-boersFavourableEffectsConsuming2014" role="doc-biblioref">Boers et al.</a> (<a href="#ref-boersFavourableEffectsConsuming2014" role="doc-biblioref">2014</a>)</span> study as <a href="#boers">noted above</a></p></li>
<li><p><span class="citation"><a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">Ghaedi et al.</a> (<a href="#ref-ghaediEffectsPaleolithicDiet2019" role="doc-biblioref">2019</a>)</span>, and <span class="citation"><a href="#ref-demenezesInfluencePaleolithicDiet2019a" role="doc-biblioref">de Menezes et al.</a> (<a href="#ref-demenezesInfluencePaleolithicDiet2019a" role="doc-biblioref">2019</a>)</span>, and any other promising analyses that come from the aforementioned systematic literature survey.</p></li>
</ul>
<!--chapter:end:meta_anal_and_open_science/paleo_example/paleo_example.Rmd-->
</div>
</div>
</div>
<div id="data-sci" class="section level1" number="20">
<h1 number="20"><span class="header-section-number">20</span> Getting, cleaning and using data</h1>
<p>This will build on my content <a href="https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data">here</a>, and integrate with it.</p>
<p>Some key resources are in a continually updated airtable <a href="https://airtable.com/shrqNt0YAa3eLiK5S">HERE</a></p>
<p>See especially:</p>
<p><a href="r4ds.had.co.nz">R for data science</a></p>
<p><a href="https://adv-r.hadley.nz/">Advanced R</a></p>
<p><a href="https://bookdown.org/yihui/bookdown/">bookdown: Authoring Books and Technical Documents with R Markdown:</a></p>
<p>[OSF: ‘PhD Toolkit on Transparent, Open, Reproducible Research’ <a href="https://osf.io/g8yjz/" class="uri">https://osf.io/g8yjz/</a>](OSF: ‘PhD Toolkit on Transparent, Open, Reproducible Research’ <a href="https://osf.io/g8yjz/" class="uri">https://osf.io/g8yjz/</a>)</p>
<p><a href="https://happygitwithr.com/">Happy Git and GitHub for the useR</a></p>
<p>“Data science for business” see <a href="#n_ds4bs">WIP notes here</a></p>
<p><br />
</p>
<p>“Code and Data for the Social Sciences” (Gentzkow/Shapiro)</p>

<div class="note">
<p>See also <a href="#n_ds4bs">“notes on Data Science for Business”</a></p>
</div>
<div id="data-whatwhywherehow" class="section level2" number="20.1">
<h2 number="20.1"><span class="header-section-number">20.1</span> Data: What/why/where/how</h2>
</div>
<div id="organizing-a-project" class="section level2" number="20.2">
<h2 number="20.2"><span class="header-section-number">20.2</span> Organizing a project</h2>
</div>
<div id="dynamic-documents-esp-rmdbookdown" class="section level2" number="20.3">
<h2 number="20.3"><span class="header-section-number">20.3</span> Dynamic documents (esp Rmd/bookdown)</h2>
<p>Some guidelines from a particular project:</p>
<p><a href="https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html">Appendix: Tech for creating, editing and collaborating on this ‘Bookdown’ web book/project (and starting your own)</a></p>
<div id="managing-referencescitations" class="section level3" number="20.3.1">
<h3 number="20.3.1"><span class="header-section-number">20.3.1</span> Managing references/citations</h3>

<div class="note">
<p>A letter to my co-authors…</p>
<p>Hi all.</p>
<p>Hope you are doing well. I’ve just invited you to a shared Zotero group managing my bibliography/references. I think this should be useful. (I prefer Zotero to Mendeley because it’s open source and… I forgot the other reason.)
On my computer it synchronizes with a .bib (bibtex) file in a dropbox folder …</p>
<p>For latex files we just refer to this as normal.
In the Rmd files/bookdown (producing output like <a href="#https://daaronr.github.io/ea_giving_barriers/outline.html">EA barriers</a> or Metrics notes (present book) this is referenced in the YAML header to the index.Rmd file as</p>
<blockquote>
<p>bibliography: [reinstein_bibtex.bib]</p>
</blockquote>
<p>Then, to keep this file, I have a “download block” included in that same file (the first line with ‘dropbox’ is the key one).</p>
</div>
<p><br />
</p>
<p>The download code follows (remove the ‘eval=FALSE’ to get it to actually run)…</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tryCatch</span>( <span class="co">#trycatch lets us &#39;try&#39; to execute and if there is an error, it does the thing *after* the braces, rather than crashing</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">download.file</span>(<span class="at">url =</span> <span class="st">&quot;https://www.dropbox.com/s/3i8bjrgo8u08v5w/reinstein_bibtex.bib?raw=1&quot;</span>, <span class="at">destfile =</span> <span class="st">&quot;reinstein_bibtex.bib&quot;</span>) <span class="co">#download the bibtex database</span></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>        <span class="fu">download.file</span>(<span class="at">url =</span> <span class="st">&quot;https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css&quot;</span>, <span class="at">destfile =</span> <span class="fu">here</span>(<span class="st">&quot;support&quot;</span>, <span class="st">&quot;tufte_plus.css&quot;</span>)) <span class="co">#this downloads the style file</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>  }, <span class="at">error =</span> <span class="cf">function</span>(e) {</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">&quot;you are not online, so we can&#39;t download&quot;</span>)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>A fairly comprehensive discussion of tools for citation in R-markdown:</p>
<p><a href="https://ropensci.org/technotes/2020/05/07/rmd-citations/">A Roundup of R Tools for Handling BibTeX</a></p>
</div>
<div id="an-example-of-dynamic-code" class="section level3" number="20.3.2">
<h3 number="20.3.2"><span class="header-section-number">20.3.2</span> An example of dynamic code</h3>
<p>Shapiro Wilk test for normality; professor salaries at some US university from the built in Cars data…</p>
<div class="marginnote">
<p>By the way, if anyone wants me to offer me a job at that university, it looks like a great deal!</p>
</div>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>prof_sal_shapiro_test <span class="ot">&lt;-</span> <span class="fu">shapiro.test</span>(carData<span class="sc">::</span>Salaries<span class="sc">$</span>salary)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># ShapiroTest &lt;- map_df(list(SXDonShapiroTest, EXDonShapiroTest), tidy)</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (ShapiroTest &lt;- kable(ShapiroTest) %&gt;% kable_styling())</span></span></code></pre></div>
<p>The results from the Shapiro Wilk normality test …</p>
<p>The p-values are 6.08^{-9} suggesting this data is not normal</p>
</div>
</div>
<div id="project-management-tools-esp.-gitgithub" class="section level2" number="20.4">
<h2 number="20.4"><span class="header-section-number">20.4</span> Project management tools, esp. Git/Github</h2>
<p>(More to be added/linked here)</p>
<p>See <a href="https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html#git-and-github">‘Git and GitHub’ here… watch this space</a></p>
<p><br />
</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
<p>For students and research assistants, I’ve been sending first time git users/developers to this:<a href="https://t.co/P6KQpXHCWI">https://t.co/P6KQpXHCWI</a><br>+ <a href="https://t.co/q4R4Ei5Biw">https://t.co/q4R4Ei5Biw</a></p>
</p>
<p>— Nathan Lane (@straightedge) <a href="https://twitter.com/straightedge/status/1172694350205087744?ref_src=twsrc%5Etfw">September 14, 2019</a></p>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div id="good-coding-practices" class="section level2" number="20.5">
<h2 number="20.5"><span class="header-section-number">20.5</span> Good coding practices</h2>
<div id="new-tools-and-approaches-to-data-esp-tidyverse" class="section level3" number="20.5.1">
<h3 number="20.5.1"><span class="header-section-number">20.5.1</span> New tools and approaches to data (esp ‘tidyverse’)</h3>
<p>From <a href="https://bookdown.org/content/3890/small-worlds-and-large-worlds.html">Kurtz</a>:</p>
<blockquote>
<p>If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., <code>%&gt;%</code>). I’m not going to explain the <code>%&gt;%</code> in this project, but you might learn more about in <a href="https://www.youtube.com/watch?v=9yjhxvu-pDg">this brief clip</a>, starting around <a href="https://www.youtube.com/watch?v=K-ss_ag2k9E&amp;t=1285s">minute 21:25 in this talk by Wickham</a>, or in <a href="https://r4ds.had.co.nz/transform.html#combining-multiple-operations-with-the-pipe">section 5.6.1 from Grolemund and Wickham’s <em>R for Data Science</em></a>. Really, all of Chapter 5 of <em>R4DS</em> is just great for new R and new tidyverse users. And <em>R4DS</em> Chapter 3 is a nice introduction to plotting with ggplot2.</p>
</blockquote>
</div>
<div id="style-and-consistency" class="section level3" number="20.5.2">
<h3 number="20.5.2"><span class="header-section-number">20.5.2</span> Style and consistency</h3>
<div id="lower_snake_case" class="section level4 unnumbered">
<h4 class="unnumbered">lower_snake_case</h4>
<p>Use <code>lower_snake_case</code> to name <em>all</em> objects (that’s my preference anyways) unless there’s a strong reason to do otherwise.</p>
<p><br />
</p>
<p>This includes:</p>
<p><code>file_names.txt</code> <code>folder_names</code> <code>function_names</code> (with few exceptions) <code>names_of_data_objects_like_vectors</code> <code>names_of_data_output_objects_like_correlation_coefficients</code> <code>ex_df1</code> In R you probably should keep data frame names short to avoid excessive typing</p>
<p><br />
</p>
<p><em>And by all that is holy, never put spaces or slashes in file or object names!</em> This can make it very hard to process across systems… there are various ways of referring to spaces and other white space.</p>
</div>
<div id="indenting-and-spacing" class="section level4" number="20.5.2.1">
<h4 number="20.5.2.1"><span class="header-section-number">20.5.2.1</span> Indenting and spacing</h4>
</div>
</div>
<div id="using-functions-variable-lists-etc.-for-clean-concise-readable-code" class="section level3" number="20.5.3">
<h3 number="20.5.3"><span class="header-section-number">20.5.3</span> Using functions, variable lists, etc., for clean, concise, readable code</h3>
</div>
<div id="mapping-over-lists-to-produce-results" class="section level3" number="20.5.4">
<h3 number="20.5.4"><span class="header-section-number">20.5.4</span> Mapping over lists to produce results</h3>
<!-- ft <- list(ft_treat_no_ask, ft_treat_no_ask_19, ft_treat_no_ask_19_short, ft_treat_no_ask_19_long)

ft_names <- c("All", "2019", "2019-short", "2019-long")

ft <- map2(ft, ft_names, function(x, y) {
  broom::tidy(x) %>% add_column(Experiment = y)
}) %>%
  bind_rows() %>%
  kable(, caption = "S2 Donation incidence by S1 Ask/no-ask; Fisher tests", digits = 2) %>%
  kable_styling()
-->
</div>
<div id="building-results-based-on-lists-of-filters-of-the-data-set" class="section level3" number="20.5.5">
<h3 number="20.5.5"><span class="header-section-number">20.5.5</span> Building results based on ‘lists of filters’ of the data set</h3>
<p>In writing a paper you very often want to produce ‘statistics A-F for subsets S1-S5.’</p>
<p>Can’t we just automate this?</p>
<p><br />
</p>
<p>We can store a filter as a character vector and then apply it</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>selection_statement <span class="ot">&lt;-</span> <span class="st">&quot;Species == &#39;setosa&#39; &amp; Petal.Width&gt;0.3&quot;</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>iris <span class="sc">%&gt;%</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">as.tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(rlang<span class="sc">::</span><span class="fu">eval_tidy</span>(rlang<span class="sc">::</span><span class="fu">parse_expr</span>(selection_statement)))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-101">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-101) </caption><col><col><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Sepal.Length</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Sepal.Width</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Petal.Length</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Petal.Width</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Species</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.4</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.9</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.7</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5.7</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">4.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.9</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.3</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5.1</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3.7</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.1</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.3</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.7</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.5</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5&nbsp;&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5&nbsp;&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.6</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.8</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.9</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
</table>

<p>Making this a function for later use:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>selection_statement <span class="ot">&lt;-</span> <span class="st">&quot;Species == &#39;setosa&#39; &amp; Petal.Width&gt;0.3&quot;</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>filter_parse <span class="ot">=</span>  <span class="cf">function</span>(df, x) {</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>  {{df}} <span class="sc">%&gt;%</span></span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">filter</span>(rlang<span class="sc">::</span><span class="fu">eval_tidy</span>(rlang<span class="sc">::</span><span class="fu">parse_expr</span>({{x}})))</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>iris <span class="sc">%&gt;%</span></span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a> <span class="fu">filter_parse</span>(selection_statement)</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-102">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-102) </caption><col><col><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Sepal.Length</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Sepal.Width</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Petal.Length</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Petal.Width</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">Species</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.4</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.9</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.7</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5.7</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">4.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.9</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.3</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5.1</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3.7</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.1</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.3</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.7</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.5</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5&nbsp;&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">5&nbsp;&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3.5</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1.6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.6</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">setosa</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5.1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3.8</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1.9</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">setosa</td></tr>
</table>

<p>We can do the same for a list of character vectors of filter statements, and apply each filter from the list to the dataframe, and then the output function:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>sel_st <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Species == &#39;setosa&#39; &amp; Petal.Width&gt;0.3&quot;</span>, <span class="st">&quot;Species == &#39;virginica&#39; &amp; Petal.Width&gt;2.4&quot;</span>)</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="fu">map</span>(iris, selection_statement)</span></code></pre></div>
<pre><code>## $Sepal.Length
## NULL
## 
## $Sepal.Width
## NULL
## 
## $Petal.Length
## NULL
## 
## $Petal.Width
## NULL
## 
## $Species
## NULL</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>sel_st <span class="sc">%&gt;%</span> <span class="fu">map</span>(<span class="sc">~</span> <span class="fu">filter_parse</span>(iris, .x))</span></code></pre></div>
<pre><code>## [[1]]
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.4         3.9          1.7         0.4  setosa
## 2          5.7         4.4          1.5         0.4  setosa
## 3          5.4         3.9          1.3         0.4  setosa
## 4          5.1         3.7          1.5         0.4  setosa
## 5          5.1         3.3          1.7         0.5  setosa
## 6          5.0         3.4          1.6         0.4  setosa
## 7          5.4         3.4          1.5         0.4  setosa
## 8          5.0         3.5          1.6         0.6  setosa
## 9          5.1         3.8          1.9         0.4  setosa
## 
## [[2]]
##   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
## 1          6.3         3.3          6.0         2.5 virginica
## 2          7.2         3.6          6.1         2.5 virginica
## 3          6.7         3.3          5.7         2.5 virginica</code></pre>
<p>It works nicely if you have a list of filters aligned with a list of names or other objects ‘specific to each filter’</p>
<p>(Code below: adapt to public data and explain)</p>
<pre><code> bal_sx &lt;-
    map2(subsets_sx_dur, subsets_sx_dur_name, function(x, y) {
      filter_parse(sa, x) %&gt;%
    dplyr::filter(stage == &quot;2&quot;) %&gt;%
     tabyl(treat_1, treat_2, show_missing_levels = FALSE)  %&gt;%
    adornme_not(cap = paste(y, &quot;; 1st and 2nd stage treatments&quot;))
    }
  )</code></pre>
</div>
<div id="coding-style-and-indenting-in-stata-one-approach" class="section level3" number="20.5.6">
<h3 number="20.5.6"><span class="header-section-number">20.5.6</span> Coding style and indenting in Stata (one approach)</h3>
<p>I indent every line except</p>
<ul>
<li><p>clear, import, save, merge (‘file operations’)</p>
<ul>
<li>except where these occur as part of a loop, in which case I put in an ‘important comment’ noting these operations</li>
</ul></li>
<li><p>lines that call other do files</p></li>
<li><p>important comments/flags/to-do’s</p></li>
</ul>
<p>I only put ‘small todo’ elements having to do with code in a code file itself (and even then there may be better places). If we are going to put todos I suggest we include <code>#todo</code> to search for these later (and R has a utility to collect these in a nice way… maybe Stata does too.</p>
<p><br />
</p>
<p>Whenever there are more than 20 lines of something prosaic that cannot/has not been put into a loop or function, I suggest we put it in a separate ‘do’ file and call that do file (with no indent). That’s what I do here, giving a brief description and a ‘back link.’ Sometimes I put all those do files into an separate folder.</p>
</div>
</div>
<div id="additional-tips-integrate" class="section level2" number="20.6">
<h2 number="20.6"><span class="header-section-number">20.6</span> Additional tips (integrate)</h2>
<p><a href="https://twitter.com/gdequeiroz/status/1228722821817225216">When you have to upgrade R on Mac, how to preserve package installations - twitter thread</a></p>
<ul>
<li><a href="https://github.com/ivelasq/r-data-recipes/blob/master/README.md#reinstall-packages-after-a-major-r-update">This worked well for me</a>. Thanks @ivelasq3 !<br />
</li>
</ul>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
<p>Are you teaching/learning <a href="https://twitter.com/hashtag/r?src=hash&amp;ref_src=twsrc%5Etfw">#r</a> <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> &amp; want to teach/learn the latest <a href="https://twitter.com/hashtag/tidyverse?src=hash&amp;ref_src=twsrc%5Etfw">#tidyverse</a> <a href="https://twitter.com/hashtag/tidyr?src=hash&amp;ref_src=twsrc%5Etfw">#tidyr</a> tools? e.g. bind_rows, pivot_wider/pivot_longer, the join family (full_join, inner_join…) Check out my slides on “Advanced data manipulation” here 😺<a href="https://t.co/dr9VNx7MFf">https://t.co/dr9VNx7MFf</a></p>
</p>
<p>— Amy Willis (@AmyDWillis) <a href="https://twitter.com/AmyDWillis/status/1195498569072959490?ref_src=twsrc%5Etfw">November 16, 2019</a></p>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br />
</p>
<p><strong>Tools for structuring your workflow for reproducable code with Rmd and Git: The workflowr package</strong></p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
<p>“This paper does a thorough job setting out the rationale, design, and implementation of the workflowr package” says <a href="https://twitter.com/PeteHaitch?ref_src=twsrc%5Etfw">@PeteHaitch</a> <a href="https://twitter.com/WEHI_research?ref_src=twsrc%5Etfw">\<span class="citation">(<a href="#ref-WEHI_research" role="doc-biblioref"><strong>WEHI_research?</strong></a>)</span></a> in his review of this <a href="https://twitter.com/hashtag/softwaretool?src=hash&amp;ref_src=twsrc%5Etfw">#softwaretool</a> article introducing workflowr by <a href="https://twitter.com/jdblischak?ref_src=twsrc%5Etfw">@jdblischak</a> and co-authors <a href="https://t.co/ZXmQkDhFuD">https://t.co/ZXmQkDhFuD</a> <a href="https://twitter.com/hashtag/OpenScience?src=hash&amp;ref_src=twsrc%5Etfw">#OpenScience</a> <a href="https://t.co/e77SVo8PhO">pic.twitter.com/e77SVo8PhO</a></p>
</p>
<p>— F1000Research (@F1000Research) <a href="https://twitter.com/F1000Research/status/1196056651691962368?ref_src=twsrc%5Etfw">November 17, 2019</a></p>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br />
</p>
<p>Switching from Latex to markdown/R-markdown? <a href="https://colinbousige.github.io/post/rmarkdown/">These tips from Colin Bousige look pretty good</a>, although I prefer the bookdown/gitbook format</p>
<p>Massive data cleaning using the Recipe package</p>
<p>Codebook package</p>
<div id="some-key-points-from-r-for-data-science-see-my-hypothesis-notes" class="section level3 unnumbered">
<h3 class="unnumbered">Some key points from <a href="r4ds.had.co.nz">R for data science</a> (see my hypothesis notes)</h3>
<div id="automating-many-models" class="section level4 unnumbered">
<h4 class="unnumbered">Automating <a href="https://r4ds.had.co.nz/many-models.html">‘many models’</a></h4>

<div class="note">
Actually, I think <a href="https://stackoverflow.com/questions/46332863/how-to-fit-multiple-models-on-multiple-dataset-in-purrr">this</a> is the automation we are usually more interested in, i.e., ‘’run this on these subsets of the data, for these variable sets.’’
)
</div>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gapminder)</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modelr)</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<blockquote>
<p>Extract out the common code with a function and repeat using a map function from <code>purrr</code>. This problem is structured a little differently to what you’ve seen before. Instead of repeating an action for each variable, we want to repeat an action for each country, a subset of rows. To do that, we need a new data structure: the nested data frame. To create a nested data frame we start with a grouped data frame, and “nest” it:</p>
</blockquote>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>by_country <span class="ot">&lt;-</span> gapminder <span class="sc">%&gt;%</span> </span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(country, continent) <span class="sc">%&gt;%</span> </span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>() <span class="co">#automatically labels a list column (column of tibbles, which are lists) as &#39;data&#39;</span></span></code></pre></div>
<blockquote>
<p>This creates a data frame that has one row per group (per country), and a rather unusual column: <code>data</code>. <code>data</code> is a list of data frames (or tibbles, to be precise).</p>
</blockquote>
<blockquote>
<p>in a nested data frame, each row is a group.</p>
</blockquote>
<blockquote>
<p>We have a model-fitting function:</p>
</blockquote>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>country_model <span class="ot">&lt;-</span> <span class="cf">function</span>(df) {</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(lifeExp <span class="sc">~</span> year, <span class="at">data =</span> df)</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<blockquote>
<p>The data frames are in a list, so we can use <code>purrr::map()</code> to apply <code>country_model</code> to each element…</p>
</blockquote>
<blockquote>
<p>However, rather than leaving the list of models as a free-floating object, I think it’s better to store it as a column in the <code>by_country</code> data frame.</p>
</blockquote>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>  by_country <span class="ot">&lt;-</span> by_country <span class="sc">%&gt;%</span> </span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">model =</span> <span class="fu">map</span>(data, country_model))</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## # A tibble: 142 x 4
## # Groups:   country, continent [142]
##    country     continent data                  model 
##    &lt;fct&gt;       &lt;fct&gt;     &lt;list&gt;                &lt;list&gt;
##  1 Afghanistan Asia      &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  2 Albania     Europe    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  3 Algeria     Africa    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  4 Angola      Africa    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  5 Argentina   Americas  &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  6 Australia   Oceania   &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  7 Austria     Europe    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  8 Bahrain     Asia      &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
##  9 Bangladesh  Asia      &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
## 10 Belgium     Europe    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;  
## # … with 132 more rows</code></pre>
<p>Here, the <code>model</code> list-column is created, resulting from mapping the <code>data</code> list-column (list of tibbles) into the <code>country_model</code> function.</p>
<blockquote>
<p>because all the related objects are stored together, you don’t need to manually keep them in sync when you filter or arrange.</p>
</blockquote>
<p>To compute the residuals, we need to call <code>add_residuals()</code> with each model-data pair:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>by_country <span class="ot">&lt;-</span> by_country <span class="sc">%&gt;%</span> </span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">resids =</span> <span class="fu">map2</span>(data, model, add_residuals)</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>by_country</span></code></pre></div>
<pre><code>## # A tibble: 142 x 5
## # Groups:   country, continent [142]
##    country     continent data                  model  resids               
##    &lt;fct&gt;       &lt;fct&gt;     &lt;list&gt;                &lt;list&gt; &lt;list&gt;               
##  1 Afghanistan Asia      &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  2 Albania     Europe    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  3 Algeria     Africa    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  4 Angola      Africa    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  5 Argentina   Americas  &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  6 Australia   Oceania   &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  7 Austria     Europe    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  8 Bahrain     Asia      &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
##  9 Bangladesh  Asia      &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
## 10 Belgium     Europe    &lt;tibble[,4] [12 × 4]&gt; &lt;lm&gt;   &lt;tibble[,5] [12 × 5]&gt;
## # … with 132 more rows</code></pre>
<div class="marginnote">
<p>Coding note: How does this syntax work? How do <code>data</code> and <code>model</code> end up referring to columns in the <code>by_country</code> tibble?</p>
<p>Because it’s inside the ‘mutate,’ I guess, so the data frame is implied.</p>
</div>
<blockquote>
<p>[So that we can plot it] … let’s turn the list of data frames back into a regular data frame. Previously we used <code>nest()</code> to turn a regular data frame into an nested data frame, and now we do the opposite with <code>unnest()</code>:</p>
</blockquote>
<p>(Note it’s saved as a different object for now)</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>resids <span class="ot">&lt;-</span> <span class="fu">unnest</span>(by_country, resids)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>resids</span></code></pre></div>
<pre><code>## # A tibble: 1,704 x 9
## # Groups:   country, continent [142]
##    country   continent data         model  year lifeExp    pop gdpPercap   resid
##    &lt;fct&gt;     &lt;fct&gt;     &lt;list&gt;       &lt;lis&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;
##  1 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1952    28.8 8.43e6      779. -1.11  
##  2 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1957    30.3 9.24e6      821. -0.952 
##  3 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1962    32.0 1.03e7      853. -0.664 
##  4 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1967    34.0 1.15e7      836. -0.0172
##  5 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1972    36.1 1.31e7      740.  0.674 
##  6 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1977    38.4 1.49e7      786.  1.65  
##  7 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1982    39.9 1.29e7      978.  1.69  
##  8 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1987    40.8 1.39e7      852.  1.28  
##  9 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1992    41.7 1.63e7      649.  0.754 
## 10 Afghanis… Asia      &lt;tibble[,4]… &lt;lm&gt;   1997    41.8 2.22e7      635. -0.534 
## # … with 1,694 more rows</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co">#and then we plot it </span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>resids <span class="sc">%&gt;%</span> </span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(year, resid)) <span class="sc">+</span></span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> country), <span class="at">alpha =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>continent)</span></code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-111-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>glance <span class="ot">&lt;-</span> by_country <span class="sc">%&gt;%</span> </span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">glance =</span> <span class="fu">map</span>(model, broom<span class="sc">::</span>glance)) <span class="sc">%&gt;%</span> </span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(glance)</span></code></pre></div>
<p>Above, we add a column <code>glance</code>, resulting from mapping the <code>mode</code>l column to the <code>broom::glance</code> function. <code>glance</code> gets some key elements of the models’ outputs.<br />
<br />
Then we unnest this column. Note that <code>unnest</code> spreads out the elements of the <code>glance</code> output into columns, but as these are specific to each country (but <em>not</em> each year), it doesn’t add more rows (while, e,g., unnesting <code>resids</code> <em>would</em> add more rows).</p>
<p>One way of judging the ‘fit’ of these models… The worst-fitting ones, in terms of R-squared, seem to be in Africa (we save the worst ones as <code>bad_fit</code>:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>glance <span class="sc">%T&gt;%</span> </span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(r.squared) <span class="sc">%T&gt;%</span> <span class="fu">print</span>() <span class="sc">%&gt;%</span>  <span class="co">#look I get both in one pipe flow with T-pipe and the &#39;print&#39; side effect</span></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(continent, r.squared)) <span class="sc">+</span> </span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_jitter</span>(<span class="at">width =</span> <span class="fl">0.3</span>) </span></code></pre></div>
<pre><code>## # A tibble: 142 x 17
## # Groups:   country, continent [142]
##    country  continent data  model resids r.squared adj.r.squared sigma statistic
##    &lt;fct&gt;    &lt;fct&gt;     &lt;lis&gt; &lt;lis&gt; &lt;list&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
##  1 Afghani… Asia      &lt;tib… &lt;lm&gt;  &lt;tibb…     0.948         0.942 1.22      181. 
##  2 Albania  Europe    &lt;tib… &lt;lm&gt;  &lt;tibb…     0.911         0.902 1.98      102. 
##  3 Algeria  Africa    &lt;tib… &lt;lm&gt;  &lt;tibb…     0.985         0.984 1.32      662. 
##  4 Angola   Africa    &lt;tib… &lt;lm&gt;  &lt;tibb…     0.888         0.877 1.41       79.1
##  5 Argenti… Americas  &lt;tib… &lt;lm&gt;  &lt;tibb…     0.996         0.995 0.292    2246. 
##  6 Austral… Oceania   &lt;tib… &lt;lm&gt;  &lt;tibb…     0.980         0.978 0.621     481. 
##  7 Austria  Europe    &lt;tib… &lt;lm&gt;  &lt;tibb…     0.992         0.991 0.407    1261. 
##  8 Bahrain  Asia      &lt;tib… &lt;lm&gt;  &lt;tibb…     0.967         0.963 1.64      291. 
##  9 Banglad… Asia      &lt;tib… &lt;lm&gt;  &lt;tibb…     0.989         0.988 0.977     930. 
## 10 Belgium  Europe    &lt;tib… &lt;lm&gt;  &lt;tibb…     0.995         0.994 0.293    1822. 
## # … with 132 more rows, and 8 more variables: p.value &lt;dbl&gt;, df &lt;dbl&gt;,
## #   logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;,
## #   nobs &lt;int&gt;</code></pre>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-113-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>bad_fit <span class="ot">&lt;-</span> <span class="fu">filter</span>(glance, r.squared <span class="sc">&lt;</span> <span class="fl">0.25</span>)</span></code></pre></div>
<p><br />
</p>
<p>What may have caused the “bad fit,” i.e., a departure from the country-specific trends?</p>
<p>We <code>semi_join</code> the original data set to these ‘worst fitting countries’ … this keeps only those that match, i.e., only the worst-fitting countries. (Probably we could otherwise have instead expanded the <code>data</code> list?)</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>gapminder <span class="sc">%&gt;%</span> </span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">semi_join</span>(bad_fit, <span class="at">by =</span> <span class="st">&quot;country&quot;</span>) <span class="sc">%&gt;%</span> <span class="co">#I think this just &#39;keeps all elements of the first df that are also present in the second frame</span></span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(year, lifeExp, <span class="at">colour =</span> country)) <span class="sc">+</span></span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="metrics_and_tools_files/figure-html/unnamed-chunk-114-1.png" width="80%" style="display: block; margin: auto;" /></p>
<blockquote>
<p>We see two main effects here: the tragedies of the HIV/AIDS epidemic and the Rwandan genocide.</p>
</blockquote>
</div>
<div id="list-columns" class="section level4" number="20.6.0.1">
<h4 number="20.6.0.1"><span class="header-section-number">20.6.0.1</span> List-columns</h4>
<blockquote>
<p>Now that you’ve seen a basic workflow for managing many models… let’s dive… into some of the details</p>
</blockquote>
<p>We saw a workflow for managing</p>
<ul>
<li><p>split the data apart</p></li>
<li><p>run the same model model for each of the groups</p></li>
<li><p>save this all in a single organized tibble</p></li>
<li><p>report and graph the results in different ways</p></li>
</ul>
<blockquote>
<p>a data frame is a named list of equal length vectors.</p>
</blockquote>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">list</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">3</span><span class="sc">:</span><span class="dv">5</span>))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-115">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-115) </caption><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x.1.3</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x.3.5</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">4</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5</td></tr>
</table>

<p>But incorporating list columns is much easier with <code>tibble</code> and <code>tribble</code> , because <code>tibble()</code> doesn’t modify its inputs, and prints better:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">list</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">3</span><span class="sc">:</span><span class="dv">5</span>), </span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(<span class="st">&quot;1, 2&quot;</span>, <span class="st">&quot;3, 4, 5&quot;</span>)</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-116">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-116) </caption><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">y</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1:3</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1, 2</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3:5</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3, 4, 5</td></tr>
</table>

<pre><code>&lt;div class=&quot;marginnote&quot;&gt;
But note each column still has to have the same number of rows.
&lt;/div&gt;
 </code></pre>
<p>And <code>tribble()</code> (<a href="http://127.0.0.1:28503/help/library/tibble/html/tribble.html">row-wise tibble creation</a>) “can automatically work out that you need a list:”</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tribble</span>(</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>   <span class="sc">~</span>x, <span class="sc">~</span>y,</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="st">&quot;1, 2&quot;</span>,</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>  <span class="dv">3</span><span class="sc">:</span><span class="dv">5</span>, <span class="st">&quot;3, 4, 5&quot;</span></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-117">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-117) </caption><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">y</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1:3</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1, 2</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3:5</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3, 4, 5</td></tr>
</table>

<blockquote>
<p>List-columns are often most useful as intermediate data structure. [to keep things organised before later unnesting or whatever]</p>
<p>Generally there are three parts of an effective list-column pipeline:</p>
<ol style="list-style-type: decimal">
<li>You create the list-column using one of <code>nest()</code>, <code>summarise()</code> + <code>list()</code>, or <code>mutate()</code> + a map function, as described in <a href="#creating-list-columns">Creating list-columns</a>.</li>
<li>You create other intermediate list-columns by transforming existing list columns with <code>map()</code>, <code>map2()</code> or <code>pmap()</code>. For example, in the case study above, we created a list-column of models by transforming a list-column of data frames.</li>
<li>You simplify the list-column back down to a data frame or atomic vector, as described in <a href="#simplifying-list-columns">Simplifying list-columns</a>.</li>
</ol>
</blockquote>
<div id="creating-list-columns" class="section level5" number="20.6.0.1.1">
<h5 number="20.6.0.1.1"><span class="header-section-number">20.6.0.1.1</span> Creating list-columns</h5>
<blockquote>
<p>Typically, you’ll … [create] list columns them from regular columns, [either]:</p>
<ol style="list-style-type: decimal">
<li><p>With <code>tidyr::nest()</code> to convert a grouped data frame into a nested data frame where you have list-columns of data frames.</p></li>
<li><p>With <code>mutate()</code> and vectorised functions that return a list.</p></li>
<li><p>With <code>summarise()</code> and summary functions that return multiple results.</p></li>
</ol>
<p>Alternatively, you might create them from a named list, using <code>tibble::enframe()</code>.</p>
<p>… make sure they’re homogeneous: each element should contain the same type of thing.</p>
</blockquote>
</div>
<div id="with-nesting" class="section level5" number="20.6.0.1.2">
<h5 number="20.6.0.1.2"><span class="header-section-number">20.6.0.1.2</span> With nesting</h5>
<blockquote>
<p><code>nest()</code> creates … a data frame with a list-column of data frames. … each row is a meta-observation:</p>
<p>When applied to a grouped data frame, <code>nest()</code> keeps the grouping columns as is, and bundles everything else into the list-column:</p>
</blockquote>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>gapminder <span class="sc">%&gt;%</span> </span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(country, continent) <span class="sc">%&gt;%</span> </span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>()</span></code></pre></div>
<pre><code>## # A tibble: 142 x 3
## # Groups:   country, continent [142]
##    country     continent data                 
##    &lt;fct&gt;       &lt;fct&gt;     &lt;list&gt;               
##  1 Afghanistan Asia      &lt;tibble[,4] [12 × 4]&gt;
##  2 Albania     Europe    &lt;tibble[,4] [12 × 4]&gt;
##  3 Algeria     Africa    &lt;tibble[,4] [12 × 4]&gt;
##  4 Angola      Africa    &lt;tibble[,4] [12 × 4]&gt;
##  5 Argentina   Americas  &lt;tibble[,4] [12 × 4]&gt;
##  6 Australia   Oceania   &lt;tibble[,4] [12 × 4]&gt;
##  7 Austria     Europe    &lt;tibble[,4] [12 × 4]&gt;
##  8 Bahrain     Asia      &lt;tibble[,4] [12 × 4]&gt;
##  9 Bangladesh  Asia      &lt;tibble[,4] [12 × 4]&gt;
## 10 Belgium     Europe    &lt;tibble[,4] [12 × 4]&gt;
## # … with 132 more rows</code></pre>
<blockquote>
<p>You can also use it on an ungrouped data frame, specifying which columns you want to nest [i.e., all but country and continent below]</p>
</blockquote>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>gapminder <span class="sc">%&gt;%</span> </span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">data =</span> <span class="fu">c</span>(year<span class="sc">:</span>gdpPercap))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-119">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-119) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">country</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">continent</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">data</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Afghanistan</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(28.801, 30.332, 31.997, 34.02, 36.088, 38.438, 39.854, 40.822, 41.674, 41.763, 42.129, 43.828), pop = c(8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12881816, 13867957, 16317921, 22227415, 25268405, 31889923), gdpPercap = c(779.4453145, 820.8530296, 853.10071, 836.1971382, 739.9811058, 786.11336, 978.0114388, 852.3959448, 649.3413952, 635.341351, 726.7340548, 974.5803384))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Albania</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(55.23, 59.28, 64.82, 66.22, 67.69, 68.93, 70.42, 72, 71.581, 72.95, 75.651, 76.423), pop = c(1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 2780097, 3075321, 3326498, 3428038, 3508512, 3600523), gdpPercap = c(1601.056136, 1942.284244, 2312.888958, 2760.196931, 3313.422188, 3533.00391, 3630.880722, 3738.932735, 2497.437901, 3193.054604, 4604.211737, 5937.029526))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Algeria</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(43.077, 45.685, 48.303, 51.407, 54.518, 58.014, 61.368, 65.799, 67.744, 69.152, 70.994, 72.301), pop = c(9279525, 10270856, 11000948, 12760499, 14760787, 17152804, 20033753, 23254956, 26298373, 29072015, 31287142, 33333216), gdpPercap = c(2449.008185, 3013.976023, 2550.81688, 3246.991771, 4182.663766, 4910.416756, 5745.160213, 5681.358539, 5023.216647, 4797.295051, 5288.040382, 6223.367465))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Angola</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(30.015, 31.999, 34, 35.985, 37.928, 39.483, 39.942, 39.906, 40.647, 40.963, 41.003, 42.731), pop = c(4232095, 4561361, 4826015, 5247469, 5894858, 6162675, 7016384, 7874230, 8735988, 9875024, 10866106, 12420476), gdpPercap = c(3520.610273, 3827.940465, 4269.276742, 5522.776375, 5473.288005, 3008.647355, 2756.953672, 2430.208311, 2627.845685, 2277.140884, 2773.287312, 4797.231267))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Argentina</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(62.485, 64.399, 65.142, 65.634, 67.065, 68.481, 69.942, 70.774, 71.868, 73.275, 74.34, 75.32), pop = c(17876956, 19610538, 21283783, 22934225, 24779799, 26983828, 29341374, 31620918, 33958947, 36203463, 38331121, 40301927), gdpPercap = c(5911.315053, 6856.856212, 7133.166023, 8052.953021, 9443.038526, 10079.02674, 8997.897412, 9139.671389, 9308.41871, 10967.28195, 8797.640716, 12779.37964))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Australia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Oceania</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(69.12, 70.33, 70.93, 71.1, 71.93, 73.49, 74.74, 76.32, 77.56, 78.83, 80.37, 81.235), pop = c(8691212, 9712569, 10794968, 11872264, 13177000, 14074100, 15184200, 16257249, 17481977, 18565243, 19546792, 20434176), gdpPercap = c(10039.59564, 10949.64959, 12217.22686, 14526.12465, 16788.62948, 18334.19751, 19477.00928, 21888.88903, 23424.76683, 26997.93657, 30687.75473, 34435.36744))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Austria</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(66.8, 67.48, 69.54, 70.14, 70.63, 72.17, 73.18, 74.94, 76.04, 77.51, 78.98, 79.829), pop = c(6927772, 6965860, 7129864, 7376998, 7544201, 7568430, 7574613, 7578903, 7914969, 8069876, 8148312, 8199783), gdpPercap = c(6137.076492, 8842.59803, 10750.72111, 12834.6024, 16661.6256, 19749.4223, 21597.08362, 23687.82607, 27042.01868, 29095.92066, 32417.60769, 36126.4927))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Bahrain</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(50.939, 53.832, 56.923, 59.923, 63.3, 65.593, 69.052, 70.75, 72.601, 73.925, 74.795, 75.635), pop = c(120447, 138655, 171863, 202182, 230800, 297410, 377967, 454612, 529491, 598561, 656397, 708573), gdpPercap = c(9867.084765, 11635.79945, 12753.27514, 14804.6727, 18268.65839, 19340.10196, 19211.14731, 18524.02406, 19035.57917, 20292.01679, 23403.55927, 29796.04834))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Bangladesh</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.484, 39.348, 41.216, 43.453, 45.252, 46.923, 50.009, 52.819, 56.018, 59.412, 62.013, 64.062), pop = c(46886859, 51365468, 56839289, 62821884, 70759295, 80428306, 93074406, 103764241, 113704579, 123315288, 135656790, 150448339), gdpPercap = c(684.2441716, 661.6374577, 686.3415538, 721.1860862, 630.2336265, 659.8772322, 676.9818656, 751.9794035, 837.8101643, 972.7700352, 1136.39043, 1391.253792))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Belgium</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(68, 69.24, 70.25, 70.94, 71.44, 72.8, 73.93, 75.35, 76.46, 77.53, 78.32, 79.441), pop = c(8730405, 8989111, 9218400, 9556500, 9709100, 9821800, 9856303, 9870200, 10045622, 10199787, 10311970, 10392226), gdpPercap = c(8343.105127, 9714.960623, 10991.20676, 13149.04119, 16672.14356, 19117.97448, 20979.84589, 22525.56308, 25575.57069, 27561.19663, 30485.88375, 33692.60508))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Benin</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(38.223, 40.358, 42.618, 44.885, 47.014, 49.19, 50.904, 52.337, 53.919, 54.777, 54.406, 56.728), pop = c(1738315, 1925173, 2151895, 2427334, 2761407, 3168267, 3641603, 4243788, 4981671, 6066080, 7026113, 8078314), gdpPercap = c(1062.7522, 959.6010805, 949.4990641, 1035.831411, 1085.796879, 1029.161251, 1277.897616, 1225.85601, 1191.207681, 1232.975292, 1372.877931, 1441.284873))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Bolivia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(40.414, 41.89, 43.428, 45.032, 46.714, 50.023, 53.859, 57.251, 59.957, 62.05, 63.883, 65.554), pop = c(2883315, 3211738, 3593918, 4040665, 4565872, 5079716, 5642224, 6156369, 6893451, 7693188, 8445134, 9119152), gdpPercap = c(2677.326347, 2127.686326, 2180.972546, 2586.886053, 2980.331339, 3548.097832, 3156.510452, 2753.69149, 2961.699694, 3326.143191, 3413.26269, 3822.137084))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Bosnia and Herzegovina</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(53.82, 58.45, 61.93, 64.79, 67.45, 69.86, 70.69, 71.14, 72.178, 73.244, 74.09, 74.852), pop = c(2791000, 3076000, 3349000, 3585000, 3819000, 4086000, 4172693, 4338977, 4256013, 3607000, 4165416, 4552198), gdpPercap = c(973.5331948, 1353.989176, 1709.683679, 2172.352423, 2860.16975, 3528.481305, 4126.613157, 4314.114757, 2546.781445, 4766.355904, 6018.975239, 7446.298803))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Botswana</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(47.622, 49.618, 51.52, 53.298, 56.024, 59.319, 61.484, 63.622, 62.745, 52.556, 46.634, 50.728), pop = c(442308, 474639, 512764, 553541, 619351, 781472, 970347, 1151184, 1342614, 1536536, 1630347, 1639131), gdpPercap = c(851.2411407, 918.2325349, 983.6539764, 1214.709294, 2263.611114, 3214.857818, 4551.14215, 6205.88385, 7954.111645, 8647.142313, 11003.60508, 12569.85177))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Brazil</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(50.917, 53.285, 55.665, 57.632, 59.504, 61.489, 63.336, 65.205, 67.057, 69.388, 71.006, 72.39), pop = c(56602560, 65551171, 76039390, 88049823, 100840058, 114313951, 128962939, 142938076, 155975974, 168546719, 179914212, 190010647), gdpPercap = c(2108.944355, 2487.365989, 3336.585802, 3429.864357, 4985.711467, 6660.118654, 7030.835878, 7807.095818, 6950.283021, 7957.980824, 8131.212843, 9065.800825))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Bulgaria</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(59.6, 66.61, 69.51, 70.42, 70.9, 70.81, 71.08, 71.34, 71.19, 70.32, 72.14, 73.005), pop = c(7274900, 7651254, 8012946, 8310226, 8576200, 8797022, 8892098, 8971958, 8658506, 8066057, 7661799, 7322858), gdpPercap = c(2444.286648, 3008.670727, 4254.337839, 5577.0028, 6597.494398, 7612.240438, 8224.191647, 8239.854824, 6302.623438, 5970.38876, 7696.777725, 10680.79282))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Burkina Faso</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(31.975, 34.906, 37.814, 40.697, 43.591, 46.137, 48.122, 49.557, 50.26, 50.324, 50.65, 52.295), pop = c(4469979, 4713416, 4919632, 5127935, 5433886, 5889574, 6634596, 7586551, 8878303, 10352843, 12251209, 14326203), gdpPercap = c(543.2552413, 617.1834648, 722.5120206, 794.8265597, 854.7359763, 743.3870368, 807.1985855, 912.0631417, 931.7527731, 946.2949618, 1037.645221, 1217.032994))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Burundi</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(39.031, 40.533, 42.045, 43.548, 44.057, 45.91, 47.471, 48.211, 44.736, 45.326, 47.36, 49.58), pop = c(2445618, 2667518, 2961915, 3330989, 3529983, 3834415, 4580410, 5126023, 5809236, 6121610, 7021078, 8390505), gdpPercap = c(339.2964587, 379.5646281, 355.2032273, 412.9775136, 464.0995039, 556.1032651, 559.603231, 621.8188189, 631.6998778, 463.1151478, 446.4035126, 430.0706916))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Cambodia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(39.417, 41.366, 43.415, 45.415, 40.317, 31.22, 50.957, 53.914, 55.803, 56.534, 56.752, 59.723), pop = c(4693836, 5322536, 6083619, 6960067, 7450606, 6978607, 7272485, 8371791, 10150094, 11782962, 12926707, 14131858), gdpPercap = c(368.4692856, 434.0383364, 496.9136476, 523.4323142, 421.6240257, 524.9721832, 624.4754784, 683.8955732, 682.3031755, 734.28517, 896.2260153, 1713.778686))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Cameroon</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(38.523, 40.428, 42.643, 44.799, 47.049, 49.355, 52.961, 54.985, 54.314, 52.199, 49.856, 50.43), pop = c(5009067, 5359923, 5793633, 6335506, 7021028, 7959865, 9250831, 10780667, 12467171, 14195809, 15929988, 17696293), gdpPercap = c(1172.667655, 1313.048099, 1399.607441, 1508.453148, 1684.146528, 1783.432873, 2367.983282, 2602.664206, 1793.163278, 1694.337469, 1934.011449, 2042.09524))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Canada</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(68.75, 69.96, 71.3, 72.13, 72.88, 74.21, 75.76, 76.86, 77.95, 78.61, 79.77, 80.653), pop = c(14785584, 17010154, 18985849, 20819767, 22284500, 23796400, 25201900, 26549700, 28523502, 30305843, 31902268, 33390141), gdpPercap = c(11367.16112, 12489.95006, 13462.48555, 16076.58803, 18970.57086, 22090.88306, 22898.79214, 26626.51503, 26342.88426, 28954.92589, 33328.96507, 36319.23501))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Central African Republic</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(35.463, 37.464, 39.475, 41.478, 43.457, 46.775, 48.295, 50.485, 49.396, 46.066, 43.308, 44.741), pop = c(1291695, 1392284, 1523478, 1733638, 1927260, 2167533, 2476971, 2840009, 3265124, 3696513, 4048013, 4369038), gdpPercap = c(1071.310713, 1190.844328, 1193.068753, 1136.056615, 1070.013275, 1109.374338, 956.7529907, 844.8763504, 747.9055252, 740.5063317, 738.6906068, 706.016537))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Chad</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(38.092, 39.881, 41.716, 43.601, 45.569, 47.383, 49.517, 51.051, 51.724, 51.573, 50.525, 50.651), pop = c(2682462, 2894855, 3150417, 3495967, 3899068, 4388260, 4875118, 5498955, 6429417, 7562011, 8835739, 10238807), gdpPercap = c(1178.665927, 1308.495577, 1389.817618, 1196.810565, 1104.103987, 1133.98495, 797.9081006, 952.386129, 1058.0643, 1004.961353, 1156.18186, 1704.063724))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Chile</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(54.745, 56.074, 57.924, 60.523, 63.441, 67.052, 70.565, 72.492, 74.126, 75.816, 77.86, 78.553), pop = c(6377619, 7048426, 7961258, 8858908, 9717524, 10599793, 11487112, 12463354, 13572994, 14599929, 15497046, 16284741), gdpPercap = c(3939.978789, 4315.622723, 4519.094331, 5106.654313, 5494.024437, 4756.763836, 5095.665738, 5547.063754, 7596.125964, 10118.05318, 10778.78385, 13171.63885))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">China</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(44, 50.54896, 44.50136, 58.38112, 63.11888, 63.96736, 65.525, 67.274, 68.69, 70.426, 72.028, 72.961), pop = c(556263527, 637408000, 665770000, 754550000, 862030000, 943455000, 1000281000, 1084035000, 1164970000, 1230075000, 1280400000, 1318683096), gdpPercap = c(400.448611, 575.9870009, 487.6740183, 612.7056934, 676.9000921, 741.2374699, 962.4213805, 1378.904018, 1655.784158, 2289.234136, 3119.280896, <br>4959.114854))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Colombia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(50.643, 55.118, 57.863, 59.963, 61.623, 63.837, 66.653, 67.768, 68.421, 70.313, 71.682, 72.889), pop = c(12350771, 14485993, 17009885, 19764027, 22542890, 25094412, 27764644, 30964245, 34202721, 37657830, 41008227, 44227550), gdpPercap = c(2144.115096, 2323.805581, 2492.351109, 2678.729839, 3264.660041, 3815.80787, 4397.575659, 4903.2191, 5444.648617, 6117.361746, 5755.259962, 7006.580419))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Comoros</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(40.715, 42.46, 44.467, 46.472, 48.944, 50.939, 52.933, 54.926, 57.939, 60.66, 62.974, 65.152), pop = c(153936, 170928, 191689, 217378, 250027, 304739, 348643, 395114, 454429, 527982, 614382, 710960), gdpPercap = c(1102.990936, 1211.148548, 1406.648278, 1876.029643, 1937.577675, 1172.603047, 1267.100083, 1315.980812, 1246.90737, 1173.618235, 1075.811558, 986.1478792))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Congo, Dem. Rep.</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(39.143, 40.652, 42.122, 44.056, 45.989, 47.804, 47.784, 47.412, 45.548, 42.587, 44.966, 46.462), pop = c(14100005, 15577932, 17486434, 19941073, 23007669, 26480870, 30646495, 35481645, 41672143, 47798986, 55379852, 64606759), gdpPercap = c(780.5423257, 905.8602303, 896.3146335, 861.5932424, 904.8960685, 795.757282, 673.7478181, 672.774812, 457.7191807, 312.188423, 241.1658765, 277.5518587))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Congo, Rep.</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.111, 45.053, 48.435, 52.04, 54.907, 55.625, 56.695, 57.47, 56.433, 52.962, 52.97, 55.322), pop = c(854885, 940458, 1047924, 1179760, 1340458, 1536769, 1774735, 2064095, 2409073, 2800947, 3328795, 3800610), gdpPercap = c(2125.621418, 2315.056572, 2464.783157, 2677.939642, 3213.152683, 3259.178978, 4879.507522, 4201.194937, 4016.239529, 3484.164376, 3484.06197, 3632.557798))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Costa Rica</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(57.206, 60.026, 62.842, 65.424, 67.849, 70.75, 73.45, 74.752, 75.713, 77.26, 78.123, 78.782), pop = c(926317, 1112300, 1345187, 1588717, 1834796, 2108457, 2424367, 2799811, 3173216, 3518107, 3834934, 4133884), gdpPercap = c(2627.009471, 2990.010802, 3460.937025, 4161.727834, 5118.146939, 5926.876967, 5262.734751, 5629.915318, 6160.416317, 6677.045314, 7723.447195, 9645.06142))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Cote d'Ivoire</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(40.477, 42.469, 44.93, 47.35, 49.801, 52.374, 53.983, 54.655, 52.044, 47.991, 46.832, 48.328), pop = c(2977019, 3300000, 3832408, 4744870, 6071696, 7459574, 9025951, 10761098, 12772596, 14625967, 16252726, 18013409), gdpPercap = c(1388.594732, 1500.895925, 1728.869428, 2052.050473, 2378.201111, 2517.736547, 2602.710169, 2156.956069, 1648.073791, 1786.265407, 1648.800823, 1544.750112))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Croatia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(61.21, 64.77, 67.13, 68.5, 69.61, 70.64, 70.46, 71.52, 72.527, 73.68, 74.876, 75.748), pop = c(3882229, 3991242, 4076557, 4174366, 4225310, 4318673, 4413368, 4484310, 4494013, 4444595, 4481020, 4493312), gdpPercap = c(3119.23652, 4338.231617, 5477.890018, 6960.297861, 9164.090127, 11305.38517, 13221.82184, 13822.58394, 8447.794873, 9875.604515, 11628.38895, 14619.22272))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Cuba</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(59.421, 62.325, 65.246, 68.29, 70.723, 72.649, 73.717, 74.174, 74.414, 76.151, 77.158, 78.273), pop = c(6007797, 6640752, 7254373, 8139332, 8831348, 9537988, 9789224, 10239839, 10723260, 10983007, 11226999, 11416987), gdpPercap = c(5586.53878, 6092.174359, 5180.75591, 5690.268015, 5305.445256, 6380.494966, 7316.918107, 7532.924763, 5592.843963, 5431.990415, 6340.646683, 8948.102923))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Czech Republic</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(66.87, 69.03, 69.9, 70.38, 70.29, 70.71, 70.96, 71.58, 72.4, 74.01, 75.51, 76.486), pop = c(9125183, 9513758, 9620282, 9835109, 9862158, 10161915, 10303704, 10311597, 10315702, 10300707, 10256295, 10228744), gdpPercap = c(6876.14025, 8256.343918, 10136.86713, 11399.44489, 13108.4536, 14800.16062, 15377.22855, 16310.4434, 14297.02122, 16048.51424, 17596.21022, 22833.30851))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Denmark</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(70.78, 71.81, 72.35, 72.96, 73.47, 74.69, 74.63, 74.8, 75.33, 76.11, 77.18, 78.332), pop = c(4334000, 4487831, 4646899, 4838800, 4991596, 5088419, 5117810, 5127024, 5171393, 5283663, 5374693, 5468120), gdpPercap = c(9692.385245, 11099.65935, 13583.31351, 15937.21123, 18866.20721, 20422.9015, 21688.04048, 25116.17581, 26406.73985, 29804.34567, 32166.50006, 35278.41874))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Djibouti</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(34.812, 37.328, 39.693, 42.074, 44.366, 46.519, 48.812, 50.04, 51.604, 53.157, 53.373, 54.791), pop = c(63149, 71851, 89898, 127617, 178848, 228694, 305991, 311025, 384156, 417908, 447416, 496374), gdpPercap = c(2669.529475, 2864.969076, 3020.989263, 3020.050513, 3694.212352, 3081.761022, 2879.468067, 2880.102568, 2377.156192, 1895.016984, 1908.260867, 2082.481567))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Dominican Republic</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(45.928, 49.828, 53.459, 56.751, 59.631, 61.788, 63.727, 66.046, 68.457, 69.957, 70.847, 72.235), pop = c(2491346, 2923186, 3453434, 4049146, 4671329, 5302800, 5968349, 6655297, 7351181, 7992357, 8650322, 9319622), gdpPercap = c(1397.717137, 1544.402995, 1662.137359, 1653.723003, 2189.874499, 2681.9889, 2861.092386, 2899.842175, 3044.214214, 3614.101285, 4563.808154, 6025.374752))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Ecuador</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(48.357, 51.356, 54.64, 56.678, 58.796, 61.31, 64.342, 67.231, 69.613, 72.312, 74.173, 74.994), pop = c(3548753, 4058385, 4681707, 5432424, 6298651, 7278866, 8365850, 9545158, 10748394, 11911819, 12921234, 13755680), gdpPercap = c(3522.110717, 3780.546651, 4086.114078, 4579.074215, 5280.99471, 6679.62326, 7213.791267, 6481.776993, 7103.702595, 7429.455877, 5773.044512, 6873.262326))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Egypt</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(41.893, 44.444, 46.992, 49.293, 51.137, 53.319, 56.006, 59.797, 63.674, 67.217, 69.806, 71.338), pop = c(22223309, 25009741, 28173309, 31681188, 34807417, 38783863, 45681811, 52799062, 59402198, 66134291, 73312559, 80264543), gdpPercap = c(1418.822445, 1458.915272, 1693.335853, 1814.880728, 2024.008147, 2785.493582, 3503.729636, 3885.46071, 3794.755195, 4173.181797, 4754.604414, 5581.180998))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">El Salvador</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(45.262, 48.57, 52.307, 55.855, 58.207, 56.696, 56.604, 63.154, 66.798, 69.535, 70.734, 71.878), pop = c(2042865, 2355805, 2747687, 3232927, 3790903, 4282586, 4474873, 4842194, 5274649, 5783439, 6353681, 6939688), gdpPercap = c(3048.3029, 3421.523218, 3776.803627, 4358.595393, 4520.246008, 5138.922374, 4098.344175, 4140.442097, 4444.2317, 5154.825496, 5351.568666, 5728.353514))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Equatorial Guinea</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(34.482, 35.983, 37.485, 38.987, 40.516, 42.024, 43.662, 45.664, 47.545, 48.245, 49.348, 51.579), pop = c(216964, 232922, 249220, 259864, 277603, 192675, 285483, 341244, 387838, 439971, 495627, 551201), gdpPercap = c(375.6431231, 426.0964081, 582.8419714, 915.5960025, 672.4122571, 958.5668124, 927.8253427, 966.8968149, 1132.055034, 2814.480755, 7703.4959, 12154.08975))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Eritrea</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(35.928, 38.047, 40.158, 42.189, 44.142, 44.535, 43.89, 46.453, 49.991, 53.378, 55.24, 58.04), pop = c(1438760, 1542611, 1666618, 1820319, 2260187, 2512642, 2637297, 2915959, 3668440, 4058319, 4414865, 4906585), gdpPercap = c(328.9405571, 344.1618859, 380.9958433, 468.7949699, 514.3242082, 505.7538077, 524.8758493, 521.1341333, 582.8585102, 913.47079, 765.3500015, 641.3695236))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Ethiopia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(34.078, 36.667, 40.059, 42.115, 43.515, 44.51, 44.916, 46.684, 48.091, 49.402, 50.725, 52.947), pop = c(20860941, 22815614, 25145372, 27860297, 30770372, 34617799, 38111756, 42999530, 52088559, 59861301, 67946797, 76511887), gdpPercap = c(362.1462796, 378.9041632, 419.4564161, 516.1186438, 566.2439442, 556.8083834, 577.8607471, 573.7413142, 421.3534653, 515.8894013, 530.0535319, 690.8055759))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Finland</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(66.55, 67.49, 68.75, 69.83, 70.87, 72.52, 74.55, 74.83, 75.7, 77.13, 78.37, 79.313), pop = c(4090500, 4324000, 4491443, 4605744, 4639657, 4738902, 4826933, 4931729, 5041039, 5134406, 5193039, 5238460), gdpPercap = c(6424.519071, 7545.415386, 9371.842561, 10921.63626, 14358.8759, 15605.42283, 18533.15761, 21141.01223, 20647.16499, 23723.9502, 28204.59057, 33207.0844))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">France</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(67.41, 68.93, 70.51, 71.55, 72.38, 73.83, 74.89, 76.34, 77.46, 78.64, 79.59, 80.657), pop = c(42459667, 44310863, 47124000, 49569000, 51732000, 53165019, 54433565, 55630100, 57374179, 58623428, 59925035, 61083916), gdpPercap = c(7029.809327, 8662.834898, 10560.48553, 12999.91766, 16107.19171, 18292.63514, 20293.89746, 22066.44214, 24703.79615, 25889.78487, 28926.03234, 30470.0167))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Gabon</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.003, 38.999, 40.489, 44.598, 48.69, 52.79, 56.564, 60.19, 61.366, 60.461, 56.761, 56.735), pop = c(420702, 434904, 455661, 489004, 537977, 706367, 753874, 880397, 985739, 1126189, 1299304, 1454867), gdpPercap = c(4293.476475, 4976.198099, 6631.459222, 8358.761987, 11401.94841, 21745.57328, 15113.36194, 11864.40844, 13522.15752, 14722.84188, 12521.71392, 13206.48452))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Gambia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(30, 32.065, 33.896, 35.857, 38.308, 41.842, 45.58, 49.265, 52.644, 55.861, 58.041, 59.448), pop = c(284320, 323150, 374020, 439593, 517101, 608274, 715523, 848406, 1025384, 1235767, 1457766, 1688359), gdpPercap = c(485.2306591, 520.9267111, 599.650276, 734.7829124, 756.0868363, 884.7552507, 835.8096108, 611.6588611, 665.6244126, 653.7301704, 660.5855997, 752.7497265))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Germany</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(67.5, 69.1, 70.3, 70.8, 71, 72.5, 73.8, 74.847, 76.07, 77.34, 78.67, 79.406), pop = c(69145952, 71019069, 73739117, 76368453, 78717088, 78160773, 78335266, 77718298, 80597764, 82011073, 82350671, 82400996), gdpPercap = c(7144.114393, 10187.82665, 12902.46291, 14745.62561, 18016.18027, 20512.92123, 22031.53274, 24639.18566, 26505.30317, 27788.88416, 30035.80198, 32170.37442))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Ghana</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(43.149, 44.779, 46.452, 48.072, 49.875, 51.756, 53.744, 55.729, 57.501, 58.556, 58.453, 60.022), pop = c(5581001, 6391288, 7355248, 8490213, 9354120, 10538093, 11400338, 14168101, 16278738, 18418288, 20550751, 22873338), gdpPercap = c(911.2989371, 1043.561537, 1190.041118, 1125.69716, 1178.223708, 993.2239571, 876.032569, 847.0061135, 925.060154, 1005.245812, 1111.984578, 1327.60891))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Greece</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(65.86, 67.86, 69.51, 71, 72.34, 73.68, 75.24, 76.67, 77.03, 77.869, 78.256, 79.483), pop = c(7733250, 8096218, 8448233, 8716441, 8888628, 9308479, 9786480, 9974490, 10325429, 10502372, 10603863, 10706290), gdpPercap = c(3530.690067, 4916.299889, 6017.190733, 8513.097016, 12724.82957, 14195.52428, 15268.42089, 16120.52839, 17541.49634, 18747.69814, 22514.2548, 27538.41188))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Guatemala</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.023, 44.142, 46.954, 50.016, 53.738, 56.029, 58.137, 60.782, 63.373, 66.322, 68.978, 70.259), pop = c(3146381, 3640876, 4208858, 4690773, 5149581, 5703430, 6395630, 7326406, 8486949, 9803875, 11178650, 12572928), gdpPercap = c(2428.237769, 2617.155967, 2750.364446, 3242.531147, 4031.408271, 4879.992748, 4820.49479, 4246.485974, 4439.45084, 4684.313807, 4858.347495, 5186.050003))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Guinea</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(33.609, 34.558, 35.753, 37.197, 38.842, 40.762, 42.891, 45.552, 48.576, 51.455, 53.676, 56.007), pop = c(2664249, 2876726, 3140003, 3451418, 3811387, 4227026, 4710497, 5650262, 6990574, 8048834, 8807818, 9947814), gdpPercap = c(510.1964923, 576.2670245, 686.3736739, 708.7595409, 741.6662307, 874.6858643, 857.2503577, 805.5724718, 794.3484384, 869.4497668, 945.5835837, 942.6542111))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Guinea-Bissau</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(32.5, 33.489, 34.488, 35.492, 36.486, 37.465, 39.327, 41.245, 43.266, 44.873, 45.504, 46.388), pop = c(580653, 601095, 627820, 601287, 625361, 745228, 825987, 927524, 1050938, 1193708, 1332459, 1472041), gdpPercap = c(299.850319, 431.7904566, 522.0343725, 715.5806402, 820.2245876, 764.7259628, 838.1239671, 736.4153921, 745.5398706, 796.6644681, 575.7047176, 579.231743))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Haiti</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.579, 40.696, 43.59, 46.243, 48.042, 49.923, 51.461, 53.636, 55.089, 56.671, 58.137, 60.916), pop = c(3201488, 3507701, 3880130, 4318137, 4698301, 4908554, 5198399, 5756203, 6326682, 6913545, 7607651, 8502814), gdpPercap = c(1840.366939, 1726.887882, 1796.589032, 1452.057666, 1654.456946, 1874.298931, 2011.159549, 1823.015995, 1456.309517, 1341.726931, 1270.364932, 1201.637154))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Honduras</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(41.912, 44.665, 48.041, 50.924, 53.884, 57.402, 60.909, 64.492, 66.399, 67.659, 68.565, 70.198), pop = c(1517453, 1770390, 2090162, 2500689, 2965146, 3055235, 3669448, 4372203, 5077347, 5867957, 6677328, 7483763), gdpPercap = c(2194.926204, 2220.487682, 2291.156835, 2538.269358, 2529.842345, 3203.208066, 3121.760794, 3023.096699, 3081.694603, 3160.454906, 3099.72866, 3548.330846))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Hong Kong, China</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(60.96, 64.75, 67.65, 70, 72, 73.6, 75.45, 76.2, 77.601, 80, 81.495, 82.208), pop = c(2125900, 2736300, 3305200, 3722800, 4115700, 4583700, 5264500, 5584510, 5829696, 6495918, 6762476, 6980412), gdpPercap = c(3054.421209, 3629.076457, 4692.648272, 6197.962814, 8315.928145, 11186.14125, 14560.53051, 20038.47269, 24757.60301, 28377.63219, 30209.01516, 39724.97867))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Hungary</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(64.03, 66.41, 67.96, 69.5, 69.76, 69.95, 69.39, 69.58, 69.17, 71.04, 72.59, 73.338), pop = c(9504000, 9839000, 10063000, 10223422, 10394091, 10637171, 10705535, 10612740, 10348684, 10244684, 10083313, 9956108), gdpPercap = c(5263.673816, 6040.180011, 7550.359877, 9326.64467, 10168.65611, 11674.83737, 12545.99066, 12986.47998, 10535.62855, 11712.7768, 14843.93556, 18008.94444))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Iceland</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(72.49, 73.47, 73.68, 73.73, 74.46, 76.11, 76.99, 77.23, 78.77, 78.95, 80.5, 81.757), pop = c(147962, 165110, 182053, 198676, 209275, 221823, 233997, 244676, 259012, 271192, 288030, 301931), gdpPercap = c(7267.688428, 9244.001412, 10350.15906, 13319.89568, 15798.06362, 19654.96247, 23269.6075, 26923.20628, 25144.39201, 28061.09966, 31163.20196, 36180.78919))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">India</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.373, 40.249, 43.605, 47.193, 50.651, 54.208, 56.596, 58.553, 60.223, 61.765, 62.879, 64.698), pop = c(372000000, 409000000, 454000000, 506000000, 567000000, 634000000, 708000000, 788000000, 872000000, 959000000, 1034172547, 1110396331), gdpPercap = c(546.5657493, 590.061996, 658.3471509, 700.7706107, 724.032527, 813.337323, 855.7235377, 976.5126756, 1164.406809, 1458.817442, 1746.769454, 2452.210407<br>))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Indonesia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.468, 39.918, 42.518, 45.964, 49.203, 52.702, 56.159, 60.137, 62.681, 66.041, 68.588, 70.65), pop = c(82052000, 90124000, 99028000, 109343000, 121282000, 136725000, 153343000, 169276000, 184816000, 199278000, 211060000, 223547000), gdpPercap = c(749.6816546, 858.9002707, 849.2897701, 762.4317721, 1111.107907, 1382.702056, 1516.872988, 1748.356961, 2383.140898, 3119.335603, 2873.91287, 3540.651564))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Iran</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(44.869, 47.181, 49.325, 52.469, 55.234, 57.702, 59.62, 63.04, 65.742, 68.042, 69.451, 70.964), pop = c(17272000, 19792000, 22874000, 26538000, 30614000, 35480679, 43072751, 51889696, 60397973, 63327987, 66907826, 69453570), gdpPercap = c(3035.326002, 3290.257643, 4187.329802, 5906.731805, 9613.818607, 11888.59508, 7608.334602, 6642.881371, 7235.653188, 8263.590301, 9240.761975, 11605.71449))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Iraq</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(45.32, 48.437, 51.457, 54.459, 56.95, 60.413, 62.038, 65.044, 59.461, 58.811, 57.046, 59.545), pop = c(5441766, 6248643, 7240260, 8519282, 10061506, 11882916, 14173318, 16543189, 17861905, 20775703, 24001816, 27499638), gdpPercap = c(4129.766056, 6229.333562, 8341.737815, 8931.459811, 9576.037596, 14688.23507, 14517.90711, 11643.57268, 3745.640687, 3076.239795, 4390.717312, 4471.061906))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Ireland</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(66.91, 68.9, 70.29, 71.08, 71.28, 72.03, 73.1, 74.36, 75.467, 76.122, 77.783, 78.885), pop = c(2952156, 2878220, 2830000, 2900100, 3024400, 3271900, 3480000, 3539900, 3557761, 3667233, 3879155, 4109086), gdpPercap = c(5210.280328, 5599.077872, 6631.597314, 7655.568963, 9530.772896, 11150.98113, 12618.32141, 13872.86652, 17558.81555, 24521.94713, 34077.04939, 40675.99635))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Israel</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(65.39, 67.84, 69.39, 70.75, 71.63, 73.06, 74.45, 75.6, 76.93, 78.269, 79.696, 80.745), pop = c(1620914, 1944401, 2310904, 2693585, 3095893, 3495918, 3858421, 4203148, 4936550, 5531387, 6029529, 6426679), gdpPercap = c(4086.522128, 5385.278451, 7105.630706, 8393.741404, 12786.93223, 13306.61921, 15367.0292, 17122.47986, 18051.52254, 20896.60924, 21905.59514, 25523.2771))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Italy</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(65.94, 67.81, 69.24, 71.06, 72.19, 73.48, 74.98, 76.42, 77.44, 78.82, 80.24, 80.546), pop = c(47666000, 49182000, 50843200, 52667100, 54365564, 56059245, 56535636, 56729703, 56840847, 57479469, 57926999, 58147733), gdpPercap = c(4931.404155, 6248.656232, 8243.58234, 10022.40131, 12269.27378, 14255.98475, 16537.4835, 19207.23482, 22013.64486, 24675.02446, 27968.09817, 28569.7197))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Jamaica</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(58.53, 62.61, 65.61, 67.51, 69, 70.11, 71.21, 71.77, 71.766, 72.262, 72.047, 72.567), pop = c(1426095, 1535090, 1665128, 1861096, 1997616, 2156814, 2298309, 2326606, 2378618, 2531311, 2664659, 2780132), gdpPercap = c(2898.530881, 4756.525781, 5246.107524, 6124.703451, 7433.889293, 6650.195573, 6068.05135, 6351.237495, 7404.923685, 7121.924704, 6994.774861, 7320.880262))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Japan</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(63.03, 65.5, 68.73, 71.43, 73.42, 75.38, 77.11, 78.67, 79.36, 80.69, 82, 82.603), pop = c(86459025, 91563009, 95831757, 100825279, 107188273, 113872473, 118454974, 122091325, 124329269, 125956499, 127065841, 127467972), gdpPercap = c(3216.956347, 4317.694365, 6576.649461, 9847.788607, 14778.78636, 16610.37701, 19384.10571, 22375.94189, 26824.89511, 28816.58499, 28604.5919, 31656.06806))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Jordan</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(43.158, 45.669, 48.126, 51.629, 56.528, 61.134, 63.739, 65.869, 68.015, 69.772, 71.263, 72.535), pop = c(607914, 746559, 933559, 1255058, 1613551, 1937652, 2347031, 2820042, 3867409, 4526235, 5307470, 6053193), gdpPercap = c(1546.907807, 1886.080591, 2348.009158, 2741.796252, 2110.856309, 2852.351568, 4161.415959, 4448.679912, 3431.593647, 3645.379572, 3844.917194, 4519.461171))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Kenya</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.27, 44.686, 47.949, 50.654, 53.559, 56.155, 58.766, 59.339, 59.285, 54.407, 50.992, 54.11), pop = c(6464046, 7454779, 8678557, 10191512, 12044785, 14500404, 17661452, 21198082, 25020539, 28263827, 31386842, 35610177), gdpPercap = c(853.540919, 944.4383152, 896.9663732, 1056.736457, 1222.359968, 1267.613204, 1348.225791, 1361.936856, 1341.921721, 1360.485021, 1287.514732, 1463.249282))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Korea, Dem. Rep.</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(50.056, 54.081, 56.656, 59.942, 63.983, 67.159, 69.1, 70.647, 69.978, 67.727, 66.662, 67.297), pop = c(8865488, 9411381, 10917494, 12617009, 14781241, 16325320, 17647518, 19067554, 20711375, 21585105, 22215365, 23301725), gdpPercap = c(1088.277758, 1571.134655, 1621.693598, 2143.540609, 3701.621503, 4106.301249, 4106.525293, 4106.492315, 3726.063507, 1690.756814, 1646.758151, 1593.06548))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Korea, Rep.</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(47.453, 52.681, 55.292, 57.716, 62.612, 64.766, 67.123, 69.81, 72.244, 74.647, 77.045, 78.623), pop = c(20947571, 22611552, 26420307, 30131000, 33505000, 36436000, 39326000, 41622000, 43805450, 46173816, 47969150, 49044790), gdpPercap = c(1030.592226, 1487.593537, 1536.344387, 2029.228142, 3030.87665, 4657.22102, 5622.942464, 8533.088805, 12104.27872, 15993.52796, 19233.98818, 23348.13973))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Kuwait</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(55.565, 58.033, 60.47, 64.624, 67.712, 69.343, 71.309, 74.174, 75.19, 76.156, 76.904, 77.588), pop = c(160000, 212846, 358266, 575003, 841934, 1140357, 1497494, 1891487, 1418095, 1765345, 2111561, 2505559), gdpPercap = c(108382.3529, 113523.1329, 95458.11176, 80894.88326, 109347.867, 59265.47714, 31354.03573, 28118.42998, 34932.91959, 40300.61996, 35110.10566, 47306.98978))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Lebanon</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(55.928, 59.489, 62.094, 63.87, 65.421, 66.099, 66.983, 67.926, 69.292, 70.265, 71.028, 71.993), pop = c(1439529, 1647412, 1886848, 2186894, 2680018, 3115787, 3086876, 3089353, 3219994, 3430388, 3677780, 3921278), gdpPercap = c(4834.804067, 6089.786934, 5714.560611, 6006.983042, 7486.384341, 8659.696836, 7640.519521, 5377.091329, 6890.806854, 8754.96385, 9313.93883, 10461.05868))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Lesotho</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.138, 45.047, 47.747, 48.492, 49.767, 52.208, 55.078, 57.18, 59.685, 55.558, 44.593, 42.592), pop = c(748747, 813338, 893143, 996380, 1116779, 1251524, 1411807, 1599200, 1803195, 1982823, 2046772, 2012649), gdpPercap = c(298.8462121, 335.9971151, 411.8006266, 498.6390265, 496.5815922, 745.3695408, 797.2631074, 773.9932141, 977.4862725, 1186.147994, 1275.184575, 1569.331442))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Liberia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(38.48, 39.486, 40.502, 41.536, 42.614, 43.764, 44.852, 46.027, 40.802, 42.221, 43.753, 45.678), pop = c(863308, 975950, 1112796, 1279406, 1482628, 1703617, 1956875, 2269414, 1912974, 2200725, 2814651, 3193942), gdpPercap = c(575.5729961, 620.9699901, 634.1951625, 713.6036483, 803.0054535, 640.3224383, 572.1995694, 506.1138573, 636.6229191, 609.1739508, 531.4823679, 414.5073415))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Libya</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.723, 45.289, 47.808, 50.227, 52.773, 57.442, 62.155, 66.234, 68.755, 71.555, 72.737, 73.952), pop = c(1019729, 1201578, 1441863, 1759224, 2183877, 2721783, 3344074, 3799845, 4364501, 4759670, 5368585, 6036914), gdpPercap = c(2387.54806, 3448.284395, 6757.030816, 18772.75169, 21011.49721, 21951.21176, 17364.27538, 11770.5898, 9640.138501, 9467.446056, 9534.677467, 12057.49928))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Madagascar</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(36.681, 38.865, 40.848, 42.881, 44.851, 46.881, 48.969, 49.35, 52.214, 54.978, 57.286, 59.443), pop = c(4762912, 5181679, 5703324, 6334556, 7082430, 8007166, 9171477, 10568642, 12210395, 14165114, 16473477, 19167654), gdpPercap = c(1443.011715, 1589.20275, 1643.38711, 1634.047282, 1748.562982, 1544.228586, 1302.878658, 1155.441948, 1040.67619, 986.2958956, 894.6370822, 1044.770126))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Malawi</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(36.256, 37.207, 38.41, 39.487, 41.766, 43.767, 45.642, 47.457, 49.42, 47.495, 45.009, 48.303), pop = c(2917802, 3221238, 3628608, 4147252, 4730997, 5637246, 6502825, 7824747, 10014249, 10419991, 11824495, 13327079), gdpPercap = c(369.1650802, 416.3698064, 427.9010856, 495.5147806, 584.6219709, 663.2236766, 632.8039209, 635.5173634, 563.2000145, 692.2758103, 665.4231186, 759.3499101))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Malaysia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(48.463, 52.102, 55.737, 59.371, 63.01, 65.256, 68, 69.5, 70.693, 71.938, 73.044, 74.241), pop = c(6748378, 7739235, 8906385, 10154878, 11441462, 12845381, 14441916, 16331785, 18319502, 20476091, 22662365, 24821286), gdpPercap = c(1831.132894, 1810.066992, 2036.884944, 2277.742396, 2849.09478, 3827.921571, 4920.355951, 5249.802653, 7277.912802, 10132.90964, 10206.97794, 12451.6558))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Mali</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(33.685, 35.307, 36.936, 38.487, 39.977, 41.714, 43.916, 46.364, 48.388, 49.903, 51.818, 54.467), pop = c(3838168, 4241884, 4690372, 5212416, 5828158, 6491649, 6998256, 7634008, 8416215, 9384984, 10580176, 12031795), gdpPercap = c(452.3369807, 490.3821867, 496.1743428, 545.0098873, 581.3688761, 686.3952693, 618.0140641, 684.1715576, 739.014375, 790.2579846, 951.4097518, 1042.581557))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Mauritania</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(40.543, 42.338, 44.248, 46.289, 48.437, 50.852, 53.599, 56.145, 58.333, 60.43, 62.247, 64.164), pop = c(1022556, 1076852, 1146757, 1230542, 1332786, 1456688, 1622136, 1841240, 2119465, 2444741, 2828858, 3270065), gdpPercap = c(743.1159097, 846.1202613, 1055.896036, 1421.145193, 1586.851781, 1497.492223, 1481.150189, 1421.603576, 1361.369784, 1483.136136, 1579.019543, 1803.151496))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Mauritius</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(50.986, 58.089, 60.246, 61.557, 62.944, 64.93, 66.711, 68.74, 69.745, 70.736, 71.954, 72.801), pop = c(516556, 609816, 701016, 789309, 851334, 913025, 992040, 1042663, 1096202, 1149818, 1200206, 1250882), gdpPercap = c(1967.955707, 2034.037981, 2529.067487, 2475.387562, 2575.484158, 3710.982963, 3688.037739, 4783.586903, 6058.253846, 7425.705295, 9021.815894, 10956.99112))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Mexico</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(50.789, 55.19, 58.299, 60.11, 62.361, 65.032, 67.405, 69.498, 71.455, 73.67, 74.902, 76.195), pop = c(30144317, 35015548, 41121485, 47995559, 55984294, 63759976, 71640904, 80122492, 88111030, 95895146, 102479927, 108700891), gdpPercap = c(3478.125529, 4131.546641, 4581.609385, 5754.733883, 6809.40669, 7674.929108, 9611.147541, 8688.156003, 9472.384295, 9767.29753, 10742.44053, 11977.57496))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Mongolia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.244, 45.248, 48.251, 51.253, 53.754, 55.491, 57.489, 60.222, 61.271, 63.625, 65.033, 66.803), pop = c(800663, 882134, 1010280, 1149500, 1320500, 1528000, 1756032, 2015133, 2312802, 2494803, 2674234, 2874127), gdpPercap = c(786.5668575, 912.6626085, 1056.353958, 1226.04113, 1421.741975, 1647.511665, 2000.603139, 2338.008304, 1785.402016, 1902.2521, 2140.739323, 3095.772271))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Montenegro</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(59.164, 61.448, 63.728, 67.178, 70.636, 73.066, 74.101, 74.865, 75.435, 75.445, 73.981, 74.543), pop = c(413834, 442829, 474528, 501035, 527678, 560073, 562548, 569473, 621621, 692651, 720230, 684736), gdpPercap = c(2647.585601, 3682.259903, 4649.593785, 5907.850937, 7778.414017, 9595.929905, 11222.58762, 11732.51017, 7003.339037, 6465.613349, 6557.194282, 9253.896111))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Morocco</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.873, 45.423, 47.924, 50.335, 52.862, 55.73, 59.65, 62.677, 65.393, 67.66, 69.615, 71.164), pop = c(9939217, 11406350, 13056604, 14770296, 16660670, 18396941, 20198730, 22987397, 25798239, 28529501, 31167783, 33757175), gdpPercap = c(1688.20357, 1642.002314, 1566.353493, 1711.04477, 1930.194975, 2370.619976, 2702.620356, 2755.046991, 2948.047252, 2982.101858, 3258.495584, 3820.17523))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Mozambique</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(31.286, 33.779, 36.161, 38.113, 40.328, 42.495, 42.795, 42.861, 44.284, 46.344, 44.026, 42.082), pop = c(6446316, 7038035, 7788944, 8680909, 9809596, 11127868, 12587223, 12891952, 13160731, 16603334, 18473780, 19951656), gdpPercap = c(468.5260381, 495.5868333, 556.6863539, 566.6691539, 724.9178037, 502.3197334, 462.2114149, 389.8761846, 410.8968239, 472.3460771, 633.6179466, 823.6856205))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Myanmar</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(36.319, 41.905, 45.108, 49.379, 53.07, 56.059, 58.056, 58.339, 59.32, 60.328, 59.908, 62.069), pop = c(20092996, 21731844, 23634436, 25870271, 28466390, 31528087, 34680442, 38028578, 40546538, 43247867, 45598081, 47761980), gdpPercap = c(331, 350, 388, 349, 357, 371, 424, 385, 347, 415, 611, 944))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Namibia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(41.725, 45.226, 48.386, 51.159, 53.867, 56.437, 58.968, 60.835, 61.999, 58.909, 51.479, 52.906), pop = c(485831, 548080, 621392, 706640, 821782, 977026, 1099010, 1278184, 1554253, 1774766, 1972153, 2055080), gdpPercap = c(2423.780443, 2621.448058, 3173.215595, 3793.694753, 3746.080948, 3876.485958, 4191.100511, 3693.731337, 3804.537999, 3899.52426, 4072.324751, 4811.060429))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Nepal</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(36.157, 37.686, 39.393, 41.472, 43.971, 46.748, 49.594, 52.537, 55.727, 59.426, 61.34, 63.785), pop = c(9182536, 9682338, 10332057, 11261690, 12412593, 13933198, 15796314, 17917180, 20326209, 23001113, 25873917, 28901790), gdpPercap = c(545.8657229, 597.9363558, 652.3968593, 676.4422254, 674.7881296, 694.1124398, 718.3730947, 775.6324501, 897.7403604, 1010.892138, 1057.206311, 1091.359778))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Netherlands</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(72.13, 72.99, 73.23, 73.82, 73.75, 75.24, 76.05, 76.83, 77.42, 78.03, 78.53, 79.762), pop = c(10381988, 11026383, 11805689, 12596822, 13329874, 13852989, 14310401, 14665278, 15174244, 15604464, 16122830, 16570613), gdpPercap = c(8941.571858, 11276.19344, 12790.84956, 15363.25136, 18794.74567, 21209.0592, 21399.46046, 23651.32361, 26790.94961, 30246.13063, 33724.75778, 36797.93332))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">New Zealand</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Oceania</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(69.39, 70.26, 71.24, 71.52, 71.89, 72.22, 73.84, 74.32, 76.33, 77.55, 79.11, 80.204), pop = c(1994794, 2229407, 2488550, 2728150, 2929100, 3164900, 3210650, 3317166, 3437674, 3676187, 3908037, 4115771), gdpPercap = c(10556.57566, 12247.39532, 13175.678, 14463.91893, 16046.03728, 16233.7177, 17632.4104, 19007.19129, 18363.32494, 21050.41377, 23189.80135, 25185.00911))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Nicaragua</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.314, 45.432, 48.632, 51.884, 55.151, 57.47, 59.298, 62.008, 65.843, 68.426, 70.836, 72.899), pop = c(1165790, 1358828, 1590597, 1865490, 2182908, 2554598, 2979423, 3344353, 4017939, 4609572, 5146848, 5675356), gdpPercap = c(3112.363948, 3457.415947, 3634.364406, 4643.393534, 4688.593267, 5486.371089, 3470.338156, 2955.984375, 2170.151724, 2253.023004, 2474.548819, 2749.320965))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Niger</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.444, 38.598, 39.487, 40.118, 40.546, 41.291, 42.598, 44.555, 47.391, 51.313, 54.496, 56.867), pop = c(3379468, 3692184, 4076008, 4534062, 5060262, 5682086, 6437188, 7332638, 8392818, 9666252, 11140655, 12894865), gdpPercap = c(761.879376, 835.5234025, 997.7661127, 1054.384891, 954.2092363, 808.8970728, 909.7221354, 668.3000228, 581.182725, 580.3052092, 601.0745012, 619.6768924))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Nigeria</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(36.324, 37.802, 39.36, 41.04, 42.821, 44.514, 45.826, 46.886, 47.472, 47.464, 46.608, 46.859), pop = c(33119096, 37173340, 41871351, 47287752, 53740085, 62209173, 73039376, 81551520, 93364244, 106207839, 119901274, 135031164), gdpPercap = c(1077.281856, 1100.592563, 1150.927478, 1014.514104, 1698.388838, 1981.951806, 1576.97375, 1385.029563, 1619.848217, 1624.941275, 1615.286395, 2013.977305))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Norway</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(72.67, 73.44, 73.47, 74.08, 74.34, 75.37, 75.97, 75.89, 77.32, 78.32, 79.05, 80.196), pop = c(3327728, 3491938, 3638919, 3786019, 3933004, 4043205, 4114787, 4186147, 4286357, 4405672, 4535591, 4627926), gdpPercap = c(10095.42172, 11653.97304, 13450.40151, 16361.87647, 18965.05551, 23311.34939, 26298.63531, 31540.9748, 33965.66115, 41283.16433, 44683.97525, 49357.19017))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Oman</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.578, 40.08, 43.165, 46.988, 52.143, 57.367, 62.728, 67.734, 71.197, 72.499, 74.193, 75.64), pop = c(507833, 561977, 628164, 714775, 829050, 1004533, 1301048, 1593882, 1915208, 2283635, 2713462, 3204897), gdpPercap = c(1828.230307, 2242.746551, 2924.638113, 4720.942687, 10618.03855, 11848.34392, 12954.79101, 18115.22313, 18616.70691, 19702.05581, 19774.83687, 22316.19287))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Pakistan</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(43.436, 45.557, 47.67, 49.8, 51.929, 54.043, 56.158, 58.245, 60.838, 61.818, 63.61, 65.483), pop = c(41346560, 46679944, 53100671, 60641899, 69325921, 78152686, 91462088, 105186881, 120065004, 135564834, 153403524, 169270617), gdpPercap = c(684.5971438, 747.0835292, 803.3427418, 942.4082588, 1049.938981, 1175.921193, 1443.429832, 1704.686583, 1971.829464, 2049.350521, 2092.712441, 2605.94758))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Panama</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(55.191, 59.201, 61.817, 64.071, 66.216, 68.681, 70.472, 71.523, 72.462, 73.738, 74.712, 75.537), pop = c(940080, 1063506, 1215725, 1405486, 1616384, 1839782, 2036305, 2253639, 2484997, 2734531, 2990875, 3242173), gdpPercap = c(2480.380334, 2961.800905, 3536.540301, 4421.009084, 5364.249663, 5351.912144, 7009.601598, 7034.779161, 6618.74305, 7113.692252, 7356.031934, 9809.185636))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Paraguay</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(62.649, 63.196, 64.361, 64.951, 65.815, 66.353, 66.874, 67.378, 68.225, 69.4, 70.755, 71.752), pop = c(1555876, 1770902, 2009813, 2287985, 2614104, 2984494, 3366439, 3886512, 4483945, 5154123, 5884491, 6667147), gdpPercap = c(1952.308701, 2046.154706, 2148.027146, 2299.376311, 2523.337977, 3248.373311, 4258.503604, 3998.875695, 4196.411078, 4247.400261, 3783.674243, 4172.838464))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Peru</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(43.902, 46.263, 49.096, 51.445, 55.448, 58.447, 61.406, 64.134, 66.458, 68.386, 69.906, 71.421), pop = c(8025700, 9146100, 10516500, 12132200, 13954700, 15990099, 18125129, 20195924, 22430449, 24748122, 26769436, 28674757), gdpPercap = c(3758.523437, 4245.256698, 4957.037982, 5788.09333, 5937.827283, 6281.290855, 6434.501797, 6360.943444, 4446.380924, 5838.347657, 5909.020073, 7408.905561))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Philippines</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(47.752, 51.334, 54.757, 56.393, 58.065, 60.06, 62.082, 64.151, 66.458, 68.564, 70.303, 71.688), pop = c(22438691, 26072194, 30325264, 35356600, 40850141, 46850962, 53456774, 60017788, 67185766, 75012988, 82995088, 91077287), gdpPercap = c(1272.880995, 1547.944844, 1649.552153, 1814.12743, 1989.37407, 2373.204287, 2603.273765, 2189.634995, 2279.324017, 2536.534925, 2650.921068, 3190.481016))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Poland</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(61.31, 65.77, 67.64, 69.61, 70.85, 70.67, 71.32, 70.98, 70.99, 72.75, 74.67, 75.563), pop = c(25730551, 28235346, 30329617, 31785378, 33039545, 34621254, 36227381, 37740710, 38370697, 38654957, 38625976, 38518241), gdpPercap = c(4029.329699, 4734.253019, 5338.752143, 6557.152776, 8006.506993, 9508.141454, 8451.531004, 9082.351172, 7738.881247, 10159.58368, 12002.23908, 15389.92468))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Portugal</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(59.82, 61.51, 64.39, 66.6, 69.26, 70.41, 72.77, 74.06, 74.86, 75.97, 77.29, 78.098), pop = c(8526050, 8817650, 9019800, 9103000, 8970450, 9662600, 9859650, 9915289, 9927680, 10156415, 10433867, 10642836), gdpPercap = c(3068.319867, 3774.571743, 4727.954889, 6361.517993, 9022.247417, 10172.48572, 11753.84291, 13039.30876, 16207.26663, 17641.03156, 19970.90787, 20509.64777))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Puerto Rico</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(64.28, 68.54, 69.62, 71.1, 72.16, 73.44, 73.75, 74.63, 73.911, 74.917, 77.778, 78.746), pop = c(2227000, 2260000, 2448046, 2648961, 2847132, 3080828, 3279001, 3444468, 3585176, 3759430, 3859606, 3942491), gdpPercap = c(3081.959785, 3907.156189, 5108.34463, 6929.277714, 9123.041742, 9770.524921, 10330.98915, 12281.34191, 14641.58711, 16999.4333, 18855.60618, 19328.70901))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Reunion</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(52.724, 55.09, 57.666, 60.542, 64.274, 67.064, 69.885, 71.913, 73.615, 74.772, 75.744, 76.442), pop = c(257700, 308700, 358900, 414024, 461633, 492095, 517810, 562035, 622191, 684810, 743981, 798094), gdpPercap = c(2718.885295, 2769.451844, 3173.72334, 4021.175739, 5047.658563, 4319.804067, 5267.219353, 5303.377488, 6101.255823, 6071.941411, 6316.1652, 7670.122558))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Romania</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(61.05, 64.1, 66.8, 66.8, 69.21, 69.46, 69.66, 69.53, 69.36, 69.72, 71.322, 72.476), pop = c(16630000, 17829327, 18680721, 19284814, 20662648, 21658597, 22356726, 22686371, 22797027, 22562458, 22404337, 22276056), gdpPercap = c(3144.613186, 3943.370225, 4734.997586, 6470.866545, 8011.414402, 9356.39724, 9605.314053, 9696.273295, 6598.409903, 7346.547557, 7885.360081, 10808.47561))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Rwanda</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(40, 41.5, 43, 44.1, 44.6, 45, 46.218, 44.02, 23.599, 36.087, 43.413, 46.242), pop = c(2534927, 2822082, 3051242, 3451079, 3992121, 4657072, 5507565, 6349365, 7290203, 7212583, 7852401, 8860588), gdpPercap = c(493.3238752, 540.2893983, 597.4730727, 510.9637142, 590.5806638, 670.0806011, 881.5706467, 847.991217, 737.0685949, 589.9445051, 785.6537648, 863.0884639))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Sao Tome and Principe</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(46.471, 48.945, 51.893, 54.425, 56.48, 58.55, 60.351, 61.728, 62.742, 63.306, 64.337, 65.528), pop = c(60011, 61325, 65345, 70787, 76595, 86796, 98593, 110812, 125911, 145608, 170372, 199579), gdpPercap = c(879.5835855, 860.7369026, 1071.551119, 1384.840593, 1532.985254, 1737.561657, 1890.218117, 1516.525457, 1428.777814, 1339.076036, 1353.09239, 1598.435089))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Saudi Arabia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(39.875, 42.868, 45.914, 49.901, 53.886, 58.69, 63.012, 66.295, 68.768, 70.533, 71.626, 72.777), pop = c(4005677, 4419650, 4943029, 5618198, 6472756, 8128505, 11254672, 14619745, 16945857, 21229759, 24501530, 27601038), gdpPercap = c(6459.554823, 8157.591248, 11626.41975, 16903.04886, 24837.42865, 34167.7626, 33693.17525, 21198.26136, 24841.61777, 20586.69019, 19014.54118, 21654.83194))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Senegal</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(37.278, 39.329, 41.454, 43.563, 45.815, 48.879, 52.379, 55.769, 58.196, 60.187, 61.6, 63.062), pop = c(2755589, 3054547, 3430243, 3965841, 4588696, 5260855, 6147783, 7171347, 8307920, 9535314, 10870037, 12267493), gdpPercap = c(1450.356983, 1567.653006, 1654.988723, 1612.404632, 1597.712056, 1561.769116, 1518.479984, 1441.72072, 1367.899369, 1392.368347, 1519.635262, 1712.472136))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Serbia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(57.996, 61.685, 64.531, 66.914, 68.7, 70.3, 70.162, 71.218, 71.659, 72.232, 73.213, 74.002), pop = c(6860147, 7271135, 7616060, 7971222, 8313288, 8686367, 9032824, 9230783, 9826397, 10336594, 10111559, 10150265), gdpPercap = c(3581.459448, 4981.090891, 6289.629157, 7991.707066, 10522.06749, 12980.66956, 15181.0927, 15870.87851, 9325.068238, 7914.320304, 7236.075251, 9786.534714))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Sierra Leone</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(30.331, 31.57, 32.767, 34.113, 35.4, 36.788, 38.445, 40.006, 38.333, 39.897, 41.012, 42.568), pop = c(2143249, 2295678, 2467895, 2662190, 2879013, 3140897, 3464522, 3868905, 4260884, 4578212, 5359092, 6144562), gdpPercap = c(879.7877358, 1004.484437, 1116.639877, 1206.043465, 1353.759762, 1348.285159, 1465.010784, 1294.447788, 1068.696278, 574.6481576, 699.489713, 862.5407561))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Singapore</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(60.396, 63.179, 65.798, 67.946, 69.521, 70.795, 71.76, 73.56, 75.788, 77.158, 78.77, 79.972), pop = c(1127000, 1445929, 1750200, 1977600, 2152400, 2325300, 2651869, 2794552, 3235865, 3802309, 4197776, 4553009), gdpPercap = c(2315.138227, 2843.104409, 3674.735572, 4977.41854, 8597.756202, 11210.08948, 15169.16112, 18861.53081, 24769.8912, 33519.4766, 36023.1054, 47143.17964))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Slovak Republic</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(64.36, 67.45, 70.33, 70.98, 70.35, 70.45, 70.8, 71.08, 71.38, 72.71, 73.8, 74.663), pop = c(3558137, 3844277, 4237384, 4442238, 4593433, 4827803, 5048043, 5199318, 5302888, 5383010, 5410052, 5447502), gdpPercap = c(5074.659104, 6093.26298, 7481.107598, 8412.902397, 9674.167626, 10922.66404, 11348.54585, 12037.26758, 9498.467723, 12126.23065, 13638.77837, 18678.31435))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Slovenia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(65.57, 67.85, 69.15, 69.18, 69.82, 70.97, 71.063, 72.25, 73.64, 75.13, 76.66, 77.926), pop = c(1489518, 1533070, 1582962, 1646912, 1694510, 1746919, 1861252, 1945870, 1999210, 2011612, 2011497, 2009245), gdpPercap = c(4215.041741, 5862.276629, 7402.303395, 9405.489397, 12383.4862, 15277.03017, 17866.72175, 18678.53492, 14214.71681, 17161.10735, 20660.01936, 25768.25759))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Somalia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(32.978, 34.977, 36.981, 38.977, 40.973, 41.974, 42.955, 44.501, 39.658, 43.795, 45.936, 48.159), pop = c(2526994, 2780415, 3080153, 3428839, 3840161, 4353666, 5828892, 6921858, 6099799, 6633514, 7753310, 9118773), gdpPercap = c(1135.749842, 1258.147413, 1369.488336, 1284.73318, 1254.576127, 1450.992513, 1176.807031, 1093.244963, 926.9602964, 930.5964284, 882.0818218, 926.1410683))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">South Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(45.009, 47.985, 49.951, 51.927, 53.696, 55.527, 58.161, 60.834, 61.888, 60.236, 53.365, 49.339), pop = c(14264935, 16151549, 18356657, 20997321, 23935810, 27129932, 31140029, 35933379, 39964159, 42835005, 44433622, 43997828), gdpPercap = c(4725.295531, 5487.104219, 5768.729717, 7114.477971, 7765.962636, 8028.651439, 8568.266228, 7825.823398, 7225.069258, 7479.188244, 7710.946444, 9269.657808))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Spain</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(64.94, 66.66, 69.69, 71.44, 73.06, 74.39, 76.3, 76.9, 77.57, 78.77, 79.78, 80.941), pop = c(28549870, 29841614, 31158061, 32850275, 34513161, 36439000, 37983310, 38880702, 39549438, 39855442, 40152517, 40448191), gdpPercap = c(3834.034742, 4564.80241, 5693.843879, 7993.512294, 10638.75131, 13236.92117, 13926.16997, 15764.98313, 18603.06452, 20445.29896, 24835.47166, 28821.0637))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Sri Lanka</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(57.593, 61.456, 62.192, 64.266, 65.042, 65.949, 68.757, 69.011, 70.379, 70.457, 70.815, 72.396), pop = c(7982342, 9128546, 10421936, 11737396, 13016733, 14116836, 15410151, 16495304, 17587060, 18698655, 19576783, 20378239), gdpPercap = c(1083.53203, 1072.546602, 1074.47196, 1135.514326, 1213.39553, 1348.775651, 1648.079789, 1876.766827, 2153.739222, 2664.477257, 3015.378833, 3970.095407))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Sudan</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(38.635, 39.624, 40.87, 42.858, 45.083, 47.8, 50.338, 51.744, 53.556, 55.373, 56.369, 58.556), pop = c(8504667, 9753392, 11183227, 12716129, 14597019, 17104986, 20367053, 24725960, 28227588, 32160729, 37090298, 42292929), gdpPercap = c(1615.991129, 1770.337074, 1959.593767, 1687.997641, 1659.652775, 2202.988423, 1895.544073, 1507.819159, 1492.197043, 1632.210764, 1993.398314, 2602.394995))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Swaziland</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(41.407, 43.424, 44.992, 46.633, 49.552, 52.537, 55.561, 57.678, 58.474, 54.289, 43.869, 39.613), pop = c(290243, 326741, 370006, 420690, 480105, 551425, 649901, 779348, 962344, 1054486, 1130269, 1133066), gdpPercap = c(1148.376626, 1244.708364, 1856.182125, 2613.101665, 3364.836625, 3781.410618, 3895.384018, 3984.839812, 3553.0224, 3876.76846, 4128.116943, 4513.480643))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Sweden</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(71.86, 72.49, 73.37, 74.16, 74.72, 75.44, 76.42, 77.19, 78.16, 79.39, 80.04, 80.884), pop = c(7124673, 7363802, 7561588, 7867931, 8122293, 8251648, 8325260, 8421403, 8718867, 8897619, 8954175, 9031088), gdpPercap = c(8527.844662, 9911.878226, 12329.44192, 15258.29697, 17832.02464, 18855.72521, 20667.38125, 23586.92927, 23880.01683, 25266.59499, 29341.63093, 33859.74835))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Switzerland</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(69.62, 70.56, 71.32, 72.77, 73.78, 75.39, 76.21, 77.41, 78.03, 79.37, 80.62, 81.701), pop = c(4815000, 5126000, 5666000, 6063000, 6401400, 6316424, 6468126, 6649942, 6995447, 7193761, 7361757, 7554661), gdpPercap = c(14734.23275, 17909.48973, 20431.0927, 22966.14432, 27195.11304, 26982.29052, 28397.71512, 30281.70459, 31871.5303, 32135.32301, 34480.95771, 37506.41907))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Syria</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(45.883, 48.284, 50.305, 53.655, 57.296, 61.195, 64.59, 66.974, 69.249, 71.527, 73.053, 74.143), pop = c(3661549, 4149908, 4834621, 5680812, 6701172, 7932503, 9410494, 11242847, 13219062, 15081016, 17155814, 19314747), gdpPercap = c(1643.485354, 2117.234893, 2193.037133, 1881.923632, 2571.423014, 3195.484582, 3761.837715, 3116.774285, 3340.542768, 4014.238972, 4090.925331, 4184.548089))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Taiwan</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(58.5, 62.4, 65.2, 67.5, 69.39, 70.59, 72.16, 73.4, 74.26, 75.25, 76.99, 78.4), pop = c(8550362, 10164215, 11918938, 13648692, 15226039, 16785196, 18501390, 19757799, 20686918, 21628605, 22454239, 23174294), gdpPercap = c(1206.947913, 1507.86129, 1822.879028, 2643.858681, 4062.523897, 5596.519826, 7426.354774, 11054.56175, 15215.6579, 20206.82098, 23235.42329, 28718.27684))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Tanzania</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(41.215, 42.974, 44.246, 45.757, 47.62, 49.919, 50.608, 51.535, 50.44, 48.466, 49.651, 52.517), pop = c(8322925, 9452826, 10863958, 12607312, 14706593, 17129565, 19844382, 23040630, 26605473, 30686889, 34593779, 38139640), gdpPercap = c(716.6500721, 698.5356073, 722.0038073, 848.2186575, 915.9850592, 962.4922932, 874.2426069, 831.8220794, 825.682454, 789.1862231, 899.0742111, 1107.482182))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Thailand</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(50.848, 53.63, 56.061, 58.285, 60.405, 62.494, 64.597, 66.084, 67.298, 67.521, 68.564, 70.616), pop = c(21289402, 25041917, 29263397, 34024249, 39276153, 44148285, 48827160, 52910342, 56667095, 60216677, 62806748, 65068149), gdpPercap = c(757.7974177, 793.5774148, 1002.199172, 1295.46066, 1524.358936, 1961.224635, 2393.219781, 2982.653773, 4616.896545, 5852.625497, 5913.187529, 7458.396327))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Togo</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(38.596, 41.208, 43.922, 46.769, 49.759, 52.887, 55.471, 56.941, 58.061, 58.39, 57.561, 58.42), pop = c(1219113, 1357445, 1528098, 1735550, 2056351, 2308582, 2644765, 3154264, 3747553, 4320890, 4977378, 5701579), gdpPercap = c(859.8086567, 925.9083202, 1067.53481, 1477.59676, 1649.660188, 1532.776998, 1344.577953, 1202.201361, 1034.298904, 982.2869243, 886.2205765, 882.9699438))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Trinidad and Tobago</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(59.1, 61.8, 64.9, 65.4, 65.9, 68.3, 68.832, 69.582, 69.862, 69.465, 68.976, 69.819), pop = c(662850, 764900, 887498, 960155, 975199, 1039009, 1116479, 1191336, 1183669, 1138101, 1101832, 1056608), gdpPercap = c(3023.271928, 4100.3934, 4997.523971, 5621.368472, 6619.551419, 7899.554209, 9119.528607, 7388.597823, 7370.990932, 8792.573126, 11460.60023, 18008.50924))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Tunisia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(44.6, 47.1, 49.579, 52.053, 55.602, 59.837, 64.048, 66.894, 70.001, 71.973, 73.042, 73.923), pop = c(3647735, 3950849, 4286552, 4786986, 5303507, 6005061, 6734098, 7724976, 8523077, 9231669, 9770575, 10276158), gdpPercap = c(1468.475631, 1395.232468, 1660.30321, 1932.360167, 2753.285994, 3120.876811, 3560.233174, 3810.419296, 4332.720164, 4876.798614, 5722.895655, 7092.923025))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Turkey</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(43.585, 48.079, 52.098, 54.336, 57.005, 59.507, 61.036, 63.108, 66.146, 68.835, 70.845, 71.777), pop = c(22235677, 25670939, 29788695, 33411317, 37492953, 42404033, 47328791, 52881328, 58179144, 63047647, 67308928, 71158647), gdpPercap = c(1969.10098, 2218.754257, 2322.869908, 2826.356387, 3450.69638, 4269.122326, 4241.356344, 5089.043686, 5678.348271, 6601.429915, 6508.085718, 8458.276384))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Uganda</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(39.978, 42.571, 45.344, 48.051, 51.016, 50.35, 49.849, 51.509, 48.825, 44.578, 47.813, 51.542), pop = c(5824797, 6675501, 7688797, 8900294, 10190285, 11457758, 12939400, 15283050, 18252190, 21210254, 24739869, 29170398), gdpPercap = c(734.753484, 774.3710692, 767.2717398, 908.9185217, 950.735869, 843.7331372, 682.2662268, 617.7244065, 644.1707969, 816.559081, 927.7210018, 1056.380121))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">United Kingdom</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Europe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(69.18, 70.42, 70.76, 71.36, 72.01, 72.76, 74.04, 75.007, 76.42, 77.218, 78.471, 79.425), pop = c(50430000, 51430000, 53292000, 54959000, 56079000, 56179000, 56339704, 56981620, 57866349, 58808266, 59912431, 60776238), gdpPercap = c(9979.508487, 11283.17795, 12477.17707, 14142.85089, 15895.11641, 17428.74846, 18232.42452, 21664.78767, 22705.09254, 26074.53136, 29478.99919, 33203.26128))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">United States</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(68.44, 69.49, 70.21, 70.76, 71.34, 73.38, 74.65, 75.02, 76.09, 76.81, 77.31, 78.242), pop = c(157553000, 171984000, 186538000, 198712000, 209896000, 220239000, 232187835, 242803533, 256894189, 272911760, 287675526, 301139947), gdpPercap = c(13990.48208, 14847.12712, 16173.14586, 19530.36557, 21806.03594, 24072.63213, 25009.55914, 29884.35041, 32003.93224, 35767.43303, 39097.09955, 42951.65309))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Uruguay</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(66.071, 67.044, 68.253, 68.468, 68.673, 69.481, 70.805, 71.918, 72.752, 74.223, 75.307, 76.384), pop = c(2252965, 2424959, 2598466, 2748579, 2829526, 2873520, 2953997, 3045153, 3149262, 3262838, 3363085, 3447496), gdpPercap = c(5716.766744, 6150.772969, 5603.357717, 5444.61962, 5703.408898, 6504.339663, 6920.223051, 7452.398969, 8137.004775, 9230.240708, 7727.002004, 10611.46299))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Venezuela</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Americas</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(55.088, 57.907, 60.77, 63.479, 65.712, 67.456, 68.557, 70.19, 71.15, 72.146, 72.766, 73.747), pop = c(5439568, 6702668, 8143375, 9709552, 11515649, 13503563, 15620766, 17910182, 20265563, 22374398, 24287670, 26084662), gdpPercap = c(7689.799761, 9802.466526, 8422.974165, 9541.474188, 10505.25966, 13143.95095, 11152.41011, 9883.584648, 10733.92631, 10165.49518, 8605.047831, 11415.80569))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Vietnam</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(40.412, 42.887, 45.363, 47.838, 50.254, 55.764, 58.816, 62.82, 67.662, 70.672, 73.017, 74.249), pop = c(26246839, 28998543, 33796140, 39463910, 44655014, 50533506, 56142181, 62826491, 69940728, 76048996, 80908147, 85262356), gdpPercap = c(605.0664917, 676.2854478, 772.0491602, 637.1232887, 699.5016441, 713.5371196, 707.2357863, 820.7994449, 989.0231487, 1385.896769, 1764.456677, 2441.576404))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">West Bank and Gaza</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(43.16, 45.671, 48.127, 51.631, 56.532, 60.765, 64.406, 67.046, 69.718, 71.096, 72.37, 73.422), pop = c(1030585, 1070439, 1133134, 1142636, 1089572, 1261091, 1425876, 1691210, 2104779, 2826046, 3389578, 4018332), gdpPercap = c(1515.592329, 1827.067742, 2198.956312, 2649.715007, 3133.409277, 3682.831494, 4336.032082, 5107.197384, 6017.654756, 7110.667619, 4515.487575, 3025.349798))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Yemen, Rep.</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Asia</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(32.548, 33.97, 35.18, 36.984, 39.848, 44.175, 49.113, 52.922, 55.599, 58.02, 60.308, 62.698), pop = c(4963829, 5498090, 6120081, 6740785, 7407075, 8403990, 9657618, 11219340, 13367997, 15826497, 18701257, 22211743), gdpPercap = c(781.7175761, 804.8304547, 825.6232006, 862.4421463, 1265.047031, 1829.765177, 1977.55701, 1971.741538, 1879.496673, 2117.484526, 2234.820827, 2280.769906))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Zambia</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(42.038, 44.077, 46.023, 47.768, 50.107, 51.386, 51.821, 50.821, 46.1, 40.238, 39.193, 42.384), pop = c(2672000, 3016000, 3421000, 3900000, 4506497, 5216550, 6100407, 7272406, 8381163, 9417789, 10595811, 11746035), gdpPercap = c(1147.388831, 1311.956766, 1452.725766, 1777.077318, 1773.498265, 1588.688299, 1408.678565, 1213.315116, 1210.884633, 1071.353818, 1071.613938, 1271.211593))</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Zimbabwe</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">Africa</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(year = c(1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007), lifeExp = c(48.451, 50.469, 52.358, 53.995, 55.635, 57.674, 60.363, 62.351, 60.377, 46.809, 39.989, 43.487), pop = c(3080907, 3646340, 4277736, 4995432, 5861135, 6642107, 7636524, 9216418, 10704340, 11404948, 11926563, 12311143), gdpPercap = c(406.8841148, 518.7642681, 527.2721818, 569.7950712, 799.3621758, 685.5876821, 788.8550411, 706.1573059, 693.4207856, 792.4499603, 672.0386227, 469.7092981))</td></tr>
</table>

</div>
<div id="from-vectorised-functions" class="section level5" number="20.6.0.1.3">
<h5 number="20.6.0.1.3"><span class="header-section-number">20.6.0.1.3</span> From vectorised functions</h5>
<blockquote>
<p>.. functions take an atomic vector and return a list, e.g., <code>stringr::str_split()</code> …</p>
</blockquote>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>x1,</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;a,b,c&quot;</span>, </span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;d,e,f,g&quot;</span></span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">x2 =</span> stringr<span class="sc">::</span><span class="fu">str_split</span>(x1, <span class="st">&quot;,&quot;</span>)) </span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-120">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-120) </caption><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x1</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x2</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a,b,c</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c("a", "b", "c")</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">d,e,f,g</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">c("d", "e", "f", "g")</td></tr>
</table>

<p>Above, <code>str_split</code> generates a list from each element of a character vector … this <code>x2</code> is generated as a list-vector.<br />
You can then <code>unnest</code> … which will spread it out:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">x2 =</span> stringr<span class="sc">::</span><span class="fu">str_split</span>(x1, <span class="st">&quot;,&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(x2)</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-121">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-121) </caption><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x1</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x2</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a,b,c</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">a,b,c</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">b</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a,b,c</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">d,e,f,g</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">d</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">d,e,f,g</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">e</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">d,e,f,g</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">f</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">d,e,f,g</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">g</td></tr>
</table>

<p><em>(Not sure when I would use the above?)</em></p>
<blockquote>
<p>Another example … using the <code>map()</code>, <code>map2()</code>, <code>pmap()</code> from purrr.</p>
</blockquote>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>sim <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>f,      <span class="sc">~</span>params,</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;runif&quot;</span>, <span class="fu">list</span>(<span class="at">min =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">max =</span> <span class="dv">1</span>),</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;rnorm&quot;</span>, <span class="fu">list</span>(<span class="at">sd =</span> <span class="dv">5</span>),</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;rpois&quot;</span>, <span class="fu">list</span>(<span class="at">lambda =</span> <span class="dv">10</span>)</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>(xxx<span class="ot">&lt;-</span> sim <span class="sc">%&gt;%</span></span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sims =</span> <span class="fu">invoke_map</span>(f, params, <span class="at">n =</span> <span class="dv">10</span>))</span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a>) <span class="co"># the invoke_map syntax is deprecated, but I can&#39;t figure out the new syntax</span></span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-122">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-122) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">f</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">params</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">sims</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">runif</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(min = -1, max = 1)</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c(0.155458308756351, -0.365021312143654, -0.751401513814926, 0.087397244758904, -0.99003853276372, 0.571434461511672, -0.802805447950959, 0.187789742369205, 0.168766090180725, -0.77981053898111)</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">rnorm</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(sd = 5)</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">c(-9.53946006340531, 5.96623642807183, 2.18363207949048, -1.11641543741036, -6.5093933954649, 13.724766812636, -4.74671747619207, 1.12873932249231, -5.8914797238219, -11.6491818752806)</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">rpois</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(lambda = 10)</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c(10, 10, 7, 14, 12, 8, 11, 12, 14, 12)</td></tr>
</table>

</div>
<div id="from-multivalued-summaries" class="section level5" number="20.6.0.1.4">
<h5 number="20.6.0.1.4"><span class="header-section-number">20.6.0.1.4</span> From multivalued summaries</h5>
<blockquote>
<p>f <code>summarise()</code> … only works with summary functions that return a single value. … not functions like <code>quantile()</code> that return a vector of arbitrary length:</p>
</blockquote>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>mtcars <span class="sc">%&gt;%</span> </span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(cyl) <span class="sc">%&gt;%</span> </span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">q =</span> <span class="fu">quantile</span>(mpg)) <span class="co">#this now seems to run?</span></span></code></pre></div>
<pre><code>## `summarise()` has grouped output by &#39;cyl&#39;. You can override using the `.groups` argument.</code></pre>
<pre><code>## # A tibble: 15 x 2
## # Groups:   cyl [3]
##      cyl     q
##    &lt;dbl&gt; &lt;dbl&gt;
##  1     4  21.4
##  2     4  22.8
##  3     4  26  
##  4     4  30.4
##  5     4  33.9
##  6     6  17.8
##  7     6  18.6
##  8     6  19.7
##  9     6  21  
## 10     6  21.4
## 11     8  10.4
## 12     8  14.4
## 13     8  15.2
## 14     8  16.2
## 15     8  19.2</code></pre>
<blockquote>
<p>You can however, wrap the result in a list! This obeys the contract of <code>summarise()</code>, because each summary is now a list (a vector) of length 1.</p>
</blockquote>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>mtcars <span class="sc">%&gt;%</span> </span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(cyl) <span class="sc">%&gt;%</span> </span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">q =</span> <span class="fu">list</span>(<span class="fu">quantile</span>(mpg)))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-124">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-124) </caption><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">cyl</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">q</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c(`0%` = 21.4, `25%` = 22.8, `50%` = 26, `75%` = 30.4, `100%` = 33.9)</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">6</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">c(`0%` = 17.8, `25%` = 18.65, `50%` = 19.7, `75%` = 21, `100%` = 21.4)</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">8</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c(`0%` = 10.4, `25%` = 14.4, `50%` = 15.2, `75%` = 16.25, `100%` = 19.2)</td></tr>
</table>

<p>OK this is neater<br />
<br />
To make useful results with unnest, you’ll also need to capture the probabilities:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.99</span>)</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>mtcars <span class="sc">%&gt;%</span> </span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(cyl) <span class="sc">%&gt;%</span> </span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">p =</span> <span class="fu">list</span>(probs), <span class="at">q =</span> <span class="fu">list</span>(<span class="fu">quantile</span>(mpg, probs))) <span class="sc">%&gt;%</span> </span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="fu">c</span>(p, q))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-125">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-125) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">cyl</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">q</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">4</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.01</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">21.4</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.25</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">22.8</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.5&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">26&nbsp;&nbsp;</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.75</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">30.4</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">4</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.99</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">33.8</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.01</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">17.8</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.25</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">18.6</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.5&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">19.7</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.75</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">21&nbsp;&nbsp;</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">6</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.99</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">21.4</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">8</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.01</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">10.4</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">8</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.25</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">14.4</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">8</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.5&nbsp;</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">15.2</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">8</td><td style="vertical-align: top; text-align: right; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">0.75</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">16.2</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">8</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.99</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">19.1</td></tr>
</table>

</div>
</div>
</div>
<div id="from-a-named-list" class="section level3" number="20.6.1">
<h3 number="20.6.1"><span class="header-section-number">20.6.1</span> From a named list</h3>
<p>Data frames with list-columns provide a solution to a common problem: what do you do if you want to iterate over both the contents of a list and its elements? Instead of trying to jam everything into one object, it’s often easier to make a data frame: one column can contain the elements, and one column can contain the list. An easy way to create such a data frame from a list is <code>tibble::enframe()</code>.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">a =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">b =</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>, </span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">c =</span> <span class="dv">5</span><span class="sc">:</span><span class="dv">6</span></span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">enframe</span>(x)</span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-126">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-126) </caption><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">name</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">value</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1:5</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">b</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3:4</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5:6</td></tr>
</table>

<p>The advantage of this structure is that it generalises in a straightforward way - names are useful if you have character vector of metadata, but don’t help if you have other types of data, or multiple vectors.</p>
<p>Now if you want to iterate over names and values in parallel, you can use <code>map2()</code>:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">smry =</span> <span class="fu">map2_chr</span>(name, value, <span class="sc">~</span> stringr<span class="sc">::</span><span class="fu">str_c</span>(.x, <span class="st">&quot;: &quot;</span>, .y[<span class="dv">1</span>]))</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-127">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-127) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">name</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">value</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">smry</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1:5</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a: 1</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">b</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3:4</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">b: 3</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5:6</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c: 5</td></tr>
</table>

<div id="simplifying-list-columns" class="section level5" number="20.6.1.0.1">
<h5 number="20.6.1.0.1"><span class="header-section-number">20.6.1.0.1</span> Simplifying list-columns</h5>
<p>To apply the techniques of data manipulation and visualisation you’ve learned in this book, you’ll need to simplify the list-column back to a regular column (an atomic vector), or set of columns. The technique you’ll use to collapse back down to a simpler structure depends on whether you want a single value per element, or multiple values:</p>
<ol style="list-style-type: decimal">
<li><p>If you want a single value, use <code>mutate()</code> with <code>map_lgl()</code>, <code>map_int()</code>, <code>map_dbl()</code>, and <code>map_chr()</code> to create an atomic vector.</p></li>
<li><p>If you want many values, use <code>unnest()</code> to convert list-columns back to regular columns, repeating the rows as many times as necessary.</p></li>
</ol>
<p>These are described in more detail below.</p>
</div>
</div>
<div id="list-to-vector" class="section level3" number="20.6.2">
<h3 number="20.6.2"><span class="header-section-number">20.6.2</span> List to vector</h3>
<p>If you can reduce your list column to an atomic vector then it will be a regular column. For example, you can always summarise an object with its type and length, so this code will work regardless of what sort of list-column you have:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>x,</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>  letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>],</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">runif</span>(<span class="dv">5</span>)</span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(</span>
<span id="cb141-9"><a href="#cb141-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="fu">map_chr</span>(x, typeof),</span>
<span id="cb141-10"><a href="#cb141-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">length =</span> <span class="fu">map_int</span>(x, length)</span>
<span id="cb141-11"><a href="#cb141-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-128">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-128) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">type</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">length</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c("a", "b", "c", "d", "e")</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">character</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1:3</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">integer</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c(0.364103812957183, 0.842565316706896, 0.713936110259965, 0.870173309929669, 0.350114052649587)</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">double</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">5</td></tr>
</table>

<p>This is the same basic information that you get from the default tbl print method, but now you can use it for filtering. This is a useful technique if you have a heterogeneous list, and want to filter out the parts aren’t working for you.</p>
<p>Don’t forget about the <code>map_*()</code> shortcuts - you can use <code>map_chr(x, "apple")</code> to extract the string stored in <code>apple</code> for each element of <code>x</code>. This is useful for pulling apart nested lists into regular columns. Use the <code>.null</code> argument to provide a value to use if the element is missing (instead of returning <code>NULL</code>):</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>x,</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">a =</span> <span class="dv">1</span>, <span class="at">b =</span> <span class="dv">2</span>),</span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">a =</span> <span class="dv">2</span>, <span class="at">c =</span> <span class="dv">4</span>)</span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(</span>
<span id="cb142-7"><a href="#cb142-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">a =</span> <span class="fu">map_dbl</span>(x, <span class="st">&quot;a&quot;</span>),</span>
<span id="cb142-8"><a href="#cb142-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">b =</span> <span class="fu">map_dbl</span>(x, <span class="st">&quot;b&quot;</span>, <span class="at">.null =</span> <span class="cn">NA_real_</span>)</span>
<span id="cb142-9"><a href="#cb142-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-129">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-129) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">a</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">b</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">list(a = 1, b = 2)</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">2</td></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">list(a = 2, c = 4)</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;"></td></tr>
</table>

</div>
<div id="unnesting" class="section level3" number="20.6.3">
<h3 number="20.6.3"><span class="header-section-number">20.6.3</span> Unnesting</h3>
<p><code>unnest()</code> works by repeating the regular columns once for each element of the list-column. For example, in the following very simple example we repeat the first row 4 times (because there the first element of <code>y</code> has length four), and the second row once:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="at">y =</span> <span class="fu">list</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="dv">1</span>)) <span class="sc">%&gt;%</span> <span class="fu">unnest</span>(y)</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-130">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-130) </caption><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">y</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">4</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">2</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td></tr>
</table>

<p>This means that you can’t simultaneously unnest two columns that contain different number of elements:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ok, because y and z have the same number of elements in</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="co"># every row</span></span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>x, <span class="sc">~</span>y,           <span class="sc">~</span>z,</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>   <span class="dv">1</span>, <span class="fu">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>), <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,</span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>   <span class="dv">2</span>, <span class="st">&quot;c&quot;</span>,           <span class="dv">3</span></span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>df1</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-131">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-131) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">y</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">z</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c("a", "b")</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1:2</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">c</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3</td></tr>
</table>

<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>df1 <span class="sc">%&gt;%</span> <span class="fu">unnest</span>(<span class="fu">c</span>(y, z))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-131">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-131) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">y</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">z</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">b</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">2</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">c</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3</td></tr>
</table>

<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Doesn&#39;t work because y and z have different number of elements</span></span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>x, <span class="sc">~</span>y,           <span class="sc">~</span>z,</span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a>   <span class="dv">1</span>, <span class="st">&quot;a&quot;</span>,         <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,  </span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a>   <span class="dv">2</span>, <span class="fu">c</span>(<span class="st">&quot;b&quot;</span>, <span class="st">&quot;c&quot;</span>),   <span class="dv">3</span></span>
<span id="cb146-6"><a href="#cb146-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb146-7"><a href="#cb146-7" aria-hidden="true" tabindex="-1"></a>df2</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-131">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-131) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">y</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">z</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1:2</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">c("b", "c")</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3</td></tr>
</table>

<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>df2 <span class="sc">%&gt;%</span> <span class="fu">unnest</span>(<span class="fu">c</span>(y, z))</span></code></pre></div>
<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-131">
<caption style="caption-side: top; text-align: center;">(#tab:unnamed-chunk-131) </caption><col><col><col><tr>
<th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">x</th><th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">y</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">z</th></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">a</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">1</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">1</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; font-weight: normal;">a</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">2</td><td style="vertical-align: top; text-align: left; white-space: normal; padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">b</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">3</td></tr>
<tr>
<td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">2</td><td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">c</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;">3</td></tr>
</table>

<p>The same principle applies when unnesting list-columns of data frames. You can unnest multiple list-cols as long as all the data frames in each row have the same number of rows.</p>
</div>
<div id="section" class="section level3" number="20.6.4">
<h3 number="20.6.4"><span class="header-section-number">20.6.4</span> </h3>
</div>
</div>
<div id="making-tidy-data-with-broom" class="section level2" number="20.7">
<h2 number="20.7"><span class="header-section-number">20.7</span> Making tidy data with broom</h2>
<p>The broom package provides three general tools for turning models into tidy data frames:</p>
<ol style="list-style-type: decimal">
<li><p><code>broom::glance(model)</code> returns a row for each model. Each column gives a model summary: either a measure of model quality, or complexity, or a combination of the two.</p></li>
<li><p><code>broom::tidy(model)</code> returns a row for each coefficient in the model. Each column gives information about the estimate or its variability.</p></li>
<li><p><code>broom::augment(model, data)</code> returns a row for each row in <code>data</code>, adding extra values like residuals, and influence statistics.</p></li>
</ol>
<p><br />
</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>test_models <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>   <span class="sc">~</span>df, <span class="sc">~</span>filter, <span class="sc">~</span>y, <span class="sc">~</span>x, <span class="sc">~</span>model, <span class="sc">~</span>family,</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;mtcars&quot;</span>, <span class="st">&quot;disp&gt;100&quot;</span>, <span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;wt, gear&quot;</span>,<span class="st">&quot;lm&quot;</span>, <span class="st">&quot;&quot;</span>,</span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;mtcars&quot;</span>, <span class="st">&quot;disp&lt;100&quot;</span>, <span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;wt, gear, hp&quot;</span>, <span class="st">&quot;lm&quot;</span>, <span class="st">&quot;&quot;</span>,</span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;mtcars&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;vs&quot;</span>, <span class="st">&quot;wt, gear, hp&quot;</span>, <span class="st">&quot;glm&quot;</span>, <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;cars&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;speed&quot;</span>, <span class="st">&quot;dist&quot;</span>, <span class="st">&quot;lm&quot;</span>, <span class="st">&quot;&quot;</span></span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>I want it to run</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>mtcars <span class="sc">%&gt;%</span> <span class="fu">filter</span>(disp<span class="sc">&gt;</span><span class="dv">100</span>) <span class="sc">%&gt;%</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> wt <span class="sc">+</span> gear, <span class="at">data =</span>.) </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + gear, data = .)
## 
## Coefficients:
## (Intercept)           wt         gear  
##     31.3800      -3.8854       0.0609</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>mtcars <span class="sc">%&gt;%</span> <span class="fu">filter</span>(disp<span class="sc">&lt;</span><span class="dv">100</span>) <span class="sc">%&gt;%</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> wt <span class="sc">+</span> gear <span class="sc">+</span>  hp, <span class="at">data =</span>.) </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + gear + hp, data = .)
## 
## Coefficients:
## (Intercept)           wt         gear           hp  
##     25.1383       1.8527       0.7982      -0.0136</code></pre>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>mtcars <span class="sc">%&gt;%</span> <span class="fu">filter</span>() <span class="sc">%&gt;%</span> <span class="fu">glm</span>(vs <span class="sc">~</span> wt <span class="sc">+</span> gear <span class="sc">+</span>  hp, <span class="at">data =</span>., <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>) </span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = vs ~ wt + gear + hp, family = &quot;binomial&quot;, data = .)
## 
## Coefficients:
## (Intercept)           wt         gear           hp  
##     11.1757       0.5555      -0.6472      -0.0851  
## 
## Degrees of Freedom: 31 Total (i.e. Null);  28 Residual
## Null Deviance:       44 
## Residual Deviance: 16    AIC: 24</code></pre>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>cars <span class="sc">%&gt;%</span> <span class="fu">filter</span>() <span class="sc">%&gt;%</span> <span class="fu">lm</span>(speed <span class="sc">~</span> dist,<span class="at">data =</span>.) </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = speed ~ dist, data = .)
## 
## Coefficients:
## (Intercept)         dist  
##       8.284        0.166</code></pre>
<!--chapter:end:data_sci/data_sci.Rmd-->
</div>
</div>
<div id="references" class="section level1" number="21">
<h1 number="21"><span class="header-section-number">21</span> List of references</h1>
<!--chapter:end:references.Rmd-->
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-angristIdentificationEstimationLocal1995" class="csl-entry">
Angrist, Joshua D., and Guido W. Imbens. 1995. <span>“Identification and Estimation of Local Average Treatment Effects.”</span> <span>National Bureau of Economic Research</span>.
</div>
<div id="ref-angrist2008mostly" class="csl-entry">
Angrist, Joshua D, and Jörn-Steffen Pischke. 2008. <em>Mostly Harmless Econometrics: <span>An</span> Empiricist’s Companion</em>. <span>Princeton university press</span>.
</div>
<div id="ref-bakerSummaryReportAAPOR2013" class="csl-entry">
Baker, Reg, J. Michael Brick, Nancy A. Bates, Mike Battaglia, Mick P. Couper, Jill A. Dever, Krista J. Gile, and Roger Tourangeau. 2013. <span>“Summary Report of the <span>AAPOR</span> Task Force on Non-Probability Sampling.”</span> <em>Journal of Survey Statistics and Methodology</em> 1 (2): 90–143. <a href="https://doi.org/ggmdn5">https://doi.org/ggmdn5</a>.
</div>
<div id="ref-boersFavourableEffectsConsuming2014" class="csl-entry">
Boers, Inge, Frits AJ Muskiet, Evert Berkelaar, Erik Schut, Ria Penders, Karine Hoenderdos, Harry J. Wichers, and Miek C. Jong. 2014. <span>“Favourable Effects of Consuming a <span>Palaeolithic</span>-Type Diet on Characteristics of the Metabolic Syndrome: A Randomized Controlled Pilot-Study.”</span> <em>Lipids in Health and Disease</em> 13 (1): 160. <a href="https://doi.org/f22cxq">https://doi.org/f22cxq</a>.
</div>
<div id="ref-brodeurMethodsMatterPhacking2018" class="csl-entry">
Brodeur, Abel, Nikolai Cook, and Anthony G. Heyes. 2018. <span>“Methods Matter: <span>P</span>-Hacking and Causal Inference in Economics.”</span>
</div>
<div id="ref-buttonPowerFailureWhy2013" class="csl-entry">
Button, Katherine S, John P A Ioannidis, Claire Mokrysz, Brian A Nosek, Jonathan Flint, Emma S J Robinson, and Marcus R Munaf‘o. 2013. <span>“Power Failure: <span>Why</span> Small Sample Size Undermines the Reliability of Neuroscience.”</span> <em>Nature Reviews Neuroscience</em> 14 (5): 365–76. <a href="https://doi.org/k9d">https://doi.org/k9d</a>.
</div>
<div id="ref-christianAlgorithmsLiveComputer2016" class="csl-entry">
Christian, Brian, and Tom Griffiths. 2016. <em>Algorithms to Live by: <span>The</span> Computer Science of Human Decisions</em>. <span>Macmillan</span>.
</div>
<div id="ref-ConsistencyInferenceInstrumental" class="csl-entry">
<span>“Consistency Without <span>Inference</span>: <span>Instrumental Variables</span> in <span>Practical Application</span>* - <span>Google Search</span>.”</span> n.d. Accessed June 18, 2020. <a href="https://www.google.com/search?q=Consistency+without+Inference%3A+Instrumental+Variables+in+Practical+Application*&amp;oq=Consistency+without+Inference%3A+Instrumental+Variables+in+Practical+Application*&amp;aqs=chrome..69i57.176j0j4&amp;sourceid=chrome&amp;ie=UTF-8">https://www.google.com/search?q=Consistency+without+Inference%3A+Instrumental+Variables+in+Practical+Application*&amp;oq=Consistency+without+Inference%3A+Instrumental+Variables+in+Practical+Application*&amp;aqs=chrome..69i57.176j0j4&amp;sourceid=chrome&amp;ie=UTF-8</a>.
</div>
<div id="ref-cunninghamCausalInferenceMixtape2018" class="csl-entry">
Cunningham, Scott. 2018. <em>Causal Inference: <span>The</span> Mixtape</em>. <span>Yale University Press</span>.
</div>
<div id="ref-fentonPaleoDietStill2016" class="csl-entry">
Fenton, Tanis R., and Carol J. Fenton. 2016. <span>“Paleo Diet Still Lacks Evidence.”</span> <em>The American Journal of Clinical Nutrition</em> 104 (3): 844–44. <a href="https://doi.org/ghqb6v">https://doi.org/ghqb6v</a>.
</div>
<div id="ref-friedmanElementsStatisticalLearning2001" class="csl-entry">
Friedman, Jerome, Trevor Hastie, Robert Tibshirani, and others. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. <span>Springer series in statistics New York</span>.
</div>
<div id="ref-fryarMeanBodyWeight2018" class="csl-entry">
Fryar, Cheryl D., Deanna Kruszan-Moran, Qiuping Gu, and Cynthia L. Ogden. 2018. <span>“Mean Body Weight, Weight, Waist Circumference, and Body Mass Index Among Adults: <span>United States</span>, 1999–2000 Through 2015–2016.”</span>
</div>
<div id="ref-gelmanDifferenceSignificantNot2006" class="csl-entry">
Gelman, Andrew, and Hal Stern. 2006. <span>“The Difference Between <span>‘Significant’</span> and <span>‘Not Significant’</span> Is Not Itself Statistically Significant.”</span> <em>The American Statistician</em> 60 (4): 328–31.
</div>
<div id="ref-ghaediEffectsPaleolithicDiet2019" class="csl-entry">
Ghaedi, Ehsan, Mohammad Mohammadi, Hamed Mohammadi, Nahid Ramezani-Jolfaie, Janmohamad Malekzadeh, Mahdieh Hosseinzadeh, and Amin Salehi-Abargouei. 2019. <span>“Effects of a <span>Paleolithic</span> Diet on Cardiovascular Disease Risk Factors: <span>A</span> Systematic Review and Meta-Analysis of Randomized Controlled Trials.”</span> <em>Advances in Nutrition</em> 10 (4): 634–46. <a href="https://doi.org/ghp383">https://doi.org/ghp383</a>.
</div>
<div id="ref-guyattGRADEEmergingConsensus2008" class="csl-entry">
Guyatt, Gordon H., Andrew D. Oxman, Gunn E. Vist, Regina Kunz, Yngve Falck-Ytter, Pablo Alonso-Coello, and Holger J. Schünemann. 2008. <span>“<span>GRADE</span>: An Emerging Consensus on Rating Quality of Evidence and Strength of Recommendations.”</span> <em>Bmj</em> 336 (7650): 924–26. <a href="https://doi.org/dqmphq">https://doi.org/dqmphq</a>.
</div>
<div id="ref-harmsMakingNullEffects2018" class="csl-entry">
Harms, Christopher, and Daniël Lakens. 2018. <span>“Making’null Effects’ Informative: Statistical Techniques and Inferential Frameworks.”</span> <em>Journal of Clinical and Translational Research</em> 3: 382.
</div>
<div id="ref-harrerDoingMetaanalysisHandson2019" class="csl-entry">
Harrer, M., P. Cuijpers, T. A. Furukawa, and D. D. Ebert. 2019. <span>“Doing Meta-Analysis in <span>R</span>: A Hands-on Guide.”</span> <em>PROTECT Lab Erlangen</em>.
</div>
<div id="ref-Heckman2013" class="csl-entry">
Heckman, James, Rodrigo Pinto, and James Heckman. 2013. <span>“Econometric Mediation Analyses : <span>Identifying</span> the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs,”</span> no. 7552.
</div>
<div id="ref-hernanCausalInference2010" class="csl-entry">
Hernán, Miguel A., and James M. Robins. 2010. <em>Causal Inference</em>. <span>CRC Boca Raton, FL;</span>
</div>
<div id="ref-juddTreatingStimuliRandom2012" class="csl-entry">
Judd, Charles M., Jacob Westfall, and David A. Kenny. 2012. <span>“Treating Stimuli as a Random Factor in Social Psychology: <span>A</span> New and Comprehensive Solution to a Pervasive but Largely Ignored Problem.”</span> <em>Journal of Personality and Social Psychology</em> 103 (1): 54. <a href="https://doi.org/f33f3t">https://doi.org/f33f3t</a>.
</div>
<div id="ref-kasyWhyExperimentersMight2016" class="csl-entry">
Kasy, Maximilian. 2016. <span>“Why Experimenters Might Not Always Want to Randomize, and What They Could Do Instead.”</span> <em>Political Analysis</em> 24 (3): 324–38. <a href="https://doi.org/10.1093/pan/mpw012">https://doi.org/10.1093/pan/mpw012</a>.
</div>
<div id="ref-kelleherFundamentalsMachineLearning2015" class="csl-entry">
Kelleher, John D., Brian Mac Namee, and Aoife D’Arcy. 2015. <em>Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies</em>. <span>Cambridge, Massachusetts</span>: <span>The MIT Press</span>.
</div>
<div id="ref-kennedyOldFiledrawerProblem2004" class="csl-entry">
Kennedy, Donald. 2004. <span>“The Old File-Drawer Problem.”</span> <em>Science</em> 305 (5683): 451–52. <a href="https://doi.org/dz778j">https://doi.org/dz778j</a>.
</div>
<div id="ref-kennedyGuideEconometrics2003" class="csl-entry">
Kennedy, Peter. 2003. <em>A Guide to Econometrics</em>. <span>MIT press</span>.
</div>
<div id="ref-leeTrainingWagesSample2009" class="csl-entry">
Lee, David S. 2009. <span>“Training, Wages, and Sample Selection: <span>Estimating</span> Sharp Bounds on Treatment Effects.”</span> <em>Review of Economic Studies</em>. <a href="https://doi.org/10.1111/j.1467-937X.2009.00536.x">https://doi.org/10.1111/j.1467-937X.2009.00536.x</a>.
</div>
<div id="ref-manheimerReplyTRFenton2016" class="csl-entry">
Manheimer, Eric W. 2016. <span>“Reply to <span>TR Fenton</span> and <span>CJ Fenton</span>.”</span> <em>The American Journal of Clinical Nutrition</em> 104 (3): 845. <a href="https://doi.org/ghqb6t">https://doi.org/ghqb6t</a>.
</div>
<div id="ref-manheimerPaleolithicNutritionMetabolic2015" class="csl-entry">
Manheimer, Eric W., Esther J. van Zuuren, Zbys Fedorowicz, and Hanno Pijl. 2015. <span>“Paleolithic Nutrition for Metabolic Syndrome: Systematic Review and Meta-Analysis.”</span> <em>The American Journal of Clinical Nutrition</em> 102 (4): 922–32. <a href="https://doi.org/gf9r2v">https://doi.org/gf9r2v</a>.
</div>
<div id="ref-McElreath2015" class="csl-entry">
McElreath, Richard. 2015. <span>“The Golem of Prague (Chapter 1) and Multilevel Models (Chapter 12).”</span> <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em>, 469. <a href="https://doi.org/10.3102/1076998616659752">https://doi.org/10.3102/1076998616659752</a>.
</div>
<div id="ref-demenezesInfluencePaleolithicDiet2019a" class="csl-entry">
Menezes, Ehrika Vanessa Almeida de, Helena Alves de Carvalho Sampaio, Antônio Augusto Ferreira Carioca, Nara Andrade Parente, Filipe Oliveira Brito, Thereza Maria Magalhães Moreira, Ana Célia Caetano de Souza, and Soraia Pinheiro Machado Arruda. 2019. <span>“Influence of Paleolithic Diet on Anthropometric Markers in Chronic Diseases: Systematic Review and Meta-Analysis.”</span> <em>Nutrition Journal</em> 18 (1): 41. <a href="https://doi.org/ghqb6s">https://doi.org/ghqb6s</a>.
</div>
<div id="ref-osterUnobservableSelectionCoefficient2019" class="csl-entry">
Oster, Emily. 2019. <span>“Unobservable Selection and Coefficient Stability: <span>Theory</span> and Evidence.”</span> <em>Journal of Business &amp; Economic Statistics</em> 37 (2): 187–204. <a href="https://doi.org/10.1080/07350015.2016.1227711">https://doi.org/10.1080/07350015.2016.1227711</a>.
</div>
<div id="ref-paezGrayLiteratureImportant2017" class="csl-entry">
Paez, Arsenio. 2017. <span>“Gray Literature: <span>An</span> Important Resource in Systematic Reviews.”</span> <em>Journal of Evidence-Based Medicine</em> 10 (3): 233–40. <a href="https://doi.org/gbtnr9">https://doi.org/gbtnr9</a>.
</div>
<div id="ref-Quintana2015" class="csl-entry">
Quintana, Daniel S. 2015. <span>“From Pre-Registration to Publication: <span>A</span> Non-Technical Primer for Conducting a Meta-Analysis to Synthesize Correlational Data.”</span> <em>Frontiers in Psychology</em> 6: 1–9. <a href="https://doi.org/10.3389/fpsyg.2015.01549">https://doi.org/10.3389/fpsyg.2015.01549</a>.
</div>
<div id="ref-randlesAsymptoticallyDistributionfreeTest1980" class="csl-entry">
Randles, Ronald H., Michael A. Fligner, George E. Policello, and Douglas A. Wolfe. 1980. <span>“An Asymptotically Distribution-Free Test for Symmetry Versus Asymmetry.”</span> <em>Journal of the American Statistical Association</em> 75 (369): 168–72. <a href="https://doi.org/ggx9q9">https://doi.org/ggx9q9</a>.
</div>
<div id="ref-raykovMetaanalysisScaleReliability2013" class="csl-entry">
Raykov, Tenko, and George A. Marcoulides. 2013. <span>“Meta-Analysis of Scale Reliability Using Latent Variable Modeling.”</span> <em>Structural Equation Modeling: A Multidisciplinary Journal</em> 20 (2): 338–53. <a href="https://doi.org/gh3mpb">https://doi.org/gh3mpb</a>.
</div>
<div id="ref-salganikSamplingEstimationHidden2004" class="csl-entry">
Salganik, Matthew J., and Douglas D. Heckathorn. 2004. <span>“Sampling and Estimation in Hidden Populations Using Respondent-Driven Sampling.”</span> <em>Sociological Methodology</em> 34 (1): 193–240. <a href="https://doi.org/bzv8kr">https://doi.org/bzv8kr</a>.
</div>
<div id="ref-schwingshacklPerspectiveNutriGradeScoring2016" class="csl-entry">
Schwingshackl, Lukas, Sven Knüppel, Carolina Schwedhelm, Georg Hoffmann, Benjamin Missbach, Marta Stelmach-Mardas, Stefan Dietrich, Fabian Eichelmann, Evangelos Kontopanteils, and Khalid Iqbal. 2016. <span>“Perspective: <span>NutriGrade</span>: A Scoring System to Assess and Judge the Meta-Evidence of Randomized Controlled Trials and Cohort Studies in Nutrition Research.”</span> <em>Advances in Nutrition</em> 7 (6): 994–1004. <a href="https://doi.org/ghqzgr">https://doi.org/ghqzgr</a>.
</div>
<div id="ref-wooldridgeIntroductoryEconometricsModern2008" class="csl-entry">
Wooldridge, J M. 2008. <em>Introductory Econometrics: <span>A</span> Modern Approach</em>. <span>South-Western Pub</span>.
</div>
<div id="ref-wooldridgeEconometricAnalysisCross2002" class="csl-entry">
Wooldridge, Jeffrey M. 2002. <em>Econometric Analysis of Cross Section and Panel Data</em>. 2. <span>The MIT press</span>. <a href="https://doi.org/10.1515/humr.2003.021">https://doi.org/10.1515/humr.2003.021</a>.
</div>
<div id="ref-zazpeScopingReviewPaleolithic2020" class="csl-entry">
Zazpe, Itziar, J. Alfredo Martínez, Susana Santiago, Silvia Carlos, M. Ángeles Zulet, and Miguel Ruiz-Canela. 2020. <span>“Scoping Review of <span>Paleolithic</span> Dietary Patterns: A Definition Proposal.”</span> <em>Nutrition Research Reviews</em>, 1–29.
</div>
</div>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
