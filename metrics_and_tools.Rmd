---
title: "Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus"
author: "Dr. David Reinstein, "
abstract: "This 'book' organizes my notes  and helps others understand it and learn from it"
#cover-image: "images/cardcatalogue.JPG"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: "book_assets"
    includes:
      in_header: https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/header.html
    #css: https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css
      css: support/tufte_plus.css
    config:
      toc:
        after: |
          <li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>
        collapse: section
        scroll_highlight: yes
      fontsettings:
        theme: white
        size: 2
      sharing:
        facebook: yes
        twitter: yes
        google: no
        linkedin: yes
        weibo: yes
        instapaper: no
        vk: no
        all: ['facebook', 'twitter', 'linkedin', 'weibo', 'instapaper']
    highlight: tango
    download: [pdf, epub, mobi]
    sharing:
      github: yes
      facebook: no
always_allow_html: yes
bibliography: [support/giving_keywords.bib, support/packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: daaronr/metrics_discussion_work
description: ""
#url: 'https\://daaronr.github.io//'
tags: [Econometrics, Statistics, Data Science, Experiments, Notes, Methodology]
---



```{r}
knitr::opts_chunk$set(echo = TRUE)
```



<link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

https://daaronr.github.io/writing_econ_research/4.3.0/css/font-awesome.min.css

<!--

```{r htmlTemp3, echo=FALSE, eval=FALSE}

collapsejs <- readr::read_lines("js/collapse.js")
codejs <- readr::read_lines("js/codefolding.js")

transitionjs <- readr::read_lines("js/transition.js")
dropdownjs <- readr::read_lines("js/dropdown.js")

htmlhead <- c(
  paste('
<script>',
paste(transitionjs, collapse = "\n"),
'</script>
<script>',
paste(collapsejs, collapse = "\n"),
'</script>
<script>',
paste(codejs, collapse = "\n"),
'</script>
<script>',
paste(dropdownjs, collapse = "\n"),
'</script>
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}
.open > .dropdown-menu {
    display: block;
}
.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>
', sep = "\n"),
  paste0('
<script>
document.write(\'<div class="btn-group pull-right" style="position: absolute; top: 20%; right: 2%; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>\')
</script>
')
)

readr::write_lines(htmlhead, path = "header.html")
```

-->

<!--
base file created from

`pandoc -f docx -t gfm -o writing_econ_gfm.md "bookoutline3-cutting examples down-cutnamesd.docx" `

and similar from


`pandoc -f docx -t gfm -o writing_econ_gfm1.md "Adapting back for BOOK --Ec831 outline-fillingindetails_forslides_edMiriam-conflict.docx"`

replacements needed:

- "\[ \]" surrounds math -- square brackets do not need 'escape' in main text
- colors need adjusting to 'format_with_col'

-->


```{r eval=FALSE}
install.packages("bookdown")
install.packages("tufte")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

```{r packages}

library(here)
#library(checkpoint) #to avoid differential processing from different package versions
library(pacman)

here <- here::here

p_load(dplyr,magrittr,purrr,tidyverse,tidyr,broom,janitor,here,glue,dataMaid,glue,readr, lubridate,summarytools,gtools,knitr,pastecs,data.table)   #citr, reporttools, experiment, estimatr,  kableExtra, ggsignif, glmnet, glmnetUtils, rsample,snakecase,zoo
library(codebook)

```


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r somefunctions}

#possibly move these to a separate file

#multi-output text color
#https://dr-harper.github.io/rmarkdown-cookbook/changing-font-colour.html#multi-output-text-colour
#We can then use the code as an inline R expression format_with_col("my text", "red")

format_with_col = function(x, color){
  if(knitr::is_latex_output())
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(knitr::is_html_output())
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}

```

```{r html, echo=FALSE}
# globally set chunk options
knitr::opts_chunk$set(fig.align='center', out.width='80%')

my_output <- knitr::opts_knit$get("rmarkdown.pandoc.to")

```


<!---
Can define text blocks here, refer to them again and again if desired
-->

<!-- Global site tag (gtag.js) - Google Analytics -->


<html>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-QLKFNFTGXX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QLKFNFTGXX');
</script>
</html>

<!--chapter:end:index.Rmd-->

# Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioural, and Experimental focus

# Notes introduction

- Focus on the practical tools I use and the challenges I (David Reinstein) face

Microeconomics, behavioral economics, focus on charitable giving and 'returns to education' type of straightforward problems. (Limited to no focus on structural approaches.)

- Where we can *add value* to real econometric practice??

\

Data:

- Observational (esp. web-scraped and API data and national surveys/admin data}

- Experimental: esp. where with multiple crossed arms, and where the 'cleanest design' may not be possible

\

Assume familiarity with most basic statistical concepts like 'bias', 'consistency', and 'null hypothesis testing.' However, I will focus on some concepts that seem to often be misunderstood and mis-applied.


## Conceptual


### Bayesian vs. frequentist approaches

Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)

### Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

#### DAGs and Potential outcomes

### Theory, restrictions, and 'structural vs reduced form'

## Getting, cleaning and using data

### Data: What/why/where/how

### Good coding practices

#### Organizing a project

#### Dynamic documents (esp Rmd/bookdown)

### New tools and approaches to data (esp 'tidyverse')

#### Style and consistency

Indenting, snake-case,etc

#### Using functions, variable lists, etc., for clean, concise, readable code

### Data sharing and integrity

## Control strategies and prediction; Machine Learning approaches

'Identification' of causal effects with a control strategy not credible	Identification	Essentially a 'control strategy' is "control for all  or most of the reasonable determinants of the independent variable so as to make the  remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest". All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., [@oster2019unobservable].

### Machine Learning (statistical learning): Lasso, Ridge, and more

### Limitations to inference from learning approaches

## Basic regression and statistical inference: Common mistakes and issues



Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)	Identification
Random effects estimators show a lack of robustness	Specification	Clustering SE  is more standard practice

OLS/IV estimators not 'mean effect' in presence of heterogeneity
Power calculations/underpowered
Selection bias due to attrition
Selection bias due to missing variables -- impute these as a solution
Signs of p-hacking and specification-hunting
Weak diagnostic/identification tests
Dropping zeroes in a "loglinear" model is problematic
Random effects estimators show a lack of robustness
Dropping zeroes in a "loglinear" model is problematic
Random effects estimators show a lack of robustness
With heterogeneity the simple OLS estimator is not the 'mean effect'
P_augmented may *overstate* type-1 error rate
Impact size from regression of "log 1+gift amount"
Lagged dependent variable and fixed effects --> 'Nickel bias'
Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)
Weak IV bias
Bias from selecting instruments and estimating using the same data

### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)


### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: 	Simonsohn18

http://datacolada.org/62

### OLS and heterogeneity

- OLS does *not* identify the ATE

http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT


- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"


"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

#### Confidence intervals and Bayesian credible intervals

#### Comparing relative parameters

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT)

See [@verkaik2016]

### Interaction terms and pitfalls

#### 'Moderators' Confusion with nonlinearity

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if outcome  'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative


#### MHT

### Choice of test statistics (including nonparametric)

(Or get to this in the experimetrics section)

### How to display and write about regression results and tests

### Bayesian interpretations of results

## LDV and discrete choice modeling

## Robustness and diagnostics, with integrity

### (How) can diagnostic tests make sense? Where is the burden of proof?

Where a particular assumption is critical to identification and inference ...Failure to reject the violation of an assumptionis not sufficient to give us confidence that it is satisfied and the results are credible. At several points the authors cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles).


### Estimating standard errors

### Sensitivity analysis: Interactive presentation


## IV and its many issues

### Instrument validity

- Exogeneity vs. exclusion
- Very hard to 'powerfully test'

IV not credible	Identification	Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a *direct* effect on the outcome (only an indirect effect through the endogenous variable

### Heterogeneity and LATE

### Weak instruments, other issues

### Reference to the use of IV in experiments/mediation

## Causal pathways: mediation, hurdles, etc.

### Mediation modeling


### 'Corner solution' or hurdle variables and 'Conditional on Positive

"Conditional on positive"/"intensive margin" analysis ignores selection


"Conditional on positive"/"intensive margin" analysis ignores selection 	Identification	See Angrist and Pischke on "Good CoP, bad CoP".  See also bounding approaches such as [@Lee2018]	AngristJ.D.2008a,



## Causal pathways: mediation, hurdles, etc.

### Mediation modeling

### 'Corner solution' or hurdle variables and 'Conditional on Positive'

## Causal pathways: mediation, hurdles, etc.

### [Mediation modeling and its massive limitations](#mediators)

An applied review

### 'Corner solution' or hurdle variables and 'Conditional on Positive

#### Bounding approaches (Lee, Manski, etc)

## Other paths to observational identification

### Fixed effects and differencing

### DiD

FE/DiD does not rule out a correlated dynamic unobservable, causing a bias

### RD

### Time-series-ish panel approaches to micro

#### Lagged dependent variable and fixed effects --> 'Nickel bias'

## (Ex-ante) Power calculations

### What sort of 'power calculations' make sense, and what is the point?

#### The 'harm to science' from running underpowered studies

> "One worries about underpowered tests. Your result (may have)  relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.

> Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects."	verkaik2016, metzger2015

### Power calculations without real data

### Power calculations using prior data


## (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters

### Why run an experiment or study?

- Sugden and Sitzia critique here, give more motivation

### Causal channels and identification

- Ruling out alternative hypotheses, etc

### Types of experiments, 'demand effects' and more artifacts of artifical setups

### Generalizability (and heterogeneity)


## (Experimental) Study design: Background and quantitative issues

### Pre-registration and Pre-analysis plans

#### The hazards of specification-searching

### Sequential and adaptive designs


Needs to adjust significance tests for augmenting data/sequential analysis/peeking	Statistics/econometrics	new-statistics		sagarin_2014			http://www.paugmented.com/	resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/

Yet ...


P_augmented may *overstate* type-1 error rate	Statistics/econometrics	response to referees, new-statistics	"
A process involving stopping ""whenever the nominal $p.0.5$"" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5\%. Even if the subsequent data suggested a ""one in a million chance of arising under the null"" the overall process yields a 5\%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the ""likelihood of the null hypothesis"" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate.

Considering the calculations in \ref{sagarin2014}, it is clear that $p_{augmented}$ should \textit{overstate} the type-1 error of the process if there is a positive probability that after an initial experiment attains p$<0.05$, more data is collected.  A headline $p<0.05$ does \textit{not} imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded $p>0.05$, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though $p<0.05$. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine's experiment).  Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p<0.05, more data is collected, implying that $p_{augmented}$ as calculated above overstates the type I error rate that would arise  from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below $p=0.05$, in order to attain a type-1 error rate of 5% or less in our published corpus."

### Efficient assignment of treatments

(Links back to power analyses)

## 'Experimetrics' and measurement of treatment effects from RCTs

### Which error structure? Random effects?

### Randomization inference?

### Parametric and nonparametric tests of simple hypotheses

### Adjustments for exogenous (but non-random) treatment assignment

### IV in an experimental context to get at 'mediators'?

### Heterogeneity in an experimental context



## Making inferences from previous work; Meta-analysis, combining studies

### Publication bias

### Combining a few (your own) studies/estimates

### Full meta-analyses

- Models to address publication biases

<!--chapter:end:metrics_outline.Rmd-->

---
bibliography: [support/giving_keywords.bib, support/packages.bib]
biblio-style: apalike
link-citations: yes
---

# Mediators (and selection and Roy models): a review, considering two research applications

(Originally focused on issues relevant to Parey et al)


## DR initial thoughts for NL education paper

<div class="marginnote">
Here were my initial thoughts as pertaining to our paper on the returns to university. </div>

Suppose we observe treatment $T$ (e.g., 'allowed to enter first-choice institution and course'),

intermediate outcome $M$ (e.g., completion of degree in first-choice course and institution),

and final outcome $Y$ (e.g., lifetime income.)\


*Alternately, in the "substitution between charities" context... (unfold)*

```{block2,  type='fold'}

The treatment $T$ is

1. 'asked to donate in the first round' (in Reinstein, Riener and Vance-McMullen, henceforth 'RRV' experiments, and perhaps in Schmitz   2019)',
2. a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?),
3. the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others),

the intermediate outcome $M$ is the amount given to that (first-round) charity,

and the final outcome $Y$ is the amount given to that charity (or other charities) in round 2 (experiments "3": other charities in that round ).

```

The treatment $T$ (may) directly affect the final outcome $Y$.

$$T\rightarrow Y$$

\

$T$ also may affect an intermediate outcome $M$.

$$T \rightarrow M$$

The intermediate outcome also may affect the final outcome $Y$.

$$M \rightarrow Y$$

\

With exogenous variation in $T$ *and* $M$ (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions.

With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of $T$ on $Y$.

We could also compute the share of this effect that occurs *via* the intermediate effect, i.e., $T \rightarrow M\rightarrow Y$.
This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients.

\

However, there are two major challenges to this estimation.

1. We (may) have a valid instrument for (exogenous variation in) $T$ only, and $M$ may arise through a process involving selection on unobserved variables.

2. Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects.

Thus we consult the relevant literature, discussed below.

The most influential paper in Economics has been 

## Econometric Mediation Analyses (Heckman and Pinto)

**Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs**

\

### Relevance to Parey et al {-}

We have an instrument for admission to one's first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these 'intermediate outcomes', including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these.^[a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well]


## Summary and key modeling

There is a 'production function'

- cf income as a function of human capital, opportunities, etc.

- cf donation as a function of income, prices, mood, framing, etc.

\

Treatments (e.g., RCTs) may affect outcomes through the following channels:

1.  observable or proxied inputs

- Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities

- Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood
\

2.  unobservable/unmeasured inputs 

-  cf human capital, social connections
- cf unobservable generosity, wealth, or temporary mood

\

3.  the production function itself, the 'map between inputs and outputs for treatment group members'

- Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital 'matter more' at some institutions?

- Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else?


If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs.

> RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs \... \[nor distinguish effects from changes in production functions\].

\

Here "we can test some of the strong assumptions implicitly invoked".

"Direct effects" as commonly stated refer to the impact of both channels 2 and 3 above.


<div class="marginnote">
 DR: Channel 2 isn't really a direct effect imho (what was this?)
</div>

\

**Standard potential outcomes framework:**

$$Y=DY_{1}+(1-D)Y_{0}$$

$$ATE=E(Y_{1}-Y_{0})$$

**Production function**

$$Y_{d}=f_{d}(\mathbf{\mathbf{{\theta}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}$$

\... the function under treatment $d$; of proxied and unobserved inputs that occur under state $d$, and baseline variables.\

The production function implies:

$$ATE=E\Big(f_{1}(\mathbf{\mathbf{{\theta}}}_{1}^{p},\mathbf{\mathbf{{\theta}}}_{1}^{u},\mathbf{{X}})-f_{0}(\mathbf{\mathbf{{\theta}}}_{0}^{p},\mathbf{\mathbf{{\theta}}}_{0}^{u},\mathbf{{X}})$$\

We also consider counterfactual outputs, fixing treatment status and
proxied inputs:

$$Y_{d,\bar{\theta_{d}}^{p}}=f_{d}(\mathbf{\mathbf{{\bar{{\theta}}}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}$$\

This allows us to decompose ('as in the mediation literature'):

*ATE(d)=IE(d)+DE(d)*

-   *IE, Indirect effect*: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment
    statuses)

-   *DE, Direct effect*: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed
    at one of the two treatment statuses)\

    \

HP further decompose the direct effect into:

-   $DE'(d,d')$: The impact of letting the treatment vary the map only
    (fixing the rest at one of the two appropriate values)

-   $DE''(d,d')$: The impact of letting the treatment vary the
    unmeasured inputs only (fixing the rest at one of the two
    appropriate values)

They use this to give two further ways of decomposing the ATE.\

**Common assumptions**\

"The [standard literature]{.underline} on mediation analysis in psychology regresses outputs on mediator inputs" \... often adopts the strong assumptions of:

1.  no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy)
    and[^1]

2.  full invariance of the production function: $f_{1}=f_{0}$.

\... which implies $Y_{d}=f(\mathbf{\theta}_{d}^{p},d,\mathbf{X})$.\

[Sequential ignorability (Imai et al, 10, '11)]{.underline}:
Essentially, independent randomization of both treatment status and measured inputs.[^2]\

*This sentence is hard to follow:*

> In other words, input $\theta_{d'}^{p}$ is statistically independent of potential outputs when treatment is fixed at $D=d$ and measured inputs are fixed at $\bar{\theta_{d'}^{p}}$ conditional on treatment assignment $D$ and same preprogram characteristics $X$.


\

This assumption yields the 'mediation formulas':

\begin{aligned}
E(IE(d)|X)= & \int E(Y|\theta^{p}=t,D=d,X)\underbrace{\Big(dF_{(\theta^{p}|D=1,X)}(t)-dF_{(\theta^{p}|D=1,X)}(t)\Big)}_{{\text{Difference in distribution of proxy inputs}}} & (9)\\
E(DE(d)|X)= & \int\underbrace{\Big(E(Y|\theta^{p}=t,D=1,X)-E(Y|\theta^{p}=t,D=0,X)\Big)}_{\text{Dfc in expectations (unobservables, function) between treatments given proxy inputs }}expe\underbrace{{dF_{(\theta^{p}|D=1,X)}(t)}}_{\text{Distn proxy inputs for D=1}} & (10)
\end{aligned}

*(??F is presumably the distribution over the observables; where did the
unobservables go? They are in the expectations, I guess.)*\
[Difference from RCT]{.underline}

*What RCT doesn't do:*

> \[sequential ignorability\] translates into \... [no confounding
> effects]{.underline} on both treatments and measured inputs \... does
> not follow from a randomized assignment of treatment \...\[which\]
> ensures independence between treatment status and counterfactual
> inputs/outputs \... \[but *not*\] between proxied inputs
> $\theta_{d}^{p}$ and unmeasured inputs $\theta_{d}^{u}$. \[Thus *not*
> between counterfactual outputs and measured inputs is assumed in
> condition (ii).\]

Cf, randomizing 'win first-choice institution' does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution.

*What RCT* *[does]{.underline}* *do:*

RCT ensures "independence between treatment status and counterfactual
inputs/outputs", thus identifying 'treatment effects for proxied inputs
and for outputs.

CF, we can identify the impact of the treatment 'win first chosen
institution' on proxied input like 'enters a specialization' and on
outputs like 'income in observed years.'

\

## Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity

Summary {#summary .unnumbered}
-------

-   \... 4000+ families targeted, incentive to relocate from projects to
    better neighbourhoods.

-   Easy to identify impact of vouchers

-   Challenge (here) is to assess impact of *neighborhoods* on outcomes.

-   Method here to decompose the TEOT into unambiguously interpreted
    effects. Method applicable to 'unordered choice models with
    categorical instrumental variables and multiple treatments'

-   Finds significant causal effect on labour market outcomes

Relevance to Parey et al {#relevance-to-parey-et-al-1 .unnumbered}
------------------------

1.  We also have an instrument (DUO lottery numbers) cleanly identifying
    the effect of the 'opportunity to do something' (in our case, to
    enter the course at your preferred institution). However, we also
    want to measure the impact of choices 'encouraged' by the
    instrument, such as (i) attending the first choice course and
    institution and (ii) completing this course. We also deal with
    unordered choices (i. enter course and institution, enter course at
    other institution, enter other course at institution, enter neither)
    (ii. choice of medical specialisation)

2.  The geographic outcome is relevant to our second paper (impact on
    'lives close to home')

Introduction  {#introduction .unnumbered}
------------

The causal link between neighborhood characteristics and resident's
outcomes has seldom been assessed.

**Treatments:**

-   Control (no voucher)

-   Experimental: could use voucher to lease in low-poverty neighborhood

-   Section 8: Could use voucher in any () neighborhood

*Many papers evaluate the ITT or TOT effects of MTO.*

-   ITT: effect of being *offered* voucher

    -   estimated as difference in average outcome of experimental vs
        control families

-   TOT: effect for 'voucher compliers' (assuming no effect of simply
    being *offered* voucher on those who don't use it)

    -   estimated as ITT/compliance rate

> \[ITT and TOT\] are the most useful parameters to investigate the
> effects of *offering* \[EA\] rent subsidising vouchers to families.

Identification strategy brief {#identification-strategy-brief .unnumbered}
-----------------------------

-   Vouchers as IVs for choice among 3 neighborhood alternatives (no
    relocation, relocate bad, relocate good) *\[Cf: enter course and
    fp-institution, enter course at other institution, do not enter
    course\]*

-   Neighborhood causal effects as difference in counterfactual outcomes
    among 3 categories

-   Challenge: "MTO vouchers are insufficient to identify the expected
    outcomes for all possible counterfactual relocation decisions"

    -   \... "compliance with the terms of the program was highly
        selective \[Clampet-Lundquist and M, 08\]"

-   Solution: Uses theory and 'tools of causal inference. Invokes SARP
    to identify 'set of counterfactual relocation choices that are
    economically justifiable'

-   [Identifying assumption]{.underline}: "the overall quality of the
    neighborhood is not directly caused by the unobserved family
    variables even though neighborhood quality correlates with these
    unobserved family variables due to network sorting"

-   'Partition sample \... into unobserved subsets associated with
    economically justified counterfactual relocation choices and
    estimate the causal effect of neighborhood relocation conditioned on
    these partition sets.' \[*what does this mean?\]*

Results in brief {#results-in-brief .unnumbered}
----------------

"Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes \... 65% higher than the TOT effect for adult earnings."

Framework: first for binary/binary (simplification) {#framework-first-for-binarybinary-simplification .unnumbered}
---------------------------------------------------

**First, for binary outcomes (simplified)**

$Z_{\omega}$: whether family $\omega$ receives a voucher *(cf
institution-winning lottery number)*

$T_{\omega}$: whether family $\omega$ relocates (*cf enters first choice
institution and course)*\

**Counterfactuals**

-   $T_{\omega}(z)$: relocation decision $\omega$ would choose if it had
    been assigned voucher $z\in{0,1}$': vector of potential relocation
    decisions (*cf education choices)* for each voucher assignment (*cf
    lottery number)*

    -   Can partition into never-takers, compliers, always takers, and
        defiers

-   $(Y_{\omega}(0);Y_{\omega}(1$)): (Potential counterfactual) outcomes
    (*cf income, residence, etc*) when relocation decision is fixed at 0
    and 1, respectively

**Key ( standard) identification assumption: instrument independent of
counterfactual variables**

$$(Y_{\omega}(0),Y_{\omega}(1),T_{\omega}(0),T_{\omega}(1))\perp Z_{\omega}$$

**Standard result 1: ITT**

$$\begin{aligned}
ITT=E(Y_{\omega}|Z_{\omega}=1)-(Y_{\omega}|Z_{\omega}=0)\\
=E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]')P(S_{\omega}=[0,1])+E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[1,0]')P(S_{\omega}=[0,1])\end{aligned}$$

i.e., ITT computation yields the sum of the 'causal effect for
compliers' and the 'causal effect for defiers, weighted by the
probability of each.

**Standard result 2: LATE**

$$\begin{aligned}
LATE=\frac{{ITT}}{P(T_{\omega}=1|Z_{\omega}=1)-P(T_{\omega}=1|Z_{\omega}=0)}= &  & E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]')\\
if\:P(S_{\omega}=[0,1])=0\end{aligned}$$

i.e., the LATE, computed as the ITT divided by the 'first stage' impact
of the instrument, is the causal effect for compliers if there are no
defiers.

Framework for MTO multiple treatment groups, multiple choices {#framework-for-mto-multiple-treatment-groups-multiple-choices .unnumbered}
-------------------------------------------------------------

-   $Z_{\omega}\in\{z_{1,}z_{2,}z_{3}\}$ for no voucher, experimental
    voucher, and section 8 voucher, respectively

-   $T_{\omega}\in\{1,2,3\}$ \... no relocation, low poverty
    neighborhood relocation, high poverty relocation

-   $T_{\omega}(z)$: relocation decision for family $\omega$ if assigned
    voucher $z$

$\ensuremath{\rightarrow}$Response type for each family $\omega$ is that
a three-dimensional vector:
$S_{\omega}=[T_{\omega}(z_{1}),T_{\omega}(z_{2}),T_{\omega}(z_{3})]$.\
$\ensuremath{\rightarrow}$ **ITT** computation now measures a weighted
sum of effects across a subset of those response types whose responses
vary between the assignments being compared.\
\
*Cf:*

-   Considering the 'treatments': '1: enter other course at fp-inst, '2:
    enter course at fp-inst', '3: enter course at non-fp inst'

    -   (I ignore other course at other institution for now)

-   Looking among those who won the course lottery (so we have a binary
    instrument: wininst $Z_{\omega}\in{0,1\}}$

-   Our reduced-form estimates (regressions on the 'lottery number wins
    institution' dummy) measures the probablility-weighted sum of:

    -   impact of institution within course ($T_{\omega}=$2 versus 3);
        for those who would 'fully comply' (enter course at institution
        if $Z_{\omega}=1$, enter course at other institution if 0)

    -   impact of the course at fp-institution versus second-best course
        at fp-institution for 'institution-loving' noncompliers; those
        who would enter the course *only* if they get the fp-institution
        and otherwise another course at the same institution

    -   effects for perverse defiers

[^1]: Cf 'winning institution' impacts human capital, social networks,
    etc identically for everyone; e.g., not a greater effect for men
    then for women, nor a greater effect for those entering particular
    specializations.

[^2]: Cf 'winning institution' does not effect the specialization
    entered nor the location of residence, nor are both determined by a
    third factor.

<!--chapter:end:mediation/mediators_lit_pinto_etc.Rmd-->

