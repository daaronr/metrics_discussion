---
title: "Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus"
author: "Dr. David Reinstein, "
abstract: "This 'book' organizes my notes  and helps others understand it and learn from it"
#cover-image: "images/cardcatalogue.JPG"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    includes:
      in_header: support/header.html
    css: support/tufte_plus.css
    config:
      toc:
        after: |
          <li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>
        collapse: section
        scroll_highlight: yes
      fontsettings:
        theme: white
        size: 2
      sharing:
        facebook: yes
        twitter: yes
        google: no
        linkedin: yes
        weibo: yes
        instapaper: no
        vk: no
        all: ['facebook', 'twitter', 'linkedin', 'weibo', 'instapaper']
    highlight: tango
    download: [pdf, epub, mobi]
    sharing:
      github: yes
      facebook: no
always_allow_html: yes
bibliography: [support/reinstein_bibtex.bib, ./references.bib] #this second one seems to be auto-generated ?
#
biblio-style: apalike
link-citations: yes
github-repo: daaronr/metrics_discussion_work
description: ""
#url: 'https\://daaronr.github.io//'
tags: [Econometrics, Statistics, Data Science, Experiments, Notes, Methodology]
---


<!--
base file created from

`pandoc -f docx -t gfm -o writing_econ_gfm.md "bookoutline3-cutting examples down-cutnamesd.docx" `

and similar from


`pandoc -f docx -t gfm -o writing_econ_gfm1.md "Adapting back for BOOK --Ec831 outline-fillingindetails_forslides_edMiriam-conflict.docx"`

replacements needed:

- "\[ \]" surrounds math -- square brackets do not need 'escape' in main text
- colors need adjusting to 'format_with_col'

-->

Try downloading and accessing some functions here:
```{r download-formatting-to-support-folder}

library(here)
tryCatch(
  {
download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/header.html", destfile = here("support", "headerX.html"))
  download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css", destfile = here("support", "tufte_plusX.css"))
  download.file(url = "https://www.dropbox.com/s/3i8bjrgo8u08v5w/reinstein_bibtex.bib?raw=1", destfile = here("support", "reinstein_bibtexX.bib"))
},  error = function(e) {
  print("you are not online, so we can't download")
}
)

tryCatch(
file.rename(here("support", "headerX.html"), here("support", "header.html"))
)

tryCatch(
file.rename(here("support", "tufte_plusX.css"), here("support", "tufte_plus.css"))
)
tryCatch(
file.rename(here("support", "reinstein_bibtexX.bib"), here("support", "reinstein_bibtex.bib"))
)
```

```{r download-fncns}

library(here)
library(devtools)

tryCatch(
  {
download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/functions/functions.R", destfile = here("code", "functionsX.R"))
  download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/functions/baseoptions.R", destfile = here("code", "baseoptionsX.R"))
},  error = function(e) {
  print("you are not online, so we source locally instead; hope you've updated")
  source(here("code", "functions.R")) # functions grabbed from web and created by us for analysis/output
 source(here("code", "baseoptions.R")) # Basic options used across files and shortcut functions, e.g., 'pp()' for print
}
)

#Note - workaround naming and code because otherwise a failed download seems to delete the destination file -- how to fix that? Renaming files?

file.rename(here::here("code", "functionsX.R"), here::here("code", "functions.R"))
file.rename(here::here("code", "baseoptionsX.R"), here::here("code", "baseoptions.R"))

source(here("code", "functions.R"))
source(here("code", "baseoptions.R"))

# Basic options used across files and shortcut functions, e.g., 'pp()' for print
# functions grabbed from web and created by us for analysis/output
```

```{r eval=FALSE}
install.packages("bookdown")
install.packages("tufte")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

```{r packages}

##TODO: this probably repeats the above -- check and integrate

library(here)
#library(checkpoint) #to avoid differential processing from different package versions
library(pacman)

here <- here::here

p_load(dplyr,magrittr,purrr,tidyverse,tidyr,broom,janitor,here,glue,dataMaid,glue,readr, lubridate,summarytools,gtools,knitr,pastecs,data.table)   #citr, reporttools, experiment, estimatr,  kableExtra, ggsignif, glmnet, glmnetUtils, rsample,snakecase,zoo
library(codebook)

```


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r somefunctions}

#possibly move these to a separate file

#multi-output text color
#https://dr-harper.github.io/rmarkdown-cookbook/changing-font-colour.html#multi-output-text-colour
#We can then use the code as an inline R expression format_with_col("my text", "red")

format_with_col = function(x, color){
  if(knitr::is_latex_output())
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(knitr::is_html_output())
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}

```

```{r html, echo=FALSE}
# globally set chunk options
knitr::opts_chunk$set(fig.align='center', out.width='80%')

my_output <- knitr::opts_knit$get("rmarkdown.pandoc.to")

```



<!---
Can define text blocks here, refer to them again and again if desired
-->

<!-- Global site tag (gtag.js) - Google Analytics -->


<!-- <html> -->

<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLKFNFTGXX"></script> -->
<!-- <script> -->
<!--   window.dataLayer = window.dataLayer || []; -->
<!--   function gtag(){dataLayer.push(arguments);} -->
<!--   gtag('js', new Date()); -->

<!--   gtag('config', 'G-QLKFNFTGXX'); -->
<!-- </script> -->
<!-- </html> -->

<!--chapter:end:index.Rmd-->

# Introduction

My goal in putting this resource is to focus on the practical tools I use and the challenges I ([David Reinstein](https://daaronr.github.io/markdown-cv/)) face. But I am open to collaboration with others on this.

\

*My focus:* Microeconomics, behavioral economics, focus on charitable giving and 'returns to education' type of straightforward problems. (Minimal focus on structural approaches.)

What I care about: Where we can *add value* to real econometric (and statistical and experimental design) practice?

\

The data I focus on:

- Observational (esp. web-scraped and API data and national surveys/admin data)

- Experimental: esp. where with multiple crossed arms, and where the 'cleanest design' may not be possible

\

I will assume familiarity with most basic statistical concepts like 'bias', 'consistency', and 'null hypothesis testing.' However, I will focus on some concepts that seem to often be misunderstood and mis-applied.

```{block2,  type='note'}

If you are involved with this project, you can find a brief guide (somewhat WIP) on how to add content (HERE)[https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html].  This is from a different project but the setup is basically the same.

```  

\

A note to potential research assistants and student collaborators (unfold):

```{block2,  type='fold'}
 
```

## (Conceptual: approaches to statistics/inference and causality)[#conceptual]

### Bayesian vs. frequentist approaches {-}

Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)

### Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

#### DAGs and Potential outcomes

### Theory, restrictions, and 'structural vs reduced form'

## Getting, cleaning and using data; project management and coding {#data-sci}

This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.  

### Data: What/why/where/how

### Organizing a project

### Dynamic documents (esp Rmd/bookdown)

### Good coding practices


#### New tools and approaches to data (esp 'tidyverse')

#### Style and consistency

Indenting, snake-case, etc

#### Using functions, variable lists, etc., for clean, concise, readable code

### Data sharing and integrity


## Basic regression and statistical inference: Common mistakes and issues

### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: 	Simonsohn18

http://datacolada.org/62

### OLS and heterogeneity

- OLS does *not* identify the ATE

http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT


- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"

"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

#### Confidence intervals and Bayesian credible intervals

#### Comparing relative parameters

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT)

See [@verkaik2016]

### Interaction terms and pitfalls

#### 'Moderators' Confusion with nonlinearity

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if outcome  'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative

#### MHT

### Choice of test statistics (including nonparametric)

<!-- (Or get to this in the experimetrics section) -->

### How to display and write about regression results and tests

### Bayesian interpretations of results

(see 'the Bayesian Approach')

## LDV and discrete choice modeling

## Robustness and diagnostics, with integrity

### (How) can diagnostic tests make sense? Where is the burden of proof?

### Estimating standard errors

### Sensitivity analysis: Interactive presentation

## [Control strategies and prediction; Machine Learning approaches](#control-ml)

### Machine Learning (statistical learning): Lasso, Ridge, and more

### Limitations to inference from learning approaches

## IV and its many issues


### Instrument validity

- Exogeneity vs. exclusion
- Very hard to 'powerfully test'


### Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

Focusing on a binary endogenous 'treatment' variable 

- With heterogeneity 
- With imperfect compliance 
- With one-way compliance

### Weak instruments, other issues

- With a 'weak instrument' ... why does that matter?

### Reference to the use of IV in experiments/mediation

## [Other paths to observational identification](#other_paths)

### Fixed effects and differencing

### DiD

FE/DiD does not rule out a correlated dynamic unobservable, causing a bias

### RD

### Time-series-ish panel approaches to micro

#### Lagged dependent variable and fixed effects --> 'Nickel bias'

## Causal pathways: [Mediation modeling and its massive limitations](#mediators)

An applied review

## Causal pathways: selection, corners, hurdles, and 'conditional on' estimates


### 'Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection

"Conditional on positive"/"intensive margin" analysis ignores selection *identification issue* 	See Angrist and Pischke on "Good CoP, bad CoP".  See also bounding approaches such as [@Lee2018]	AngristJ.D.2008a,

\

#### Bounding approaches (Lee, Manski, etc)

See [Notes on Lee bounds](#notes_lee)


## [(Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters](#why_experiment_etc)

### Why run an experiment or study?

- Sugden and Sitzia critique here, give more motivation

### Causal channels and identification

- Ruling out alternative hypotheses, etc

### Types of experiments, 'demand effects' and more artifacts of artifical setups

### Generalizability (and heterogeneity)

## (Experimental) Study design: Background and quantitative issues

### Pre-registration and Pre-analysis plans

#### The hazards of specification-searching

### Sequential and adaptive designs

Needs to adjust significance tests for augmenting data/sequential analysis/peeking	Statistics/econometrics	new-statistics		sagarin_2014			http://www.paugmented.com/	resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/

Yet ...


P_augmented may *overstate* type-1 error rate	Statistics/econometrics	response to referees, new-statistics	"
A process involving stopping ""whenever the nominal $p.0.5$"" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5\%. Even if the subsequent data suggested a ""one in a million chance of arising under the null"" the overall process yields a 5\%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the ""likelihood of the null hypothesis"" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate.

Considering the calculations in \ref{sagarin2014}, it is clear that $p_{augmented}$ should \textit{overstate} the type-1 error of the process if there is a positive probability that after an initial experiment attains p$<0.05$, more data is collected.  A headline $p<0.05$ does \textit{not} imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded $p>0.05$, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though $p<0.05$. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine's experiment).  Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p<0.05, more data is collected, implying that $p_{augmented}$ as calculated above overstates the type I error rate that would arise  from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below $p=0.05$, in order to attain a type-1 error rate of 5% or less in our published corpus."

### Efficient assignment of treatments

(Links back to power analyses)


## (Experimental) Study design: (Ex-ante) Power calculations 

### What sort of 'power calculations' make sense, and what is the point?

#### The 'harm to science' from running underpowered studies

> "One worries about underpowered tests. Your result (may have)  relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.

> Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects."	verkaik2016, metzger2015

### Power calculations without real data

### Power calculations using prior data

## ['Experimetrics' and measurement of treatment effects from RCTs] (#experimetrics_te)

### Which error structure? Random effects?

### Randomization inference?

### Parametric and nonparametric tests of simple hypotheses

### Adjustments for exogenous (but non-random) treatment assignment

### IV in an experimental context to get at 'mediators'?

### Heterogeneity in an experimental context

## [Making inferences from previous work; Meta-analysis, combining studies](#metaanalysis)

### Publication bias

### Combining a few (your own) studies/estimates

### Full meta-analyses

- Models to address publication biases

## The Bayesian approach

## Some key resources and references 

[@AngristJ.D.2008a]

'The Mixtape' (Cunningham)

[@kennedyGuideEconometrics2003]

[@Tibshirani]

OSF guides

Christensen ea "Transparent and Reproducable Social Science Research"

[@Gentzkow2013; @wooldridgeEconometricAnalysisCross2002; @wooldridgeIntroductoryEconometricsModern2008]


An Introduction to Statistical Learning with Applications in R

R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org 

Statistical Rethinking: A Bayesian Course with Examples in R and Stan

### Consider: 

Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017
  
Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction .
Cambridge University Press, 2015

Judea Pearl

Imbens: Potential Outcomes versus DAGs

<!--chapter:end:metrics_outline.Rmd-->

#  Conceptual: approaches to statistics/inference and causality {#conceptual}

## Bayesian vs. frequentist approaches

Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)


### Interpretation of CI's (aside)

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The fact that 95% of all (correct) CIs contain the true value does not mean that 95% of those that exclude zero do so correctly. You could have (say) 59% correct coverage for 10% excluding zero and 99% for 90% including zero.</p>&mdash; Stephen John Senn (@stephensenn) <a href="https://twitter.com/stephensenn/status/1219680319114227714?ref_src=twsrc%5Etfw">January 21, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

## Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

### DAGs and Potential outcomes

See especially  Imbens, 2019: "Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics"

## Theory, restrictions, and 'structural vs reduced form'


<!--chapter:end:conceptual/conceptual.Rmd-->

# Getting, cleaning and using data {#data-sci}

This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.

Some key resources are in a continually updated airtable [HERE](https://airtable.com/shrqNt0YAa3eLiK5S)

See especially:

[R for data science](r4ds.had.co.nz)

[Advanced R](https://adv-r.hadley.nz/)

[bookdown: Authoring Books and Technical Documents with R Markdown:](https://bookdown.org/yihui/bookdown/)

[OSF: 'PhD Toolkit on Transparent, Open, Reproducible Research'	https://osf.io/g8yjz/](OSF: 'PhD Toolkit on Transparent, Open, Reproducible Research'	https://osf.io/g8yjz/)


[Happy Git and GitHub for the useR](https://happygitwithr.com/)


"Data science for business"

"Code and Data for the Social Sciences" (Gentzkow/Shapiro)


## Data: What/why/where/how

## Organizing a project

## Dynamic documents (esp Rmd/bookdown)

Some guidelines from a particular project:

[Appendix: Tech for creating, editing and collaborating on this ‘Bookdown’ web book/project (and starting your own)](https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html)

### Managing references/citations


```{block2,  type='note'}
A letter to my co-authors...

Hi all.

Hope you are doing well. I've just invited you to a shared Zotero group managing my bibliography/references. I think this should be useful. (I prefer Zotero to Mendeley because it's open source and... I forgot the other reason.)
On my computer it synchronizes with a .bib (bibtex) file in a dropbox folder ...

For latex files we just refer to this as normal.
In the Rmd files/bookdown (producing output like [EA barriers](#https://daaronr.github.io/ea_giving_barriers/outline.html) or Metrics notes (present book) this is referenced in the YAML header to the index.Rmd file as

> bibliography: [reinstein_bibtex.bib]

Then, to keep this file, I have a "download block" included in that same file (the first line with 'dropbox' is the key one).

```

\

The download code follows (remove the 'eval=FALSE' to get it to actually run)...

```{r download-formatting-to-support-folder-example, eval=FALSE}

tryCatch( #trycatch lets us 'try' to execute and if there is an error, it does the thing *after* the braces, rather than crashing
  {
   download.file(url = "https://www.dropbox.com/s/3i8bjrgo8u08v5w/reinstein_bibtex.bib?raw=1", des tfile = "reinstein_bibtex.bib") #download the bibtex database

        download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css", destfile = here("support", "tufte_plus.css")) #this downloads the style file

  }, error = function(e) {
    print("you are not online, so we can't download")
  }
)
```


A fairly comprehensive discussion of tools for citation in R-markdown:

[A Roundup of R Tools for Handling BibTeX](https://ropensci.org/technotes/2020/05/07/rmd-citations/)


### An example of dynamic code


Shapiro Wilk test for normality; professor salaries at some US university from the built in Cars data...

<div class="marginnote">
By the way, if anyone wants me to offer me a job at that university, it looks like a great deal!
</div>

```{r Diagnostic-tests, include=TRUE}


prof_sal_shapiro_test <- shapiro.test(carData::Salaries$salary)

  # ShapiroTest <- map_df(list(SXDonShapiroTest, EXDonShapiroTest), tidy)
  # (ShapiroTest <- kable(ShapiroTest) %>% kable_styling())
```

The results from the Shapiro Wilk normality test ...

The p-values are  `r prof_sal_shapiro_test[2]` suggesting this data `r ifelse(prof_sal_shapiro_test[2]<0.10, "is not", "is")` normal

## Project management tools, esp. Git/Github

(More to be added/linked here)

See ['Git and GitHub' here... watch this space](https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html#git-and-github)

\

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">For students and research assistants, I&#39;ve been sending first time git users/developers to this:<a href="https://t.co/P6KQpXHCWI">https://t.co/P6KQpXHCWI</a><br>+ <a href="https://t.co/q4R4Ei5Biw">https://t.co/q4R4Ei5Biw</a></p>&mdash; Nathan Lane (@straightedge) <a href="https://twitter.com/straightedge/status/1172694350205087744?ref_src=twsrc%5Etfw">September 14, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

## Good coding practices

### New tools and approaches to data (esp 'tidyverse')

From [Kurtz](https://bookdown.org/content/3890/small-worlds-and-large-worlds.html):

> If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., `%>%`). I'm not going to explain the `%>%` in this project, but you might learn more about in [this brief clip](https://www.youtube.com/watch?v=9yjhxvu-pDg), starting around [minute 21:25 in this talk by Wickham](https://www.youtube.com/watch?v=K-ss_ag2k9E&t=1285s), or in [section 5.6.1 from Grolemund and Wickham's *R for Data Science*](https://r4ds.had.co.nz/transform.html#combining-multiple-operations-with-the-pipe). Really, all of Chapter 5 of *R4DS* is just great for new R and new tidyverse users. And *R4DS* Chapter 3 is a nice introduction to plotting with ggplot2.

### Style and consistency

#### lower_snake_case {-}

Use `lower_snake_case` to name *all* objects (that's my preference anyways)
unless there's a strong reason to do otherwise.

\

This includes:

`file_names.txt`
`folder_names`
`function_names` (with few exceptions)
`names_of_data_objects_like_vectors`
`names_of_data_output_objects_like_correlation_coefficients`
`ex_df1` In R you probably should keep data frame names short to avoid excessive typing

\

*And by all that is holy, never put spaces or slashes in file or object names!* This can make it very hard to process across systems... there are various ways of referring to spaces and other white space.

#### Indenting and spacing


### Using functions, variable lists, etc., for clean, concise, readable code

### Mapping over lists to produce results

<!-- ft <- list(ft_treat_no_ask, ft_treat_no_ask_19, ft_treat_no_ask_19_short, ft_treat_no_ask_19_long)

ft_names <- c("All", "2019", "2019-short", "2019-long")

ft <- map2(ft, ft_names, function(x, y) {
  broom::tidy(x) %>% add_column(Experiment = y)
}) %>%
  bind_rows() %>%
  kable(, caption = "S2 Donation incidence by S1 Ask/no-ask; Fisher tests", digits = 2) %>%
  kable_styling()
-->

### Building results based on 'lists of filters' of the data set

We can store a filter as a character vector and then apply it
```{r}

selection_statement <- "Species == 'setosa' & Petal.Width>0.3"

iris %>%  
   as.tibble() %>%
    filter(rlang::eval_tidy(rlang::parse_expr(selection_statement)))
```

Making this a function for later use:

```{r}

selection_statement <- "Species == 'setosa' & Petal.Width>0.3"


filter_parse =  function(df, x) {
  {{df}} %>%
   filter(rlang::eval_tidy(rlang::parse_expr({{x}})))
}

iris %>%
  as.tibble() %>%
 filter_parse(selection_statement)

```



We can do the same for a list of character vectors of filter statements, and apply each filter from the list to the dataframe, and then the output function:

```{r}

sel_st <- c("Species == 'setosa' & Petal.Width>0.3", "Species == 'virginica' & Petal.Width>2.4")

map(iris, selection_statement)

sel_st %>% map(~ filter_parse(iris, .x))

```

It works nicely if you have a list of filters aligned with a list of names or other objects 'specific to each filter'


(Code below: adapt to public data and explain)

```
 bal_sx <-
    map2(subsets_sx_dur, subsets_sx_dur_name, function(x, y) {
      filter_parse(sa, x) %>%
    dplyr::filter(stage == "2") %>%
     tabyl(treat_1, treat_2, show_missing_levels = FALSE)  %>%
    adornme_not(cap = paste(y, "; 1st and 2nd stage treatments"))
    }
  )
```


### Coding style and indenting in Stata (one approach)

I indent every line except

- clear, import, save, merge ('file operations')
  - except where these occur as part of a loop, in which case I put in an 'important comment' noting these operations

- lines that call other do files

- important comments/flags/to-do's


I only put 'small todo' elements having to do with code in a code file itself (and even then there may be better places). If we are going to put todos I suggest we include `#todo` to search for these later (and R has a utility to collect these in a nice way... maybe Stata does too.


\


Whenever there are more than 20 lines of something prosaic that cannot/has not been put into a loop or function, I suggest we put it in a separate 'do' file and call that do file (with no indent).  That's what I do here, giving a brief description and a 'back link'.  Sometimes I put all those do files into an separate folder.


## Additional tips (integrate)

[When you have to upgrade R on Mac, how to preserve package installations - twitter thread](https://twitter.com/gdequeiroz/status/1228722821817225216)


- [This worked well for me](https://github.com/ivelasq/r-data-recipes/blob/master/README.md#reinstall-packages-after-a-major-r-update). Thanks
@ivelasq3 !
\

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Are you teaching/learning <a href="https://twitter.com/hashtag/r?src=hash&amp;ref_src=twsrc%5Etfw">#r</a> <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> &amp; want to teach/learn the latest <a href="https://twitter.com/hashtag/tidyverse?src=hash&amp;ref_src=twsrc%5Etfw">#tidyverse</a> <a href="https://twitter.com/hashtag/tidyr?src=hash&amp;ref_src=twsrc%5Etfw">#tidyr</a> tools? e.g. bind_rows, pivot_wider/pivot_longer, the join family (full_join, inner_join...) Check out my slides on &quot;Advanced data manipulation&quot; here 😺<a href="https://t.co/dr9VNx7MFf">https://t.co/dr9VNx7MFf</a></p>&mdash; Amy Willis (@AmyDWillis) <a href="https://twitter.com/AmyDWillis/status/1195498569072959490?ref_src=twsrc%5Etfw">November 16, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

\

**Tools for structuring your workflow for reproducable code with Rmd and Git: The workflowr package** 

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;This paper does a thorough job setting out the rationale, design, and implementation of the workflowr package&quot; says <a href="https://twitter.com/PeteHaitch?ref_src=twsrc%5Etfw">@PeteHaitch</a> <a href="https://twitter.com/WEHI_research?ref_src=twsrc%5Etfw">@WEHI_research</a> in his review of this <a href="https://twitter.com/hashtag/softwaretool?src=hash&amp;ref_src=twsrc%5Etfw">#softwaretool</a> article introducing workflowr by <a href="https://twitter.com/jdblischak?ref_src=twsrc%5Etfw">@jdblischak</a> and co-authors <a href="https://t.co/ZXmQkDhFuD">https://t.co/ZXmQkDhFuD</a> <a href="https://twitter.com/hashtag/OpenScience?src=hash&amp;ref_src=twsrc%5Etfw">#OpenScience</a> <a href="https://t.co/e77SVo8PhO">pic.twitter.com/e77SVo8PhO</a></p>&mdash; F1000Research (@F1000Research) <a href="https://twitter.com/F1000Research/status/1196056651691962368?ref_src=twsrc%5Etfw">November 17, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

\

Switching from Latex to markdown/R-markdown? [These tips from Colin Bousige look pretty good](https://colinbousige.github.io/post/rmarkdown/), although I prefer the bookdown/gitbook format

<!--chapter:end:data_sci/data_sci.Rmd-->

# Basic regression and statistical inference: Common mistakes and issues {#reg-follies}



## Basic regression and statistical inference: Common mistakes and issues briefly listed


Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)	Identification

Random effects estimators show a lack of robustness	Specification	Clustering SE  is more standard practice

OLS/IV estimators not 'mean effect' in presence of heterogeneity

Power calculations/underpowered

Selection bias due to attrition

Selection bias due to missing variables -- impute these as a solution

Signs of p-hacking and specification-hunting

Weak diagnostic/identification tests

Dropping zeroes in a "loglinear" model is problematic

Random effects estimators show a lack of robustness

Dropping zeroes in a "loglinear" model is problematic

Random effects estimators show a lack of robustness

-(Some notes on multi-level modeling) and RE linked [in this Twitter thread](https://twitter.com/DavidPoe223/status/1239447381172727809)
 
With heterogeneity the simple OLS estimator is not the 'mean effect'

P_augmented may *overstate* type-1 error rate

Impact size from regression of "log 1+gift amount"

Lagged dependent variable and fixed effects --> 'Nickel bias'

Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)

Weak IV bias

Bias from selecting instruments and estimating using the same data


### Bad control



From MHE:

> some variables are bad controls and should not be included in a regression model even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notational experiment at hand. That is, bad controls might just as well be dependent variables too."

– They could also be interpreted as endogenous variables.

Example of looking at a regression of wages in schooling, controlling for college degree completion:

> Once we acknowledge the fact that college affects occupation, comparison of wages by college degree status within occupation are no longer apples to apples, even if college degree completion is randomly assigned."

– The question here was whether to control for the category of occupation, not the college degree.

> It is also incorrect to say that the conditional comparison captures the part of the effect of college that is 'not explained by occupation' ... so we would do better to control only for variables that are not themselves caused by education."



### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: 	Simonsohn18

http://datacolada.org/62

### OLS and heterogeneity

#### OLS does *not* identify the ATE (-)

In general, with heterogeneity, OLS does *not* identify the ATE. It weights observations from different parts of the sample differently. Parts with greater residual variation in the treatment (outcome) variable are more (less) heavily weighted. 

<div class="marginnote">
E.g., if the treatment is binary, the estimator will most heavily weight those parts of the sample where the probability of treatment is closest to 1/2.
</div>
 
The formula is ...

\

```{block2,  type='note'}
**Some intuition**

Why is this the case?  The OLS type estimators we are taught in Econometrics are 'BLUE'  under the assumption of a *single homogenous 'effect'* (the 'slope'...  although the discussion itself is often agnostic as to whether this represents a causal effect). 

\
 
It is 'best' in a minimizing MSE sense under certain assumptions;  in particular, we must also know the true functional form and the set of variables to be included. See 'overfitting' issues.
 
In  order to have the estimate of the true slope that minimizes the squared errors, OLS (and related estimators like FGLS; as well as 2SLS in a more complicated sense) weights  some observations more than others. The 'influence' of an observation on the estimated slope depends on the nature of the variation in the  dependent and independent variables in the region that observation is drawn from.  Think of drawing a line  through a set of points  that were drawn with some noise from the true distribution.  If you drew it based on a bunch of points (from a region where) the treatment varies very little and the outcomes have a lot of noise,  the line you draw will be very sensitive to the latter noise and thus unreliable. So,  would optimally 'down-weight'  these observations in drawing the line. 

\

However, if the *actual* slope varies by region, this also means you are under-representing certain regions, and  thus getting  a biased  estimate of the average slope.

```

How can we deal with this?  If we think  that the  treatment effect varies with *observable* variables, we could include 'interactions';  essentially making separate estimates of the slope for each share of the population (but potentially  allowing other control variables to have a homogenous effects, and pooled or clustered estimation of underlying variance.)

<div class="marginnote">
 ...Although we may want to consider both overfitting here and the idea that there may be *some* shared component, so the fully-interacted model may be sub-optimal. See mixed modeling (?) 
</div>

\

However, this does not tell us how to recover the *average* of these slopes (approximately, the ATE).  Should we weight  each of the slopes by the share of the population that this group represents?

Mechanically,  the standard way of estimating and representing these  interactions and economics has been with simple dummies (0,1) for each compared group. This yields a 'base group' (e.g., males aged 35-60) --  this obviously does not recover the average slope-- as well as the 'adjustment' coefficients.

\

Another way of expressing interactions, particularly helpful with multi-level interactions is called 'effect coding': each group is coded as a 'difference from 0' (e.g,. males are -1/2 and females +1/2), before doing the interactions. This could allow for a more straightforward interpretation: at each level, the uninteracted term represents the average treatment effects, and the interacted terms  represent adjustments relative to this average. *But under which conditions is this in fact the case?*


[insert here].

[WB blog - your-go-to regression-specification is -biased-here-s-simple-way-fix-it](http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT)

A key paper: http://www.jcsuarez.com/Files/Suarez_Serrato-BFE.pdf

> In particular, we compare treatment effect estimates using a fixed effects estimator (FE) to the average treatment effect (ATE) by replicating eight influential papers from the American Economic Review published between 2004 and 2009.1 Using these examples, we consider a randomized experiment in Section 1 as a case study and, in Section 3, we show generally that heterogeneous treatment effects are common and that the FE and ATE are often different in statistically and economically significant degrees. In all but one paper, there is at least one statistically significant source of treatment effect heterogeneity. In five papers, this heterogeneity induces the ATE to be statistically different from the FE estimate at the 5% level (7 of 8 are statistically different at the 10% level). Five of these differences are economically significant, which we define as an absolute difference exceeding 10%. Based upon these results, we conclude that methods that consistently estimate the ATE offer more interpretable results than standard FE models

By "FE" here I think they mean group dummies; they are focused on cross-sectional and not panel data!

> While fixed effects permit different mean outcomesamong groups,  the estimates of treatment effects are typically required to be the same;  in more colloquial  terms,  the  intercepts  of  the  conditional  expectation  functions  may  differ,  but  not  the slopes

DGP

$$y_i = x_i \beta_{g(i)} + \mathbf{z_i}' \gamma + \epsilon_i$$

> where $y_i is the outcome for observation i among N [N what?],
$x_i$ is treatment or another variable of interest, and $z_i$ contains control variables, including group-specific fixed effects.  

> The treatment effects aregroup-specific  for  each  of  the $g=1,...,G$ groups,  where  group  membership  is  known  for  each observation. 

Defining ATE

$$\beta^{ATE}=\sum_g \pi_g \beta_g $$

where the $\pi$ terms are population frequencies

\


The use of interaction terms is delicate... 

<!-- 
On the book, I saw the section on the problems of using OLS as an estimator of FE without fully interacting the variables (4.2.4, this blog post), but d
By idn’t really understand the issue/intuition behind the problem/solution – I would be keen to chat about what it means.. and if I then get it, would more than happily contribute a lay summary for the book. Also there are three papers by Ferraro (links below) that I think you might find interesting/offer well explained insights into difficulties in how to do empirical econ/problems within it for the Book. 

Plus there is this paper – https://www.nber.org/papers/w25636 which applies the changes-in-changes method of Athey and Imbens 2006 a method which (proponents claims) is able to get at heterogeneous treatment effects better than simple DiD by bin/something similar.
--> 


<!-- #TODO: recover conversations with Winston Lin and write these up --> 

- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"

"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

#### Confidence intervals and Bayesian credible intervals

#### Comparing relative parameters

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT)

See [@verkaik2016]

### Interaction terms and pitfalls

See also 'effect coding'

#### 'Moderators' Confusion with nonlinearity

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if outcome  'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative


#### MHT

### Choice of test statistics (including nonparametric)

(Or get to this in the experimetrics section)

### How to display and write about regression results and tests

### Bayesian interpretations of results

<!--chapter:end:regression_follies/regression_follies.Rmd-->

# Robustness and diagnostics, with integrity; Open Science resources {#robust-diag}

## (How) can diagnostic tests make sense? Where is the burden of proof?

Where a particular assumption is critical to identification and inference ...Failure to reject the violation of an assumptionis not sufficient to give us confidence that it is satisfied and the results are credible. At several points the authors cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles).

I am concerned with the interpretation of diagnostic testing, both in model selection, and in the defense of the exclusion restrictions or identification assumptions. It is problematic, when the basic consistency of the estimator (or a main finding of the paper) critically depends on such tests failing to reject a null hypothesis, to merely state that the 'test failed to reject, therefore we maintain the null hypothesis'. 

- How powerful are these tests?

- I.e. what is the probability of a false negative Type II error?
- How large a bias would be compatible with reasonable confidence intervals for these tests?


## Estimating standard errors

## Sensitivity analysis: Interactive presentation

## Supplement: open science resources, tools and considerations

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;m in psychology research (just finished PhD) and I want to get into <a href="https://twitter.com/hashtag/OpenScience?src=hash&amp;ref_src=twsrc%5Etfw">#OpenScience</a> to make sure I&#39;m following best practices. But this is something that wasn&#39;t explicitly taught to me. What are some good resources? Thanks! <a href="https://twitter.com/AcademicChatter?ref_src=twsrc%5Etfw">@AcademicChatter</a> <a href="https://twitter.com/OSFramework?ref_src=twsrc%5Etfw">@OSFramework</a> <a href="https://twitter.com/OpenAcademics?ref_src=twsrc%5Etfw">@OpenAcademics</a> <a href="https://twitter.com/dsquintana?ref_src=twsrc%5Etfw">@dsquintana</a></p>&mdash; Alessa Teunisse (@alessateunisse) <a href="https://twitter.com/alessateunisse/status/1252481892240125952?ref_src=twsrc%5Etfw">April 21, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

\



<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A couple of months ago I made a guide on how to use Binder to make our <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> code <a href="https://twitter.com/hashtag/reproducible?src=hash&amp;ref_src=twsrc%5Etfw">#reproducible</a>. E.g., Binder will make your code runnable using the versions of R and r-packages used when you analyzed your data. <a href="https://t.co/srYNazwy0q">https://t.co/srYNazwy0q</a> <a href="https://twitter.com/hashtag/reproducibility?src=hash&amp;ref_src=twsrc%5Etfw">#reproducibility</a> <a href="https://twitter.com/hashtag/openscience?src=hash&amp;ref_src=twsrc%5Etfw">#openscience</a> <a href="https://t.co/gcTlVFpaY5">pic.twitter.com/gcTlVFpaY5</a></p>&mdash; Erik Marsja (@Emarsja) <a href="https://twitter.com/Emarsja/status/1206850126444204032?ref_src=twsrc%5Etfw">December 17, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


## Diagnosing p-hacking (see also [meta-analysis](#metaanalysis))

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Ever wonder &quot;Were those results p-hacked?&quot; Brodeur et al. propose a useful new check (&quot;speccheck&quot; on Stata. R/etc. coming soon). <a href="https://twitter.com/hashtag/ASSA2020?src=hash&amp;ref_src=twsrc%5Etfw">#ASSA2020</a> <a href="https://t.co/NCZ1jZTaO5">pic.twitter.com/NCZ1jZTaO5</a></p>&mdash; Eva Vivalt (@evavivalt) <a href="https://twitter.com/evavivalt/status/1213608282486525952?ref_src=twsrc%5Etfw">January 4, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<!--chapter:end:meta_anal_and_open_science/robust_diagnos.Rmd-->

# Control strategies and prediction, Machine Learning (Statistical Learning) approaches {#control-ml}

> 'Identification' of causal effects with a control strategy not credible	

Essentially a 'control strategy' is "control for all  or most of the reasonable determinants of the independent variable so as to make the  remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest". All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., [@oster2019unobservable].

## Machine Learning (statistical learning): Lasso, Ridge, and more

### Limitations to inference from learning approaches

## Notes Hastie: Statistical Learning with Sparsity

[google books link](https://books.google.co.uk/books?hl=en&lr=&id=f-A_CQAAQBAJ&oi=fnd&pg=PP1&ots=G4RMC-gZU-&sig=u_EoI3-ZcX0phwsZgLt2Ux7DXyc#v=onepage&q&f=true)


### Introduction

> One form of simplicity is sparsity, the central theme of this book. Loosely speaking, a sparse statistical model is one in which only a relatively small number of parameters (or predictors) play an important role.

"the $\ell_1$ norm is special" (abs value). Other norms yield nonconvex problems, hard to minimize.

> “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems.

- Examples from gene mapping

#### Book roadmap

- Chapter 2 ... lasso for linear regression, and a simple coordinate descent algorithm for its computation.

- Chapter 3 application of $\ell_1$ [lasso-type] penalties to generalized linear models such as multinomial and survival models, as well as support vector machines. [?]

- Chapter 4: Generalized penalties such as the elastic net and group lasso are discussed in Chapter 4.

- Chapter 5: numerical methods for optimization (skip for now]

- Chapter 6:  statistical inference for fitted (lasso) models, including the bootstrap, Bayesian methods and more recent stuff

- Chapter 7: Sparse matrix decomposition [?] (Skip?)
- Ch 8: sparse multivariate analysis of that (Skip?)
- Ch 9: Graphical models and their selection (Skip?)
- Ch 10: compressed sensing (Skip?)
- Ch 11: a survey of theoretical results for the lasso (Skip?)

### Ch2: Lasso for linear models

- N samples (?N observations), want to approx the response variable using a linear combination of the predoctors

***

**OLS** minimizes squared-error loss but

1. Prediction accuracy
- OLS unbiased but 'often has large variance'
- prediction accuracy can be improved by shrinking coefficients (even to zero)
    - yielding biased but perhaps better predictive estimators

2. Interpretation: too many predictors hard to interpret
- DR: I do not care about this for fitting background noise in experiments

#### 2.2 The Lasso Estimator

Lasso bounds the sum of the abs values of coefficients, an "$\ell_1" constraint.

Lasso is OLS subject to


\

$\sum_{j=1..p}{\abs(\beta_j)}\leq t$

\

"compactly" $||\beta||_1\leq t$

with notation for the "$\ell_1$ norm"


\

- Bound $t$ acts as a 'budget', must be specified by an 'external procedure' such as cross-validation

- typically we must *standardize the predictors* $\mathbf{X}** so that each column is centered with unit variance ... as well as  the outcome variables (?) ... can ignore intercept

    - DR: Not clear here whether standardisation is necessary for the procedure to be valid or just convenient for explaining and deriving its properties.

\

<div class="marginnote">
 
Aside: Can re-write Lasso minimization st constraint as a Lagrangian. $\lambda$ plays the same role as $t$ in the constraint. Thus we can speak of the solution to the Lagrangian minimisation problem $\hat{\beta)_{\lambda}$ which also solves the bound problem with $t=||\hat_{\lambda}||_1$.

</div>


<div class="marginnote">
 
Aside: We often remove the $1/2n$ term at the beginning of the minimization problem. Same minimization, minimizing sum of squared deviations rather than something like an average of this.

</div>
\

Express (Karush-Kuhn-Tucker) optimisation conditions for this ...


***

Example from Thomas (1990) on crime data

> Typically ... lasso is most useful for much larger problems, including "wide" data for which $p>>N$

\

Fig 2.1: Lasso vs ridge regression; coefficients of each for a set of considered variables plotted against their respective norms (as shares of maximal bound on coefficient sum measure, i.e., ols, for each)

- Note ridge regression penalises *squared* sums of betas
    - Fig 2.2., in $\beta_1,\beta_2$ space   illustrates the difference well: contour lines of Resid SS elipses, 'budget constraint' for each (disc vs diamond)

(Note: lasso bound was chosen via cross-validation)

- No analytical statistical inference after lasso (some being developed?), bootstrap is common

>  lasso sets two of the five coefficients to zero, and tends to shrink the coefficients of the others toward zero relative to the full least-squares estimate.

- DR: analytically and intuitively, I do not yet understand why lasso should shrink coefficients but not all the way to zero.
    - The penalty is linear in the coefficient size, so I would think the solution would be bang-bang, either drop a coeficient or leave it unchanged. But it is not.
    - Adding an increment to a $\hat{\beta}$ when it is below the OLS estimate should have a linear effect on the RSS (according to my memory and according to Sebastian).
    - But that would mean that shrinking one parameter always yields a better benefit to cost ratio. Thus I should shrink each parameter to zero before beginning to shrink any others. This cannot be right!

I looked up this derivative wrt the beta vector (one needs to set this to 0 to get the ols estimates

[stackexchange](https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression)

$\frac{d RSS}{d \beta}=-2X^{T}(y-X\beta}$

or

$−\frac{d e'e}{d b}=2X′y+2X′Xb$

The answer to this question: while the impact of changing each coefficient on SSR is in fact constant (a constant own-derivative), there is *also* an impact of changing one coefficient on the *other* derivatives. As one coefficient shrinks to zero the marginal impact of the other coefficients on the SSR may (will?) increase.

    - At the same time, we need that the effect of increasing it from zero need not be infinite, so it might not outweigh the linear penalty, thus some coefficients might be set to zero

**Relaxed lasso**

> the least-squares fit on the subset of the three predictors tends to expand the lasso estimates away from zero. The nonzero estimates from the lasso tend to be biased toward zero, so the debiasing in the right panel can often improve the prediction error of the model. This two-stage process is also known as the relaxed lasso (Meinshausen 2007).


- DR: When is this likely to help/hurt relative to pure lasso?

- [Stackexchange discussion](https://stats.stackexchange.com/questions/285501/why-do-we-use-ols-to-estimate-the-final-model-chosen-by-lars/285518#285518) Contrasts a 'relaxed-lasso' from a 'lars-ols'

***

Aside: which seems better for *Control variable selection for prediction/reducing noise to enable better inference of treatment effects*?

Ridge? better than Lasso here? We do not care about *interpreting* the predictors here... so if we allow $\beta$‘s to be shrunk towards zero for each coefficient maybe that should yield better prediction than making them exactly zero?

\

On the other hand if we know the true model is ‘parsimonious’ (as in the genes problem) it might boost efficiency to allow inference about coefficients that should be exactly zero (edited)

***

#### 2.3 Cross-Validation and Inference

Generalization ability
: accuracy  for predicting independent test data from the same population

... find the value of t that does best


**Cross-validation procedure*

1. randomly divide ... dataset into K groups.

"Typical choices ... might be 5 or 10, and sometimes N."

2. One 'test', remaining K-1 'training'

3. Apply lasso to training data for a range of t values,
    - use each fitted model to predict the responses in the test set, recording mean-squared prediction errors for each value of t.

4. Repeat the previous step K times
    - each time, one of the K groups is the test data, remaining K − 1 are training data.
    - yields K different estimates of the prediction error over a range of t values.

5. Average K estimates of prediction error for each value of t $\rightarrow$  cross-validation error curve.

Fig 2.3 plots an example with K=10 splits for cross validation

- ... of the estimated MS prediction error vs the relative bound $\tilde{t}$(summed absolute value of Lasso betas divided by summed abs value of OLS betas).
- Also draw dotted line at the 1-std-error rule choice of $\tilde{t$}
- Number of nonzero coefficients plotted at top

#### 2.4 Computation of the Lasso solution

DR: I think I will skip this for now

least angle/LARS is mentioned at the bottom as a 'homotopy method' which "produce the entire path of solutions in a sequential fashion, starting at zero"

#### 2.5 Degrees of freedom

...

Jumping to

#### 2.10 Some perspective

**Good properties of the Lasso ($\ell_1$ penalty)**

- Natural interpretation (enforce sparsity and simplicity)

- Statistical efficiency ... if the underlying true signal is sparse (but if it is not sparse "no method can do well relative to the Bayes error")

- Computational efficiency, as $\ell_1$ penalties are convex

### Chapter 3: Generalized linear models

### Chapter 4: Generalizations of the Lasso penalty

> lasso does not handle highly correlated variables very well; the coefficient paths tend to be erratic and can sometimes show wild behavior.

The elastic net makes a compromise between the ridge and the lasso penalties (Zou and Hastie 2005)1] is a parameter that can be varied.

For an individual coefficient the penalty is
$\frac{1}{2} (1-\alpha)\beta_j^2 + \alpha|\beta_j|$

(a convex combo of the lasso and ridge penalties)

multiplied by a 'regularization weight' $\lambda>0$ which plays the same role (I think) as in lasso

- elastic net is also *strictly convex*

## Notes: Mullainathan

> The fundamental insight behind these breakthroughs is as much statistical as computational. Machine intelligence became possible once researchers
stopped approaching intelligence tasks procedurally and began tackling them
empirically. Face recognition algorithms, for example, do not consist of hard-wired
rules to scan for certain pixel combinations, based on human understanding of
what constitutes a face. Instead, these algorithms use a large dataset of photos
labeled as having a face or not to estimate a function f (x) that predicts the presence y of a face from pixels x

(p2)
> supervised- machine learning, the focus of this article) revolves around the problem of prediction: produce predictions of y from x

... 

> manages to fit complex and very flexible functional forms to the data without simply overfitting; it finds functions that work well out-of-sample 

> danger in using these tools is taking an algorithm built for [predicting $y$-] and presuming their [parameters $\beta$] - have the properties we typically associate with estimation output

> One category of such applications appears when using new kinds of data for traditional questions; for example, in measuring economic activity using satellite images or in classifying industries using corporate 10-K filings.
Making sense of complex data such as images and text often involves a prediction
pre-processing step

<div class="marginnote">
This middle category is most relevant for me
</div>

> In another category of applications, the key object of interest is actually a parameter ... but the inference procedures (often implicitly) contain a prediction task. For example, the first stage of a linear instrumental variables regression is effectively prediction. The same is true when estimating heterogeneous treatment effects, testing for effects on multiple outcomes in experiments, and
flexibly controlling for observed confounders

> A final category is in direct policy applications. Deciding which teacher to hire implicitly involves a prediction task (what added value will a given teacher have?), one that is intimately tied to the causal question of the value of an additional teacher.


(p3)

**A useful (interactive?) example:**

> We consider 10,000 randomly selected owner-occupied units from the 2011 metropolitan sample of the American Housing Survey. In addition to the values of each unit, we also include 150 variables that contain information about the unit and its location, such as the number of rooms, the base area, and the census region within the United States. To compare different prediction techniques, we evaluate how well each approach predicts (log) unit value on a separate hold-out set of 41,808 units from the same sample. All details on the sample and our empirical exercise can be found in an online appendix available with this paper at http://e-jep.org

>  In-sample performance may overstate performance; this is especially true for certain machine learning algorithms like random forests that have a strong tendency to overfit. Second, on out-of-sample performance, machine learning algorithms such as random forests can do significantly better than ordinary least squares, even at moderate sample sizes and with a limited number of covariates

(p4)

> algorithms are fitted on the same, randomly drawn training sample of 10,000 units and evaluated on the 41,808 remaining held-out units.

> Simply including all pairwise interactions would be infeasible as it produces more regressors than data points (especially considering that some variables are categorical

> Machine learning searches for these interactions automatically

(p5)

> Shallow Regression Tree Predicting House Values

<div class="marginnote">
not sure what's going on here. is this the random forest thing?
</div>

> The prediction function takes the form of a tree that splits in two at every
node. At each node of the tree, the value of a single variable (say, number of bathrooms) determines whether the left (less than two bathrooms) or the right (two or
more) child node is considered next. When a terminal node-a leaf—is reached, a
prediction is returned. An


So how
does machine learning manage to do out-of-sample prediction?
The first part of the solution is regularization. In the tree case, instead of
choosing the -best” overall tree, we could choose the best tree among those of a
certain depth.

(p5)
Tree depth is an example of a regularizer. It measures the complexity of a
function. As we regularize less, we do a better job at approximating the in-sample
variation, but for the same reason, the wedge between in-sample and out-of-sample

(p6)
how do we choose the level of regularization (-tune the algorithm”)? This is the
second key insight: empirical tuning.

(p6)
-tuning within the training sample
 In
empirical tuning, we create an out-of-sample experiment inside the original sample.
We fit on one part of the data and ask which level of regularization leads to the best
performance on the other part of the data.4
 We can increase the efficiency of this
procedure through cross-validation: we randomly partition the sample into equally
sized subsamples (-folds”). The estimation process then involves successively holding
out one of the folds for evaluation while fitting the prediction function for a range
of regularization parameters on all remaining folds. Finally, we pick the parameter
with the best estimated average performance.5
 The

(p6)
-!
This procedure works because prediction quality is observable: both predictions y- and outcomes y are observed. Contrast this with parameter estimation, where
typically we must rely on assumptions about the data-generating process to ensure
consistency

(p7)
Some Machine Learning Algorithms
Function class - (and its parametrization) Regularizer R( f )
Global/parametric predictors
Linear -′x (and generalizations) Subset selection|

(p7)
-very useful table
Some Machine Learning Algorithms
Function class - (and its parametrization) Regularizer R( f )
Global/parametric predictors
Linear -′x (and generalizations) Subset selection||β|

(p7)
Random forest (linear combination of
trees

(p7)
-kernel in an ml framework!
Kernel regression

(p6)
-but can we make inferences about the structure? hypothesis testing?
Regularization combines with the observability of prediction quality to allow us to fit flexible functional forms and still find generalizable
structure.

(p7)
Picking the prediction function then involves two steps: The first step is, conditional on a level of complexity,
to pick the best in-sample loss-minimizing function.8
 The second step is to estimate
the optimal level of complexity using empirical tuning (as we saw in cross-validating
the depth of the tree).

(p8)
-but they forgot to mention that others are shrunk
linear regression in which only a small number of predictors from all possible variables are chosen to have nonzero values: the absolute-value regularizer encourages
a coefficient vector where many are exactly zero.

(p4)
-why no ridge or elastic net?
LASSO

(p8)
-ensembles usually win contests
While it may be unsurprising that such ensembles perform well on average-
after all, they can cover a wider array of functional forms-it may be more surprising
that they come on top in virtually every prediction competition

(p8)
-neural nets broadly  explained
neural nets are popular prediction algorithms for image recognition tasks. For one standard implementation in binary prediction, the underlying
function class is that of nested logistic regressions: The final prediction is a logistic
transformation of a linear combination of variables (-neurons”) that are themselves
such logistic transformations, creating a layered hierarchy of logit regressions. The
complexity of these functions is controlled by the number of layers, the number of
neurons per layer, and their connectivity (that is, how many variables from one level
enter each logistic regression on the next)

(p9)
These choices about how to represent the features will interact with the regularizer
and function class: A linear model can reproduce the log base area per room from
log base area and log room number easily, while a regression tree would require
many splits to do so.

(p9)
In a traditional estimator, replacing one set of variables by a set
of transformed variables from which it could be reconstructed would not change the
predictions, because the set of functions being chosen from has not changed. But
with regularization, including these variables can improve predictions because-at
any given level of regularization-the set of functions might change

(p9)
-!!
Economic theory and content expertise play a crucial role in guiding where
the algorithm looks for structure first. This is the sense in which -simply throw it
all in- is an unreasonable way to understand or run these machine learning algorithms

(p9)
-I need hear of using adjusted r square for this
Should out-ofsample performance be estimated using some known correction for overfitting
(such as an adjusted R2
 when it is available) or using cross-validation

(p9)
-big unknowns
available
finite-sample guidance on its implementation-such as heuristics for the number
of folds (usually five to ten) or the -one standard-error rule” for tuning the LASSO
(Hastie, Tibshirani, and Friedman 2009)-has a more ad-hoc flavor

(p9)
firewall principle:
none of the data involved in fitting the prediction function-which includes crossvalidation to tune the algorithm—is used to evaluate the prediction function that
is produced

(p10)
-how?
First, econometrics can guide
design choices, such as the number of folds or the function class

(p10)
with the fitted function. Why not also use it to learn
something about the -underlying model

(p10)
-!!
the lack of standard errors on the coefficients. Even when machine-learning predictors produce
familiar output like linear functions, forming these standard errors can be more
complicated than seems at first glance as they would have to account for the model
selection itself. In fact, Leeb and P-tscher (2006, 2008) develop conditions under
which it is impossible to obtain (uniformly) consistent estimates of the distribution
of model parameters after data-driven selection

(p11)
-lasso chosen variables are unstable because of multicollinearity. a problem for making inferences from estimated coefficients
the variables are correlated with each other (say the number of rooms of a house and
its square-footage), then such variables are substitutes in predicting house prices.
Similar predictions can be produced using very different variables. Which variables
are actually chosen depends on the specific finite sample

(p11)
this creates an Achilles-
heel: more functions mean a greater chance that two functions with very different

(p12)
coefficients can produce similar prediction quality

(p12)
In econometric terms, while the lack of standard errors illustrates the limitations to making inference after model selection, the challenge here is (uniform)
model selection consistency itself

(p12)
-is this equally a problem for non sparsity based procedures like ridge?
First, it encourages the choice
of less complex, but wrong models. Even if the best model uses interactions of
number of bathrooms with number of rooms, regularization may lead to a choice
of a simpler (but worse) model that uses only number of fireplaces. Second, it can
bring with it a cousin of omitted variable bias, where we are typically concerned with
correlations between observed variables and unobserved ones. Here, when regularization excludes some variables, even a correlation between observed variables and
other observed (but excluded) ones can create bias in the estimated coefficients

(p12)
 Some econometric results also show the converse: when there is structure,
it will be recovered at least asymptotically (for example, for prediction consistency
of LASSO-type estimators in an approximately sparse linear framework, see Belloni,
Chernozhukov, and Hansen 2011).

(p12)
-unrealistic for micro economic applications
Zhao and Yu (2006) who establish asymptotic model-selection consistency for the
LASSO. Besides assuming that the true model is -sparse”—only a few variables are
relevant-they also require the “irrepresentable condition” between observables:
loosely put, none of the irrelevant covariates can be even moderately related to the
set of relevant ones.
In practice, these assumptions are strong.

(p13)
Machine learning can
deal with unconventional data that is too high-dimensional for standard estimation
methods, including image and language information that we conventionally had
not even thought of as data we can work with, let alone include in a regression

(p13)
satellite data

(p13)
they provide us with a large x vector of image-based
data; these images are then matched (in what we hope is a representative sample)
to yield data which form the y variable. This translation of satellite images to yield
measures is a prediction problem

(p13)
 particularly relevant where reliable data on
economic outcomes are missing, such as in tracking and targeting poverty in developing countries (Blumenstock 2016

(p13)
cell-phone data to measure
wealth

(p13)
Google Street View to
measure block-level income in New York City and Boston

(p13)
online posts can be made meaningful by labeling them with machine
learning

(p14)
extract similarity of firms from their
10-K business description texts, generating new time-varying industry classifications
for these firms

(p14)
and imputing even in traditional datasets. In this vein, Feigenbaum (2015a, b) applies machine-learning
classifiers to match individuals in historical records

(p13)
-the first prediction applications
New Data

(p14)
Prediction in the Service of Estimation

(p14)
linear instrumental variables understood as a two-stage procedure

(p14)
The first stage is typically handled as an estimation step. But this is effectively a
prediction task: only the predictions x- enter the second stage; the coefficients in the
first stage are merely a means to these fitted values.
Understood this way, the finite-sample biases in instrumental variables are a
consequence of overfitting

(p14)
-ll
overfitting. Overfitting means that the in-sample fitted values x- pick
up not only the signal -′z, but also the noise δ. As a consequence, xˆ is biased towards
x, and the second-stage instrumental variable estimate -
- is thus biased towards the
ordinary least squares estimate of y on x. Since overfit will be larger when sample
size is low, the number of instruments is high, or the instruments are weak, we can
see why biases arise in these cases

(p14)
 same techniques applied here result in split-sample instrumental variables
(Angrist and Krueger 1995) and -jackknife” instrumental variables (Angrist,
Imbens, and Krueger 1999)

(p15)
-worth referencing
In particular, a set of papers has already introduced regularization into the first stage in a high-dimensional setting, including the LASSO
(Belloni, Chen, Chernozhukov, and Hansen 2012) and ridge regression (Carrasco
2012; Hansen and Kozbur 2014). More recent extensions include nonlinear functional forms, all the way to neural nets (Hartford, Leyton-Brown, and Taddy 2016

(p15)
Practically, even when there appears to be only a few instruments, the problem
is effectively high-dimensional because there are many degrees of freedom in how
instruments are actually constructed

(p15)
-a note of caution
It allows us to let the data
explicitly pick effective specifications, and thus allows us to recover more of the variation and construct stronger instruments, provided that predictions are constructed
and used in a way that preserves the exclusion restriction

(p15)
-this seems similar to my idea of regularising on a subset
Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey
(2016) take care of high-dimensional controls in treatment effect estimation by
solving two simultaneous prediction problems, one in the outcome and one in the
treatment equation

(p15)
the problem of verifying balance between treatment and control groups
(such as when there is attrition

(p15)
-!
 Or consider the seemingly different problem of
testing for effects on many outcomes. Both can be viewed as prediction problems
(Ludwig, Mullainathan, and Spiess 2017). If treatment assignment can be predicted
better than chance from pretreatment covariates, this is a sign of imbalance. If treatment assignment can be predicted from a set of outcomes, the treatment must have
had an effect

(p15)
 prediction task of mapping
unit-level attributes to individual effect estimates

(p15)
 Athey
and Imbens (2016) use sample-splitting to obtain valid (conditional) inference on

(p16)
treatment effects that are estimated using decision trees,

(p16)
-look into the implication for treatment assignment with heterogeneity
heterogenous treatment effects can be used to assign treatments;
Misra and Dub- (2016) illustrate this on the problem of price targeting, applying
Bayesian regularized methods to a large-scale experiment where prices were
randomly assigned

(p16)
-caveat
 Suppose the algorithm chooses a tree that splits on
education but not on age. Conditional on this tree, the estimated coefficients are
consistent. But that does not imply that treatment effects do not also vary by age,
as education may well covary with age; on other draws of the data, in fact, the same
procedure could have chosen a tree that split on age instead

(p16)
Prediction in Policy

(p16)
-no .. can we predict who will gain most from admission? but even if we can what can we report?
Prediction in Policy


<!--chapter:end:ml-and-mlreading-group/control_prediction_ml.Rmd-->

# IV and its many issues


## Instrument validity

- Exogeneity vs. exclusion
- Very hard to 'powerfully test'

IV not credible?	Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a *direct* effect on the outcome (only an indirect effect through the endogenous variable

## Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

Focusing on a binary endogenous 'treatment' variable

- With heterogeneity
- With imperfect compliance
- With one-way compliance

## Weak instruments, other issues

- With a 'weak instrument' ... why does that matter?

## Reference to the use of IV in experiments/mediation



<!--chapter:end:causal_inference_general_notes/iv-issues.Rmd-->

# [Other paths to observational identification]{#other_paths}


## Fixed effects and differencing

## DiD

Key issue: FE/DiD does not rule out a correlated dynamic unobservable, causing a bias

\

Helpful links from a Twitter thread:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Ok, let’s say you’ve neither written nor refereed a Diff-in-Diff paper in the last two yeasts. What are the key methodological papers I need to brush up on? <a href="https://twitter.com/hashtag/EconTwitter?src=hash&amp;ref_src=twsrc%5Etfw">#EconTwitter</a><br><br>Didn’t someone put together a list?</p>&mdash; Damon Jones (@nomadj1s) <a href="https://twitter.com/nomadj1s/status/1263225306275614722?ref_src=twsrc%5Etfw">May 20, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




## RD

## Time-series-ish panel approaches to micro


### Lagged dependent variable and fixed effects --> 'Nickel bias'

<!--chapter:end:causal_inference_general_notes/other_paths.Rmd-->

# Causal pathways - mediators {#mediators}

## Mediators (and selection and Roy models): a review, considering two research applications

<div class="marginnote">
Originally focused on issues relevant to Parey et al project on 'returns to HE institution' using data from the Netherlands (flagged as \@NL); also relevant to Reinstein et al work on substitution in charitable giving (flagged as \@subst).
</div>

## DR initial thoughts (for NL education paper)

<div class="marginnote">
Here were my initial thoughts as pertaining to our paper on the returns to university. </div>

Suppose we observe treatment $T$ (e.g., 'allowed to enter first-choice institution and course'),

intermediate outcome $M$ (e.g., completion of degree in first-choice course and institution),

and final outcome $Y$ (e.g., lifetime income.)\


*Alternately, in the "substitution between charities" (\@subst) context... (unfold)*

```{block2,  type='fold'}

The treatment $T$ is

1. 'asked to donate in the first round' (in Reinstein, Riener and Vance-McMullen, henceforth 'RRV' experiments, and perhaps in Schmitz 2019)',

2. a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?),

3. the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others),

the intermediate outcome $M$ is the amount given to that (first-round) charity,

and the final outcome $Y$ is the amount given to that charity (or other charities) in round 2 (experiments "3": other charities in that round ).

```

The treatment $T$ (may) directly affect the final outcome $Y$.

<div class="marginnote">
Do: show a diagram here
</div>


$$T\rightarrow Y$$

\

$T$ also may affect an intermediate outcome $M$.

$$T \rightarrow M$$

The intermediate outcome also may affect the final outcome $Y$.

$$M \rightarrow Y$$

\

With exogenous variation in $T$ *and* $M$ (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions.

With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of $T$ on $Y$.

We could also compute the share of this effect that occurs *via* the intermediate effect, i.e., $T \rightarrow M\rightarrow Y$.
This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients.

\

However, there are two major challenges to this estimation.

1. We (may) have a valid instrument for (exogenous variation in) $T$ only, and $M$ may arise through a process involving selection on unobserved variables.

2. Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects.

Thus we consult the relevant literature, discussed below.

The most influential paper in Economics has been [@Heckman2013].

It is cited in more recent applied work such as Fagereng, 2018  (unfold).

```{block2,  type='fold'}

 ... We follow Heckman et al. (2013) and Heckman and Pinto (2015) in using mediation analysis. The goal of this analysis is to disentangle the average causal effect on outcomes that operate through two channels: a) Indirect effects arising from the effect of treatment on measured mediators, and b) direct effects that operate through channels other
than changes in the measured mediators (including changes in mediators that are not observed by the analyst and changes in the mapping between mediators and outcomes).

 It is therefore necessary to assume that the mediators we do not
observe are uncorrelated with both $\mathbf{X}$ and the measured mediators for all values of $D$.

```

\

Antonakis, coming from the Psychology and Leadership disciplines, considers the mediation question in a much simpler set of models.


## Econometric Mediation Analyses (Heckman and Pinto)

**Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs**

\

### Relevance to Parey et al {-}

We have an instrument for admission to one's first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these 'intermediate outcomes', including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these.

<div class="marginnote">
a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well
</div>


### Summary and key modeling

There is a 'production function'

- cf income as a function of human capital, opportunities, etc.

- cf donation as a function of income, prices, mood, framing, etc.

\

Treatments (e.g., RCTs) may affect outcomes through the following channels:

1.  observable or proxied inputs

- Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities

- Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood
\

2.  unobservable/unmeasured inputs

-  cf human capital, social connections
- cf unobservable generosity, wealth, or temporary mood

\

3.  the production function itself, the 'map between inputs and outputs for treatment group members'

- Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital 'matter more' at some institutions?

- Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else?


If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs.

> RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs \... [nor distinguish effects from changes in production functions].

\

Here "we can test some of the strong assumptions implicitly invoked".

"Direct effects" as commonly stated refer to the impact of both channels 2 and 3 above.


<div class="marginnote">
 DR: Channel 2 isn't really a direct effect imho (what was this?)
</div>

\

**Standard potential outcomes framework:**

$$Y=DY_{1}+(1-D)Y_{0}$$

$$ATE=E(Y_{1}-Y_{0})$$

**Production function**

$$Y_{d}=f_{d}(\mathbf{\mathbf{{\theta}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}$$

\... the function under treatment $d$; of proxied and unobserved inputs that occur under state $d$, and baseline variables.\

The production function implies:

$$ATE=E\Big(f_{1}(\mathbf{\mathbf{{\theta}}}_{1}^{p},\mathbf{\mathbf{{\theta}}}_{1}^{u},\mathbf{{X}})-f_{0}(\mathbf{\mathbf{{\theta}}}_{0}^{p},\mathbf{\mathbf{{\theta}}}_{0}^{u},\mathbf{{X}})\Big)$$\

We also consider counterfactual outputs, fixing treatment status and
proxied inputs:

$$Y_{d,\bar{\theta_{d}}^{p}}=f_{d}(\mathbf{\mathbf{{\bar{{\theta}}}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}$$\

This allows us to decompose ('as in the mediation literature'):

$$ATE(d)=IE(d)+DE(d)$$

-   *IE, Indirect effect*: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment
    statuses)

-   *DE, Direct effect*: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed
    at one of the two treatment statuses)\

    \

HP further decompose the direct effect into:

-   $DE'(d,d')$: The impact of letting the treatment vary the map only
    (fixing the rest at one of the two appropriate values)

-   $DE''(d,d')$: The impact of letting the treatment vary the
    unmeasured inputs only (fixing the rest at one of the two
    appropriate values)

They use this to give two further ways of decomposing the ATE.\

### Common assumptions and their implications

"The [standard literature]{.underline} on mediation analysis in psychology regresses outputs on mediator inputs" \... often adopts the strong assumptions of:

1.  no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy)
    and
    
<div class="marginnote">

Cf 'winning institution' impacts human capital, social networks,
    etc identically for everyone; e.g., not a greater effect for men
    then for women, nor a greater effect for those entering particular
    specializations.
    
</div>

2.  full invariance of the production function: $f_{1}=f_{0}$.

\... which implies $Y_{d}=f(\mathbf{\theta}_{d}^{p},d,\mathbf{X})$.\

[Sequential ignorability (Imai et al, 10, '11)]{.underline}:
Essentially, independent randomization of both treatment status and measured inputs.

<div class="marginnote">
Cf 'winning institution' does not effect the specialization
    entered nor the location of residence, nor are both determined by a
    third factor.
    
</div>



\


*This sentence is hard to follow:*

> In other words, input $\theta_{d'}^{p}$ is statistically independent of potential outputs when treatment is fixed at $D=d$ and measured inputs are fixed at $\bar{\theta_{d'}^{p}}$ conditional on treatment assignment $D$ and same preprogram characteristics $X$.


\

This assumption yields the 'mediation formulas':

\begin{aligned}
E(IE(d)|X)= & \int E(Y|\theta^{p}=t,D=d,X)\underbrace{\Big(dF_{(\theta^{p}|D=1,X)}(t)-dF_{(\theta^{p}|D=1,X)}(t)\Big)}_{{\text{Difference in distribution of proxy inputs}}} & (9)\\
E(DE(d)|X)= & \int\underbrace{\Big(E(Y|\theta^{p}=t,D=1,X)-E(Y|\theta^{p}=t,D=0,X)\Big)}_{\text{Dfc in expectations (unobservables, function) between treatments given proxy inputs }}expe\underbrace{{dF_{(\theta^{p}|D=1,X)}(t)}}_{\text{Distn proxy inputs for D=1}} & (10)
\end{aligned}

*(??F is presumably the distribution over the observables; where did the
unobservables go? They are in the expectations, I guess.)*\
[Difference from RCT]{.underline}

*What RCT doesn't do:*

> [sequential ignorability] translates into \... [no confounding
> effects]{.underline} on both treatments and measured inputs \... does
> not follow from a randomized assignment of treatment \...[which]
> ensures independence between treatment status and counterfactual
> inputs/outputs \... [but *not*] between proxied inputs
> $\theta_{d}^{p}$ and unmeasured inputs $\theta_{d}^{u}$. [Thus *not*
> between counterfactual outputs and measured inputs is assumed in
> condition (ii).]

Cf, randomizing 'win first-choice institution' does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution.

*What RCT* *[does]{.underline}* *do:*

RCT ensures "independence between treatment status and counterfactual
inputs/outputs", thus identifying 'treatment effects for proxied inputs
and for outputs.

CF, we can identify the impact of the treatment 'win first chosen
institution' on proxied input like 'enters a specialization' and on
outputs like 'income in observed years.'

\

## Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity

### Summary {-}

-   \... 4000+ families targeted, incentive to relocate from projects to
    better neighbourhoods.

-   Easy to identify impact of vouchers

-   Challenge (here) is to assess impact of *neighborhoods* on outcomes.

-   Method here to decompose the TEOT into unambiguously interpreted
    effects. Method applicable to 'unordered choice models with
    categorical instrumental variables and multiple treatments'

-   Finds significant causal effect on labour market outcomes

### Relevance to Parey et al {-}

1.  We also have an instrument (DUO lottery numbers) cleanly identifying
    the effect of the 'opportunity to do something' (in our case, to
    enter the course at your preferred institution). However, we also
    want to measure the impact of choices 'encouraged' by the
    instrument, such as (i) attending the first choice course and
    institution and (ii) completing this course. We also deal with
    unordered choices (i. enter course and institution, enter course at
    other institution, enter other course at institution, enter neither)
    (ii. choice of medical specialisation)

2.  The geographic outcome is relevant to our second paper (impact on
    'lives close to home')

### Introduction  {-}

The causal link between neighborhood characteristics and resident's
outcomes has seldom been assessed.

**Treatments:**

-   Control (no voucher)

-   Experimental: could use voucher to lease in low-poverty neighborhood

-   Section 8: Could use voucher in any () neighborhood

*Many papers evaluate the ITT or TOT effects of MTO.*

-   ITT: effect of being *offered* voucher
    -   estimated as difference in average outcome of experimental vs control families

-   TOT: effect for 'voucher compliers' (assuming no effect of simply
    being *offered* voucher on those who don't use it)

    -   estimated as ITT/compliance rate

> [ITT and TOT] are the most useful parameters to investigate the
> effects of *offering* [EA] rent subsidising vouchers to families.

### Identification strategy brief {-}

-   Vouchers as IVs for choice among 3 neighborhood alternatives (no
    relocation, relocate bad, relocate good)

<div class="marginnote">
Cf \@NL: enter course and
    fp-institution, enter course at other institution, do not enter
    course
</div>

-   Neighborhood causal effects as difference in counterfactual outcomes
    among 3 categories

-   Challenge: "MTO vouchers are insufficient to identify the expected
    outcomes for all possible counterfactual relocation decisions"

    -   \... "compliance with the terms of the program was highly
        selective [Clampet-Lundquist and M, 08]"

-   Solution: Uses theory and 'tools of causal inference. Invokes SARP
    to identify 'set of counterfactual relocation choices that are
    economically justifiable'

-   [Identifying assumption]{.underline}: "the overall quality of the
    neighborhood is not directly caused by the unobserved family
    variables even though neighborhood quality correlates with these
    unobserved family variables due to network sorting"

-   'Partition sample \... into unobserved subsets associated with
    economically justified counterfactual relocation choices and
    estimate the causal effect of neighborhood relocation conditioned on
    these partition sets.' [*what does this mean?*]

### Results in brief {-}

"Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes \... 65% higher than the TOT effect for adult earnings."

### Framework: first for binary/binary (simplification) {-}

**First, for binary outcomes (simplified)**

$Z_{\omega}$: whether family $\omega$ receives a voucher *(cf
institution-winning lottery number)*

$T_{\omega}$: whether family $\omega$ relocates (*cf enters first choice
institution and course)*\

**Counterfactuals**

-   $T_{\omega}(z)$: relocation decision $\omega$ would choose if it had
    been assigned voucher $z\in{0,1}$': vector of potential relocation
    decisions (*cf education choices)* for each voucher assignment (*cf
    lottery number)*

    -   Can partition into never-takers, compliers, always takers, and
        defiers

-   $(Y_{\omega}(0);Y_{\omega}(1$)): (Potential counterfactual) outcomes
    (*cf income, residence, etc*) when relocation decision is fixed at 0
    and 1, respectively

**Key ( standard) identification assumption: instrument independent of
counterfactual variables**

$$(Y_{\omega}(0),Y_{\omega}(1),T_{\omega}(0),T_{\omega}(1))\perp Z_{\omega}$$

**Standard result 1: ITT**

$$\begin{aligned}
ITT=E(Y_{\omega}|Z_{\omega}=1)-(Y_{\omega}|Z_{\omega}=0)\\
=E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]')P(S_{\omega}=[0,1])+E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[1,0]')P(S_{\omega}=[0,1])\end{aligned}$$

i.e., ITT computation yields the sum of the 'causal effect for
compliers' and the 'causal effect for defiers, weighted by the
probability of each.

**Standard result 2: LATE**

$$\begin{aligned}
LATE=\frac{{ITT}}{P(T_{\omega}=1|Z_{\omega}=1)-P(T_{\omega}=1|Z_{\omega}=0)}= &  & E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]')\\
if\:P(S_{\omega}=[0,1])=0\end{aligned}$$

i.e., the LATE, computed as the ITT divided by the 'first stage' impact
of the instrument, is the causal effect for compliers if there are no
defiers.

### Framework for MTO multiple treatment groups, multiple choices {-}

-   $Z_{\omega}\in\{z_{1,}z_{2,}z_{3}\}$ for no voucher, experimental
    voucher, and section 8 voucher, respectively

-   $T_{\omega}\in\{1,2,3\}$ \... no relocation, low poverty
    neighborhood relocation, high poverty relocation

-   $T_{\omega}(z)$: relocation decision for family $\omega$ if assigned
    voucher $z$

$\rightarrow$ Response type for each family $\omega$ is  a three-dimensional vector:

$$S_{\omega}=[T_{\omega}(z_{1}),T_{\omega}(z_{2}),T_{\omega}(z_{3})]$$.

$\rightarrow$

**ITT** computation now measures a weighted sum of effects across a subset of those response types whose responses vary between the assignments being compared.\
\

*Cf:*

-   Considering the 'treatments': '1: enter other course at fp-inst, '2:
    enter course at fp-inst', '3: enter course at non-fp inst'

    -   (I ignore other course at other institution for now)

-   Looking among those who won the course lottery (so we have a binary
    instrument: wininst $Z_{\omega}\in{0,1\}}$

-   Our reduced-form estimates (regressions on the 'lottery number wins
    institution' dummy) measures the probablility-weighted sum of:

    -   impact of institution within course ($T_{\omega}=$2 versus 3);
        for those who would 'fully comply' (enter course at institution
        if $Z_{\omega}=1$, enter course at other institution if 0)

    -   impact of the course at fp-institution versus second-best course
        at fp-institution for 'institution-loving' noncompliers; those
        who would enter the course *only* if they get the fp-institution
        and otherwise another course at the same institution

    -   effects for perverse defiers


## Antonakis approaches

Insert notes here

<!--chapter:end:mediation/mediators_lit_pinto_etc.Rmd-->

# Causal pathways: selection, corners, hurdles, and 'conditional on' estimates {#selection_cop}

## 'Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection

"Conditional on positive"/"intensive margin" analysis ignores selection *identification issue* 	See Angrist and Pischke on "Good CoP, bad CoP".  See also bounding approaches such as [@Lee2018]	AngristJ.D.2008a,

\

## Bounding approaches (Lee, Manski, etc)

See [Notes on Lee bounds](#notes_lee)



### Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD

Notes David Reinstein

##### Introduction

> even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research

- Intuitive trimming procedure for bounding average treatment effects in the presence of sample selection…

- Requires neither exclusion restrictions nor a bounded support for the outcome of interest."

- (Also) applicable to "nonrandom sample selection/attrition", as well as to the 'conditional on positive'/hurdle/mediation effect discussed here

> analyses and evaluations typically focus on "reduced form impacts on total earnings, a first-order issue for cost-benefit analysis. Unfortunately, exclusively studying the effect on total earnings leaves open the question of whether any earnings gains are achieved through raising individuals hypothesis wage rates (price affects or hours of work (quantity effects).

*Important methodological point to constantly bring up:* "even a randomized experiment cannot guarantee the treatment and control individuals will be comparable conditional on being employed."

Claims that standard "parametric or semi-parametric methods for correcting sample selection require exclusion restrictions that have little justification in this case." Notes that most of the baseline variables could affect employment probabilities or have a direct impact on wage rates.

\

*Summary of the method*: "...amounts to first identifying the excess number of individuals who were induced to be selected (employed) because of the treatment and then trimming the upper and lower tails of the outcome... distribution by this number, yielding a worst-case scenario bound."

Uses same assumptions as in "conventional models for sample selection"

1. regressor of interest is independent of the errors in the outcome and selection models selection equations – this is ensured by random assignment.

2. "the selection equation can be written as a standard latent variable binary response model"

– what meaningful restriction does this impose?

He proves this procedure "yields the tightest bounds for the average treatment effect that are consistent with the observed data."

```{block2,  type='note'}

The bounds estimator is shown to be $\sqrt(n)$ consistent and asymptotically normal with an intuitive expression for its asymptotic variance which depends on the variance of the trimmed outcome and the trimming threshold, an estimated quantifiable; (and an added term accounting for the estimation of which quantile to trim on)

```  


Note for charity experiment (unfold) (\@subst)

```{block2,  type='fold'}

– *DR, Note, charity data: We can make confidence statements over the bounds themselves. Will this procedure be easy to bring into our code?*
    – In our (charity) experiment we in fact do have upper bounds on the outcome variable. Could this yield even greater efficiency?

```

Note for the Netherlands data: (unfold, \@NL)

```{block2,  type='fold'}

it is not immediately clear how this could be adapted to instrumental variables; we shall see. Can we recover something meaningful from the reduced form model they are? Can it be applied to the (instrumental variables) estimates to disentangle the impact of changing courses from the impact of the institution itself?

```

In Lee's paper, the estimate seems to give very narrow and informative bounds even though they have a great many people who do not earn any wages as a share of the population, about 54\%. These are much narrower than the bounds proposed by Horowitz and M then what those bounds produce.

<!- ask @Gerhard whether his Horowitz/Manski estimator incorporated the natural bounds on the outcome. -->


##### The National Job  Corps Study and Sample Selection [prior approaches]

> In the experiment discussed here those in the control  group were embargoed from the program for three years but could join afterwards, thus "when I use the phrase 'effect of the program' I am referring to this reduced-form treatment effect", i.e., the intent to treat effect.

– "some subpopulations were randomized into the program group with differing, but known probabilities. Thus analyzing the data requires the use of design weights."
 
<div class="marginnote">
*Note:* (\\@NL) this bears some resemblance to our Dutch data situation, and we can probably use examples from analyses of these programs. We can check their code against ours.
</div>

– Note also that they impute means of the baseline variables with their means; this seems to be an accepted practice.

<div class="marginnote">
Lee notes that he focuses exclusively on the "sample selection on wages caused by employment" and not the attrition/nonresponse problem, to focus attention on this, but they could have used it for the other as well.
</div>
 

– *DR:* (\@NL) Note again that their desire to separate the employment hours and wage effects of the program is very similar to our desire to separate out different margins of the impact of winning an institution.  ...Namely the impact on completing a course or starting a course versus other impacts and the impact of entering a specialization versus remaining impacts. ...Similar decompositions for the geography outcomes.

    – To do: check whether any papers cite Lee using an IV approach, extending the technique and the estimation of variance.

***

> "the problem of nonrandom sample selection is well-understood in the training literature; ... may be one of the reasons why most evaluations of job-training programs focus on total earnings, including zeros for those without a job, rather than on wages conditional on employment" "of the 24 studies referenced in a survey ... (Heckman et al.)... Most examine annual, quarterly, or monthly earnings without discussing the sample selection problem examining rage rates."

– *DR:* (\@NL)Note that this is relevant to our question of whether to exclude zeros in log models, etc. While there will be less unemployment in our data, it still may be a relevant influence made have a strong effect on the estimates.

***

**...previous conventional approaches to the sample selection problem (skip if desired).** One may explicitly model the process determining selection, such as in Heckman (1979) ...

Separate equations for the wage and the propensity to be employed, where employment occurs if the latter crosses a particular threshold, in which case a wage is observed. It is reasonable to think that the treatment variable can have effects on both terms..

"sample selection bias can be seen as specification error in the conditional expectation..."

The expected wage conditional on treatment exogenous variables and the selection into working (that is the underlying propensity to work variable exceeding zero) his status is equal to the true effect of the treatment an adjustment for the differences in the observable's exogenous variables and a bias term representing the expectation of the idiosyncratic unobservables given the treatment and the exogenous variables exceeding the value necessary to induce work participation. The unobservable term needs to exceed the prediction based on the observable term for the entire term to exceed zero inducing labor force participation.

One may assume the data are missing at random, perhaps conditional on a set of covariates (Rubin, 1976;  essentially assuming the error terms in each equation are independent of one another, here "employment status is unrelated to the determination of wages"… This "is strictly inconsistent with standard models of labor supply that account for the participation decision (Heckman, 1974)."

A more common assumption is that some exogenous variables "determine sample selection but do not have their own direct impact on the outcome of interest.... Exclusion restrictions are used in parametric and semi-parametric models..."

but "there may not exist credible 'instruments... excluded from the outcome equation"


***

– *DR, aside:*  We can return to (our) previous papers to impose these Lee bounds!  One example would be the Siskel and Ebert your reviews paper and perhaps incorporating us with subsequent approaches, considering the "selection to review" equation.

***


**Second approach  "the construction of worst-case scenario bounds of the treatment effect"**

"Impute missing data with either the largest or smallest possible values to compute the largest and smallest possible treatment effects consistent with the data" as in Horwitz and Manski (2000a) who provide a general framework for this.

- Particularly useful with binary outcomes.

This cannot be used when the support is unbounded. ... note in their replication example they are actually using the equivalent of the bottom 5th percentile and the top 95th percentile. Strictly using a procedure would provide even wider bounds.

Lee considers his approach to be a hybrid of the two previous general approaches.

...end of section 2.. .a statement of the Horwitz upper bound for the treatment effect; very intuitive: "what if everyone in the treatment who dropped out would have had the largest possible wage and everyone in the control group that drop out would've had the smallest possible wage; this will give the upper bound." Switching this the other  way  around will give a lower bound.


***

*DR, an aside thought:* (\@NL) Something akin to the Horwitz and M approach (or maybe Lee bounds) could be applied to our issue of swapping into institutions directly. Suppose we only focus on those who *actually* complied: those assigned to an institution who also went to that institution. Our concern was that this would under-represent those who had particularly strong institutional preferences. Suppose you are interested in looking at the impact of winning the lottery (for once preferred institution) itself, as that was our most simply identifiable outcome.

... Let's consider evaluating a treatment effect for those who happened to swap in. Those who swapped in might be assigned a counter-factual outcome of the lowest value of the lifetime income among those who did not get their institution of choice. Similarly, the small group who swapped out might be assigned a counterfactual outcome (had they no swapped out) representing the highest outcome value for those who did get their institution of choice. This should give us an upper bound on the treatment effect for these two groups of what we might call non-compliers. Making the opposite assumptions, precisely that those who swapped *into* their institution of choice would've had a very good counterfactual outcome (if they had not got their institution of choice) that comes from the highest outcomes for those who didn't get their institution of choice (and also reversing this for those who swapped out of their preferred institution) would give us a lower bound for the treatment effect for this group. We can then combine these bounded treatment effects for these non-compliers with the treatment effect for the compliers to get a measure of the average treatment effect with bounds for this sort of behavior.

This will also allow us to come up with estimates with bounds *without* having to use the instrumental variable strategy which has issues of its own.

##### Section 3: identification of bounds on treatment effects; the main meat of the model

He starts with a simple example. He begins with a model with a treatment indicator and no other covariates, and a continuous outcome variable, but notes that this will clearly apply to discrete outcome variables and will also apply conditional on controls.

Nest, he brings forward the statement... from the earlier selection models. In each case the latent variable must overcome a hurdle for the outcome to be observed and in fact *the hurdle differs depending on the impact of the treatment itself*.  In general *when the errors in the selection and outcome equations are correlated the difference in these means differs from the actual treatment effect*. In other words through a slightly complicated story, when those who have unobservables that make them more likely to work also tend to have unobservables that would make them likely to earn more the standard difference in outcomes between control and treatment will *not* describe the true treatment effect.

<br> \bigskip

*A key insight* seems to be that we could identify the treatment effect if we could estimate the expected outcome given treatment *and* given that the unobservable component in the selection equation would lead to an observable outcome had the person *not* been given treatment. If so, we could subtract the observed mean control outcome from the above to yield the true treatment effect (for those who would be observed always). However, we obviously do not observe this because we only observe the outcomes for those who are treated where the selection equation *is* in fact positive and not "where the selection equation *would have* been positive had they not been treated."

However, the insight here is that this term can in fact be bounded. We *do* observe these outcomes for the treated people (note we are assuming without loss of generality that the treatment raises the probability of selection for this discussion) but we don't know exactly which ones they are. In other words, we observe outcomes for more people in the treatment group than we need; we wish we could figure out what is the subset of these that would have *also* been observed had they not been treated, so we could compare like-to-like. The observed treatment mean is a weighted average of the thing we are seeking (to difference from the control) and "the mean for a subpopulation of marginal individuals... that are induced to be selected into the sample *because* of the treatment"

This then gets us the upper bound for the term expressing the treatment outcome for those who would have been observed even if they had been in the control. The upper bound for this is the expected outcome for those in the treated group (who are observed of course) and who are in quantile-p or above of the outcome, where this $p$ is the share of the treated population that are in the marginal group we referred to that were only induced to be selected into the sample because of the treatment.

In other words the worst case scenario is that the smallest share $p$ values of $Y$ are in the marginal group and the largest one (which is share 1-pone are in the inframarginal group. We don't know which observations are inframarginal and which ones are marginal.

$p$:  the share of marginal individuals and (1-p) the share of inframarginal individuals (the latter is group we want the average outcome for). The highest could be would be the average outcomefor the largest (1-p) share of this group.  We are looking for the expectation given that they are at or  above at will at or above percentile p within this group.

In other words we trim the lower tail of the Y distribution by the portion $p$, (so what remains is the 1-p share) to get the upper bound for the inframarginal groups mean. We can then subtract the mean for the control group to get an upper  bound for the treatment effect.

To compute this "trimming proportion  p": this p is equal to the share of the treated group  whose outcome is observed minus the share of the control group whose outcome is deserved is observed, divided by the share of the treatment group where the outcome is observed.
Something like the *increased likelihood of observation that is driven by the treatment, as a share of the total number as a share of the probability of observation in the treatment group*.

The average observed outcome for the treatment group is including too many observations; we need to difference out the share of observations that are observed only because the treatment caused them to be observed; this share is certainly no larger than the increased probability of observation in  the treatment group as a share of the probability of observations the treatment group.

Another much simpler way of saying this is "trimming the data by the known proportion of excess individuals" in the treatment group. (To gain bounds on the mean for the inframarginal group which we can then difference from the control-group mean get the treatment effect).

Perhaps some intuition for why this improves on the Horwitz model: we don't need to assume that those observed in the treatment group that wouldn't have been observed in the control would've had the highest possible outcomes. No, we only need to assume (to get the upper bound) that these came from the highest *distribution*  because they had to come from somewhere. These were the people in the upper tail of the relevant group but they couldn't *all* have been the individual highest achiever.

***

The model is extended to heterogeneity and heteroscedasticity. This begins with the independence of treatment assignment the "potential sample selection indicators" for either treatment or control, in other words whether that individual will have an observed outcome under treatment and whether that the individual would have an observed outcome under control, and the latent potential outcomes.

Experimental or random assignment ensures that each of the potential outcomes (and the correspondence to observability under each treatment) is independent of the actual assigned treatment.

The second assumption is monotonicity: treatment assignment can only affect sample selection in one direction.

– DR: For our (substitution) experiments, it is in fact not clear to me whether this should necessarily be the case, as some (less generous?) people may be induced to leave because of having been asked to donate, while potentially other (more generous people) might be induced to return given that they were asked to donate. (This proposed nonmonotonicity implies that the 'asked twice' sample tends to weed out the less generous, which would lead to a bias *against* substitution, strengthening the case for our result.)
    - DR, aside: However, even though the paper doesn't say it, I suspect this assumption could be weakened and you would still get some similar bounds.
To put it another way, I would imagine that these bounds could be adjusted based on some reasonable ad hoc assumptions about the share of the population who is affected in either direction.

– \@NL: I'm coming to think that our Dutch data problems are more things involving "hurdle models". Can this technique also be applied to such hurdle models?

Next proposition 1a states that given these assumptions we can derive sharp lower and upper bounds for the average treatment effect (conditional on 'would be observed in both states'). Note that for this estimator if the probability of observation is greater under the treatment we need to trim the treatment groups outcome distribution and if the probability of observation is greater under the control we need to trim the control group's outcome distribution.

- DR, aside comment: we seem to be throwing out a bit of the data in these estimates, which would suggest that something more efficient could be generated.

(The stated bounds you can estimate are exactly the same as the bounds from the previous specification, at least as I had interpreted the way they would be produced.)

Their remark 2 notes that an implication is that as $P_0$, that as the "difference between the relative probability of observation of an outcome under treatment versus control" tends to zero,
i.e., as the probability of having an observed outcome (or the conditional probability of this) is the same for treatment and control) then there is no sample selection bias.

Their estimate convergences to the estimate he calls an estimate for the "always takers subpopulation... except that taking... is selection into the [outcome-observed] sample."

*So, a very vanilla estimator is acceptable if we find the same conditional probability of selection for each group, under monotonicity, which, for this case, we can test (see Remark 4 below).*

– (DR: To me this suggests that there might be something wrong going on here. Intuitively, If I simply observe the same rate of attrition in the treatment and control groups this *shouldn't* be enough to tell me that attrition did not matter, as it could occur differentially for both groups, but it seems to be a result here; this is probably due to the assumption of monotonicity of the selection/observation term, as well as  the random/exogenous assignment to each group.)

***

Remark 3 discusses the importance of monotonicity for the bounds, saying this assumption is "minimally sufficient" (I think it would be better to say minimally sufficient for these particular bounds that he computed). To demonstrate this he gives an extreme example. Without monotonicity it could be (note: this would seem like a very unlikely outcome!) that every observation in the control group comes from the population in the treatment group that would *not* have been observed had they been treated and every observation in the treatment group happens to come from the set of people that would *not* have been observed had they been in the control group. These two "subpopulations do not overlap, so the difference in the means could not be interpreted as a causal effect."

– DR, aside : there must be some way to impose some restrictions on this even allowing for this non-monotonicity. (He notes that this can be improved upon somewhat by thinking about the total the idea that the total masses of  unobserved that would've been observed in the other group can't be greater than the share that is not observed in the other treatment group, but this doesn't seem like a particularly fruitful route as it in most reasonable cases will still allow for very wide bounds.)

***

Remark 4 suggests that if we can assume (or somehow observe?) that the conditional probabilities of selection are the same for treatment and control, we can *test whether monotonicity in fact holds* and the simple difference in means will be an appropriate estimate of the treatment effect. Here, the assumption implies that everyone in the treatment or control group would have been observed under the opposite treatment as well. This in fact implies that the distribution of the exogenous variables should be the same in the treatment and control groups conditional on being selected. This seems fairly intuitive, we look at whether selection seems to be occurring in different ways are on different margins for the two groups treatment versus control.

Apparently for this test to have *power* we need that the subpopulations of "noncompliant errors in opposite directions" (quotation mine) must have *distinct* distributions of baselines exogenous characteristics. If these were the same then whether or not monotonicity holds the test doesn't tell us anything.

– DR: *I wonder if anyone uses this test for  Monotonicity under non-differential selection?*

Another relevant note that he bundles in this remark is that the technique here only yields estimates *for those who would be with an observed outcome for either treatment or control.* One could *additionally* try to bound this as an estimate for the entire population using the Horwitz and Manski bounds for this latter thing. However, in many contexts there are reasons that the bounded estimates they mainly use are the relevant ones, such as "the impact of the program on wage rates for those whose employment status was not affected by the program."

- DR: In our substitution experiment case, the substitution patterns for those for whom attrition was not affected by the first-round-charity treatment
- \@NL: E.g., the impact of an institution on income for those whose choice to remain in the course was not affected by their institutional assignment

***

"Narrowing bounds using covariates"

All of the above could be done conditional on a particular set of baseline characteristics such as gender or race. The average treatment effect could be estimated separately for each. (Note: and perhaps combined in a fruitful way?)


One can alternately use covariates to reduce the width of these bounds. To give intuition, we can imagine a baseline covariate that perfectly predicts an individual's wage. Because treatments are randomly assigned the maintained assumptions will still hold conditionally on this X. The results  the methods can be applied separately for each value of this covariate, and for each such value the trimming procedure will actually have no impact on the estimate.

DR: I think this is the "estimate and sum things up in a weighted way" procedure I thought about a moment ago.

<br> \bigskip

Proposition 1B gives the balance estimator for a model involving exogenous variables. Essentially, this computes the corresponding bounds estimator at each X, where the differential selection probability is computed for that particular X, the upper quantile value of the outcome is given conditional on the same X and on being in the treatment group. These are then integrated (or summed up) weighted by the distribution or the cdf of this  covariate in the control group. These bounds will necessarily be sharper than the balance without controls.

##### Section 4: estimation and inference

The asymptotic variance depends on components reflecting the variance of the trimmed distribution, the variance of the estimated trimming threshold, and the variance in the estimate of "how much of the distribution to trim" (the relative selection probability differential).

Equation 6 formally defines the estimator

Estimated bounds consistent for 'true bounds' under standard conditions

Two ways to compute CI's -- CI's for the 'true bounds' or CI's for the TE itself. A 95\%  CI for the former will contain the latter with even greater probability.

Imbens and Manski '04 can be used to derive the latter which are 'more apppropriate here' since the object of interest is the TA and not the 'region of all rationalizable treatment effects.
These are built off of a transformation of the estimate UB and LB and max estimated sd of each of these.

- the latter are reported by the 'cie' option in 'leebounds'

Generalisation to monotonicity (without  knowing direction of impact of treatment on selection)...

> As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative... [do similar]

> though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small and p0 is “close” to zero
>...  A simple, conservative approach to combining the trimmed and untrimmed intervals is to compute their union


##### Section 5: Empirical Results

Table 4 gives a step-by-step that is a good way of seeing and understanding the construction of the estimator, and where the 'action' is, in treimming, in components of the SE, etc.

Intervals are 1/14 the width of  the equivalent Horowitz/Manski bounds

#######   5.2 using covariates to narrow bounds

> Any baseline covariate will do, as will any function of all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to be a proxy for the predicted wage potential for each individual—is constructed from a linear combination of all observed baseline characteristics. This single covariate is then discretized, so that effectively five groups are formed according to whether the predicted wage is within intervals defined by $6·75, $7, $7·50, and $8·50.

- @Substitution: this is essentially what I propose we do, but using Ridge Regressions or something similar

> To compute the bounds for the overall average...the group-specific bounds must be averaged, weighted by the proportion (sPr Group J|S0=1,S1=1)

> The estimated asymptotic variance for these overall averages is the sum of (1) a weighted average of the group-specific variances and (2) the (weighted-) mean squared deviation of the group-specific estimates from the overall mean. This second term takes into account the sampling variability of the weights

$\rightarrow$ result: 11\% narrower bounds

<br> \bigskip

*Interesting; possibly do similar for \@NL-ed*:

>By statistically ruling out any effect more negative than −0·037, this suggests that after 4 years, the Job Corps enabled program group members to offset at least 35\% (and perhaps more) of the potential 0·058 loss in wages due to lost labour market experience that could have been caused by the program


#### Section 6: Conclusions: implications and applications

Interesting intuitive argument:

>Another reason to interpret the evidence as pointing to positive wage effects is that the lower bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly negatively correlated with the propensity to be employed. From a purely theoretical standpoint, a simple labour supply model suggests that, all other things equal, those on the margin of being employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be the smallest wage that draws the individual into the labour force). In addition, the empirical evidence in Table 2 suggests that there is positive selection into employment: those who are predicted to have higher wages are more likely to be employed (i.e., U and V are positively correlated). If this is true, it seems relatively more plausible to trim the lower rather than the upper tail of the distribution to get an estimate of the treatment effect.

<!--chapter:end:sample_selection/selection_models_lee.Rmd-->

# (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters  {#why_experiment_design}

## Why run an experiment or study?

I claim an experiment should:

1. Have a reasonable chance of an outcome that would not have been predicted in advance. 

2. The realized outcome should meaningfully inform  our understanding of the world in other words. In other words, if the outcome comes out one way it should cause us to update our beliefs about a particular hypothesis about the world in one direction (and if it comes out the other way we should update in the other direction.)

> Experimenter should always ask: "What uncertainty (about real-world  preferences, decision-making etc.) is 'entangled' (ala @ESYudkowsky) with the results of this experiment?"... i.e., 'how might my beliefs change depending on the results?' 

- @givingtools on twitter


### Sitzia and Sugden on what theoretically driven experiments can and should do

**"Sitzia, Stefania, and Robert Sugden. "Implementing theoretical models in the laboratory, and what this can and cannot achieve." Journal of Economic Methodology 18.4 (2011): 323-343.**

This paper is a critique of how models are claimed to be "tested", through a literal implementation, in the laboratory. They argue this misinterprets the intention of a model, and use of economic modelling in general. Ultimately, such experiments (they say) don't really tell us one way or another about the truth or usefulness of the model for the real-world domain that was intended. Some key quotes..

> My reductio ad absurdum on this is an experimenter who 'tests mechanism-design' by asking subjects "do you want to choose this optimal mechanism and earn $20, or this inefficient mechanism and earn $10"? -- @givingtools on twitter

They single-out  two examples of well-published experiments  for criticism: "an investigation of price dispersion by John Morgan,

Henrik Orzen and Martin Sefton (2006), and an investigation of information cascades by Lisa Anderson and Charles Holt (1997)"...

> In each case, the experimenters create a laboratory environment that closely resembles the model itself. The only important difference between the experiment and the model is that, whereas the model world contains imaginary agents who act according to certain principles of rational choice, the laboratory contains real human beings who are free to act as they wish. The decision problems that the human subjects face are exactly the problems specified by the model. We argue that such an experiment is not, in any useful sense, a test of what the model purports to say about the target domain. Instead, it is a test of those principles of rational choice that the modeller has attributed to the model world. Those principles are not specific to that model; they are generic theoretical components that are used in many economic models across a wide range of applications.

\

> Surprisingly, these doubts are not expressed in terms of the applicability of MSNE [mixed strategy Nash Equilibrium] to the model’s target domain, pricing decisions by retail firms. The doubts are about whether experimental subjects will act according to MSNE when placed in a laboratory environment that reproduces the main features of the model.

\

> If one takes the viewpoint of the subjects themselves, there seems to be very little resemblance between the decision problems they face and those by which retail firms set their prices. The connection between the two is given by the model: the subjects’ decision problems are like those of the firms in the model, and the firms in the model are supposed to represent firms in the world.

> However, MOS are no more concrete than Varian in explaining how the comparative-static properties of the model relate to the real world of retail pricing.

\

> The suggestion in these passages is that the clearinghouse model’s claim to be informative about the world is strengthened if its results are confirmed in the laboratory. In this sense the experiment is informative about the world. But the experiment itself is a test of the model, not of what the model says about the world.

\

> The procedure of random and anonymous rematching of subjects is explained as a means of eliminating ‘unintended repeated game effects’, such as tacit collusion among sellers (pp. 142–3). This argument illustrates how tightly the laboratory environment is being configured to match the model. In a test of MSNE, repeated game effects are indeed a source of contamination; and MSNE is a property of Varian’s model. But in the target domain of retail trade, the same firms interact repeatedly in the same markets, with opportunities for tacit collusion.

\

> Clearly, if an experiment implemented a model in its entirety, all that it could test would be the mathematical validity of the model’s results. Provided one were confident in the modeller’s mathematics, experimental testing would be pointless. Thus, when an experiment implements almost every feature of a model, all it can test in addition to mathematical validity are those features that have not been implemented.

\

> Thus, the experiment is a test of MSNE in a specific class of games. [emphasis added]

\

> MSNE is what we will call a generic component of economic models – a piece of ready-to-use theory which economists insert into models with disparate target domain

- ibid

Relating back to the discussion of the different conceptions of theory:

> Is it informative at all to run experimental tests of theoretical principles such as MSNE and Bayesian rationality, viewed as generic components of economic models? ... A strict instrumentalist (taking a position that is often attributed to Friedman) might answer ‘No’ to the first question, on the grounds that tests should be directed only at the predictions of theories and not at their assumptions.

\

> Such an experimental design should not be appraised in terms of what the model purports to say about its target domain. It should be appraised in terms of what it can tell us about the relevant generic component, considered generically. When (as in the cases of MSNE and Bayesian rationality) the same theoretical component appears in many different models, an experimenter can afford to be selective in looking for a suitable design for a test


\

> Considered simply as a test of MSNE, MOS’s experiment uses extraordinarily complicated games. Many of the canonical experiments in game theory use 2×2 games. Depending on the treatment, MOS’s games are either 101×101 (for two players) or 101×101×101×101 (for four players). Payoffs to combinations of strategies are determined by a formula which, although perhaps intuitive to an economist (it replicates the demand conditions of the clearinghouse model), might not be easy for a typical subject to grasp.

\

## Causal channels and identification

- Ruling out alternative hypotheses, etc

## Types of experiments, 'demand effects' and more artifacts of artifical setups

## Generalizability (and heterogeneity)


```{block2,  type='note'}

**"But all the other papers do it!"**

A common response to critiques (particularly critiques of the  generalizability of experimental work) is that "all the other papers have the same problem" and that excepting this critique would require rejecting all previous work too.  In politics this has been referred to as "what-about-ism".

You can guess that I'm not a fan of this.  I think one always needs to defend the paper and approach  on its own merits. Generalisability is an important issue. Each of the other published papers that also suffers from such issues has a specific response and justification for that particular case, and if it doesn't this is sorely lacking.

I think we *should* be reading and publishing papers that consider, discuss, and acknowledge their own limitations, and future work can test and build on this. This should promote to robust, reproducable science. 

\

Just because I say "'"this is something we should be concerned with doesn't mean I'm saying "this paper has no value'. I just mean "let's discuss reasons why this may or may not threaten internal or external validity/generalisability, and how we can design the study and analysis minimise these potential problems""

\

In writing a paper, I find it important that *we the authors* feel the results are credible and not overstated. So I feel like the best approach is "let's write the best paper we can and consider every issue seriously, and then hopefully the good publication/peer-review outcome will follow". That's also the most motivating and least stressful way for me to work. (Rather than thinking 'how can I sneak this paper into the best journal?') 

<div class="marginnote">
In fact I consider peer review and high rating and use as the important outcome, not the publication itself. We live in a world where anyone can publish their work immediately on the WWW. The journals themselves are providing little or no service: it is the reviewers and editors offering feedback and evaluation that matters.
</div>
  
```  


\

[A thought: Replace reviews (accept/reject/R&R) with ratings (0-10)?](https://twitter.com/GivingTools/status/1258704179306147841?s=20)

<!--chapter:end:experiments_and_study_design/why_experiment_design.Rmd-->

# (Experimental) Study design: Background and quantitative issues {#quant_design_power}

## Pre-registration and Pre-analysis plans {#pre-reg-pap}


### The benefits and costs of pre-registration:  a typical discussion 


> BB: That said, I would be interested to think about the benefits – and more importantly limitations to – pre-registration. I think it could solve some of the p-hacking problems but not much else. How to not relegate exploratory analyses too far is also unclear to me.


> DR: I'm much more on the 'pro' side pre-registration and PaPs. It also helps deal with publication bias and file drawers. And p-hacking is a huge issue IMHO.  But it is also good to have some consideration of the pros and cons, so this would be great. 


> BB: RE pre-reg: yes I think it is enough that it prevents p-hacking (there could be very little cost associated with pre-reg) but I fear that it could prevent other advancements if it relegates exploratory analyses too far.


> DR: I don't think it should be binary. Systems need to be worked out for *adjustments* to the meaning of reported estimates depending on whether they were or were not preregistered, and how many were preregistered. While reported significance levels could be adjusted in the frequentist framework, this will all presumably based on  measures of the likelihood that such a result would have been estimated/reported.  Thus I think this could most easily  be incorporated into a Bayesian framework but I'm not saying it would be easy. Still, they have done some good work on adjustments for 'sequential designs'.

\

> BB: I think that it could also stifle students a bit – it may reduce further the number of students who have access to funding that allows for experiments that will be able to be published if all experiments have to be high-powered. 

\

> DR:   Statistical power is an important issue.  I was skeptical at first about the 'dangers of underpowered studies' but I'm coming around a bit.

<div class="marginnote">
My thinking was that 'we can simply make downward adjustments to the estimates reported in underpowered studies'. </div>
 

> Anyways, we don't want to put the cart before the horse: as Gelman  said at a conference we should be supporting science not the careers of scientists.   I tend to think there are strong arguments for more centralization in social science. 

> And my impression is that we actually have too many different studies and distinct research programs being run, and too many papers being published and not carefully brought together into a framework. Going through the studies on the https://www.replicationmarkets.com/ reinforces this impression for me.

> Still, I think there are ways around this to enable early career people.  'Underpowered'  experiments could be registered as part of a longer/sequential research program,  perhaps collaborative and enabling meta-analysis.

\

> BB:  I also don’t think it gets at publication bias very much unless pre-reg’ed studies are followed up on. Only then do you know why the study didn’t come out – and quite a lot of the time I think it will be attrition/inability to gather the necessary data. Someone could launch that journal though – the Journal of Failed Studies – to have a place for a record that they have been run and what happened to be kept. So I am pro pre-reg, I just think the system needs a bit of work.

\

> DR:  If preregistration is made public and well-organize, then the 'failed' exercises willtbe integrated into future meta-analyses; so that's at least a partial solution here.

> Agreed, we need to build better systems for incentivising pre-registration and careful data sharing. We need  to give career credit to people for  planning designing and reporting  credible  experiments and projects, even if they 'fail'. Part  this is publishing/rewarding tight null results,  which actually do add a lot of value. 

> We might also consider offering some reward careerwise to experiments that fail -- in terms of being deeply inconclusive-- for some arbitrary or random reason even though they were well-planned and executed.  But I think it is hard to get the incentives right for the latter.


### The hazards of specification-searching

## Sequential and adaptive designs

Needs to adjust significance tests for augmenting data/sequential analysis/peeking	Statistics/econometrics	new-statistics		sagarin_2014			http://www.paugmented.com/	resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/

Yet ...


$P_{augmented}$ may *overstate* type-1 error rate	Statistics/econometrics	response to referees, new-statistics	"

A process involving stopping "whenever the nominal $p < 0.05$" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5\%. Even if the subsequent data suggested a "one in a million chance of arising under the null" the overall process yields a 5\%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the "likelihood of the null hypothesis" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate.

Considering the calculations in \ref{sagarin2014}, it is clear that $p_{augmented}$ should \textit{overstate} the type-1 error of the process if there is a positive probability that after an initial experiment attains p$<0.05$, more data is collected.  A headline $p<0.05$ does \textit{not} imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded $p>0.05$, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though $p<0.05$. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine's experiment).  Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p<0.05, more data is collected, implying that $p_{augmented}$ as calculated above overstates the type I error rate that would arise  from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below $p=0.05$, in order to attain a type-1 error rate of 5% or less in our published corpus."

## Efficient assignment of treatments

(Links back to power analyses)


### How many treatment arms can you 'afford'?


<!--
A guiding principle might be:
"Will we have statistical power to identify a small true effect from this pairing? If not, we drop the pairing."

A caveat to this is that we may be able to pool some of the pairings to answer certain questions, but then it is only worth having the distinct variations that are being pooled if that doing so gives us power to answer some other question.
-->

### Other notes and resources

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ICYMI:<br>Recording of my talk on experimental design in the Chamberlain seminar, with discussions by <a href="https://twitter.com/dmckenzie001?ref_src=twsrc%5Etfw">@dmckenzie001</a> and Max Tabord-Meehan:<a href="https://t.co/xKtTrH8X1U">https://t.co/xKtTrH8X1U</a> <a href="https://t.co/e7Cq90D6Sl">https://t.co/e7Cq90D6Sl</a></p>&mdash; Maximilian Kasy (@maxkasy) <a href="https://twitter.com/maxkasy/status/1264966489595162627?ref_src=twsrc%5Etfw">May 25, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<!--chapter:end:experiments_and_study_design/quant_design_power.Rmd-->

# (Experimental) Study design: (Ex-ante) Power calculations {#power}

## What sort of 'power calculations' make sense, and what is the point?

### The 'harm to science' from running underpowered studies

> "One worries about underpowered tests. Your result (may have)  relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.

> Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects."	verkaik2016, metzger2015


\

On magnitude error due to underpowered studies:

https://www.pauljferraro.com/publications/2020/2/1/is-there-a-replicability-crisis-on-the-horizon-for-environmental-and-resource-economics

Gelman and Carlin 2014 beyond power calculations: Assessing the probabilities that estimated coefficients are of the wrong sign or overestimated
https://journals.sagepub.com/doi/full/10.1177/1745691614551642

## Power calculations without real data

## Power calculations using prior data

Adapt example in 'scopingwork.Rmd' to this



<!--chapter:end:experiments_and_study_design/power_calc.Rmd-->

# 'Experimetrics' and measurement of treatment effects from RCTs {#experimetrics_te}


## Which error structure? Random effects?

## Randomization inference?

## Parametric and nonparametric tests of simple hypotheses

## Adjustments for exogenous (but non-random) treatment assignment

## IV in an experimental context to get at 'mediators'?

## Heterogeneity in an experimental context

## Incorporate above: Notes on "The econometrics of randomised experiments" (Athey and Imbens)

(with an eye towards giving experiments_


Page 7

>   Fundamentally, most concerns  with external validity are related to treatment effect heterogeneity ...
> [ considering extrapolation between settings A and B] Units  in the two settings may differ in observed or unobserved characteristics, or treatments may differ in some aspect.
>  to assess these issues it is helpful to have ...  randomized experiments,  in multiple settings [varying] in  the distribution of characteristics of the units,  and possibly ...  the nature of the treatments or the treatment rate,  in order to assess the credibility of generalizing to other settings

- Shall we do Fisher's test based on computing the distribution of differences in means randomly reassigning the  "treatment"?

- F-tests to consider multiple outcomes for any cases?

- (When) shall we use covariates (esp those for interactions) in the 'deviations from mean' form?

- Considering when to use controls (and interactions?)

> the asymptotic variance for $\hat{\tau}}$ is less than that of the simple difference estimator by a factor equal to $1-R^2$ from including the covariates relative to not including the covariates
> If... the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial

- DR: Could there not ever be a loss from doing interactions dividing up the sample too fine in doing this interactive estimation? This should depend on the true $R^2$ I think. Try to remember what is the real tradeoff?

- Statistics adjusted for stratification:

> One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance

<div class="marginnote">
  DR: Is it valid to simply say "we choose the lower value of the estimated variances"? Are they advocating this? Such a procedure seems like it would have a bias.
  </div>


** Things to potential incorporate in NL HE lottery paper(s) **

- (When) shall we use covariates (esp those for interactions) in the 'deviations from mean' form?

"randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received"

- Consider a "partial identification or bounds analysis" to deal with noncompliance at each margin

- Look up "randomization-based approach to IV" (Imbens and Rosenbaum, 2005)




### Abstract and intro

1. randomisation-based inference as opposed to sampling based inference

- DR: I Disagree as the object of interest is ultimately not the experimental sample, particularly not in the lab

2. efficiency gains from stratification into small strata, adjust se to capture these gains  (We have only done this to a limited extent)
3. (Non-compliance, intention-to-treat, and IV)
4. Estimation and inference for **heterogeneous treatment with covariates… ** subpopulations… Maintaining the ability to construct valid confidence (mht etc). "Conditional average treatment effects"
5. (interaction between units)

Why careful statistics are important even for randomised experiments

"Randomisation approach": potential outcomes fixed, Assignment to treatments random

Example of why the randomisation-based inference approach matters:

"in the conventional sampling paradigm… Controlling for observable heterogeneity using a regression model" required for the assumptions to be justified with this approach. With randomisation approach it makes more sense to put data into strata by covariates, analyse within-group experiments and average results.

Recommend small strata but not too small as variances cannot be estimated within pairs

- DR: Section 10 on heterogeneity is particularly relevant for us

- Other experimetrics methodology surveys mentioned (Duflo et al '06, Glennerster and T '13, Glennerster '16); present one is more theoretical

### randomised experiments and validity

Defined as settings "where the assignment mechanism does not depend on characteristics of the units"
- That seems to be "pure randomisation"

… Debate about the supremacy of randomised experiments

Definition of internal validity (DR:: it is a bit imprecise here).

\

Typical argument about how external validity is no more guaranteed in observational studies then and randomised experiments "there is nothing in non-experimental methods which made some superior randomised experiments with the same population and sample size in this regard."

-DR: I think this is a bit of a strawman and a weak argument here
- GR: This is the Deaton argument, very strange

They argue for experiments in multiple settings varying in the characteristics of the units and perhaps the treatments to assess the credibility of generalizing to other settings.

\

(?Graphical methods to deal with external validity issues?)

\

**Finite population versus random sample from super-population**

We can interpret the uncertainty as unobserved potential outcomes rather than sampling uncertainty.

- DR: I don't see why these are mutually exclusive.
- GR: agreed
- DR: Viewing the sample of the full population of interest may not even work I'm considering some experiments such as those using within subject treatments

\

The differences in these approaches matter in some settings but not others.
Sometimes "...conventional sampling-based standard errors will be unnecessarily conservative"

- DR: This could be helpful

### Potential outcomes/ Rubin causal model framework (covered earlier)

(This is somewhat familiar by now)

\

**Potential outcomes**

If we do not impose limitations on interactions between units (like SUTVA) there  will be a dimensionality problem


$$p\:\{0,1\}^N \times Y^{2N}\times X^N \rightarrow [0,1]$$

- DR: I am not sure I understand this notation, particularly the $\{0,1\}$ bit

\

" For randomized experiments we disallow dependence on the potential outcomes, and we assume that the functional form of the assignment mechanism is known"
(this goes further than "completely randomized")

### 3.2 Classification of assignment mechanisms

(Formula for probabilities of each assignment combination of control and treatment given for each one)

1. Completely Randomized: $N_t$ Units drawn at random from a population of $N$ to receive the treatment, remaining $N_c$ get control.

2. Stratified randomized: Partition into $G$ strata based on covariates values, "Disallowing assignments that are likely to be uninformative about the treatment effects of interest"

3. Paired randomized (Extreme stratification)

4. Cluster randomized: Treatments assigned randomly to entire clusters. Maybe cheaper to implement and more valid in the presence of interactions between units within but not across clusters.

### The analysis of Completely randomized experiments

**Exact p-values for sharp null hypotheses (Fisher etc)** 

"Sharp": "Under which we can an for all the missing potential outcomes from the observed data" ... So we can infer the distribution of any statistics under the Null.

E.g., $H0$, The treatment has no effect $Y_i(0)=Y_i(1)\forall i$, vs $Ha$, At least one unit i has $Y_i(0)\neq Y_i(1)$

\

Difference in means by treatment status: Calculate the probability over the randomization distribution of a value with as large an absolute value as the one observed given the actual assignments.

This is done by reassigning what we call the "treatments" to all possible combinations (keeping the number of treated units constant) and calculating the "placebo" treatment effect. Calculate the fraction of assignment vectors with statistic at least as large (in absolute value) as the observed one.

- DR: What is the statistic called and is there preprogrammed code? Is it the Fisher's exact test?

- Can do for means or means of the ranks by treatment status (rank sum?) or any stat.
    - Latter is less sensitive to outliers and thick-tailed distributions

\

With multiple outcomes, multiple comparisons issues

- Use statistics it takes into account all the outcomes (e.g., F-stat, calculate exact P value using the 'Fisher randomization distribution' as in Young, '16)

- Or use adjustments to P values e.g., Bonferroni or tighter bounds (Which are still more conservative than the Fisher thing); Romano ea survey (2010)

- Rosenbaum '92 on estimating treatment effects based on rank statistics (DR: I don't get this at all)

### Randomization inference for Average treatment effects

Neyman wanted to estimate the ATE for the sample at hand

$$\tau=\frac{1}{N}\Sum_{i=1..N}{(Y_i(1)-Y_i(0)i)}=\bar{Y}(1)-\bar{Y}(0)$$

- DR: Again, this is really not what we care about particularly not in a small-scale experiment.

    \

Proposed the estimator "Difference in average outcomes by treatment status" (DR: Same as in last section)

\

Defining $D_i$, a term representing "assignment minus the average assignment"

Allows a restatement of the estimator which makes it clear that this is unbiased for the average treatment effect $\tau$

$$\hat{tau}=\tau+\frac{1}{N}\Sum_{i=1..N}{(D_i(\frac{N}{N_t}Y_i(1)+\frac{N}{N_c}Y_i(0)}$$



Sampling variance of $\hat{\tau}$ over the randomization distribution decomposed as

$$V{\hat{\tau})=\frac{S_c^2}{N_c}+\frac{S_t^2}{N_t}-\frac{S_{tc}^2}{N}$$

Where $S_c^2$ and $S_t^2$ Are the variances of the control and treated outcomes,
and $S_{tc}^2$ is the variance of the unit level treatment effect (DR: This must be related to the covariance)

We can estimate rhe first two terms but not the latter term as we have no observations with both a control and a treatment.

\

In practice researchers ignore the third term, which leads to an upward bias for the *sample* treatment effect but an unbiased estimator of the population ATE

- DR: Any intuition for this?

\

We still need to make large sample approximations to construct confidence intervals for the ATE.

- DR: Does this yield any practical strategy for us to use?

### Quantile treatment effect (Infinite population context)

Usefulness:

1. Uncover "Treatment effects in the tails"

1. Results robust to thick tails

S-th quantile treatment effect defined as the difference in quantiles between the $Y_i(1)$ and$Y_i(0)$ distributions:

$$\tau_s=q_{Y(1)}(s)-q_{Y(0)}(s)$$ ...

- this is distinct from the "quantile of the differences": $q_{Y(1)-Y(0)}(s)$,  which is in general not identified
    - DR:  the letter is truly more interesting; we care about the distribution of the *impact of the treatment*  and not so much about the impact of the treatment on the distribution of  outcomes.

-  the two are equal if  there is "perfect rank correlation between the two potential outcomes" (DR: I think this simply means that the unit ranked n'th  if not treated would also be the unit ranked n'th if treated ...  no crossing over).

\

Making lemonade: they argue here that the (identifiable) difference in quantiles would be more interesting  to a policymaker considering exposing all units to the treatment ... (DR:  presumably because she should not care *who*  get the particular outcome but only about the distribution of outcomes, a  common  axiom for social
 welfare functions).

 \

Estimates and tests:   use the difference in quantiles as a statistic in  and exact P value computation ...  results for such exact tests are quite different than those based on estimated effects and standard errors because "Quantile estimates are far from normally distributed."

### Covariates (if not stratified) in completely randomized experiments

(They strongly recommend stratifying instead of ex post controls.)

Why use controls  if a simple difference in means is unbiased for the ATE?

1.  "incorporating covariates may make analyses more informative" (greater  precision)

-  Can incorporate covariates in exact P value analysis, or estimate average treatment effects  within subpopulations and average these up appropriately

- DR: How to do these things in practice?

2.  correcting for compromised randomization ...  which may occur because of missing data and selective attrition

\
They give an example with the data from Lalonde where they estimate the average treatment effect for two groups those with and without prior earnings. They then add these up weighted by the estimated probability of being in each group.

- DR:  I understand this correctly, as they do not define all variables
    - also, how do they compute the standard error of the combined estimator here?!
    - this seems more like an interaction than a standard control here, which would allow a different intercept (control outcome) but not a different treatment effect.


? se of
$$\hat{p}(\bar{Y}_t|Y^{t-1}=0-\bar{Y}_c|Y^{t-1}=0)+(1-\hat{p})(\bar{Y}_t|Y^{t-1}=1-\bar{Y}_c|Y^{t-1}=1)$$

- DR: I think there is a simple formula for difference in means that could be applied to this

    \

### Randomization inference and regression estimators

They urge caution in using reg. "Since randomization does not justify the models, almost anything can happen" (Freedman 08)

But using only "indicator variables based on partitioning the covariate space" preserves many of the finite simple properties of simple comparisons of means.
\

**Regression estimators for average treatment effects**

With a single variable, the least-squares estimate of $\tau$ is identical to the simple difference in means:


$$\hat{tau}_{ols} = \bar{Y^o}_t - \bar{Y}^o_c$$

The intercept is the control value of course: $\hat{\alpha}_{ols}=\bar{Y}^o-\hat{\tau_{ols}}\bar{W}=\bar{Y^0}_c$.

*Conceptually important:*

> the unbiasedness claim in the Neyman analysis is conceptually different from the one in conventional regression analysis: in the first case the repeated sampling paradigm keeps the potential outcomes fixed and varies the assignments, whereas in the latter the realized outcomes and assignments are fixed but different units with different residuals, but the same treatment status, are sampled.

\

Redefining the residual in randomisation-based inference terms

> Now the error term has a clear meaning as the difference between potential outcomes and their population expectation
[DR: I think they mean the expectation conditional on treatment]

> The randomization  implies that the average residuals for treated and control units are zero ...

DR: They mean it implies mean independence (?) but not full independence, heteroskedasticity still likely


> Because the general robust variance estimator has no natural degrees-of-freedom adjustment [DR: ??], these standard [Randomisation-based?] robust variance estimators differs slightly from the Neyman unbiased variance estimator $\hat{V}_{neyman}$

\


$\hat{V}_{robust} =\frac{s^2_c}{N_c}\frac{N_c-1}{N_c}+\frac{s^2_t}{N_t}\frac{N_t-1}{N_t}$

Compared to the previously stated estimator for the TE variance for the sample (which we argued overstates the true sample TE variance)

\

$\hat{V}_{neyman} =\frac{s^2_c}{N_c}+\frac{s^2_t}{N_t}$

\

> The Eicker-Huber-White variance estimator is not unbiased, and in settings where one of the treatment arms is rare, the difference may matter


They give an example where it does not matter.

- DR: This point seems ignorable for most of our designs, as we intentionally avoid such rare arms (but in NL lottery maybe)


### Regression Estimators with Additional Covariates [DR: seems important]


For now they continue to focus on 'pure randomisation', not stratified nor merely exogenous conditional on observables

\

1. Can include these additively:

$$Y^{obs}_i=\alpha+\tau W_i + \beta'\dot{X}_i +  \epsilon_i$$

2. Can allow a 'full set of interactions'

\

$Y^{obs}_i=\alpha+\tau W_i + \beta'\dot{X}_i + \gamma'\dot{X}_i W_i + \epsilon_i$

\

- DR: They do not do much discussion here of whether to do additive or full interactions; maybe it comes later (causal trees etc)

\

> In general the least squares estimates based on these regression functions are not unbiased for the average treatment effects over the randomization distribution given the finite population.

- DR: Why not?  Intuition? Which regression function of the ones above is referred to here?

- DR: The discussion below suggests it will *still* be consistent (asymptotically unbiased)

\

> There is one exception. If the covariates are all indicators and they partition the population, and we estimate the model with a full set of interactions, Equation (5.4), then the least squares estimate of $\tau$ is unbiased for the average treatment effect

\

If $\bar{X}$ is the average value of $X_i$ in the sample, then \hat{\tau}=\hat{\tau}_1\bar{X}+\hat{\tau}_0(1−\bar{X})$, and $\hat{\gamma}=\hat{\tau}_1-\hat{\tau}_0$

\

With large sample approximations we can 'say something about the case with multivalued covariates' ... "$\tau$ [DR: estimated how?] is asymptotically unbiased for the average treatment effect ..."

> the asymptotic variance for $\hat{\tau}}$ is less than that of the simple difference estimator by a factor equal to $1-R^2$ from including the covariates relative to not including the covariates

- DR: This motivates the use of covariates even in a randomized design, and even if we don't take the 'model of the covariates' seriously.
    - "results do not rely on the regression model being true in the sense that the conditional expectation of Y obs i is actually linear in the covariates and the treatment indicator in the population"
- DR: Is this for the linear controls model or for the full interactions model?


However, ...

> If... the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial

- DR: Intuition?

The presence of non-zero values for γ imply treatment effect heterogeneity.


*Best argument for using only binary/categorical interactions: interpretation*

"Only if the covariates partition the population do these $\gamma$ have a clear interpretation as differences in average treatment effects."

\

- DR: Could there not ever be a loss from including interactions and dividing up the sample too fine in doing this interactive estimation? This should depend on the true $R^2$ I think. Try to remember what is the real tradeoff.

**  6 The Analysis of Stratified and paired randomized experiments ** 

### Stratified randomized experiments: analysis

*Case for stratification*

> capture the gains from ex post regression adjustment without the potential costs of linear regression, and the potential costs of linear regression, and therefore stratification is generally preferable over regression adjustment

> Within this stratum we can estimate the average effect as the difference in average outcomes for treated and control units: $\tauˆg = \bar{Y}^{obs}_{t,g} − \bar{Y}^{obs}_{c,g}$,

> and we can estimate the within-stratum variance, using the Neyman results, as

$\hat{V}(\hat{\tau}) =\frac{s^2_{t,g}}{N_{t,g}} + \frac{s^2_{c,g}}{N_{c,g}}$

> where the g-subscript indexes the stratum [They wrote 'j' but I think its a typo]

Next just average weighted by stratum shares:

$\hat{\tau} = \sum_{g=1..G}{\hat{tau}_g \frac{N_g}{N}$

with estimated variance $\sum_{g=1..G}$\hat{V}(\hat{tau}_g)(\frac{N_g}{N})^2$

- DR: Presumably they mean the above mentioned Neyman variance
    - Also note the squared term in the variance estimation, this may be how they computed the variance in the above empirical example

"Special case":  proportion treat units the same in all strata $\rightarrow$ ATE estimator equals difference in means by treatment status:
$\hat{\tau} = \sum_{g=1..G}{\hat{tau}_g \frac{N_g}{N}=\bar{Y}^{obs}_t-\bar{Y}^{obs}_c$
... same as estimator for completely randomized experiment

But the estimated variance for the latter will be overly conservative.

- DR: But I thought stratifying sometime ends up yielding a larger *estimated* variance?

##Paired randomized experiments: analysis

(Skipping note-taking for now)


### 7 The Design of randomised experiments and the benefits of stratification

> ... Our recommendation is that one should always stratify as much as possible, up to the point that each stratum contains at least two treated and two control units

### 7.1 Power calculations

- DR: This section is fairly basic and trivial, largely what we already know

> we largely focus on the formulation where the output is the minimum sample size required to find treatment effects of a pre-specified size with a pre-specified probability

- DR: My usual formulation

- DR: Why are they doing these calculations based on the t-statistic, when they recommend using other measures?

- DR: They claim equal sample sizes is "typically close to optimal" in cases without homoskedasticity. I think this is pure speculation.

### Stratified randomized experiments: Benefits

> Stratifying does not remove any bias, it simply leads more precise inferences than complete randomization

> confusion in the literature concerning the benefits of stratification in small samples if this correlation is weak [between the stratifying variables and the outcome]

> in fact there is no tradeoff. We present formal results that show that in terms of expected-squared-error, stratification (with the same treatment probabilities in each stratum) cannot be worse than complete randomization.

> if one stratifies on a covariate that is independent of all other variables, then stratification is obviously equivalent to complete randomization.

> Ex ante, committing to stratification can only improve precision, not lower it

*Qualifications to this:*

1.

> Ex-post, given the joint distribution of the covariates in the sample, a particular stratification may be inferior to complete randomization.

2.

> ... Second, the result requires that the sample can be viewed as a (stratified) random sample from an infinitely large population... guarantees that outcomes within strata cannot be negatively correlated.

(Note)

> The lack of any finite sample cost ... contrasts with ... regression adjustment. [which] may increase the finite sample variance, and in fact it will strictly increase the variance for any sample size, if the covariates have no predictive power at all.

3.
>  Although there is no cost to stratification in terms of the variance, there is a cost in terms of estimation of the variance.


*Still*

> One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance

- DR: Is it valid to simply say "we choose the lower value of the estimated variances"? Are they advocating this? Such a procedure seems like it would have a bias.




> exact variance for a completely randomized experiment can be written as ... variance for the corresponding stratified randomized experiment is...  the difference in the two variances is $V_C − V_S =... \geq 0$

- DR: I am curious how these terms are derived and compared

> if the strata we draw from are small, say litters of puppies, it may well be that the within-stratum correlation is negative, but that is not possible if all the strata are large: in that case the correlation has to be non-negative

- DR: unless sutva violated perhaps (?)


> consider two estimators for the variance [both unbiased]

$\hat{V}_C=\frac{s^2_{t,g}}{N_{t,g}} + \frac{s^2_{c,g}}{N_{c,g}}$
> the natural estimator for the variance under the completely randomized experiment is: $\hat{V}_c=\frac{s^2_{t}}{N_{t}} + \frac{s^2_{c,g}}{N_{c,g}}$

> or a stratified randomized experiment the natural variance estimator, taking into account the stratification, is:
$\hat{V}_S=\frac{N_f}{N_f+N_m}\Big(\frac{s^2_{fc}}{N_{fc}}\frac{s^2_{ft}}{N_{ft}}\Big)+\frac{N_f}{N_f+N_m}\Big(\frac{s^2_{mc}}{N_{mc}}\frac{s^2_{mt}}{N_{mt}}\Big)$


> Hence, $E\hat{V}_S\leq\hat{V}_C$.

- DR: Because we know both are unbiased and we know the true variance of $\hat{V}_C$ is larger.

Nevertheless, the reverse may hold in a particular sample

> where the stratification is not related to the potential outcomes ... the two variances are identical in expectation

but the $var\Big(hat{V}_S\Big) < var\Big(hat{V}_C\Big)$

- DR: This seems contradictory at first but I think it's correct. The expectation of the estimated variance can be smaller or identical, while the *variance of the estimated variance* can still be larger.

### Re-randomization

Basically, they argue that if the first pre-implementation experiment comes out very unbalanced, you can randomize again -- this will be an indirect method of stratifying.

P-values could/should be adjusted to take into account that you are basically stratifying imprecisely.

### Analysis of Clustered Randomised Experiments

> our main recommendation is to include analyses that are based on the cluster as the unit of analysis. Although more sophisticated analyses may be more informative than simple analyses using the clusters as units, it is rare that these differences in precision are substantial, and a cluster-based analysis has the virtue of great transparency

*DR: skipping most of this section for now*

### Noncompliance in randomized experiments (DR: Relevant to NL lottery, not to charity experiments)


> randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received.

- DR: good quote for Nlmed

Responses to noncompliance:

1. ITT
2. LATE
3. Partial identification or bounds analysis

- Latter: "to obtain the range of values for the average causal effect of the receipt of treatment for the full population."

> Another approach, not further discussed here, is the randomization-based approach to instrumental variables developed in Imbens and Rosenbaum (2005).
[check into that]

They recommend against:
> The first of these is an as-treated analysis, where units are compared by the treatment received; this relies on an unconfoundedness or selectionon-observables assumption. A second type of analysis is a per protocol analysis, where units are dropped who do not receive the treatment they were assigned to. We need some additional notation in this section.

- DR: Skipping full note-taking on this for now but *COME BACK TO IT* as it is very relevant to NL Med; the bounds analysis could be particularly interesting


### Heterogenous Treatment Effects and Pretreatment Variables

- Crump et al setup (?)

Multiple splits and tests may lead to overstated statistical significance for differences in TE's.

- Bonferroni "overly conservative in an environment where many covariates are correlated with one another"
    - List, Shaikh, and Xu (2016) propose an approach accounting for this; it uses bootstrapping, and requires pre-specifying list of tests to conduct

** 10.3 Estimating Treatment Effect Heterogeneity ** 

- Parametric estimators, 'all interactions' (presumably with a correction as noted above)
- Nonparametric estimator of $\tau(x)$

>  The approach of List, Shaikh, and Xu (2016) works for an arbitrary set of null hypotheses, so the researcher could generate a long list of hypotheses using the causal tree approach restricted to different subsets of covariates, and then test them with a correction for multiple testing. Since in datasets with many covariates, there are often many ways to describe what are essentially the same sub-groups, we expect a lot of correlation in test statistics, reducing the magnitude of the correction for multiple hypothesis testing.

### 10.3.1 Data-driven Subgroup Analysis: Recursive Partitioning for Treatment Effects

- Partition sample by "region of covariate space"
- Determine which partition produces subgroups that differ the most in terms of treatment effects.
- The method avoids introducing biases in the estimated average treatment effects and allows for valid confidence intervals using “sample splitting,” or “honest” estimation
- Output of the method ... is a set of subgroups, selected to optimize for treatment effect heterogeneity (to minimize expected mean-squared error of treatment effects), together with treatment effect estimates and standard errors for each subgroup.

\

If instead...
> we estimate the average treatment effect on the two subsamples using the same sample, the fact that this particular split led to a high value of the criterion would often imply that the average treatment effect estimate is biased.

\

But here ,,,
> The treatment effect estimates are unbiased on the two subsamples, and the corresponding confidence intervals are valid, even in settings with a large number of pretreatment variables or covariates.

\

Because unit level TE is not observed, it is difficult to use standard protocols
\

... suggest transforming outcome from Y_i^{obs} to $Y_i^\ast=Y_i^{obs}\frac{W_i−p}{p(1−p)}$

... "so that standard methods for recursive partitioning based on prediction apply"

Which implies $E[Y_i^\ast|X_i=x]=\tau(x)=E[Y_i(1)−Y_i(0)|X_i = x]$


- DR: Are these p's conditional on the x's? Probably it doesn't matter here as they are assuming pure randomisation.

AI criterion

> focuses directly on the expected squared error of the treatment effect estimator ... which turns out to depend both on the t-statistic and on the fit measures.
> ... further modified to anticipate ... that the treatment effects will be re-estimated on an independent sample after the subgroups are selected

- This penalises too small groups and too much variance,
- (in general) rewards explain outcomes but not treatment effect heterogeneity...enables a lower-variance estimate of the treatment effect.

Wager and W argue for inflating SE's rather than partitioning
- DR: I see an advantage there, as the AI approach throws away data

### 10.3.2 Non-Parametric Estimation of Treatment Effect Heterogeneity

- Many allow descriptive evidence and prediction, but few methods available that allow for confidence intervals

- K-nearest neighbors, hurdle methods
    - Do not prioritise 'more important' covariates


> ... can work well and provide satisfactory coverage of confidence intervals with one or two covariates, but performance deteriorates quickly after that.
> The output of the nonparametric estimator is a treatment effect for an arbitrary x. The estimates generally must be further summarized or visualized since the model produces a distinct prediction for each x.

> A key problem with kernels and nearest neighbor matching is that all covariates are treated symmetrically; if one unit is close to another in 20 dimensions, the units are probably not particularly similar in any given dimension. We would ideally like to prioritize dimensions that are most important for heterogeneous treatment effects, as is done in many machine learning methods, including the highly successful random forest algorithm.

But these are often "bias-dominated asymptotically" ... except the ones proposed by Wager and Athey  (2015) :)

>  asymptotically normal and centered on the true value of the treatment effect,... consistent estimator for the asymptotic variance.

> Averages over the many "trees" of the form developed in Athey and Imbens (2016)

> ... different subsamples are used for each tree [plus some randomness]
> Each tree is “honest,” in that one subsample is used to determine a partition and [another] to estimate treatment effects within the leaves.
> Unlike the case of a single tree, no data is “wasted” because each observation is used to determine the partition in some trees and used to estimate treatment effects in other trees, and subsampling is already an inherent part of the method.

What does this mean?:

> can obtain nominal coverage with more covariates than K-Nearest Neighbour matching or kernel methods,

(but still "eventually becomes bias-dominated when the number of covariates grows" ... but "much more robust to irrelevant covariates than kernels or nearest neighbor matching.")

- Also, approaches fitting separately for treatment and control

- Also, Bayesian perspectives on this: Green and Kern (2011), Hill (2012), others ... but unknown asymptotic properties (DR: do we care?)

### 10.3.3 Treatment Effect Heterogeneity Using Regularized Regression

- Lasso-like (Imai and Ratkovic (2013), etc.)

- With few important covariates (a 'sparse' model), can derive valid CI's w/o sample-splitting

- Some proposed modeling heterogeneity separately for treatment and control;...   can be inefficient if the covariates that affect the level of outcomes are distinct from those that affect treatment effect heterogeneity.
    - alternative ... incorporate interactions ... as covariates, and then allow LASSO to select which covariates are important.

### 10.3.4 Comparison of Methods

- Lasso: more sparsity restrictions, better handle linear or polynomial relationships between covariates and outcomes;
    - outputs a regression; but CI's justified only under strict conditions
- Random forest methods ... are more localized, ... capture complex, multi-dimensional interactions among covariates, or highly nonlinear interactions.
    - Less sensitive to sparsity, CI's do not 'deteriorate' as covariates grow (but MSE of predictions suffer)
    - Inference more justifiable by random assignment (Lasso requires stronger assumptions)




<!--chapter:end:experiments_and_study_design/experimetrics_te.Rmd-->

# Meta-analysis and combining studies: Making inferences from previous work {#metaanalysis}

My opinion on why this is so important (unfold):

```{block2,  type='fold'}

it is lame how often I see 'new experiments' and 'new studies' that tread most of the ground as old studies, spend lots of money, get a publication and ... ignore or pay lip service to the previous findings. There is tons of data out there that can inform new questions and bring better through re-analysis and combination with other data. Otherwise we are not actually building progress. This is why I became involved with a project I called 'ExpArchive', later working with projects such as GESIS' X-Econ to try to encourage and facilitate data sharing in experimental economics, as well as  the innovationsinfundraising.org project, which is now collaborating with the Lily Institute's "revolutionizing philanthropy research" (RPR) project.

```


## Notes: Christensen et al 2019, ch 5, 'Using all evidence, registration and meta-analysis

> how the research community can systematically collect, organize, and analyze a body of research work

- Limitations to the 'narrative literature review': subjectivity, too much info to narrate

### The origins [and importance] of study [pre-]registration

... Make details of planned and ongoing studies available to the community .... including those not (yet) published

- Required by FDA in 1997, many players in medical community followed soon after

- Turner ea (08) and others documented massive publication bias and misrepresentation

... but registration far from fully enforced (Mathieu ea '09) found 46% clealy registered, and discrepancies between registered and published outcomes
!

### Social science study registries

Jameel 2009, AEA 2013, 2100 registrations to date
RIDIE, EGAP, AsPredicted, OSF allowing a DOI (25,000+)

### Meta-analysis

Key references: Borenstein ea '09, Cooper, Hedges, and V '09

#### Selecting studies

"some scholarly discretion regarding which measures are 'close enough' to be included... contemperanous meta-analyses on the same topic finding opposit e conclusions

'asses the robustness... to different inclusion conditions'... see Doucouliagos ea '17 on inclusion options


<div class="marginnote">


My opinion: this is the key barrier to meta-analysis in social science! How do we weight studies using different methodologies and in different contexts? The Bayesian Random Effects approach seems to offer some help here (this not to be confused with the random effects panel-data models discussed in standard Econometrics texts).

</div>

#### Assembling estimates

- Which statistic to collect?

\


Studies $j \in J, j= 1..N_j$

Relevant estimate of stat from each study is $\hat{
\beta_j}$ with SE $\hat{\sigma_j}$

- Papers report several estimates (e.g., in robustness checks): which to choose, esp if author's preferred approach differs from other scholars.

\

*Ex from Hsiang, B, Miguel, '13*: links between extreme climate and violence

- how to classify outcomes... interpersonal and intergroup... normalised as pct changes wrt the meanoutcome in that dataset

- how to standardice climate varn measures... chose SD from local area mean
(DR: this choice implicitly reflects a behavioural assumption)

$\rightarrow$ 'pct  change in a conflict outcome as a fncn of a 1 SD schock to local climate'

### Combining estimates

'Fixed-effect meta-analysis approach': assumes a single true effect'

<div class="marginnote">

DR: I'm  not sure I agree on this assesment of *why* this is unlikely to be true in practice... 'differences in measures' (etc) seem to be a different issue

</div>

*Equal weight approach*: (Simply the average across studies... ugh)

\

*Precision-weighted approach*:

$$\hat{\beta}_{PW}=
\sum_{j}p_j\hat{\beta}_j/
\sum_{j}p_j$$

where $p_j$ is the estimated precision for study $j$: $\frac{1}{\hat{\sigma_i}^2}$

Thus the weight $\omega_j$ placed on study $j$ is proportional to it's precision.


<div class="marginnote">

'implies weight in proportion to sample size'? I think that's loosely worded, it must be nonlinear.

</div>

$\rightarrow$ This minimises the variance in the resulting meta-analytical estimate:

$$var(\hat{\beta}_{PW}) =\sum_j \omega_j\hat{\sigma_j}^2 = \frac{1}{\sum_j(p_j)}$$


'inclusion of additional estimates always reduces the SE of $\hat{\beta_{PW}}$ [in expectation].' ... so more estimtes can't hurt as long as you know their precision.

(they give a numerical example here with 3 estimates)

<!-- Todo: add R code explicitly doing these calculations -->

### Heterogeneous estimates...

#### WLS estimate

(Stanley and Doucouliagos '15)

Interpreted as 'an estimate of the average of potentially heterogenous estimates'

This may feel like a more familiar to Economists but it is also seems to be far less useful than the Bayesian approach.

\


#### Random-effects (more common)

*Focus here on hierarchical Bayesian approach* (Gelman and Hill '06; Gelman ea '13)

'The magnitude and precision of the common component represents the generalizable conclusions we might draw from a literature'

... continuing from above notation

'cross-study differences we observe might not be driven solely by sampling variability... [even with] infinite data, they would not converge to the exact same [estimate] '

True Treatment Effect (TE) $\beta_j$ for study j drawn from a normal distribution...

$$\beta_j \sim N(\mu, \tau^2)$$

'Hyperparameters' $\mu$ determines central tendency of findings...
$\tau$ the extent of hety across contexts.

Considering $\tau$ vs $\mu$ is informative  in itself.
And a large $\mu$ may suggest looking into sample splits for hety on obsl lines.

\

Uniform prior for $\mu$ $\rightarrow$ conditional posterior:

$$\mu|\tau,y \sim N(\hat{\mu}, V_{\mu})$$ where the estimated common effect $\hat{\mu}$ is

$$\hat{\mu}=
\frac{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))\hat{\beta}}
{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))}$$

(Similar to precision-weighted approach but now the between-study dispersion is incorporated into the weights)

and where the estimated variance  of the generalizable component $V_\mu$ is:

$$Var(\hat{\mu})= \frac{1}{\sum_j\big(1/(\hat{\sigma_i}^2 + \hat{tau}^2)}$$


<div class="marginnote">

Confusion/correction? Is this the estimated variance or the variance of the estimate?

</div>


- and how do we estimate some of the components of these, like $\hat{\tau}$?

> Intuitively, if estimated [TE] in all studies are near one another and have relatively wide and overlapping [CI's], then most of the difference in estimates is likely the result of sampling variation [and $\tau$] is likely to be close to zero.


<div class="marginnote">

DR: But if the TE have wide CI's, do we have power to idfy btwn-study hety? ... I guess that's what the 'estimated TE are all near each other' gives us?

</div>

... Alternatively, if there is extensive variation in the estimated ATEs but each is precise... $\tau$ is likely to be relatively large.

```{block2,  type='note'}
Coding meta-analyses in R

"A Review of Meta-Analysis Packages in R" offers a helpful guide to the various packages, such as `metafor`.

[Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) appears extremely helpful; see, e.g., their chapter [Bayesian Meta-Analysis in R using the brms package](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-meta-analysis-in-r-using-the-brms-package.html)

```


<!-- TODO: some code exercises should be put or linked here? Perhaps drawn from the above references? -->


</div>

\ The $I^2$ stat is a measure of the proportion of total variation attributed to cross-study variation; if $\hat{\sigma}_j$ is the same across all studies we have:
$I^2(.) = \hat{\tau}^2/(\hat{\tau}^2 + \hat{\sigma}^2)$

<!-- *DR: more detail would be welcome here. Material from [this syllabus]() may be helpful.

https://docs.google.com/document/d/1oImg-ojUFqak5KyZ-ETD2qGvkvUgx8Ym6b8gG4GwfM8/edit?usp=drivesdk

-->

## Excerpts and notes from 'Doing Meta-Analysis in R: A Hands-on Guide' (Harrer et al) {#doing-meta}

Some notes follow excerpting and commenting on 

[Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/)

<div class="marginnote">

Note that installation of the required packages can be tricky here. 
For Mac Catalina with R 4.0 I followed the instructions [HERE](https://discourse.mc-stan.org/t/dealing-with-catalina-iii/12731/30)

</div>
 
```{r, warning=FALSE, message=FALSE}

#devtools::install_github("MathiasHarrer/dmetar") 
#...I did not 'update new packages'
#install.packages("extraDistr")

library(pacman)

p_load(tidyverse, meta, brms, dmetar, extraDistr, ggplot2, tidybayes, dplyr, ggplot2, ggridges, glue, stringr, forcats)

```

### Pooling effect sizes
FE model calculates weighted average: 

FIX THESE FORMULAS

<div class="marginnote">
Fixed the equations according to https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/fixed.html 
</div>

$$\hat{\theta_F} = \frac{\sum\limits_{k=1}^K\hat{\theta_k}/\hat{\sigma}^2_k}{\sum\limits_{k=1}^K1/\hat{\sigma}^2_k} $$ 
$$\hat{\sigma^2_k}=\sum\limits_{k=1}^K \frac{1}{K}\hat{\sigma}^2_k $$



- note that this process does not 'dig in' to the raw data, it just needs the summary statistics, neither does the "RE model" they refer to:

> Both of these models only require an effect size, and a dispersion (variance) estimate for each study, of which the inverse is taken. This is why the methods are often called generic inverse-variance methods.

Nor the Bayesian models, apparently (they use the same 'madata' dataset)

### Bayesian Meta-analysis {#doing-bayes-meta}


```{block2,  type='note'}
"The model we apply in Bayesian Meta-Analysis is a so-called Bayesian Hierarchical Model...
every meta-analytical model inherently possesses a multilevel, and thus 'hierarchical', structure."

"A Bayesian heriarchical model has three layers:
- A data layer (the likelihood)
- A process layer (the parameters describing the underlying process)
- A prior layer (priors on hyper parameters) "
```  

#### The setup {-}
Underlying RE model (as before)

Study-specific estimate: 

$$ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) $$

True study-specific effects distributed: 

$$ \theta_k \sim \mathcal{N}(\mu,\tau^2) $$

... simplified to the 'marginal' form:

$$ \hat\theta_k | \mu, \tau, \sigma_k \sim \mathcal{N}(\mu,\sigma_k^2 + \tau^2)$$

\


And now we specify priors for these parameters, 'making it Bayesian'


$$(\mu, \tau^2) \sim p(.)$$
$$ \tau^2 > 0 $$
\

Estimation will...

> involve[] Markov Chain Monte Carlo based sampling procedures, such as the Gibbs Sampler. In the brms package we will be using in this chapter, the No-U-Turn Sampler, or NUTS (Hoffman and Gelman 2014), is used. MCMC is used to sample from the posterior distribution. Without MCMC, to sample from the posterior the normalising constant would have to be found. But with most prior/likelihood combinations, the integral to find it is intractable.

\

```{block2,  type='note'}

**Why use Bayesian?**

- to  "directly model the uncertainty when estimating [the between-study variance] $\tau^2$"

- "have been found to be superior in estimating the between-study heterogeneity and pooled effect, particularly when the number of included studies is small"

- "produce full posterior distributions for both $\mu$ and $\tau$" ... so we can make legitimate statements about the probabilities of true parameters

- "allow us to integrate prior knowledge and assumptions when calculating meta-analyses" (including methodological uncertainty perhaps)

```  
\

#### Setting weakly informative' priors for the mean and cross-study variance of the TE sizes {-}

> It has been argued that a good approach is to use weakly informative priors (Williams, Rast, and Bürkner 2018) [rather than 'non-informative priors'!]. With a non-informative prior, we are essentially saying that we do not have any prior information on the effect size, whereas often we do have some information, and would like to include it in our analyis. Be careful of falling into traps - for instance, consider the uniform prior, which gives equal probability to every effect size. This is actually very informative, as it is saying that extreme values are as likely as moderate values. 


**For $\mu$**: 

> include distributions which represent that we do indeed have **some confidence that some values are more credible than others**, while still not making any overly specific statements about the exact true value of the parameter. ... In most applied cases, it seems reasonable to assume that the true effect size we want to estimate must lie somewhere between, for example, Cohen's $d=-2.0$ and $d=2.0$, but will unlikely be hovering around $d=50$. A good starting point for our $\mu$ prior may therefore be a normal distribution with mean $0$ and variance $1$. This means that we grant a 95% prior probability that the true pooled effect size $\mu$ lies between $d=-2.0$ and $d=2.0$. There has to be very good reason for setting the prior mean to be different from zero, as doing so may induce some form of bias. With regards to the variance, consider the order of magnitude of the response, and how much each effect will have. A variance of one is large enough to explore the parameter space, without being too large to allow for extreme values:

$$ \mu \sim \mathcal{N}(0,1)$$

**For  $\tau^2$**

- must be non-negative, but might be very close to zero.

- Recommended distribution for this case (for variances in general):  *Half-Cauchy prior* (a censored Cauchy)

$\mathcal{HC}(x_0,s)$

- with  *location parameter* $x_0$ (peak on x-axis)
- and  $s$, scaling parameter 'how heavy-tailed'

\

Half-Cauchy distribution for varying $s$, with $x_0=0$:

```{r, echo=FALSE, fig.width=6, fig.align='center'}

library(png)
library(grid)
img <- readPNG(here("images", "half_cauchy.png"))
grid.raster(img)

```


HC is 'heavy-tailed;... gives some probability to very high values but low values are still more likely.

Note that the Half-Cauchy distribution has an undefined mean, variance, skewness, and kurtosis. It does have a mode, at $x_0$.

One might consider $s=0.3$ 

<div class="marginnote">
$s$ corresponds to the std deviation here? ... so an SD of the effect size about 1/3 of it's mean size?
SD: Since the Cauchy distribution has infinite mean (hence undefined first moment) and variance, it doesn't make sense to discuss $s$ in this way. $s$ is a way of capturing the dispersion of the distribution. 
</div>


Checking the share of this distribution below 0.3...
```{r, message=F, warning=F}
phcauchy(0.3, sigma = 0.3) #cumulative share of distribution below 0.3 ... is 1/2 ... with sigma=0.3
```

\

... But they go for the 'more conservative' $s=0.5$.

> In general, it is advised to always conduct sensitivity analyses with different prior specifications to check if they affect the results substantially. With enough data, the likelihood should start to "overload" the prior. Some priors never allow the data to overload them, so we should be careful using them. 

\

Complete model:

$$ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) $$
$$ \theta_k \sim \mathcal{N}(\mu,\tau^2) $$
$$ \mu \sim \mathcal{N}(0,1)$$
$$ \tau \sim \mathcal{HC}(0,0.5)$$

#### Bayesian Meta-Analysis in R using the `brms` package {-}

You specify the priors as a vector of elements, each of which invokes the 'prior' function, which makes some sort of data frame. The priors function takes a distribution function with parameters, and a 'class'. 

```{r priors}
priors <- c(prior(normal(0,1), class = Intercept), prior(cauchy(0,0.5), class = sd))
```


A quick look at the data we're using here: 

```{r}

str(ThirdWave[,1:3])
```

`TE`:  calculated effect size of each study, expressed as the Standardized Mean Difference (SMD)

`seTE`: the standard error corresponding to each effect size

`Author`: a unique identifier for each study/effect size.

\

To actually run the model, he uses the following code:

<div class="marginnote">
This requires careful installation of packages. See [here](https://discourse.mc-stan.org/t/dealing-with-catalina-iii/12731/30) for Mac OS Catalina, R 4.9 instructions.
</div>

I find it surprising how long this procedure takes to run this simulation, given that the actual data used (estimates and SE's) is rather small. It seems to be that the C++ model takes long to compile.
 
<div class="marginnote">
The issue here seems to be that it takes a long time to compile the C++ model each time; it might be something we can write to avoid this step.

SD: If I can get this to work in Stan, I can write a .stan file and there should be some code so that the model does not need to be compiled in C++ every time it is run when unchanged. This is a work in progress
</div>

```{r echo=TRUE, eval=TRUE}

m.brm <- brm(TE|se(seTE) ~ 1 + (1|Author),
             data = ThirdWave,
             prior = priors,
             iter = 2000)

m.brm

```

<div class="marginnote">
The number of iterations can be any high number. A commonly used amount is 2000 iterations and four chains. The reason the number should be high is that the MCMC sampler needs to converge in distribution to the target distribution.
</div>

The *formula for the model* is specified using 'regression formula notation' (unfold)...

```{block2,  type='note'}


- As there is no 'predictor variable' in such analyses (unless it's meta-regression), `x` is replaced with `1`.

- But we want to give studies that more precisely estimate the effect size (perhaps because they have a larger sample) a greater weight. 
  - Coded using the `y|se(se_y)` element
  
- For the *random effects terms* he adds `(1|study)` to the predictor part (or here `(1|author)`.

-  `prior`: Plugs in the priors created above plug in the `priors` object we created previously here.

- `iter`:  Number of iterations of MCMC algorithm... the more complex your model, the higher this number should be. [DR: but what's a rule of thumb here?]

```


#### Assesing convergence (has MCMC algo found an optimum?) {-}

- If it hasn't converged, don't trust it! 
  - You may need to boost the number of iterations

"Posterior predictive checks": If it's converged, then the density of the replications should resemble the original data. 

<div class="marginnote">
Not sure I understand this; may need to revise and give a clear explanation. I guess it's something like 'the estimated terms are can then be run to simulate a dgp, which should yield data looking like the original data set'...
</div>


```{r}
pp_check(m.brm)

```
 

A commonly used method of assessing convergence is to look at the traceplots. There should be no distinctive patterns, and they should not have large spikes. Good traceplots look "well mixed", suggesting that the parameter space is being explored. Assessing convergence is a subjective judgement. 

```{r}
library(bayesplot)
mcmc_trace(m.brm)
```


\

Also check for a Potential Scale Reduction Factor (PSRF), or $\hat{R}$ below 1.01.

<div class="marginnote">
I've no idea what this is at this point; I need to read earlier chapters.
SD: Rhat is a Bayesian measure used to determine convergence - I don't think that Harrer will go into detail. It has a complicated formula, but all we should focus on is that Rhat converges to 1 as n tends to infinity. Read more here: https://mc-stan.org/rstan/reference/Rhat.html
</div>

```{r}
summary(m.brm)

```

 
#### Interpreting the Results {-}

Above we see the estimated 'sd of the mean effect' and the 'mean effect', and 'est. error' and CI's for each. 

<div class="marginnote">
Is 'est error' this like a measure of the standard deviation of the estimated coefficient?

Here CI's are 'credible intervals'. 
</div>

\

We can also extract the estimated deviation of each study’s “true” effect size from the pooled effect:
 
```{r}
ranef(m.brm)

```

<div class="marginnote">

These are measures of *deviations*. But they don't exactly equal the difference between the input effect size and the estimated pooled effect size. In fact this is coming from an estimate of the true effect for each study which  'averages towards the mean' following some criteria (this is mentioned later).

</div>


- No p-values listed because this is Bayesian. P-values are not really as helpful here as with frequentist approaches. With a classical analysis, we would ask if there is enough evidence of an effect to reject the null hypothesis of no impact at a (subjective) significance level. With a Bayesian approach, the entire distribution of the parameter is calcuated, is it is better to observe the distribution of the parameter. Frequentist analyses give binary answers to continuous questions, whereas with Bayesian analyses, continuous questions have continuous answers. 

Instead he states:

>  the Estimate of the pooled effect size is SMD = 0.57,
> with the 95% credibility interval (not confidence interval!) ranging from   95% CrI: 0.40 − 0.77. This indicates that there is in fact a moderate-sized overall effect of the interventions studied in this meta-analysis.

But now we can model the parameters we want to estimate probabilistically...

Taking samples from the (?) simulated posterior density of the population intercept (mean effect size) and sd of the effect...

```{r}

post.samples <- posterior_samples(m.brm, c("^b", "^sd"))
names(post.samples)

names(post.samples) <- c("smd", "tau")

```

"... make a **density plot** of the posterior distributions"


```{r, eval=T}

# Plot for SMD

smd_density  <- ggplot(aes(x = smd), data = post.samples) +
  geom_density(fill = "lightblue", color = "lightblue", alpha = 0.7) +
  geom_point(y = 0, x = mean(post.samples$smd)) +
  labs(x = expression(italic(SMD)),
       y = element_blank()) +
  theme_minimal() +
  ggtitle("Standardized mean difference", subtitle = "Posterior density plot")


# Plot for tau
tau_density <- ggplot(aes(x = tau), data = post.samples) +
  geom_density(fill = "lightgreen", color = "lightgreen", alpha = 0.7) +
  geom_point(y = 0, x = mean(post.samples$tau)) +
    labs(x = expression(tau),
       y = element_blank()) +
  theme_minimal() +
  ggtitle("Between-study variation (SD = tau)", subtitle = "Posterior density plot")

#Display plots together

require(gridExtra)
grid.arrange(smd_density, tau_density, ncol=2)

```

- posterior distributions unimodal, roughly normal distribution
- ... "peaking around the values for $\mu$ and $\tau$ we saw in the output"

<div class="marginnote">
Consider: why are the peaks not exactly these values? Mean versus mode, I guess.
</div>
 
\
Maybe we want to know (e.g.) "the probability that the pooled effect is $SMD=0.30$ or smaller, based on our model."
<div class="marginnote">
Because maybe an effect of 0.30 or smaller means it's not worth using this drug or something </div>
 
 
Consider the *Empirical Cumulative Distribution Function* (ECDF) "of the posterior distribution for the pooled effect size"...

> Use the `ecdf` function to implement the ECDF...  then check...

```{r}
smd.ecdf <- ecdf(post.samples$smd)
smd.ecdf(0.3)
```

> We see that the probability of our pooled effect being smaller than $SMD = 0.30$ is **very, very low**, so  the effects of the interventions we find in this meta-analysis are very likely to be meaningful.

 
\

Plotting the ECDF  below:

```{r, warning=F, message=F, fig.width=7, fig.height=5, fig.align='center', echo=F}

smd.ecdf <- ecdf(post.samples$smd) #cumulative distribution computed on the posterior samples/ simulations of the SMD
ecdf.dat <- data.frame(smd = 1:1000/1000, p = smd.ecdf(1:1000/1000)) #makes a data frame from a 0-1 counter in .001 units, and of the increments of the aforementioned cdf (I guess)

ggplot(aes(x = smd, y = p), data = ecdf.dat) +
  geom_vline(xintercept = mean(post.samples$smd), color = "grey") +
  geom_line(size = 2, color = "darkblue") +
  theme_minimal() +
  labs(x = "SMD", y = "Cumulative Probability") +
  ggtitle("ECDF: Posterior Distribution of the Pooled Effect Size")

```

### Forest plots

Forest plots are great, esp. with Bayesian, where we've 'sampled posterior distributions'... but there's no prepackaged tool yet. So we've to build it with help from the `tidybayes` package.

\

First we prepare the data, extracting the posterior distribution for each study individually.

<div class="marginnote">
Use the `spread_draws` function ... takes, as input 1. the fitted `brms` model, 2. the random-effects index factor and the parameter to extract (here `b_Intercept`).
...calculate actual effect sizes for each by adding the pooled effect size `b_Intercept` to the estimated deviation for each study.
</div>

```{r}
study.draws <- spread_draws(m.brm, r_Author[Author,], b_Intercept) %>% 
  mutate(b_Intercept = r_Author + b_Intercept)
```

\

Next, generate the distribution of the pooled effect (usually out in the last row of forest plots).

```{r}
pooled.effect.draws <- spread_draws(m.brm, b_Intercept) %>% 
  mutate(Author = "Pooled Effect")
```

Bind this together, clean labels and reorder:

```{r}
forest.data <- bind_rows(study.draws, pooled.effect.draws) %>% 
   ungroup() %>%
   mutate(Author = str_replace_all(Author, "[.]", " ")) %>% 
   mutate(Author = reorder(Author, b_Intercept))
```

Generate summarized data (the mean and credibility interval) of each study. Group the above by author, use the `mean_qi` function (generates 95pct intervals) to calculate these.

```{r}
forest.data.summary <- group_by(forest.data, Author) %>% 
  mean_qi(b_Intercept)
```

\

Now generate the forest plot: 

```{r, message=F, fig.align="center"}
ggplot(aes(b_Intercept, relevel(Author, "Pooled Effect", after = Inf)), 
       data = forest.data) +
  geom_vline(xintercept = fixef(m.brm)[1, 1], color = "grey", size = 1) +
  geom_vline(xintercept = fixef(m.brm)[1, 3:4], color = "grey", linetype = 2) +
  geom_vline(xintercept = 0, color = "black", size = 1) +
  geom_density_ridges(fill = "blue", rel_min_height = 0.01, col = NA, scale = 1,
                      alpha = 0.8) +
  geom_pointintervalh(data = forest.data.summary, size = 1) +
  geom_text(data = mutate_if(forest.data.summary, is.numeric, round, 2),
    aes(label = glue("{b_Intercept} [{.lower}, {.upper}]"), x = Inf), hjust = "inward") +
  labs(x = "Standardized Mean Difference",
       y = element_blank()) +
  theme_minimal()
```

<div class="marginnote">
Remember, these are not the effect sizes from the original studies. There has been some bayesian updating of each of these, considering all the others.
</div>

## Other notes, links, and commentary

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A high quality meta-analysis should:<br><br>- Have a pre-registered protocol<br>- Appropriately deal with dependent effect sizes<br>- Explore effect size heterogeneity <br>- Have a clear methods description<br>- Report COIs<br>- Publish data and code <a href="https://t.co/cHj11wv5vm">https://t.co/cHj11wv5vm</a></p>&mdash; Dan Quintana (@dsquintana) <a href="https://twitter.com/dsquintana/status/1196551674132914176?ref_src=twsrc%5Etfw">November 18, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



<!--chapter:end:meta_anal_and_open_science/metaanalysis.Rmd-->

# Bayesian approaches


```{block2,  type='note'}
I take notes on several different resources/texts below. Ultimately I'll try to integrate these into a single set of notes.
```  

## My (David Reinstein's) uses for Bayesian approaches (brainstorm)

### Meta-analysis of previous evidence

- Of prior work, especially on motivators of (effective) charitable giving and responses to effectiveness information

- Of my own series' of experiments (potentially joint with prior work)

### Inference, particularly about 'null effects'

When/what can we say about the 'absence of an effect'

How to integrate into inferences from diagnostic testing (e.g., common-trend assumption)?  

### 'Policy' and business implications and recommendations

In particular, in a charitable giving social-media fundraising context, we might consider whether it is worth offering 'seed contributions' to encourage giving on existing pages. If so, 'which pages should we seed and how much?'

\

### Theory-driven inference about optimizing agents, esp. in strategic settings

- Especially in the context od 'predicted contributions to public goods... and 2nd order beliefs'

### Experimental design

- Optimal treatment assignment, with previous observables and a track record

- Sequential designs

- Bayesian Power calculation
\

```{r}
#sessionInfo()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Package loadings from Kurtz:
```{r, echo = TRUE, message = F, warning = F, results = "hide"}

pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")

library(tidyverse) #adds in next chapter

```


## 'Statistical thinking' (McElreath) and [AJ Kurtz 'recoded' (bookdown)](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded): highlights and notes 


```{block2,  type='note'}

McElreath's course and text looks great. I'm taking selective notes here; I'll try to incorporate content from both text and [youtube video lectures](https://www.youtube.com/watch?v=4WVelCswXo4&list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI).

[AJ Kurtz has re-written the code](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded) using the `brms` package, which he finds superior. More crucially for me, he redoes the code using ggplot and tidyverse?

I'm planning to through this here, adding my own notes, questions, and considerations and (hopefully) incorporating some of my own work.
```  

<div class="marginnote">
I've also forked Kurtz's repo [here](https://github.com/daaronr/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse), which I may play with.
</div>
 
### The Golem of Prague (Chapter 1)

Don't let your model or approach turn into a Golem you can't control. Don't 'believe the model'; continuously validate it. The map is not the territory.

'Statistical decision trees' lend a false sense of security... and almost never fit the actual case we are dealing with. (fig 1.1)
\

Statistical models are non-unique maps to 'process models' which are non-unique maps to *hypotheses*. (He offers the example of neutral evolutionary selection' example.) 

This makes strict falsification impossible:  How can you falsify a hypothesis/theory if it corresponds to a wide set of process models and statistical models, many of which overlap other hypotheses? 
\

But this warning is at least as relevant for Bayesian analyses, which must be based on specifically defined (term) models of the DGP etc. Thus he recommends caution and continuous (?) interplay between the model and the data. (See next chapter ... 'small worlds and large worlds'.)
\

He also suggests we refer not to 'Confidence intervals' or even 'Credible intervals', but to 'Consistent intervals' ... as in 'these intervals are consistent with the model and data'.

\

And... 

> [so you should] '...Explicitly compare predictions of more than one model'


#### Rethinking: Is NHST falsificationist? {-}

```{r failure_of_falsification.png, fig.cap='From McElreath video lecture 1', out.width='70%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("images/failure_of_falsification.png")

```

> Null hypothesis significance testing, NHST, is often identified with the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify a null hypothesis, not the actual research hypothesis. So the falsification is being done to something other than the explanatory model. This seems the reverse from Karl Popper’s philosophy.

<div class="marginnote">
 
I.e., scientists have turned things upside down; originally the idea was that you had substitute of hypotheses that you would want to falsify and now we try to falsify silly null hypotheses that “nothing is going on”. You should try to really build a hypothesis and test it not just reject that nothing is going on.

</div>


#### Book's foci

1. Bayesian data analysis
2. Multilevel modeling
3. Model comparison using information criteria

\

### Small Worlds and Large Worlds (Ch 2)

> ... The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20)

\

We imagine a bag filled with four marbles, each of which is blue or white. 

"So, if we're willing to code the marbles as 0 = "white" 1 = "blue", we can arrange the possibility data in a tibble as follows." I.e., we can consider the five possible worlds, in each of which the bag has a different number of white and blue marbles, and represent each of these worlds as a column vector:

```{r, warning = F, message = F}
d <-
  tibble(p_1 = 0,
         p_2 = rep(1:0, times = c(1, 3)),
         p_3 = rep(1:0, times = c(2, 2)),
         p_4 = rep(1:0, times = c(3, 1)),
         p_5 = 1)

d

```

\

We visualize this in the plot below, where each column is one 'world':

```{r,  out.width='50%'}
d %>% 
  gather() %>% #make it long, with an ket variable for the possibility 'world'
  mutate(x = rep(1:4, times = 5), #an index for 'which ball'
         possibility = rep(1:5, each = 4)) %>% #distributing the 'which world' index
  ggplot(aes(x = x, y = possibility, 
             fill = value %>% as.character())) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) +
  theme(legend.position = "none")
```

\

Simple combinatorics (permutations rule) tells us how many 'ways' we can draw 1, 2, and 3 marbles... Here we think about 'which' marble is drawn, and not just 'which color' it is. We can draw marble 1-4, the first time, then 1-4 the second time, and then 1-4 the third time... so `possibilities=marbles ^ draw`.

```{r}

tibble(draw    = 1:3,
       marbles = 4) %>% 
  mutate(possibilities = marbles ^ draw) %>% 
  knitr::kable()

```

```{block2,  type='note'}
Next, there is a huge amount of code explaining how to make the 'garden of forking paths' diagrams. I'm basically going to skip all that code, and paste in a few images. You can find  all the code [HERE](https://bookdown.org/content/3890/small-worlds-and-large-worlds.html#the-garden-of-forking-data)

```  
\

Suppose there is only one blue ball and three white balls, possibility '2' above. For this world, we see the full 'garden of forking paths' --- the number of ways to select 1, 2, and 3 balls (with replacement) --- below. 


Every path starting from the center is a possible (sequence of) draws. 

```{r, fig.cap='All possible draws of three balls', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-13-1.png")

```

\

Now the inferential exercise: we want to know (the likelihood) of each of the five possible 'worlds'. As we draw data we know we are  proceeding along one of some subset of the forking paths.
\

For example, under possible world 2,  if we draw Blue, then White, then Blue, this could have occured with any of the following paths (consider a draw of each of the white balls as distinct):

```{r, fig.cap='All possible draws of three balls', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-14-1.png")

```

We see that under World 2 there are 3 ways of getting this sequence. 3 out of $4^3$ equally likely paths under World 2, or a $3/64$ chance  (about 5%).

\

We can do similar for the other possible worlds; multiplying the 'ways to produce each draw' in the path yields the 'total ways to produce the path', under each world.  

```{r}
# if we make two custom functions, here, it will simplify the code within `mutate()`, below
n_blue <- function(x) {
  rowSums(x == "b")
}

n_white <- function(x) {
  rowSums(x == "w")
}

t <-
  # for the first four columns, `p_` indexes position
  tibble(p_1 = rep(c("w", "b"), times = c(1, 4)),
         p_2 = rep(c("w", "b"), times = c(2, 3)),
         p_3 = rep(c("w", "b"), times = c(3, 2)),
         p_4 = rep(c("w", "b"), times = c(4, 1))) %>% 
  mutate(`draw 1: blue`  = n_blue(.),
         `draw 2: white` = n_white(.),
         `draw 3: blue`  = n_blue(.)) %>% 
  mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`)

t %>% 
  knitr::kable()
```

Among of all possible worlds, we see the most number of ways to get B-W-B in a world 4; with three blues and one white -- here there are 9 ways in total to get B-W-B. Under world 4 this sequence occurs 9/64, or roughly 14% of the time. 


\

We can see this in the following plot. (We leave out the worlds with only one color ball, as these will have no paths that produce B-W-B). Below, each partitioned section represents one world, and the paths in that world that could produce B-W-B are shown.

\ 

```{r, fig.cap='All possible draws of three balls',  out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-20-1.png")

```

Three paths, versus 8 paths, versus nine paths...

Does this reveal world 4 to be the most likely contents of the present bag? Not necessarily. Suppose we knew ex-ante, from the factory, that '99 bags out of 100 have equal numbers of whites and blues.' Then, it would be much more likely that this bag was from an equal-color bag (world 3), even though this *draw* is more likely conditional on the bag being from world 4. We need will to consider the base-rate probabilities as well. 

<div class="marginnote">
This in turn motivates the standard 'false positive/false negative HIV test' example.
</div>
 

### Using prior information 

> We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25)

This seems to easy to be true, but our garden illustration helps us understand why it is the case. 


#### "Multiply in" new information

First consider, what if we had another draw from the bag, how would this adjust the 'number of paths' for each world? Remember, each draw is independent (replacement). We simply record the number of ways (permutations) that could lead to this draw in each world, and we *multiply* the previous count by this number. You can consider this visually in seeing how 'adding an additional fork at the end of each path' changes the count. 

\

This is given in the table below:

```{r}
t <-
  t %>% 
  rename(`previous counts` = `ways to produce`,
         `ways to produce` = `draw 1: blue`) %>% 
  select(p_1:p_4, `ways to produce`, `previous counts`) %>% 
  mutate(`new count` = `ways to produce` * `previous counts`)

t %>% 
  knitr::kable()
```
\
How to incorporate *prior* information about the probability of each world? 
\

Suppose your friend in the factory tells you (reliably) that "we produce 3 bags with (just) 1 blue for every 2 bags with equal counts, for every 1 bag with 3 blues."

We can think of the 'factory choosing which bag to produce' as another draw, thus another path.

\
Here the *sequence* in which the information is recieved shouldn't matter. The draws are independent (we presume).  

We can thus multiply the number of paths for each marbles-in-bag world by the (relative) frequency with which the factory 'draws' that bag... as shown below:


```{r}
t <- t %>% 
  select(p_1:p_4, `new count`) %>% 
  rename(`prior count` = `new count`) %>% 
  mutate(`factory count` = c(0, 3:0)) %>% 
  mutate(`new count` = `prior count` * `factory count`)

t %>% 
  knitr::kable()
```

### From counts to probability.

\

## Title: "Introduction to Bayesian analysis in R and Stata - Katz, Qstep"

*Content from notes from this lecture*

### Why and when use Bayesian (MCMC) methods?

#### Pros
1. No need for asymptotics ... good when sample sizes are small

2. Incorporate previous information

You can consider the 'robustness to other priors'

3. Fit complex nonstandard models
... e.g., with difficult functional forms or likelihood settings (more computation, less thinking)

4. Easy to make predictions (e.g., simulate scenarios) after estimation

5. Incorporate evidence, results, expert judgement

('restrictions' with some lee-way?)

(ISn't this the same as number 2?)

6. Cleaner treatment/imputation of missing values ... these are just parameters

#### Cons

1. Must specify prior distributions ... allows subjective judgement

2. Different way of thinking about stats and inference; probability distributions and simulations, not much about p-values, point estimates and standard errors ... path dependence

3. Computational cost

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


#### Why more popular today?

- Starting from around 2005 in Political Science and Sociology

Computational revolution comes from Markov chain Monte Carlo (MCMC) methods ... don't need analytical solutions

Software implementations -- many in R, specialised software like EWinBugs, JAGS, STAN; also increasingly in Stata


### Theory

Bayes theorem ... inverting conditional probability thing ... 'inversion' to make inferences about the parameters

- In Bayesian stats the *parameters* (and sometimes missing values) are random variables, we make probability statements about them

$$P(A|B)=P(B|A)P(A)/P(B)$$


Frequentist: Point estimates, unknown fixed parameters, data from a hyol repeataable random sample

Bayesian: Fixed data (from the experiment), parameters are random variables ... results based on probability distributions about rthese

Classical statistics: likelihood of data given parameter: $p(y|\theta)$

Bayes we want, $p(\theta|y) = p(y|\theta)p(\theta)/p(y)$

$p(y)$  is a 'constant' in our estimation ... the data is fixed.

So it's proportional to $p(\theta|y) = p(y|\theta)\times p(\theta)$

$p(y|\theta)$ is what we max when we do ML

$ p(\theta)$: prior distribution capturing beliefs about $\theta$

#### So how do we estimate it?

1.  Specify a probability model, a distribution for Y (likelihood function) and the priors for $\theta$

2. Solve (find) the posterior distribution $p(\theta|Y)$ and summarise the parameters of interest

In practice, step 2 is usually done via MCMC simulation rather than analytically.

... via simulations, I approach the 'true' value on $\theta$

(Given 'regularity conditions')

#### Linear regression model example

$$Y = x'\beta+\epsilon$$ with n obs

only random term is epsilon ... natural candidate is a normal distribution, so $Y \sim N(x'\beta,\sigma^2_e)$


So we want to find $p(\beta, \sigma^2_\epsilon|Y,X)$. This depends on the choices of $p(\beta)$ and $p(\epsilon)$. Could choose conjugate priors, leading to a particular joint posterior, you can solve it analytically.

Can yield a joint posterior.

Instead, let's assume that the latter (variance) parameter is known, you can show that the posterior for $\beta$ is also normally distributed. (Conjugate)

Similarly, if we assume $\beta$ is known, if the variance term had an inverse gamma distribution (prior), so will the posterior.

In these conjugate priors, the posterior mean will be a weighted average of the priors and the data.

#### Gibbs

Needs closed form conditional posterior for every parameter.

What Gibbs sampler does is break the parameter space into sets of parameters

1. Choose starting values, $\theta^0_1,...\theta^0_k$

2. sample from the first parameter's distribution given the others
... the second one, ... the k'th one .

3. Repeat step 2 ... thousands of times (starting with the parameters from the previous iteration)
Eventually 'we obtain samples of $p(\theta|y)$'

*But if we don't have a closed form, we cannot simply sample from known distributions in each step*

E.g., in case of Logit distribution.

#### Metropolis Hastings

1. Choose 'proposal distribution' to sample parameter values (a candidate like normal, uniform)
1. Start w a prelim guess for parameter values $\theta_0$
1. At iteration t sample a proposal $\theta_t$  from $p(\theta_t|\theta_{t-1})$ ?? what does this come from?
1. If $p(\theta_t|y)>p(\theta_{t-1}|y)$ accept it as the new value of $\theta$. ??? how is this computed if we don't have conjugate closed-form posteriors?
1. Otherwise flip a coin  with probability  r = (ratio of those probabilities)
- if coin tosses heads, accept as new theta, otherwise stay at previous theta
-  allows algorithm to avoid getting stuck at local maxima

Commonly used proposal: random walk sample: $\theta_t=\theta_{t-1}+z_t$, $z_t \sim f$

?? I do this because there is no analytical way to derive this, unlike in the conjugate case, where we might use the Gibbs

- can combine Gibbs with Metropolis steps; relevant to some problems

#### Assessing convergence

- previous ... 'eyeballing'
- formal:
  - single-chain tests (Geweke/Heidel) ... is the last part of the chain stable (stationary)... compare simulation at middle and end, is there much variation?
  - multiple-chain test... (starting from different values), do they end similar ... Gelman-Rubin diagnosting $\hat{R}$
  - typically either a very long chain and use GH convergence, or multiple shorter chains and use $\hat{R}$

Gabriel: Gelman-Rubin is probably preferred; more conservative

?? What am I iterating towards? Converging on what?

#### Assesing 'fit' in Bayesian

- No r-squared
- Typical measure is 'posterior predictive comparisons'

$p(y_{replicated}|y_{observed}= ...$

1. Simulate data from estimated parameters
2. Compare to observed data
3. Use an overall fit measure to assess model fit

E.g.,   percent correct predictions (binary), whether the true data is within the 95\% CI of the replicates, deviance

For each replicate Choose statistic D, compare the replicated 

$D(y^s_{replicated})$ 
against $D(y^s_{observed})$

Quantify the discrepancy ... percent of correct predictions, proportion of times replicated y is below true y ... compute 'bayesian p-value's'

Systematic differences between replicate and actual data indicate model limitations

(?? what are reasonable values here??)

### Comparing models ... Equivalent of 'likelihood'

'Deviance Information Criterion' (most used); specific for MCMC simulations: compares expected LL of the model (of the data given the estimated parameters; average here across much of the later points in the chain) against the llhd at the posterior parameter mean.   Always select model with lowest DIC.

Bayes Factor (less used): Ratio of llhd of the models; higher BF means model is more supported; BF>10 seen to provide strong evidence for model w higher value

### On choosing priors

Most social scientists use non-informative or vague priors; i.e., large variance... e.g., $\beta \sim N(0,1000)$

But its often useful to incorporate information into your priors

Small pilot to test, $\rightarrow$ data $Y_1$, another study gives data $Y_2$; repeated application of Bayes theorem gives the posterior.

Same result whether you obtained these together, or whether you did one and then updated (e.g., via an MCMC, starting with the first one as a prior)

Conjugate priors (mentioned before)

- Jeffrey's priors (??)

### Implementation

If you don't need to do fancy things, and don't want to (?) generate the full posterior distribution (or something)

Some Stata/R commands that make Bayesian look frequentist.

In Jags and Winbugs, we only have to specify the prior... rest is done for us

Jags is great ... you only need to do self-coding with lots of data and super complicated models as it can freeze up

We went through it the fancy way in Probit.R

Then the easy way with 'script probit Jags.R'

### Generate predictions from a WinBUGS model

You can just generate these outcomes ...

Prediction: generate a new observation #note, he is doing one per iteration, but since these are convergent it would be basically the same if you just chose a random iteration and did all the draws from that one

### Missing data case

One solution -- multiple imputation

- choose imputation model to predict missings,
- generate many copies of orig data set, imputing missibg value for each
- 2 more steps here

Need a model for X|alpha, because missing variables are random variables

### Stata

Has some rather simple implementations; e.g., just using commands like ```bayes: regress y x```

### R mcmc pac

Also simple code; great for standard use

Speedup with parallelization; see "script for parallel probit.R" and "parallelprobit.R"

More advanced: C++; can integrate it with Rcpp, or even use Exeter's ISCA cluster


```{r cars}
summary(cars)
```

## Other resources and notes to integrate

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Hey stats twitter: got a very sharp psych UG student wanting to dive into Bayes. Many resources are too technical (i.e., not good teaching texts for UG level, but useful references). Where should I point her?</p>&mdash; Tom Carpenter (@tcarpenter216) <a href="https://twitter.com/tcarpenter216/status/1223406990325534720?ref_src=twsrc%5Etfw">February 1, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<!--chapter:end:bayesian/bayes_notes.Rmd-->

# List of references {#references}

<!--chapter:end:references.Rmd-->

