---
title: "Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus"
author: "Dr. David Reinstein, "
abstract: "This 'book' organizes my notes  and helps others understand it and learn from it"
#cover-image: "images/cardcatalogue.JPG"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    includes:
      in_header: support/header.html
    css: support/tufte_plus.css
    config:
      toc:
        after: |
          <li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>
        collapse: section
        scroll_highlight: yes
      fontsettings:
        theme: white
        size: 2
      sharing:
        facebook: yes
        twitter: yes
        google: no
        linkedin: yes
        weibo: yes
        instapaper: no
        vk: no
        all: ['facebook', 'twitter', 'linkedin', 'weibo', 'instapaper']
    highlight: tango
    download: [pdf, epub, mobi]
    sharing:
      github: yes
      facebook: no
always_allow_html: yes
bibliography: [support/reinstein_bibtex.bib, ./references.bib] #this second one seems to be auto-generated ?
#
biblio-style: apalike
link-citations: yes
github-repo: daaronr/metrics_discussion_work
description: ""
#url: 'https\://daaronr.github.io//'
tags: [Econometrics, Statistics, Data Science, Experiments, Notes, Methodology]
---


<!--
base file created from

`pandoc -f docx -t gfm -o writing_econ_gfm.md "bookoutline3-cutting examples down-cutnamesd.docx" `

and similar from


`pandoc -f docx -t gfm -o writing_econ_gfm1.md "Adapting back for BOOK --Ec831 outline-fillingindetails_forslides_edMiriam-conflict.docx"`

replacements needed:

- "\[ \]" surrounds math -- square brackets do not need 'escape' in main text
- colors need adjusting to 'format_with_col'

-->

Try downloading and accessing some functions here:
```{r download-formatting-to-support-folder}

library(here)
tryCatch(
  {
download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/header.html", destfile = here("support", "headerX.html"))
  download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css", destfile = here("support", "tufte_plusX.css"))
  download.file(url = "https://www.dropbox.com/s/3i8bjrgo8u08v5w/reinstein_bibtex.bib?raw=1", destfile = here("support", "reinstein_bibtexX.bib"))
},  error = function(e) {
  print("you are not online, so we can't download")
}
)

tryCatch(
file.rename(here("support", "headerX.html"), here("support", "header.html"))
)

tryCatch(
file.rename(here("support", "tufte_plusX.css"), here("support", "tufte_plus.css"))
)
tryCatch(
file.rename(here("support", "reinstein_bibtexX.bib"), here("support", "reinstein_bibtex.bib"))
)
```

```{r download-fncns}

library(here)
library(devtools)

tryCatch(
  {
download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/functions/functions.R", destfile = here("code", "functionsX.R"))
  download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/functions/baseoptions.R", destfile = here("code", "baseoptionsX.R"))
},  error = function(e) {
  print("you are not online, so we source locally instead; hope you've updated")
  source(here("code", "functions.R")) # functions grabbed from web and created by us for analysis/output
 source(here("code", "baseoptions.R")) # Basic options used across files and shortcut functions, e.g., 'pp()' for print
}
)

#Note - workaround naming and code because otherwise a failed download seems to delete the destination file -- how to fix that? Renaming files?

file.rename(here::here("code", "functionsX.R"), here::here("code", "functions.R"))
file.rename(here::here("code", "baseoptionsX.R"), here::here("code", "baseoptions.R"))

source(here("code", "functions.R"))
source(here("code", "baseoptions.R"))

# Basic options used across files and shortcut functions, e.g., 'pp()' for print
# functions grabbed from web and created by us for analysis/output
```

```{r eval=FALSE}
install.packages("bookdown")
install.packages("tufte")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

```{r packages}

##TODO: this probably repeats the above -- check and integrate

library(here)
#library(checkpoint) #to avoid differential processing from different package versions
library(pacman)

here <- here::here

p_load(dplyr,magrittr,purrr,tidyverse,tidyr,broom,janitor,here,glue,dataMaid,glue,readr, lubridate,summarytools,gtools,knitr,pastecs,data.table)   #citr, reporttools, experiment, estimatr,  kableExtra, ggsignif, glmnet, glmnetUtils, rsample,snakecase,zoo
library(codebook)

```


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r somefunctions}

#possibly move these to a separate file

#multi-output text color
#https://dr-harper.github.io/rmarkdown-cookbook/changing-font-colour.html#multi-output-text-colour
#We can then use the code as an inline R expression format_with_col("my text", "red")

format_with_col = function(x, color){
  if(knitr::is_latex_output())
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(knitr::is_html_output())
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}

```

```{r html, echo=FALSE}
# globally set chunk options
knitr::opts_chunk$set(fig.align='center', out.width='80%')

my_output <- knitr::opts_knit$get("rmarkdown.pandoc.to")

```



<!---
Can define text blocks here, refer to them again and again if desired
-->

<!-- Global site tag (gtag.js) - Google Analytics -->


<!-- <html> -->

<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLKFNFTGXX"></script> -->
<!-- <script> -->
<!--   window.dataLayer = window.dataLayer || []; -->
<!--   function gtag(){dataLayer.push(arguments);} -->
<!--   gtag('js', new Date()); -->

<!--   gtag('config', 'G-QLKFNFTGXX'); -->
<!-- </script> -->
<!-- </html> -->

<!--chapter:end:index.Rmd-->

# Introduction

My goal in putting this resource is to focus on the practical tools I use and the challenges I ([David Reinstein](https://daaronr.github.io/markdown-cv/)) face. But I am open to collaboration with others on this.

\

*My focus:* Microeconomics, behavioral economics, focus on charitable giving and 'returns to education' type of straightforward problems. (Minimal focus on structural approaches.)

What I care about: Where we can *add value* to real econometric (and statistical and experimental design) practice?

\

The data I focus on:

- Observational (esp. web-scraped and API data and national surveys/admin data)

- Experimental: esp. where with multiple crossed arms, and where the 'cleanest design' may not be possible

\

I will assume familiarity with most basic statistical concepts like 'bias', 'consistency', and 'null hypothesis testing.' However, I will focus on some concepts that seem to often be misunderstood and mis-applied.

```{block2,  type='note'}

If you are involved with this project, you can find a brief guide (somewhat WIP) on how to add content (HERE)[https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html].  This is from a different project but the setup is basically the same.

```  

\

A note to potential research assistants and student collaborators (unfold):

```{block2,  type='fold'}
 
```

## (Conceptual: approaches to statistics/inference and causality)[#conceptual]

### Bayesian vs. frequentist approaches {-}

Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)

### Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

#### DAGs and Potential outcomes

### Theory, restrictions, and 'structural vs reduced form'

## Getting, cleaning and using data; project management and coding {#data-sci}

This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.  

### Data: What/why/where/how

### Organizing a project

### Dynamic documents (esp Rmd/bookdown)

### Good coding practices


#### New tools and approaches to data (esp 'tidyverse')

#### Style and consistency

Indenting, snake-case, etc

#### Using functions, variable lists, etc., for clean, concise, readable code

### Data sharing and integrity


## Basic regression and statistical inference: Common mistakes and issues

### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: 	Simonsohn18

http://datacolada.org/62

### OLS and heterogeneity

- OLS does *not* identify the ATE

http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT


- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"

"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

#### Confidence intervals and Bayesian credible intervals

#### Comparing relative parameters

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT)

See [@verkaik2016]

### Interaction terms and pitfalls

#### 'Moderators' Confusion with nonlinearity

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if outcome  'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative

#### MHT

### Choice of test statistics (including nonparametric)

<!-- (Or get to this in the experimetrics section) -->

### How to display and write about regression results and tests

### Bayesian interpretations of results

(see 'the Bayesian Approach')

## LDV and discrete choice modeling

## Robustness and diagnostics, with integrity

### (How) can diagnostic tests make sense? Where is the burden of proof?

### Estimating standard errors

### Sensitivity analysis: Interactive presentation

## [Control strategies and prediction; Machine Learning approaches](#control-ml)

### Machine Learning (statistical learning): Lasso, Ridge, and more

### Limitations to inference from learning approaches

## IV and its many issues


### Instrument validity

- Exogeneity vs. exclusion
- Very hard to 'powerfully test'


### Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

Focusing on a binary endogenous 'treatment' variable 

- With heterogeneity 
- With imperfect compliance 
- With one-way compliance

### Weak instruments, other issues

- With a 'weak instrument' ... why does that matter?

### Reference to the use of IV in experiments/mediation

## [Other paths to observational identification](#other_paths)

### Fixed effects and differencing

### DiD

FE/DiD does not rule out a correlated dynamic unobservable, causing a bias

### RD

### Time-series-ish panel approaches to micro

#### Lagged dependent variable and fixed effects --> 'Nickel bias'

## Causal pathways: [Mediation modeling and its massive limitations](#mediators)

An applied review

## Causal pathways: selection, corners, hurdles, and 'conditional on' estimates


### 'Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection

"Conditional on positive"/"intensive margin" analysis ignores selection *identification issue* 	See Angrist and Pischke on "Good CoP, bad CoP".  See also bounding approaches such as [@Lee2018]	AngristJ.D.2008a,

\

#### Bounding approaches (Lee, Manski, etc)

See [Notes on Lee bounds](#notes_lee)


## [(Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters](#why_experiment_etc)

### Why run an experiment or study?

- Sugden and Sitzia critique here, give more motivation

### Causal channels and identification

- Ruling out alternative hypotheses, etc

### Types of experiments, 'demand effects' and more artifacts of artifical setups

### Generalizability (and heterogeneity)

## (Experimental) Study design: Background and quantitative issues

### Pre-registration and Pre-analysis plans

#### The hazards of specification-searching

### Sequential and adaptive designs

Needs to adjust significance tests for augmenting data/sequential analysis/peeking	Statistics/econometrics	new-statistics		sagarin_2014			http://www.paugmented.com/	resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/

Yet ...


P_augmented may *overstate* type-1 error rate	Statistics/econometrics	response to referees, new-statistics	"
A process involving stopping ""whenever the nominal $p.0.5$"" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5\%. Even if the subsequent data suggested a ""one in a million chance of arising under the null"" the overall process yields a 5\%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the ""likelihood of the null hypothesis"" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate.

Considering the calculations in \ref{sagarin2014}, it is clear that $p_{augmented}$ should \textit{overstate} the type-1 error of the process if there is a positive probability that after an initial experiment attains p$<0.05$, more data is collected.  A headline $p<0.05$ does \textit{not} imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded $p>0.05$, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though $p<0.05$. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine's experiment).  Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p<0.05, more data is collected, implying that $p_{augmented}$ as calculated above overstates the type I error rate that would arise  from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below $p=0.05$, in order to attain a type-1 error rate of 5% or less in our published corpus."

### Efficient assignment of treatments

(Links back to power analyses)


## (Experimental) Study design: (Ex-ante) Power calculations 

### What sort of 'power calculations' make sense, and what is the point?

#### The 'harm to science' from running underpowered studies

> "One worries about underpowered tests. Your result (may have)  relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.

> Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects."	verkaik2016, metzger2015

### Power calculations without real data

### Power calculations using prior data

## ['Experimetrics' and measurement of treatment effects from RCTs] (#experimetrics_te)

### Which error structure? Random effects?

### Randomization inference?

### Parametric and nonparametric tests of simple hypotheses

### Adjustments for exogenous (but non-random) treatment assignment

### IV in an experimental context to get at 'mediators'?

### Heterogeneity in an experimental context

## [Making inferences from previous work; Meta-analysis, combining studies](#metaanalysis)

### Publication bias

### Combining a few (your own) studies/estimates

### Full meta-analyses

- Models to address publication biases

## The Bayesian approach

## Some key resources and references 

[@AngristJ.D.2008a]

'The Mixtape' (Cunningham)

[@kennedyGuideEconometrics2003]

[@Tibshirani]

OSF guides

Christensen ea "Transparent and Reproducable Social Science Research"

[@Gentzkow2013; @wooldridgeEconometricAnalysisCross2002; @wooldridgeIntroductoryEconometricsModern2008]


An Introduction to Statistical Learning with Applications in R

R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org 

Statistical Rethinking: A Bayesian Course with Examples in R and Stan

### Consider: 

Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017
  
Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction .
Cambridge University Press, 2015

Judea Pearl

Imbens: Potential Outcomes versus DAGs

<!--chapter:end:metrics_outline.Rmd-->

#  Conceptual: approaches to statistics/inference and causality {#conceptual}

### Bayesian vs. frequentist approaches

Folder: bayesian
Notes: [bayes_notes](bayesian/bayes_notes.Rmd)

### Causal vs. descriptive; 'treatment effects' and the potential outcomes causal model

#### DAGs and Potential outcomes

### Theory, restrictions, and 'structural vs reduced form'


<!--chapter:end:conceptual/conceptual.Rmd-->

# Getting, cleaning and using data {#data-sci}

This will build on my content [here](https://daaronr.github.io/writing_econ_research/economic-theory-modeling-and-empirical-work.html#getting-and-using-data), and integrate with it.

## Data: What/why/where/how

## Organizing a project

## Dynamic documents (esp Rmd/bookdown)

### Managing references/citations


```{block2,  type='note'}
A letter to my co-authors...

Hi all.

Hope you are doing well. I've just invited you to a shared Zotero group managing my bibliography/references. I think this should be useful. (I prefer Zotero to Mendeley because it's open source and... I forgot the other reason.)
On my computer it synchronizes with a .bib (bibtex) file in a dropbox folder ...

For latex files we just refer to this as normal. 
In the Rmd files/bookdown (producing output like [EA barriers](#https://daaronr.github.io/ea_giving_barriers/outline.html) or Metrics notes (present book) this is referenced in the YAML header to the index.Rmd file as

> bibliography: [reinstein_bibtex.bib]

Then, to keep this file, I have a "download block" included in that same file (the first line with 'dropbox' is the key one).

```  

\

The download code follows (remove the 'eval=FALSE' to get it to actually run)...

```{r download-formatting-to-support-folder-example, eval=FALSE}

tryCatch( #trycatch lets us 'try' to execute and if there is an error, it does the thing *after* the braces, rather than crashing
  {
   download.file(url = "https://www.dropbox.com/s/3i8bjrgo8u08v5w/reinstein_bibtex.bib?raw=1", des tfile = "reinstein_bibtex.bib") #download the bibtex database

        download.file(url = "https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css", destfile = here("support", "tufte_plus.css")) #this downloads the style file
    
  }, error = function(e) {
    print("you are not online, so we can't download")
  }
)
```


## Good coding practices

### New tools and approaches to data (esp 'tidyverse')

From [Kurtz](https://bookdown.org/content/3890/small-worlds-and-large-worlds.html):

> If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., `%>%`). I'm not going to explain the `%>%` in this project, but you might learn more about in [this brief clip](https://www.youtube.com/watch?v=9yjhxvu-pDg), starting around [minute 21:25 in this talk by Wickham](https://www.youtube.com/watch?v=K-ss_ag2k9E&t=1285s), or in [section 5.6.1 from Grolemund and Wickham's *R for Data Science*](https://r4ds.had.co.nz/transform.html#combining-multiple-operations-with-the-pipe). Really, all of Chapter 5 of *R4DS* is just great for new R and new tidyverse users. And *R4DS* Chapter 3 is a nice introduction to plotting with ggplot2. 

### Style and consistency

#### lower_snake_case {-} 

Use `lower_snake_case` to name *all* objects (that's my preference anyways) 
unless there's a strong reason to do otherwise.

\

This includes:

`file_names.txt`
`folder_names`
`function_names` (with few exceptions)
`names_of_data_objects_like_vectors`
`names_of_data_output_objects_like_correlation_coefficients`
`ex_df1` In R you probably should keep data frame names short to avoid excessive typing

\

*And by all that is holy, never put spaces or slashes in file or object names!* This can make it very hard to process across systems... there are various ways of referring to spaces and other white space. 

#### Indenting and spacing


### Using functions, variable lists, etc., for clean, concise, readable code


<!--chapter:end:data_sci/data_sci.Rmd-->

# Basic regression and statistical inference: Common mistakes and issues {#reg-follies}



## Basic regression and statistical inference: Common mistakes and issues briefly listed


Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)	Identification

Random effects estimators show a lack of robustness	Specification	Clustering SE  is more standard practice

OLS/IV estimators not 'mean effect' in presence of heterogeneity

Power calculations/underpowered

Selection bias due to attrition

Selection bias due to missing variables -- impute these as a solution

Signs of p-hacking and specification-hunting

Weak diagnostic/identification tests

Dropping zeroes in a "loglinear" model is problematic

Random effects estimators show a lack of robustness

Dropping zeroes in a "loglinear" model is problematic

Random effects estimators show a lack of robustness

With heterogeneity the simple OLS estimator is not the 'mean effect'

P_augmented may *overstate* type-1 error rate

Impact size from regression of "log 1+gift amount"

Lagged dependent variable and fixed effects --> 'Nickel bias'

Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper)

Weak IV bias

Bias from selecting instruments and estimating using the same data


## Bad control



From MEH:

> some variables are bad controls and should not be included in a regression model even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notational experiment at hand. That is, bad controls might just as well be dependent variables too."

– They could also be interpreted as endogenous variables.

Example of looking at a regression of wages in schooling, controlling for college degree completion:

> Once we acknowledge the fact that college affects occupation, comparison of wages by college degree status within occupation are no longer apples to apples, even if college degree completion is randomly assigned."

– The question here was whether to control for the category of occupation, not the college degree.

> It is also incorrect to say that the conditional comparison captures the part of the effect of college that is 'not explained by occupation' ... so we would do better to control only for variables that are not themselves caused by education."




### "Bad control" ("colliders")

Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita)

### Choices of lhs and rhs variables

- Missing data
- Choice of control variables and interactions
- Which outcome variable/variables

### Functional form

- Logs and exponentials
- Nonlinear modeling (and interpreting coefficients)


#### 'Testing for nonlinear terms'

Quadratic regressions are not diagnostic regarding u-shapedness: 	Simonsohn18

http://datacolada.org/62

### OLS and heterogeneity

#### OLS does *not* identify the ATE (-)

In general, with heterogeneity, OLS does *not* identify the ATE. It weights observations from different parts of the sample differently. Parts with greater residual variation in the treatment (outcome) variable are more (less) heavily weighted. 

<div class="marginnote">
E.g., if the treatment is binary, the estimator will most heavily weight those parts of the sample where the probability of treatment is closest to 1/2.
</div>
 
The formula is ...

\

```{block2,  type='note'}
**Some intuition**

Why is this the case?  The OLS type estimators we are taught in Econometrics are 'BLUE'  under the assumption of a *single homogenous 'effect'* (the 'slope'...  although the discussion itself is often agnostic as to whether this represents a causal effect). 

\
 
It is 'best' in a minimizing MSE sense under certain assumptions;  in particular, we must also know the true functional form and the set of variables to be included. See 'overfitting' issues.
 
In  order to have the estimate of the true slope that minimizes the squared errors, OLS (and related estimators like FGLS; as well as 2SLS in a more complicated sense) weights  some observations more than others. The 'influence' of an observation on the estimated slope depends on the nature of the variation in the  dependent and independent variables in the region that observation is drawn from.  Think of drawing a line  through a set of points  that were drawn with some noise from the true distribution.  If you drew it based on a bunch of points (from a region where) the treatment varies very little and the outcomes have a lot of noise,  the line you draw will be very sensitive to the latter noise and thus unreliable. So,  would optimally 'down-weight'  these observations in drawing the line. 

\

However, if the *actual* slope varies by region, this also means you are under-representing certain regions, and  thus getting  a biased  estimate of the average slope.

```

How can we deal with this?  If we think  that the  treatment effect varies with *observable* variables, we could include 'interactions';  essentially making separate estimates of the slope for each share of the population (but potentially  allowing other control variables to have a homogenous effects, and pooled or clustered estimation of underlying variance.)

<div class="marginnote">
 ...Although we may want to consider both overfitting here and the idea that there may be *some* shared component, so the fully-interacted model may be sub-optimal. See mixed modeling (?) 
</div>

\

However, this does not tell us how to recover the *average* of these slopes (approximately, the ATE).  Should we weight  each of the slopes by the share of the population that this group represents?

Mechanically,  the standard way of estimating and representing these  interactions and economics has been with simple dummies (0,1) for each compared group. This yields a 'base group' (e.g., males aged 35-60) --  this obviously does not recover the average slope-- as well as the 'adjustment' coefficients.

\

Another way of expressing interactions, particularly helpful with multi-level interactions is called 'effect coding': each group is coded as a 'difference from 0' (e.g,. males are -1/2 and females +1/2), before doing the interactions. This could allow for a more straightforward interpretation: at each level, the uninteracted term represents the average treatment effects, and the interacted terms  represent adjustments relative to this average. *But under which conditions is this in fact the case?*



[insert here].


[WB blog - your-go-to regression-specification is -biased-here-s-simple-way-fix-it](http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT)

A key paper: http://www.jcsuarez.com/Files/Suarez_Serrato-BFE.pdf

> In particular, we compare treatment effect estimates using a fixed effects estimator (FE) to the average treatment effect (ATE) by replicating eight influential papers from the American Economic Review published between 2004 and 2009.1 Using these examples, we consider a randomized experiment in Section 1 as a case study and, in Section 3, we show generally that heterogeneous treatment effects are common and that the FE and ATE are often different in statistically and economically significant degrees. In all but one paper, there is at least one statistically significant source of treatment effect heterogeneity. In five papers, this heterogeneity induces the ATE to be statistically different from the FE estimate at the 5% level (7 of 8 are statistically different at the 10% level). Five of these differences are economically significant, which we define as an absolute difference exceeding 10%. Based upon these results, we conclude that methods that consistently estimate the ATE offer more interpretable results than standard FE models


The use of interaction ters is delicate... 

<!-- 
On the book, I saw the section on the problems of using OLS as an estimator of FE without fully interacting the variables (4.2.4, this blog post), but didn’t really understand the issue/intuition behind the problem/solution – I would be keen to chat about what it means.. and if I then get it, would more than happily contribute a lay summary for the book. Also there are three papers by Ferraro (links below) that I think you might find interesting/offer well explained insights into difficulties in how to do empirical econ/problems within it for the Book. 

Plus there is this paper – https://www.nber.org/papers/w25636 which applies the changes-in-changes method of Athey and Imbens 2006 a method which (proponents claims) is able to get at heterogeneous treatment effects better than simple DiD by bin/something similar.
--> 



- Modeling heterogeneity: the limits of Quantile re regression

### "Null effects"

"While the classical statistical framework is not terribly clear about when one should ""accept"" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a ""zero or minimal effect"" one would want to find these closely bounded around zero under a variety of conditions for generalizability.

In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making 'null effects' informative: statistical techniques and inferential frameworks”."	Harms-lakens-18

#### Confidence intervals and Bayesian credible intervals

#### Comparing relative parameters

E.g., "the treatment had a heterogeneous effect... we see a statistically significant positive effect for women but not for men".   This doesn't cut it: we need to see a *statistical test* for the *difference* in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing).


### Multiple hypothesis testing (MHT)

See [@verkaik2016]

### Interaction terms and pitfalls

#### 'Moderators' Confusion with nonlinearity

Moderators: Heterogeneity mixed with nonlinearity/corners

In the presence of nonlinearity, e.g., diminishing returns, if outcome  'starts' at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from 'the diminishing returns kicking in'.  Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative


#### MHT

### Choice of test statistics (including nonparametric)

(Or get to this in the experimetrics section)

### How to display and write about regression results and tests

### Bayesian interpretations of results

<!--chapter:end:regression_follies/regression_follies.Rmd-->

# Robustness and diagnostics, with integrity {#robust-diag}

## (How) can diagnostic tests make sense? Where is the burden of proof?

Where a particular assumption is critical to identification and inference ...Failure to reject the violation of an assumptionis not sufficient to give us confidence that it is satisfied and the results are credible. At several points the authors cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles).


## Estimating standard errors

## Sensitivity analysis: Interactive presentation



<!--chapter:end:meta_anal_and_open_science/robust_diagnos.Rmd-->

# Control strategies and prediction, Machine Learning (Statistical Learning) approaches {#control-ml}

> 'Identification' of causal effects with a control strategy not credible	

Essentially a 'control strategy' is "control for all  or most of the reasonable determinants of the independent variable so as to make the  remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest". All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., [@oster2019unobservable].

## Machine Learning (statistical learning): Lasso, Ridge, and more

### Limitations to inference from learning approaches

## Notes Hastie: Statistical Learning with Sparsity

[google books link](https://books.google.co.uk/books?hl=en&lr=&id=f-A_CQAAQBAJ&oi=fnd&pg=PP1&ots=G4RMC-gZU-&sig=u_EoI3-ZcX0phwsZgLt2Ux7DXyc#v=onepage&q&f=true)


### Introduction

> One form of simplicity is sparsity, the central theme of this book. Loosely speaking, a sparse statistical model is one in which only a relatively small number of parameters (or predictors) play an important role.

"the $\ell_1$ norm is special" (abs value). Other norms yield nonconvex problems, hard to minimize.

> “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems.

- Examples from gene mapping

#### Book roadmap

- Chapter 2 ... lasso for linear regression, and a simple coordinate descent algorithm for its computation.

- Chapter 3 application of $\ell_1$ [lasso-type] penalties to generalized linear models such as multinomial and survival models, as well as support vector machines. [?]

- Chapter 4: Generalized penalties such as the elastic net and group lasso are discussed in Chapter 4.

- Chapter 5: numerical methods for optimization (skip for now]

- Chapter 6:  statistical inference for fitted (lasso) models, including the bootstrap, Bayesian methods and more recent stuff

- Chapter 7: Sparse matrix decomposition [?] (Skip?)
- Ch 8: sparse multivariate analysis of that (Skip?)
- Ch 9: Graphical models and their selection (Skip?)
- Ch 10: compressed sensing (Skip?)
- Ch 11: a survey of theoretical results for the lasso (Skip?)

### Ch2: Lasso for linear models

- N samples (?N observations), want to approx the response variable using a linear combination of the predoctors

***

**OLS** minimizes squared-error loss but

1. Prediction accuracy
- OLS unbiased but 'often has large variance'
- prediction accuracy can be improved by shrinking coefficients (even to zero)
    - yielding biased but perhaps better predictive estimators

2. Interpretation: too many predictors hard to interpret
- DR: I do not care about this for fitting background noise in experiments

#### 2.2 The Lasso Estimator

Lasso bounds the sum of the abs values of coefficients, an "$\ell_1" constraint.

Lasso is OLS subject to


\

$\sum_{j=1..p}{\abs(\beta_j)}\leq t$

\

"compactly" $||\beta||_1\leq t$

with notation for the "$\ell_1$ norm"


\

- Bound $t$ acts as a 'budget', must be specified by an 'external procedure' such as cross-validation

- typically we must *standardize the predictors* $\mathbf{X}** so that each column is centered with unit variance ... as well as  the outcome variables (?) ... can ignore intercept

    - DR: Not clear here whether standardisation is necessary for the procedure to be valid or just convenient for explaining and deriving its properties.

\

<div class="marginnote">
 
Aside: Can re-write Lasso minimization st constraint as a Lagrangian. $\lambda$ plays the same role as $t$ in the constraint. Thus we can speak of the solution to the Lagrangian minimisation problem $\hat{\beta)_{\lambda}$ which also solves the bound problem with $t=||\hat_{\lambda}||_1$.

</div>


<div class="marginnote">
 
Aside: We often remove the $1/2n$ term at the beginning of the minimization problem. Same minimization, minimizing sum of squared deviations rather than something like an average of this.

</div>
\

Express (Karush-Kuhn-Tucker) optimisation conditions for this ...


***

Example from Thomas (1990) on crime data

> Typically ... lasso is most useful for much larger problems, including "wide" data for which $p>>N$

\

Fig 2.1: Lasso vs ridge regression; coefficients of each for a set of considered variables plotted against their respective norms (as shares of maximal bound on coefficient sum measure, i.e., ols, for each)

- Note ridge regression penalises *squared* sums of betas
    - Fig 2.2., in $\beta_1,\beta_2$ space   illustrates the difference well: contour lines of Resid SS elipses, 'budget constraint' for each (disc vs diamond)

(Note: lasso bound was chosen via cross-validation)

- No analytical statistical inference after lasso (some being developed?), bootstrap is common

>  lasso sets two of the five coefficients to zero, and tends to shrink the coefficients of the others toward zero relative to the full least-squares estimate.

- DR: analytically and intuitively, I do not yet understand why lasso should shrink coefficients but not all the way to zero.
    - The penalty is linear in the coefficient size, so I would think the solution would be bang-bang, either drop a coeficient or leave it unchanged. But it is not.
    - Adding an increment to a $\hat{\beta}$ when it is below the OLS estimate should have a linear effect on the RSS (according to my memory and according to Sebastian).
    - But that would mean that shrinking one parameter always yields a better benefit to cost ratio. Thus I should shrink each parameter to zero before beginning to shrink any others. This cannot be right!

I looked up this derivative wrt the beta vector (one needs to set this to 0 to get the ols estimates

[stackexchange](https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression)

$\frac{d RSS}{d \beta}=-2X^{T}(y-X\beta}$

or

$−\frac{d e'e}{d b}=2X′y+2X′Xb$

The answer to this question: while the impact of changing each coefficient on SSR is in fact constant (a constant own-derivative), there is *also* an impact of changing one coefficient on the *other* derivatives. As one coefficient shrinks to zero the marginal impact of the other coefficients on the SSR may (will?) increase.

    - At the same time, we need that the effect of increasing it from zero need not be infinite, so it might not outweigh the linear penalty, thus some coefficients might be set to zero

**Relaxed lasso**

> the least-squares fit on the subset of the three predictors tends to expand the lasso estimates away from zero. The nonzero estimates from the lasso tend to be biased toward zero, so the debiasing in the right panel can often improve the prediction error of the model. This two-stage process is also known as the relaxed lasso (Meinshausen 2007).


- DR: When is this likely to help/hurt relative to pure lasso?

- [Stackexchange discussion](https://stats.stackexchange.com/questions/285501/why-do-we-use-ols-to-estimate-the-final-model-chosen-by-lars/285518#285518) Contrasts a 'relaxed-lasso' from a 'lars-ols'

***

Aside: which seems better for *Control variable selection for prediction/reducing noise to enable better inference of treatment effects*?

Ridge? better than Lasso here? We do not care about *interpreting* the predictors here... so if we allow $\beta$‘s to be shrunk towards zero for each coefficient maybe that should yield better prediction than making them exactly zero?

\

On the other hand if we know the true model is ‘parsimonious’ (as in the genes problem) it might boost efficiency to allow inference about coefficients that should be exactly zero (edited)

***

#### 2.3 Cross-Validation and Inference

Generalization ability
: accuracy  for predicting independent test data from the same population

... find the value of t that does best


**Cross-validation procedure*

1. randomly divide ... dataset into K groups.

"Typical choices ... might be 5 or 10, and sometimes N."

2. One 'test', remaining K-1 'training'

3. Apply lasso to training data for a range of t values,
    - use each fitted model to predict the responses in the test set, recording mean-squared prediction errors for each value of t.

4. Repeat the previous step K times
    - each time, one of the K groups is the test data, remaining K − 1 are training data.
    - yields K different estimates of the prediction error over a range of t values.

5. Average K estimates of prediction error for each value of t $\rightarrow$  cross-validation error curve.

Fig 2.3 plots an example with K=10 splits for cross validation

- ... of the estimated MS prediction error vs the relative bound $\tilde{t}$(summed absolute value of Lasso betas divided by summed abs value of OLS betas).
- Also draw dotted line at the 1-std-error rule choice of $\tilde{t$}
- Number of nonzero coefficients plotted at top

#### 2.4 Computation of the Lasso solution

DR: I think I will skip this for now

least angle/LARS is mentioned at the bottom as a 'homotopy method' which "produce the entire path of solutions in a sequential fashion, starting at zero"

#### 2.5 Degrees of freedom

...

Jumping to

#### 2.10 Some perspective

**Good properties of the Lasso ($\ell_1$ penalty)**

- Natural interpretation (enforce sparsity and simplicity)

- Statistical efficiency ... if the underlying true signal is sparse (but if it is not sparse "no method can do well relative to the Bayes error")

- Computational efficiency, as $\ell_1$ penalties are convex

### Chapter 3: Generalized linear models

### Chapter 4: Generalizations of the Lasso penalty

> lasso does not handle highly correlated variables very well; the coefficient paths tend to be erratic and can sometimes show wild behavior.

The elastic net makes a compromise between the ridge and the lasso penalties (Zou and Hastie 2005)1] is a parameter that can be varied.

For an individual coefficient the penalty is
$\frac{1}{2} (1-\alpha)\beta_j^2 + \alpha|\beta_j|$

(a convex combo of the lasso and ridge penalties)

multiplied by a 'regularization weight' $\lambda>0$ which plays the same role (I think) as in lasso

- elastic net is also *strictly convex*

## Notes: Mullainathan

> The fundamental insight behind these breakthroughs is as much statistical as computational. Machine intelligence became possible once researchers
stopped approaching intelligence tasks procedurally and began tackling them
empirically. Face recognition algorithms, for example, do not consist of hard-wired
rules to scan for certain pixel combinations, based on human understanding of
what constitutes a face. Instead, these algorithms use a large dataset of photos
labeled as having a face or not to estimate a function f (x) that predicts the presence y of a face from pixels x

(p2)
> supervised- machine learning, the focus of this article) revolves around the problem of prediction: produce predictions of y from x

... 

> manages to fit complex and very flexible functional forms to the data without simply overfitting; it finds functions that work well out-of-sample 

> danger in using these tools is taking an algorithm built for [predicting $y$-] and presuming their [parameters $\beta$] - have the properties we typically associate with estimation output

> One category of such applications appears when using new kinds of data for traditional questions; for example, in measuring economic activity using satellite images or in classifying industries using corporate 10-K filings.
Making sense of complex data such as images and text often involves a prediction
pre-processing step

<div class="marginnote">
This middle category is most relevant for me
</div>

> In another category of applications, the key object of interest is actually a parameter ... but the inference procedures (often implicitly) contain a prediction task. For example, the first stage of a linear instrumental variables regression is effectively prediction. The same is true when estimating heterogeneous treatment effects, testing for effects on multiple outcomes in experiments, and
flexibly controlling for observed confounders

> A final category is in direct policy applications. Deciding which teacher to hire implicitly involves a prediction task (what added value will a given teacher have?), one that is intimately tied to the causal question of the value of an additional teacher.


(p3)

**A useful (interactive?) example:**

> We consider 10,000 randomly selected owner-occupied units from the 2011 metropolitan sample of the American Housing Survey. In addition to the values of each unit, we also include 150 variables that contain information about the unit and its location, such as the number of rooms, the base area, and the census region within the United States. To compare different prediction techniques, we evaluate how well each approach predicts (log) unit value on a separate hold-out set of 41,808 units from the same sample. All details on the sample and our empirical exercise can be found in an online appendix available with this paper at http://e-jep.org

>  In-sample performance may overstate performance; this is especially true for certain machine learning algorithms like random forests that have a strong tendency to overfit. Second, on out-of-sample performance, machine learning algorithms such as random forests can do significantly better than ordinary least squares, even at moderate sample sizes and with a limited number of covariates

(p4)

> algorithms are fitted on the same, randomly drawn training sample of 10,000 units and evaluated on the 41,808 remaining held-out units.

> Simply including all pairwise interactions would be infeasible as it produces more regressors than data points (especially considering that some variables are categorical

> Machine learning searches for these interactions automatically

(p5)

> Shallow Regression Tree Predicting House Values

<div class="marginnote">
not sure what's going on here. is this the random forest thing?
</div>

> The prediction function takes the form of a tree that splits in two at every
node. At each node of the tree, the value of a single variable (say, number of bathrooms) determines whether the left (less than two bathrooms) or the right (two or
more) child node is considered next. When a terminal node-a leaf—is reached, a
prediction is returned. An


So how
does machine learning manage to do out-of-sample prediction?
The first part of the solution is regularization. In the tree case, instead of
choosing the -best” overall tree, we could choose the best tree among those of a
certain depth.

(p5)
Tree depth is an example of a regularizer. It measures the complexity of a
function. As we regularize less, we do a better job at approximating the in-sample
variation, but for the same reason, the wedge between in-sample and out-of-sample

(p6)
how do we choose the level of regularization (-tune the algorithm”)? This is the
second key insight: empirical tuning.

(p6)
-tuning within the training sample
 In
empirical tuning, we create an out-of-sample experiment inside the original sample.
We fit on one part of the data and ask which level of regularization leads to the best
performance on the other part of the data.4
 We can increase the efficiency of this
procedure through cross-validation: we randomly partition the sample into equally
sized subsamples (-folds”). The estimation process then involves successively holding
out one of the folds for evaluation while fitting the prediction function for a range
of regularization parameters on all remaining folds. Finally, we pick the parameter
with the best estimated average performance.5
 The

(p6)
-!
This procedure works because prediction quality is observable: both predictions y- and outcomes y are observed. Contrast this with parameter estimation, where
typically we must rely on assumptions about the data-generating process to ensure
consistency

(p7)
Some Machine Learning Algorithms
Function class - (and its parametrization) Regularizer R( f )
Global/parametric predictors
Linear -′x (and generalizations) Subset selection|

(p7)
-very useful table
Some Machine Learning Algorithms
Function class - (and its parametrization) Regularizer R( f )
Global/parametric predictors
Linear -′x (and generalizations) Subset selection||β|

(p7)
Random forest (linear combination of
trees

(p7)
-kernel in an ml framework!
Kernel regression

(p6)
-but can we make inferences about the structure? hypothesis testing?
Regularization combines with the observability of prediction quality to allow us to fit flexible functional forms and still find generalizable
structure.

(p7)
Picking the prediction function then involves two steps: The first step is, conditional on a level of complexity,
to pick the best in-sample loss-minimizing function.8
 The second step is to estimate
the optimal level of complexity using empirical tuning (as we saw in cross-validating
the depth of the tree).

(p8)
-but they forgot to mention that others are shrunk
linear regression in which only a small number of predictors from all possible variables are chosen to have nonzero values: the absolute-value regularizer encourages
a coefficient vector where many are exactly zero.

(p4)
-why no ridge or elastic net?
LASSO

(p8)
-ensembles usually win contests
While it may be unsurprising that such ensembles perform well on average-
after all, they can cover a wider array of functional forms-it may be more surprising
that they come on top in virtually every prediction competition

(p8)
-neural nets broadly  explained
neural nets are popular prediction algorithms for image recognition tasks. For one standard implementation in binary prediction, the underlying
function class is that of nested logistic regressions: The final prediction is a logistic
transformation of a linear combination of variables (-neurons”) that are themselves
such logistic transformations, creating a layered hierarchy of logit regressions. The
complexity of these functions is controlled by the number of layers, the number of
neurons per layer, and their connectivity (that is, how many variables from one level
enter each logistic regression on the next)

(p9)
These choices about how to represent the features will interact with the regularizer
and function class: A linear model can reproduce the log base area per room from
log base area and log room number easily, while a regression tree would require
many splits to do so.

(p9)
In a traditional estimator, replacing one set of variables by a set
of transformed variables from which it could be reconstructed would not change the
predictions, because the set of functions being chosen from has not changed. But
with regularization, including these variables can improve predictions because-at
any given level of regularization-the set of functions might change

(p9)
-!!
Economic theory and content expertise play a crucial role in guiding where
the algorithm looks for structure first. This is the sense in which -simply throw it
all in- is an unreasonable way to understand or run these machine learning algorithms

(p9)
-I need hear of using adjusted r square for this
Should out-ofsample performance be estimated using some known correction for overfitting
(such as an adjusted R2
 when it is available) or using cross-validation

(p9)
-big unknowns
available
finite-sample guidance on its implementation-such as heuristics for the number
of folds (usually five to ten) or the -one standard-error rule” for tuning the LASSO
(Hastie, Tibshirani, and Friedman 2009)-has a more ad-hoc flavor

(p9)
firewall principle:
none of the data involved in fitting the prediction function-which includes crossvalidation to tune the algorithm—is used to evaluate the prediction function that
is produced

(p10)
-how?
First, econometrics can guide
design choices, such as the number of folds or the function class

(p10)
with the fitted function. Why not also use it to learn
something about the -underlying model

(p10)
-!!
the lack of standard errors on the coefficients. Even when machine-learning predictors produce
familiar output like linear functions, forming these standard errors can be more
complicated than seems at first glance as they would have to account for the model
selection itself. In fact, Leeb and P-tscher (2006, 2008) develop conditions under
which it is impossible to obtain (uniformly) consistent estimates of the distribution
of model parameters after data-driven selection

(p11)
-lasso chosen variables are unstable because of multicollinearity. a problem for making inferences from estimated coefficients
the variables are correlated with each other (say the number of rooms of a house and
its square-footage), then such variables are substitutes in predicting house prices.
Similar predictions can be produced using very different variables. Which variables
are actually chosen depends on the specific finite sample

(p11)
this creates an Achilles-
heel: more functions mean a greater chance that two functions with very different

(p12)
coefficients can produce similar prediction quality

(p12)
In econometric terms, while the lack of standard errors illustrates the limitations to making inference after model selection, the challenge here is (uniform)
model selection consistency itself

(p12)
-is this equally a problem for non sparsity based procedures like ridge?
First, it encourages the choice
of less complex, but wrong models. Even if the best model uses interactions of
number of bathrooms with number of rooms, regularization may lead to a choice
of a simpler (but worse) model that uses only number of fireplaces. Second, it can
bring with it a cousin of omitted variable bias, where we are typically concerned with
correlations between observed variables and unobserved ones. Here, when regularization excludes some variables, even a correlation between observed variables and
other observed (but excluded) ones can create bias in the estimated coefficients

(p12)
 Some econometric results also show the converse: when there is structure,
it will be recovered at least asymptotically (for example, for prediction consistency
of LASSO-type estimators in an approximately sparse linear framework, see Belloni,
Chernozhukov, and Hansen 2011).

(p12)
-unrealistic for micro economic applications
Zhao and Yu (2006) who establish asymptotic model-selection consistency for the
LASSO. Besides assuming that the true model is -sparse”—only a few variables are
relevant-they also require the “irrepresentable condition” between observables:
loosely put, none of the irrelevant covariates can be even moderately related to the
set of relevant ones.
In practice, these assumptions are strong.

(p13)
Machine learning can
deal with unconventional data that is too high-dimensional for standard estimation
methods, including image and language information that we conventionally had
not even thought of as data we can work with, let alone include in a regression

(p13)
satellite data

(p13)
they provide us with a large x vector of image-based
data; these images are then matched (in what we hope is a representative sample)
to yield data which form the y variable. This translation of satellite images to yield
measures is a prediction problem

(p13)
 particularly relevant where reliable data on
economic outcomes are missing, such as in tracking and targeting poverty in developing countries (Blumenstock 2016

(p13)
cell-phone data to measure
wealth

(p13)
Google Street View to
measure block-level income in New York City and Boston

(p13)
online posts can be made meaningful by labeling them with machine
learning

(p14)
extract similarity of firms from their
10-K business description texts, generating new time-varying industry classifications
for these firms

(p14)
and imputing even in traditional datasets. In this vein, Feigenbaum (2015a, b) applies machine-learning
classifiers to match individuals in historical records

(p13)
-the first prediction applications
New Data

(p14)
Prediction in the Service of Estimation

(p14)
linear instrumental variables understood as a two-stage procedure

(p14)
The first stage is typically handled as an estimation step. But this is effectively a
prediction task: only the predictions x- enter the second stage; the coefficients in the
first stage are merely a means to these fitted values.
Understood this way, the finite-sample biases in instrumental variables are a
consequence of overfitting

(p14)
-ll
overfitting. Overfitting means that the in-sample fitted values x- pick
up not only the signal -′z, but also the noise δ. As a consequence, xˆ is biased towards
x, and the second-stage instrumental variable estimate -
- is thus biased towards the
ordinary least squares estimate of y on x. Since overfit will be larger when sample
size is low, the number of instruments is high, or the instruments are weak, we can
see why biases arise in these cases

(p14)
 same techniques applied here result in split-sample instrumental variables
(Angrist and Krueger 1995) and -jackknife” instrumental variables (Angrist,
Imbens, and Krueger 1999)

(p15)
-worth referencing
In particular, a set of papers has already introduced regularization into the first stage in a high-dimensional setting, including the LASSO
(Belloni, Chen, Chernozhukov, and Hansen 2012) and ridge regression (Carrasco
2012; Hansen and Kozbur 2014). More recent extensions include nonlinear functional forms, all the way to neural nets (Hartford, Leyton-Brown, and Taddy 2016

(p15)
Practically, even when there appears to be only a few instruments, the problem
is effectively high-dimensional because there are many degrees of freedom in how
instruments are actually constructed

(p15)
-a note of caution
It allows us to let the data
explicitly pick effective specifications, and thus allows us to recover more of the variation and construct stronger instruments, provided that predictions are constructed
and used in a way that preserves the exclusion restriction

(p15)
-this seems similar to my idea of regularising on a subset
Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey
(2016) take care of high-dimensional controls in treatment effect estimation by
solving two simultaneous prediction problems, one in the outcome and one in the
treatment equation

(p15)
the problem of verifying balance between treatment and control groups
(such as when there is attrition

(p15)
-!
 Or consider the seemingly different problem of
testing for effects on many outcomes. Both can be viewed as prediction problems
(Ludwig, Mullainathan, and Spiess 2017). If treatment assignment can be predicted
better than chance from pretreatment covariates, this is a sign of imbalance. If treatment assignment can be predicted from a set of outcomes, the treatment must have
had an effect

(p15)
 prediction task of mapping
unit-level attributes to individual effect estimates

(p15)
 Athey
and Imbens (2016) use sample-splitting to obtain valid (conditional) inference on

(p16)
treatment effects that are estimated using decision trees,

(p16)
-look into the implication for treatment assignment with heterogeneity
heterogenous treatment effects can be used to assign treatments;
Misra and Dub- (2016) illustrate this on the problem of price targeting, applying
Bayesian regularized methods to a large-scale experiment where prices were
randomly assigned

(p16)
-caveat
 Suppose the algorithm chooses a tree that splits on
education but not on age. Conditional on this tree, the estimated coefficients are
consistent. But that does not imply that treatment effects do not also vary by age,
as education may well covary with age; on other draws of the data, in fact, the same
procedure could have chosen a tree that split on age instead

(p16)
Prediction in Policy

(p16)
-no .. can we predict who will gain most from admission? but even if we can what can we report?
Prediction in Policy


<!--chapter:end:ml-and-mlreading-group/control_prediction_ml.Rmd-->

# IV and its many issues


## Instrument validity

- Exogeneity vs. exclusion
- Very hard to 'powerfully test'

IV not credible?	Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a *direct* effect on the outcome (only an indirect effect through the endogenous variable

## Heterogeneity and LATE

*Basic consideration: what does IV identify and when:*?

Focusing on a binary endogenous 'treatment' variable

- With heterogeneity
- With imperfect compliance
- With one-way compliance

## Weak instruments, other issues

- With a 'weak instrument' ... why does that matter?

## Reference to the use of IV in experiments/mediation



<!--chapter:end:causal_inference_general_notes/iv-issues.Rmd-->

# [Other paths to observational identification]{#other_paths}


## Fixed effects and differencing

## DiD

FE/DiD does not rule out a correlated dynamic unobservable, causing a bias

## RD

## Time-series-ish panel approaches to micro


### Lagged dependent variable and fixed effects --> 'Nickel bias'

<!--chapter:end:causal_inference_general_notes/other_paths.Rmd-->

# Causal pathways - mediators {#mediators}

## Mediators (and selection and Roy models): a review, considering two research applications

<div class="marginnote">
Originally focused on issues relevant to Parey et al project on 'returns to HE institution' using data from the Netherlands (flagged as \@NL); also relevant to Reinstein et al work on substitution in charitable giving (flagged as \@subst).
</div>

## DR initial thoughts (for NL education paper)

<div class="marginnote">
Here were my initial thoughts as pertaining to our paper on the returns to university. </div>

Suppose we observe treatment $T$ (e.g., 'allowed to enter first-choice institution and course'),

intermediate outcome $M$ (e.g., completion of degree in first-choice course and institution),

and final outcome $Y$ (e.g., lifetime income.)\


*Alternately, in the "substitution between charities" (\@subst) context... (unfold)*

```{block2,  type='fold'}

The treatment $T$ is

1. 'asked to donate in the first round' (in Reinstein, Riener and Vance-McMullen, henceforth 'RRV' experiments, and perhaps in Schmitz 2019)',

2. a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?),

3. the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others),

the intermediate outcome $M$ is the amount given to that (first-round) charity,

and the final outcome $Y$ is the amount given to that charity (or other charities) in round 2 (experiments "3": other charities in that round ).

```

The treatment $T$ (may) directly affect the final outcome $Y$.

<div class="marginnote">
Do: show a diagram here
</div>


$$T\rightarrow Y$$

\

$T$ also may affect an intermediate outcome $M$.

$$T \rightarrow M$$

The intermediate outcome also may affect the final outcome $Y$.

$$M \rightarrow Y$$

\

With exogenous variation in $T$ *and* $M$ (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions.

With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of $T$ on $Y$.

We could also compute the share of this effect that occurs *via* the intermediate effect, i.e., $T \rightarrow M\rightarrow Y$.
This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients.

\

However, there are two major challenges to this estimation.

1. We (may) have a valid instrument for (exogenous variation in) $T$ only, and $M$ may arise through a process involving selection on unobserved variables.

2. Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects.

Thus we consult the relevant literature, discussed below.

The most influential paper in Economics has been [@Heckman2013].

It is cited in more recent applied work such as Fagereng, 2018  (unfold).

```{block2,  type='fold'}

 ... We follow Heckman et al. (2013) and Heckman and Pinto (2015) in using mediation analysis. The goal of this analysis is to disentangle the average causal effect on outcomes that operate through two channels: a) Indirect effects arising from the effect of treatment on measured mediators, and b) direct effects that operate through channels other
than changes in the measured mediators (including changes in mediators that are not observed by the analyst and changes in the mapping between mediators and outcomes).

 It is therefore necessary to assume that the mediators we do not
observe are uncorrelated with both $\mathbf{X}$ and the measured mediators for all values of $D$.

```

\

Antonakis, coming from the Psychology and Leadership disciplines, considers the mediation question in a much simpler set of models.


## Econometric Mediation Analyses (Heckman and Pinto)

**Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs**

\

### Relevance to Parey et al {-}

We have an instrument for admission to one's first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these 'intermediate outcomes', including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these.

<div class="marginnote">
a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well
</div>


### Summary and key modeling

There is a 'production function'

- cf income as a function of human capital, opportunities, etc.

- cf donation as a function of income, prices, mood, framing, etc.

\

Treatments (e.g., RCTs) may affect outcomes through the following channels:

1.  observable or proxied inputs

- Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities

- Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood
\

2.  unobservable/unmeasured inputs

-  cf human capital, social connections
- cf unobservable generosity, wealth, or temporary mood

\

3.  the production function itself, the 'map between inputs and outputs for treatment group members'

- Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital 'matter more' at some institutions?

- Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else?


If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs.

> RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs \... [nor distinguish effects from changes in production functions].

\

Here "we can test some of the strong assumptions implicitly invoked".

"Direct effects" as commonly stated refer to the impact of both channels 2 and 3 above.


<div class="marginnote">
 DR: Channel 2 isn't really a direct effect imho (what was this?)
</div>

\

**Standard potential outcomes framework:**

$$Y=DY_{1}+(1-D)Y_{0}$$

$$ATE=E(Y_{1}-Y_{0})$$

**Production function**

$$Y_{d}=f_{d}(\mathbf{\mathbf{{\theta}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}$$

\... the function under treatment $d$; of proxied and unobserved inputs that occur under state $d$, and baseline variables.\

The production function implies:

$$ATE=E\Big(f_{1}(\mathbf{\mathbf{{\theta}}}_{1}^{p},\mathbf{\mathbf{{\theta}}}_{1}^{u},\mathbf{{X}})-f_{0}(\mathbf{\mathbf{{\theta}}}_{0}^{p},\mathbf{\mathbf{{\theta}}}_{0}^{u},\mathbf{{X}})\Big)$$\

We also consider counterfactual outputs, fixing treatment status and
proxied inputs:

$$Y_{d,\bar{\theta_{d}}^{p}}=f_{d}(\mathbf{\mathbf{{\bar{{\theta}}}}}_{d}^{p},\mathbf{\mathbf{{\theta}}}_{d}^{u},\mathbf{{X}}),d\in\left\{ 0,1\right\}$$\

This allows us to decompose ('as in the mediation literature'):

$$ATE(d)=IE(d)+DE(d)$$

-   *IE, Indirect effect*: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment
    statuses)

-   *DE, Direct effect*: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed
    at one of the two treatment statuses)\

    \

HP further decompose the direct effect into:

-   $DE'(d,d')$: The impact of letting the treatment vary the map only
    (fixing the rest at one of the two appropriate values)

-   $DE''(d,d')$: The impact of letting the treatment vary the
    unmeasured inputs only (fixing the rest at one of the two
    appropriate values)

They use this to give two further ways of decomposing the ATE.\

### Common assumptions and their implications

"The [standard literature]{.underline} on mediation analysis in psychology regresses outputs on mediator inputs" \... often adopts the strong assumptions of:

1.  no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy)
    and
    
<div class="marginnote">

Cf 'winning institution' impacts human capital, social networks,
    etc identically for everyone; e.g., not a greater effect for men
    then for women, nor a greater effect for those entering particular
    specializations.
    
</div>

2.  full invariance of the production function: $f_{1}=f_{0}$.

\... which implies $Y_{d}=f(\mathbf{\theta}_{d}^{p},d,\mathbf{X})$.\

[Sequential ignorability (Imai et al, 10, '11)]{.underline}:
Essentially, independent randomization of both treatment status and measured inputs.

<div class="marginnote">
Cf 'winning institution' does not effect the specialization
    entered nor the location of residence, nor are both determined by a
    third factor.
    
</div>



\


*This sentence is hard to follow:*

> In other words, input $\theta_{d'}^{p}$ is statistically independent of potential outputs when treatment is fixed at $D=d$ and measured inputs are fixed at $\bar{\theta_{d'}^{p}}$ conditional on treatment assignment $D$ and same preprogram characteristics $X$.


\

This assumption yields the 'mediation formulas':

\begin{aligned}
E(IE(d)|X)= & \int E(Y|\theta^{p}=t,D=d,X)\underbrace{\Big(dF_{(\theta^{p}|D=1,X)}(t)-dF_{(\theta^{p}|D=1,X)}(t)\Big)}_{{\text{Difference in distribution of proxy inputs}}} & (9)\\
E(DE(d)|X)= & \int\underbrace{\Big(E(Y|\theta^{p}=t,D=1,X)-E(Y|\theta^{p}=t,D=0,X)\Big)}_{\text{Dfc in expectations (unobservables, function) between treatments given proxy inputs }}expe\underbrace{{dF_{(\theta^{p}|D=1,X)}(t)}}_{\text{Distn proxy inputs for D=1}} & (10)
\end{aligned}

*(??F is presumably the distribution over the observables; where did the
unobservables go? They are in the expectations, I guess.)*\
[Difference from RCT]{.underline}

*What RCT doesn't do:*

> [sequential ignorability] translates into \... [no confounding
> effects]{.underline} on both treatments and measured inputs \... does
> not follow from a randomized assignment of treatment \...[which]
> ensures independence between treatment status and counterfactual
> inputs/outputs \... [but *not*] between proxied inputs
> $\theta_{d}^{p}$ and unmeasured inputs $\theta_{d}^{u}$. [Thus *not*
> between counterfactual outputs and measured inputs is assumed in
> condition (ii).]

Cf, randomizing 'win first-choice institution' does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution.

*What RCT* *[does]{.underline}* *do:*

RCT ensures "independence between treatment status and counterfactual
inputs/outputs", thus identifying 'treatment effects for proxied inputs
and for outputs.

CF, we can identify the impact of the treatment 'win first chosen
institution' on proxied input like 'enters a specialization' and on
outputs like 'income in observed years.'

\

## Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity

### Summary {-}

-   \... 4000+ families targeted, incentive to relocate from projects to
    better neighbourhoods.

-   Easy to identify impact of vouchers

-   Challenge (here) is to assess impact of *neighborhoods* on outcomes.

-   Method here to decompose the TEOT into unambiguously interpreted
    effects. Method applicable to 'unordered choice models with
    categorical instrumental variables and multiple treatments'

-   Finds significant causal effect on labour market outcomes

### Relevance to Parey et al {-}

1.  We also have an instrument (DUO lottery numbers) cleanly identifying
    the effect of the 'opportunity to do something' (in our case, to
    enter the course at your preferred institution). However, we also
    want to measure the impact of choices 'encouraged' by the
    instrument, such as (i) attending the first choice course and
    institution and (ii) completing this course. We also deal with
    unordered choices (i. enter course and institution, enter course at
    other institution, enter other course at institution, enter neither)
    (ii. choice of medical specialisation)

2.  The geographic outcome is relevant to our second paper (impact on
    'lives close to home')

### Introduction  {-}

The causal link between neighborhood characteristics and resident's
outcomes has seldom been assessed.

**Treatments:**

-   Control (no voucher)

-   Experimental: could use voucher to lease in low-poverty neighborhood

-   Section 8: Could use voucher in any () neighborhood

*Many papers evaluate the ITT or TOT effects of MTO.*

-   ITT: effect of being *offered* voucher
    -   estimated as difference in average outcome of experimental vs control families

-   TOT: effect for 'voucher compliers' (assuming no effect of simply
    being *offered* voucher on those who don't use it)

    -   estimated as ITT/compliance rate

> [ITT and TOT] are the most useful parameters to investigate the
> effects of *offering* [EA] rent subsidising vouchers to families.

### Identification strategy brief {-}

-   Vouchers as IVs for choice among 3 neighborhood alternatives (no
    relocation, relocate bad, relocate good)

<div class="marginnote">
Cf \@NL: enter course and
    fp-institution, enter course at other institution, do not enter
    course
</div>

-   Neighborhood causal effects as difference in counterfactual outcomes
    among 3 categories

-   Challenge: "MTO vouchers are insufficient to identify the expected
    outcomes for all possible counterfactual relocation decisions"

    -   \... "compliance with the terms of the program was highly
        selective [Clampet-Lundquist and M, 08]"

-   Solution: Uses theory and 'tools of causal inference. Invokes SARP
    to identify 'set of counterfactual relocation choices that are
    economically justifiable'

-   [Identifying assumption]{.underline}: "the overall quality of the
    neighborhood is not directly caused by the unobserved family
    variables even though neighborhood quality correlates with these
    unobserved family variables due to network sorting"

-   'Partition sample \... into unobserved subsets associated with
    economically justified counterfactual relocation choices and
    estimate the causal effect of neighborhood relocation conditioned on
    these partition sets.' [*what does this mean?*]

### Results in brief {-}

"Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes \... 65% higher than the TOT effect for adult earnings."

### Framework: first for binary/binary (simplification) {-}

**First, for binary outcomes (simplified)**

$Z_{\omega}$: whether family $\omega$ receives a voucher *(cf
institution-winning lottery number)*

$T_{\omega}$: whether family $\omega$ relocates (*cf enters first choice
institution and course)*\

**Counterfactuals**

-   $T_{\omega}(z)$: relocation decision $\omega$ would choose if it had
    been assigned voucher $z\in{0,1}$': vector of potential relocation
    decisions (*cf education choices)* for each voucher assignment (*cf
    lottery number)*

    -   Can partition into never-takers, compliers, always takers, and
        defiers

-   $(Y_{\omega}(0);Y_{\omega}(1$)): (Potential counterfactual) outcomes
    (*cf income, residence, etc*) when relocation decision is fixed at 0
    and 1, respectively

**Key ( standard) identification assumption: instrument independent of
counterfactual variables**

$$(Y_{\omega}(0),Y_{\omega}(1),T_{\omega}(0),T_{\omega}(1))\perp Z_{\omega}$$

**Standard result 1: ITT**

$$\begin{aligned}
ITT=E(Y_{\omega}|Z_{\omega}=1)-(Y_{\omega}|Z_{\omega}=0)\\
=E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]')P(S_{\omega}=[0,1])+E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[1,0]')P(S_{\omega}=[0,1])\end{aligned}$$

i.e., ITT computation yields the sum of the 'causal effect for
compliers' and the 'causal effect for defiers, weighted by the
probability of each.

**Standard result 2: LATE**

$$\begin{aligned}
LATE=\frac{{ITT}}{P(T_{\omega}=1|Z_{\omega}=1)-P(T_{\omega}=1|Z_{\omega}=0)}= &  & E(Y_{\omega}(1)-Y_{\omega}(0)|S_{\omega}=[0,1]')\\
if\:P(S_{\omega}=[0,1])=0\end{aligned}$$

i.e., the LATE, computed as the ITT divided by the 'first stage' impact
of the instrument, is the causal effect for compliers if there are no
defiers.

### Framework for MTO multiple treatment groups, multiple choices {-}

-   $Z_{\omega}\in\{z_{1,}z_{2,}z_{3}\}$ for no voucher, experimental
    voucher, and section 8 voucher, respectively

-   $T_{\omega}\in\{1,2,3\}$ \... no relocation, low poverty
    neighborhood relocation, high poverty relocation

-   $T_{\omega}(z)$: relocation decision for family $\omega$ if assigned
    voucher $z$

$\rightarrow$ Response type for each family $\omega$ is  a three-dimensional vector:

$$S_{\omega}=[T_{\omega}(z_{1}),T_{\omega}(z_{2}),T_{\omega}(z_{3})]$$.

$\rightarrow$

**ITT** computation now measures a weighted sum of effects across a subset of those response types whose responses vary between the assignments being compared.\
\

*Cf:*

-   Considering the 'treatments': '1: enter other course at fp-inst, '2:
    enter course at fp-inst', '3: enter course at non-fp inst'

    -   (I ignore other course at other institution for now)

-   Looking among those who won the course lottery (so we have a binary
    instrument: wininst $Z_{\omega}\in{0,1\}}$

-   Our reduced-form estimates (regressions on the 'lottery number wins
    institution' dummy) measures the probablility-weighted sum of:

    -   impact of institution within course ($T_{\omega}=$2 versus 3);
        for those who would 'fully comply' (enter course at institution
        if $Z_{\omega}=1$, enter course at other institution if 0)

    -   impact of the course at fp-institution versus second-best course
        at fp-institution for 'institution-loving' noncompliers; those
        who would enter the course *only* if they get the fp-institution
        and otherwise another course at the same institution

    -   effects for perverse defiers


## Antonakis approaches

Insert notes here

<!--chapter:end:mediation/mediators_lit_pinto_etc.Rmd-->

# Causal pathways: selection, corners, hurdles, and 'conditional on' estimates {#selection_cop}

## 'Corner solution' or hurdle variables and 'Conditional on Positive'

"Conditional on positive"/"intensive margin" analysis ignores selection

"Conditional on positive"/"intensive margin" analysis ignores selection *identification issue* 	See Angrist and Pischke on "Good CoP, bad CoP".  See also bounding approaches such as [@Lee2018]	AngristJ.D.2008a,

\

## Bounding approaches (Lee, Manski, etc)

See [Notes on Lee bounds](#notes_lee)



### Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD

Notes David Reinstein

##### Introduction

> even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research

- Intuitive trimming procedure for bounding average treatment effects in the presence of sample selection…

- Requires neither exclusion restrictions nor a bounded support for the outcome of interest."

- (Also) applicable to "nonrandom sample selection/attrition", as well as to the 'conditional on positive'/hurdle/mediation effect discussed here

> analyses and evaluations typically focus on "reduced form impacts on total earnings, a first-order issue for cost-benefit analysis. Unfortunately, exclusively studying the effect on total earnings leaves open the question of whether any earnings gains are achieved through raising individuals hypothesis wage rates (price affects or hours of work (quantity effects).

*Important methodological point to constantly bring up:* "even a randomized experiment cannot guarantee the treatment and control individuals will be comparable conditional on being employed."

Claims that standard "parametric or semi-parametric methods for correcting sample selection require exclusion restrictions that have little justification in this case." Notes that most of the baseline variables could affect employment probabilities or have a direct impact on wage rates.

\

*Summary of the method*: "...amounts to first identifying the excess number of individuals who were induced to be selected (employed) because of the treatment and then trimming the upper and lower tails of the outcome... distribution by this number, yielding a worst-case scenario bound."

Uses same assumptions as in "conventional models for sample selection"

1. regressor of interest is independent of the errors in the outcome and selection models selection equations – this is ensured by random assignment.

2. "the selection equation can be written as a standard latent variable binary response model"

– what meaningful restriction does this impose?

He proves this procedure "yields the tightest bounds for the average treatment effect that are consistent with the observed data."

```{block2,  type='note'}

The bounds estimator is shown to be $\sqrt(n)$ consistent and asymptotically normal with an intuitive expression for its asymptotic variance which depends on the variance of the trimmed outcome and the trimming threshold, an estimated quantifiable; (and an added term accounting for the estimation of which quantile to trim on)

```  


Note for charity experiment (unfold) (\@subst)

```{block2,  type='fold'}

– *DR, Note, charity data: We can make confidence statements over the bounds themselves. Will this procedure be easy to bring into our code?*
    – In our (charity) experiment we in fact do have upper bounds on the outcome variable. Could this yield even greater efficiency?

```

Note for the Netherlands data: (unfold, \@NL)

```{block2,  type='fold'}

it is not immediately clear how this could be adapted to instrumental variables; we shall see. Can we recover something meaningful from the reduced form model they are? Can it be applied to the (instrumental variables) estimates to disentangle the impact of changing courses from the impact of the institution itself?

```

In Lee's paper, the estimate seems to give very narrow and informative bounds even though they have a great many people who do not earn any wages as a share of the population, about 54\%. These are much narrower than the bounds proposed by Horowitz and M then what those bounds produce.

<!- ask @Gerhard whether his Horowitz/Manski estimator incorporated the natural bounds on the outcome. -->


##### The National Job  Corps Study and Sample Selection [prior approaches]

> In the experiment discussed here those in the control  group were embargoed from the program for three years but could join afterwards, thus "when I use the phrase 'effect of the program' I am referring to this reduced-form treatment effect", i.e., the intent to treat effect.

– "some subpopulations were randomized into the program group with differing, but known probabilities. Thus analyzing the data requires the use of design weights."
 
<div class="marginnote">
*Note:* (\\@NL) this bears some resemblance to our Dutch data situation, and we can probably use examples from analyses of these programs. We can check their code against ours.
</div>

– Note also that they impute means of the baseline variables with their means; this seems to be an accepted practice.

<div class="marginnote">
Lee notes that he focuses exclusively on the "sample selection on wages caused by employment" and not the attrition/nonresponse problem, to focus attention on this, but they could have used it for the other as well.
</div>
 

– *DR:* (\@NL) Note again that their desire to separate the employment hours and wage effects of the program is very similar to our desire to separate out different margins of the impact of winning an institution.  ...Namely the impact on completing a course or starting a course versus other impacts and the impact of entering a specialization versus remaining impacts. ...Similar decompositions for the geography outcomes.

    – To do: check whether any papers cite Lee using an IV approach, extending the technique and the estimation of variance.

***

> "the problem of nonrandom sample selection is well-understood in the training literature; ... may be one of the reasons why most evaluations of job-training programs focus on total earnings, including zeros for those without a job, rather than on wages conditional on employment" "of the 24 studies referenced in a survey ... (Heckman et al.)... Most examine annual, quarterly, or monthly earnings without discussing the sample selection problem examining rage rates."

– *DR:* (\@NL)Note that this is relevant to our question of whether to exclude zeros in log models, etc. While there will be less unemployment in our data, it still may be a relevant influence made have a strong effect on the estimates.

***

**...previous conventional approaches to the sample selection problem (skip if desired).** One may explicitly model the process determining selection, such as in Heckman (1979) ...

Separate equations for the wage and the propensity to be employed, where employment occurs if the latter crosses a particular threshold, in which case a wage is observed. It is reasonable to think that the treatment variable can have effects on both terms..

"sample selection bias can be seen as specification error in the conditional expectation..."

The expected wage conditional on treatment exogenous variables and the selection into working (that is the underlying propensity to work variable exceeding zero) his status is equal to the true effect of the treatment an adjustment for the differences in the observable's exogenous variables and a bias term representing the expectation of the idiosyncratic unobservables given the treatment and the exogenous variables exceeding the value necessary to induce work participation. The unobservable term needs to exceed the prediction based on the observable term for the entire term to exceed zero inducing labor force participation.

One may assume the data are missing at random, perhaps conditional on a set of covariates (Rubin, 1976;  essentially assuming the error terms in each equation are independent of one another, here "employment status is unrelated to the determination of wages"… This "is strictly inconsistent with standard models of labor supply that account for the participation decision (Heckman, 1974)."

A more common assumption is that some exogenous variables "determine sample selection but do not have their own direct impact on the outcome of interest.... Exclusion restrictions are used in parametric and semi-parametric models..."

but "there may not exist credible 'instruments... excluded from the outcome equation"


***

– *DR, aside:*  We can return to (our) previous papers to impose these Lee bounds!  One example would be the Siskel and Ebert your reviews paper and perhaps incorporating us with subsequent approaches, considering the "selection to review" equation.

***


**Second approach  "the construction of worst-case scenario bounds of the treatment effect"**

"Impute missing data with either the largest or smallest possible values to compute the largest and smallest possible treatment effects consistent with the data" as in Horwitz and Manski (2000a) who provide a general framework for this.

- Particularly useful with binary outcomes.

This cannot be used when the support is unbounded. ... note in their replication example they are actually using the equivalent of the bottom 5th percentile and the top 95th percentile. Strictly using a procedure would provide even wider bounds.

Lee considers his approach to be a hybrid of the two previous general approaches.

...end of section 2.. .a statement of the Horwitz upper bound for the treatment effect; very intuitive: "what if everyone in the treatment who dropped out would have had the largest possible wage and everyone in the control group that drop out would've had the smallest possible wage; this will give the upper bound." Switching this the other  way  around will give a lower bound.


***

*DR, an aside thought:* (\@NL) Something akin to the Horwitz and M approach (or maybe Lee bounds) could be applied to our issue of swapping into institutions directly. Suppose we only focus on those who *actually* complied: those assigned to an institution who also went to that institution. Our concern was that this would under-represent those who had particularly strong institutional preferences. Suppose you are interested in looking at the impact of winning the lottery (for once preferred institution) itself, as that was our most simply identifiable outcome.

... Let's consider evaluating a treatment effect for those who happened to swap in. Those who swapped in might be assigned a counter-factual outcome of the lowest value of the lifetime income among those who did not get their institution of choice. Similarly, the small group who swapped out might be assigned a counterfactual outcome (had they no swapped out) representing the highest outcome value for those who did get their institution of choice. This should give us an upper bound on the treatment effect for these two groups of what we might call non-compliers. Making the opposite assumptions, precisely that those who swapped *into* their institution of choice would've had a very good counterfactual outcome (if they had not got their institution of choice) that comes from the highest outcomes for those who didn't get their institution of choice (and also reversing this for those who swapped out of their preferred institution) would give us a lower bound for the treatment effect for this group. We can then combine these bounded treatment effects for these non-compliers with the treatment effect for the compliers to get a measure of the average treatment effect with bounds for this sort of behavior.

This will also allow us to come up with estimates with bounds *without* having to use the instrumental variable strategy which has issues of its own.

##### Section 3: identification of bounds on treatment effects; the main meat of the model

He starts with a simple example. He begins with a model with a treatment indicator and no other covariates, and a continuous outcome variable, but notes that this will clearly apply to discrete outcome variables and will also apply conditional on controls.

Nest, he brings forward the statement... from the earlier selection models. In each case the latent variable must overcome a hurdle for the outcome to be observed and in fact *the hurdle differs depending on the impact of the treatment itself*.  In general *when the errors in the selection and outcome equations are correlated the difference in these means differs from the actual treatment effect*. In other words through a slightly complicated story, when those who have unobservables that make them more likely to work also tend to have unobservables that would make them likely to earn more the standard difference in outcomes between control and treatment will *not* describe the true treatment effect.

<br> \bigskip

*A key insight* seems to be that we could identify the treatment effect if we could estimate the expected outcome given treatment *and* given that the unobservable component in the selection equation would lead to an observable outcome had the person *not* been given treatment. If so, we could subtract the observed mean control outcome from the above to yield the true treatment effect (for those who would be observed always). However, we obviously do not observe this because we only observe the outcomes for those who are treated where the selection equation *is* in fact positive and not "where the selection equation *would have* been positive had they not been treated."

However, the insight here is that this term can in fact be bounded. We *do* observe these outcomes for the treated people (note we are assuming without loss of generality that the treatment raises the probability of selection for this discussion) but we don't know exactly which ones they are. In other words, we observe outcomes for more people in the treatment group than we need; we wish we could figure out what is the subset of these that would have *also* been observed had they not been treated, so we could compare like-to-like. The observed treatment mean is a weighted average of the thing we are seeking (to difference from the control) and "the mean for a subpopulation of marginal individuals... that are induced to be selected into the sample *because* of the treatment"

This then gets us the upper bound for the term expressing the treatment outcome for those who would have been observed even if they had been in the control. The upper bound for this is the expected outcome for those in the treated group (who are observed of course) and who are in quantile-p or above of the outcome, where this $p$ is the share of the treated population that are in the marginal group we referred to that were only induced to be selected into the sample because of the treatment.

In other words the worst case scenario is that the smallest share $p$ values of $Y$ are in the marginal group and the largest one (which is share 1-pone are in the inframarginal group. We don't know which observations are inframarginal and which ones are marginal.

$p$:  the share of marginal individuals and (1-p) the share of inframarginal individuals (the latter is group we want the average outcome for). The highest could be would be the average outcomefor the largest (1-p) share of this group.  We are looking for the expectation given that they are at or  above at will at or above percentile p within this group.

In other words we trim the lower tail of the Y distribution by the portion $p$, (so what remains is the 1-p share) to get the upper bound for the inframarginal groups mean. We can then subtract the mean for the control group to get an upper  bound for the treatment effect.

To compute this "trimming proportion  p": this p is equal to the share of the treated group  whose outcome is observed minus the share of the control group whose outcome is deserved is observed, divided by the share of the treatment group where the outcome is observed.
Something like the *increased likelihood of observation that is driven by the treatment, as a share of the total number as a share of the probability of observation in the treatment group*.

The average observed outcome for the treatment group is including too many observations; we need to difference out the share of observations that are observed only because the treatment caused them to be observed; this share is certainly no larger than the increased probability of observation in  the treatment group as a share of the probability of observations the treatment group.

Another much simpler way of saying this is "trimming the data by the known proportion of excess individuals" in the treatment group. (To gain bounds on the mean for the inframarginal group which we can then difference from the control-group mean get the treatment effect).

Perhaps some intuition for why this improves on the Horwitz model: we don't need to assume that those observed in the treatment group that wouldn't have been observed in the control would've had the highest possible outcomes. No, we only need to assume (to get the upper bound) that these came from the highest *distribution*  because they had to come from somewhere. These were the people in the upper tail of the relevant group but they couldn't *all* have been the individual highest achiever.

***

The model is extended to heterogeneity and heteroscedasticity. This begins with the independence of treatment assignment the "potential sample selection indicators" for either treatment or control, in other words whether that individual will have an observed outcome under treatment and whether that the individual would have an observed outcome under control, and the latent potential outcomes.

Experimental or random assignment ensures that each of the potential outcomes (and the correspondence to observability under each treatment) is independent of the actual assigned treatment.

The second assumption is monotonicity: treatment assignment can only affect sample selection in one direction.

– DR: For our (substitution) experiments, it is in fact not clear to me whether this should necessarily be the case, as some (less generous?) people may be induced to leave because of having been asked to donate, while potentially other (more generous people) might be induced to return given that they were asked to donate. (This proposed nonmonotonicity implies that the 'asked twice' sample tends to weed out the less generous, which would lead to a bias *against* substitution, strengthening the case for our result.)
    - DR, aside: However, even though the paper doesn't say it, I suspect this assumption could be weakened and you would still get some similar bounds.
To put it another way, I would imagine that these bounds could be adjusted based on some reasonable ad hoc assumptions about the share of the population who is affected in either direction.

– \@NL: I'm coming to think that our Dutch data problems are more things involving "hurdle models". Can this technique also be applied to such hurdle models?

Next proposition 1a states that given these assumptions we can derive sharp lower and upper bounds for the average treatment effect (conditional on 'would be observed in both states'). Note that for this estimator if the probability of observation is greater under the treatment we need to trim the treatment groups outcome distribution and if the probability of observation is greater under the control we need to trim the control group's outcome distribution.

- DR, aside comment: we seem to be throwing out a bit of the data in these estimates, which would suggest that something more efficient could be generated.

(The stated bounds you can estimate are exactly the same as the bounds from the previous specification, at least as I had interpreted the way they would be produced.)

Their remark 2 notes that an implication is that as $P_0$, that as the "difference between the relative probability of observation of an outcome under treatment versus control" tends to zero,
i.e., as the probability of having an observed outcome (or the conditional probability of this) is the same for treatment and control) then there is no sample selection bias.

Their estimate convergences to the estimate he calls an estimate for the "always takers subpopulation... except that taking... is selection into the [outcome-observed] sample."

*So, a very vanilla estimator is acceptable if we find the same conditional probability of selection for each group, under monotonicity, which, for this case, we can test (see Remark 4 below).*

– (DR: To me this suggests that there might be something wrong going on here. Intuitively, If I simply observe the same rate of attrition in the treatment and control groups this *shouldn't* be enough to tell me that attrition did not matter, as it could occur differentially for both groups, but it seems to be a result here; this is probably due to the assumption of monotonicity of the selection/observation term, as well as  the random/exogenous assignment to each group.)

***

Remark 3 discusses the importance of monotonicity for the bounds, saying this assumption is "minimally sufficient" (I think it would be better to say minimally sufficient for these particular bounds that he computed). To demonstrate this he gives an extreme example. Without monotonicity it could be (note: this would seem like a very unlikely outcome!) that every observation in the control group comes from the population in the treatment group that would *not* have been observed had they been treated and every observation in the treatment group happens to come from the set of people that would *not* have been observed had they been in the control group. These two "subpopulations do not overlap, so the difference in the means could not be interpreted as a causal effect."

– DR, aside : there must be some way to impose some restrictions on this even allowing for this non-monotonicity. (He notes that this can be improved upon somewhat by thinking about the total the idea that the total masses of  unobserved that would've been observed in the other group can't be greater than the share that is not observed in the other treatment group, but this doesn't seem like a particularly fruitful route as it in most reasonable cases will still allow for very wide bounds.)

***

Remark 4 suggests that if we can assume (or somehow observe?) that the conditional probabilities of selection are the same for treatment and control, we can *test whether monotonicity in fact holds* and the simple difference in means will be an appropriate estimate of the treatment effect. Here, the assumption implies that everyone in the treatment or control group would have been observed under the opposite treatment as well. This in fact implies that the distribution of the exogenous variables should be the same in the treatment and control groups conditional on being selected. This seems fairly intuitive, we look at whether selection seems to be occurring in different ways are on different margins for the two groups treatment versus control.

Apparently for this test to have *power* we need that the subpopulations of "noncompliant errors in opposite directions" (quotation mine) must have *distinct* distributions of baselines exogenous characteristics. If these were the same then whether or not monotonicity holds the test doesn't tell us anything.

– DR: *I wonder if anyone uses this test for  Monotonicity under non-differential selection?*

Another relevant note that he bundles in this remark is that the technique here only yields estimates *for those who would be with an observed outcome for either treatment or control.* One could *additionally* try to bound this as an estimate for the entire population using the Horwitz and Manski bounds for this latter thing. However, in many contexts there are reasons that the bounded estimates they mainly use are the relevant ones, such as "the impact of the program on wage rates for those whose employment status was not affected by the program."

- DR: In our substitution experiment case, the substitution patterns for those for whom attrition was not affected by the first-round-charity treatment
- \@NL: E.g., the impact of an institution on income for those whose choice to remain in the course was not affected by their institutional assignment

***

"Narrowing bounds using covariates"

All of the above could be done conditional on a particular set of baseline characteristics such as gender or race. The average treatment effect could be estimated separately for each. (Note: and perhaps combined in a fruitful way?)


One can alternately use covariates to reduce the width of these bounds. To give intuition, we can imagine a baseline covariate that perfectly predicts an individual's wage. Because treatments are randomly assigned the maintained assumptions will still hold conditionally on this X. The results  the methods can be applied separately for each value of this covariate, and for each such value the trimming procedure will actually have no impact on the estimate.

DR: I think this is the "estimate and sum things up in a weighted way" procedure I thought about a moment ago.

<br> \bigskip

Proposition 1B gives the balance estimator for a model involving exogenous variables. Essentially, this computes the corresponding bounds estimator at each X, where the differential selection probability is computed for that particular X, the upper quantile value of the outcome is given conditional on the same X and on being in the treatment group. These are then integrated (or summed up) weighted by the distribution or the cdf of this  covariate in the control group. These bounds will necessarily be sharper than the balance without controls.

##### Section 4: estimation and inference

The asymptotic variance depends on components reflecting the variance of the trimmed distribution, the variance of the estimated trimming threshold, and the variance in the estimate of "how much of the distribution to trim" (the relative selection probability differential).

Equation 6 formally defines the estimator

Estimated bounds consistent for 'true bounds' under standard conditions

Two ways to compute CI's -- CI's for the 'true bounds' or CI's for the TE itself. A 95\%  CI for the former will contain the latter with even greater probability.

Imbens and Manski '04 can be used to derive the latter which are 'more apppropriate here' since the object of interest is the TA and not the 'region of all rationalizable treatment effects.
These are built off of a transformation of the estimate UB and LB and max estimated sd of each of these.

- the latter are reported by the 'cie' option in 'leebounds'

Generalisation to monotonicity (without  knowing direction of impact of treatment on selection)...

> As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative... [do similar]

> though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small and p0 is “close” to zero
>...  A simple, conservative approach to combining the trimmed and untrimmed intervals is to compute their union


##### Section 5: Empirical Results

Table 4 gives a step-by-step that is a good way of seeing and understanding the construction of the estimator, and where the 'action' is, in treimming, in components of the SE, etc.

Intervals are 1/14 the width of  the equivalent Horowitz/Manski bounds

#######   5.2 using covariates to narrow bounds

> Any baseline covariate will do, as will any function of all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to be a proxy for the predicted wage potential for each individual—is constructed from a linear combination of all observed baseline characteristics. This single covariate is then discretized, so that effectively five groups are formed according to whether the predicted wage is within intervals defined by $6·75, $7, $7·50, and $8·50.

- @Substitution: this is essentially what I propose we do, but using Ridge Regressions or something similar

> To compute the bounds for the overall average...the group-specific bounds must be averaged, weighted by the proportion (sPr Group J|S0=1,S1=1)

> The estimated asymptotic variance for these overall averages is the sum of (1) a weighted average of the group-specific variances and (2) the (weighted-) mean squared deviation of the group-specific estimates from the overall mean. This second term takes into account the sampling variability of the weights

$\rightarrow$ result: 11\% narrower bounds

<br> \bigskip

*Interesting; possibly do similar for \@NL-ed*:

>By statistically ruling out any effect more negative than −0·037, this suggests that after 4 years, the Job Corps enabled program group members to offset at least 35\% (and perhaps more) of the potential 0·058 loss in wages due to lost labour market experience that could have been caused by the program


#### Section 6: Conclusions: implications and applications

Interesting intuitive argument:

>Another reason to interpret the evidence as pointing to positive wage effects is that the lower bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly negatively correlated with the propensity to be employed. From a purely theoretical standpoint, a simple labour supply model suggests that, all other things equal, those on the margin of being employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be the smallest wage that draws the individual into the labour force). In addition, the empirical evidence in Table 2 suggests that there is positive selection into employment: those who are predicted to have higher wages are more likely to be employed (i.e., U and V are positively correlated). If this is true, it seems relatively more plausible to trim the lower rather than the upper tail of the distribution to get an estimate of the treatment effect.

<!--chapter:end:sample_selection/selection_models_lee.Rmd-->

# (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters  {#why_experiment_design}

## Why run an experiment or study?

- Sugden and Sitzia critique here, give more motivation

## Causal channels and identification

- Ruling out alternative hypotheses, etc

## Types of experiments, 'demand effects' and more artifacts of artifical setups

## Generalizability (and heterogeneity)


<!--chapter:end:experiments_and_study_design/why_experiment_design.Rmd-->

# (Experimental) Study design: Background and quantitative issues {#quant_design_power}

## Pre-registration and Pre-analysis plans {#pre-reg-pap}


### The benefits and costs of pre-registration:  a typical discussion 


> BB: That said, I would be interested to think about the benefits – and more importantly limitations to – pre-registration. I think it could solve some of the p-hacking problems but not much else. How to not relegate exploratory analyses too far is also unclear to me.


> DR: I'm much more on the 'pro' side pre-registration and PaPs. It also helps deal with publication bias and file drawers. And p-hacking is a huge issue IMHO.  But it is also good to have some consideration of the pros and cons, so this would be great. 


> BB: RE pre-reg: yes I think it is enough that it prevents p-hacking (there could be very little cost associated with pre-reg) but I fear that it could prevent other advancements if it relegates exploratory analyses too far.


> DR: I don't think it should be binary. Systems need to be worked out for *adjustments* to the meaning of reported estimates depending on whether they were or were not preregistered, and how many were preregistered. While reported significance levels could be adjusted in the frequentist framework, this will all presumably based on  measures of the likelihood that such a result would have been estimated/reported.  Thus I think this could most easily  be incorporated into a Bayesian framework but I'm not saying it would be easy. Still, they have done some good work on adjustments for 'sequential designs'.

\

> BB: I think that it could also stifle students a bit – it may reduce further the number of students who have access to funding that allows for experiments that will be able to be published if all experiments have to be high-powered. 

\

> DR:   Statistical power is an important issue.  I was skeptical at first about the 'dangers of underpowered studies' but I'm coming around a bit.

<div class="marginnote">
My thinking was that 'we can simply make downward adjustments to the estimates reported in underpowered studies'. </div>
 

> Anyways, we don't want to put the cart before the horse: as Gelman  said at a conference we should be supporting science not the careers of scientists.   I tend to think there are strong arguments for more centralization in social science. 

> And my impression is that we actually have too many different studies and distinct research programs being run, and too many papers being published and not carefully brought together into a framework. Going through the studies on the https://www.replicationmarkets.com/ reinforces this impression for me.

> Still, I think there are ways around this to enable early career people.  'Underpowered'  experiments could be registered as part of a longer/sequential research program,  perhaps collaborative and enabling meta-analysis.

\

> BB:  I also don’t think it gets at publication bias very much unless pre-reg’ed studies are followed up on. Only then do you know why the study didn’t come out – and quite a lot of the time I think it will be attrition/inability to gather the necessary data. Someone could launch that journal though – the Journal of Failed Studies – to have a place for a record that they have been run and what happened to be kept. So I am pro pre-reg, I just think the system needs a bit of work.

\

> DR:  If preregistration is made public and well-organize, then the 'failed' exercises willtbe integrated into future meta-analyses; so that's at least a partial solution here.

> Agreed, we need to build better systems for incentivising pre-registration and careful data sharing. We need  to give career credit to people for  planning designing and reporting  credible  experiments and projects, even if they 'fail'. Part  this is publishing/rewarding tight null results,  which actually do add a lot of value. 

> We might also consider offering some reward careerwise to experiments that fail -- in terms of being deeply inconclusive-- for some arbitrary or random reason even though they were well-planned and executed.  But I think it is hard to get the incentives right for the latter.

```

### The hazards of specification-searching

## Sequential and adaptive designs

Needs to adjust significance tests for augmenting data/sequential analysis/peeking	Statistics/econometrics	new-statistics		sagarin_2014			http://www.paugmented.com/	resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/

Yet ...


$P_{augmented}$ may *overstate* type-1 error rate	Statistics/econometrics	response to referees, new-statistics	"

A process involving stopping "whenever the nominal $p < 0.05$" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5\%. Even if the subsequent data suggested a "one in a million chance of arising under the null" the overall process yields a 5\%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the "likelihood of the null hypothesis" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate.

Considering the calculations in \ref{sagarin2014}, it is clear that $p_{augmented}$ should \textit{overstate} the type-1 error of the process if there is a positive probability that after an initial experiment attains p$<0.05$, more data is collected.  A headline $p<0.05$ does \textit{not} imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded $p>0.05$, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though $p<0.05$. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine's experiment).  Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p<0.05, more data is collected, implying that $p_{augmented}$ as calculated above overstates the type I error rate that would arise  from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below $p=0.05$, in order to attain a type-1 error rate of 5% or less in our published corpus."

## Efficient assignment of treatments

(Links back to power analyses)


### How many treatment arms can you 'afford'?


<!--
A guiding principle might be:
"Will we have statistical power to identify a small true effect from this pairing? If not, we drop the pairing."

A caveat to this is that we may be able to pool some of the pairings to answer certain questions, but then it is only worth having the distinct variations that are being pooled if that doing so gives us power to answer some other question.
-->

<!--chapter:end:experiments_and_study_design/quant_design_power.Rmd-->

# (Experimental) Study design: (Ex-ante) Power calculations {#power}

## What sort of 'power calculations' make sense, and what is the point?

### The 'harm to science' from running underpowered studies

> "One worries about underpowered tests. Your result (may have)  relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results.

> Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects."	verkaik2016, metzger2015


\

On magnitude error due to underpowered studies:

https://www.pauljferraro.com/publications/2020/2/1/is-there-a-replicability-crisis-on-the-horizon-for-environmental-and-resource-economics




## Power calculations without real data

## Power calculations using prior data

Adapt example in 'scopingwork.Rmd' to this



<!--chapter:end:experiments_and_study_design/power_calc.Rmd-->

# 'Experimetrics' and measurement of treatment effects from RCTs {#experimetrics_te}

## Which error structure? Random effects?

## Randomization inference?

## Parametric and nonparametric tests of simple hypotheses

## Adjustments for exogenous (but non-random) treatment assignment

## IV in an experimental context to get at 'mediators'?

## Heterogeneity in an experimental context


<!--chapter:end:experiments_and_study_design/experimetrics_te.Rmd-->

# Making inferences from previous work; Meta-analysis, combining studies {#metaanalysis}

My opinion on why this is so important (unfold):

```{block2,  type='fold'}

it is lame how often I see 'new experiments' and 'new studies' that tread most of the ground as old studies, spend lots of money, get a publication and ... ignore or pay lip service to the previous findings. There is tons of data out there that can inform new questions and bring better through re-analysis and combination with other data. Otherwise we are not actually building progress. This is why I became involved with a project I called 'ExpArchive', later working with projects such as GESIS' X-Econ to try to encourage and facilitate data sharing in experimental economics, as well as  the innovationsinfundraising.org project, which is now collaborating with the Lily Institute's "revolutionizing philanthropy research" (RPR) project.

```


## Notes: Christensen et al 2019, ch 5, 'Using all evidence, registration and meta-analysis

> how the research community can systematically collect, organize, and analyze a body of research work

- Limitations to the 'narrative literature review': subjectivity, too much info to narrate

### The origins [and importance] of study [pre-]registration

... Make details of planned and ongoing studies available to the community .... including those not (yet) published

- Required by FDA in 1997, many players in medical community followe d soon after

- Turner ea (08) and others documented massive publication bias and misrepresentation

... but registration far from fully enforced (Mathieu ea '09) found 46% clealy registered, and discrepancies between registered and published outcomes
!

### Social science study registries

Jameel 2009, AEA 2013, 2100 registrations to date
RIDIE, EGAP, AsPredicted, OSF allowing a DOI (25,000+)

### Meta-analsis

Key references: Borenstein ea '09, Cooper, Hedges, and V '09

#### Selecting studies

"some scholarly discretion regarding which measures are 'close enough' to be included... contemperanous meta-analyses on the same topic finding opposit e conclusions

'asses the robustness... to diffrent inclusion conditions'... see Doucouliagos ea '17 on inclusion options


<div class="marginnote">


My opinion: this is the key barrier to meta-analysis in social science! How do we weight studies using different methodologies and in different contexts? The Bayesian Random Effects approach seems to offer some help here (this not to be confused with the random effects panel-data models discussed in standard Econometrics texts).

</div>

#### Assembling estimates

- Which statistic to collect?

\


Studies $j \in J, j= 1..N_j$

Relevant estimate of stat from each study is $\hat{
\beta_j}$ with SE $\hat{\sigma_j}$

- Papers report several estimates (e.g., in robustness checks): which to choose, esp if author's preferred approach differs from other scholars.

\

*Ex from Hsiang, B, Miguel, '13*: links between extreme climate and violence

- how to classify outcomes... interpersonal and intergroup... normalised as pct changes wrt the meanoutcome in that dataset

- how to standardice climate varn measures... chose SD from local area mean
(DR: this choice implicitly reflects a behavioural assumption)

$\rightarrow$ 'pct  change in a conflict outcome as a fncn of a 1 SD schock to local climate'

### Combining estimates

'Fixed-effect meta-analysis approach': assumes a single true effect'

<div class="marginnote">

DR: I'm  not sure I agree on theis assesment of *why* this is unlikely to be true in practice... 'differences in measures' (etc) seem to be a different issue

</div>

*Equal weight approach*: (Simply the average across studies... ugh)

\

*Precision-weighted approach*:

$$\hat{\beta}_{PW}=
\sum_{j}p_j\hat{\beta}_j/
\sum_{j}p_j$$

where $p_j$ is the estimated precision for study $j$: $\frac{1}{\hat{\sigma_i}^2}$

Thus the weight $\omega_j$ placed on study $j$ is proportional to it's precision.


<div class="marginnote">

'implies weight in proportion to sample size'? I think that's loosely worded, it must be nonlinear.

</div>

$\rightarrow$ This minimises the variance in the resulting meta-analtical estimate:

$$Var(\hat{\beta}_{PW) =\sum_j \omega_j^\hat{\sigma_j}^2=\frac{1}{\sum_j(p_j)}$$

'inclusion of additional estimates always reduces the SE of $\hat{\beta_{PW}}$ [in expectation].' ... so more estimtes can't hurt as long as you know their precision.

(they give a numerical example here with 3 estimates)

<!-- Todo: add R code explicitly doing these calculations -->

### Heterogeneous estimates...

#### WLS estimate

(Stanley and Doucouliagos '15)

Interpreted as 'an estimate of the average of potentially heterogenous estimates'

This may feel like a more familiar to Economists but it is also seems to be far less useful than the Bayesian approach.

\


#### Random-effects (more common)

*Focus here on hierarchical Bayesian approach* (Gelman and Hill '06; Gelman ea '13)

'The magnitude and precision of the common component represents the generalizable conclusions we might draw from a literature'

... continuing from above notation

'cross-study differences we observe might not be driven solely by sampling variability... [even with] infinite data, they would not converge to the exact same [estimate] '

True Treatment Effect (TE) $\beta_j$ for study j drawn from a normal distribution...

$$\beta_j \sim N(\mu, \tau^2)$$

'Hyperparameters' $\mu$ determines central tendency of findings...
$\tau$ the extent of hety across contexts.

Considering $\tau$ vs $\mu$ is informative  in itself.
And a large $\mu$ may suggest looking into sample splits for hety on obsl lines.

\

Uniform prior for $\mu$ $\rightarrow$ conditional posterior:

$$\mu|\tau,y \sim N(\hat{\mu}, V_{\mu})$$ where the estimated common effect $\hat{\mu}$ is

$$\hat{\mu}=
\frac{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))\hat{\beta}}
{\sum_{j}(1/(\hat{\sigma}^2_j+\hat{\tau}^2))}$$

(Similar to precision-weighted approach but now the between-study dispersion is incorporated into the weights)

and where the estimated variance  of the generalizable component $V_\mu$ is:

$$Var(\hat{\mu})= \frac{1}{\sum_j\big(1/(\hat{\sigma_i}^2 + \hat{tau}^2)}$$


<div class="marginnote">

Confusion/correction? Is this the estimated variance or the variance of the estimate?

</div>


- and how do we estimate some of the components of these, like $\hat{\tau}$?

> Intuitively, if estimated [TE] in all studies are near one another and have relatively wide and overlapping [CI's], then most of the difference in estimates is likely the result of sampling variation [and $\tau$] is likely to be close to zero.


<div class="marginnote">

DR: But if the TE have wide CI's, do we have power to idfy btwn-study hety? ... I guess that's what the 'estimated TE are all near each other' gives us?

</div>

... Alternatively, if there is extensive variation in the estimated ATEs but each is precise... $\tau$ is likely to be relatively large.

```{block2,  type='note'}
Coding meta-analyses in R

"A Review of Meta-Analysis Packages in R" offers a helpful guide to the various packages, such as `metafor`.

[Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) appears extremely helpful; see, e.g., their chapter [Bayesian Meta-Analysis in R using the brms package](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-meta-analysis-in-r-using-the-brms-package.html)

```


<!-- TODO: some code exercises should be put or linked here? Perhaps drawn from the above references? -->


</div>

\ The $I^2$ stat is a measure of the proportion of total variation attributed to cross-study variation; if $\hat{\sigma}_j$ is the same across all studies we have:
$I^2(.) = \hat{\tau}^2/(\hat{\tau}^2 + \hat{\sigma}^2)$

<!-- *DR: more detail would be welcome here. Material from [this syllabus]() may be helpful.

https://docs.google.com/document/d/1oImg-ojUFqak5KyZ-ETD2qGvkvUgx8Ym6b8gG4GwfM8/edit?usp=drivesdk

-->

## Excerpts and notes from 'Doing Meta-Analysis in R: A Hands-on Guide' (Harrer et al) {#doing-meta}

Some notes follow excerpting and commenting on 

[Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/)


```{r, warning=FALSE, message=FALSE}

#devtools::install_github("MathiasHarrer/dmetar")
#install.packages("extraDistr")

library(meta)

# I followed a lot of steps before this worked! ... https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started ... 
#install.packages("brms")
library(brms)
library(dmetar)

library(extraDistr)

```

### Pooling effect sizes
FE model calculates weighted average: 

FIX THESE FORMULAS

$$\hat{\theta_F} = \frac{\sum\limits_{k=1}^K \hat{\theta_k} $$ 
$$\hat{\sigma^2_k}=\sum\limits_{k=1}^K \frac{1}{K}\hat{\sigma}^2_k $$



- note that this process does not 'dig in' to the raw data, it just needs the summary statistics, neither does the "RE model" they refer to:

> Both of these models only require an effect size, and a dispersion (variance) estimate for each study, of which the inverse is taken. This is why the methods are often called generic inverse-variance methods.

Nor the Bayesian models, apparently (they use the same 'madata' dataset)

### Bayesian Meta-analysis {#doing-bayes-meta}


```{block2,  type='note'}
"The model we apply in Bayesian Meta-Analysis is a so-called Bayesian Hierarchical Model...
every meta-analytical model inherently possesses a multilevel, and thus 'hierarchical', structure."

```  

#### The setup {-}
Underlying RE model (as before)

Study-specific estimate: 

$$ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) $$

True study-specific effects distributed: 

$$ \theta_k \sim \mathcal{N}(\mu,\tau^2) $$

... simplified to the 'marginal' form:

$$ \hat\theta_k | \mu, \tau, \sigma_k \sim \mathcal{N}(\mu,\sigma_k^2 + \tau^2)$$

\


And now we specify priors for these parameters, 'making it Bayesian'


$$(\mu, \tau^2) \sim p(.)$$
$$ \tau^2 > 0 $$
\

Estimation will...

> involve[] Markov Chain Monte Carlo based sampling procedures, such as the Gibbs Sampler. In the brms package we will be using in this chapter, the No-U-Turn Sampler, or NUTS (Hoffman and Gelman 2014), is used.

\

```{block2,  type='note'}

**Why use Bayesian?**

- to  "directly model the uncertainty when estimating [the between-study variance] $\tau^2$"

- "have been found to be superior in estimating the between-study heterogeneity and pooled effect, particularly when the number of included studies is small"

- "produce full posterior distributions for both $\mu$ and $\tau$" ... so we can make legitimate statements about the probabilities of true parameters

- "allow us to integrate prior knowledge and assumptions when calculating meta-analyses" (including methodological uncertainty perhaps)

```  
\

#### Setting weakly informative' priors for the mean and cross-study variance of the TE sizes {-}

> It has been argued that a good approach is to use weakly informative priors (Williams, Rast, and Bürkner 2018) [rather than 'non-informative priors'!]


**For $\mu$**: 

> include distributions which represent that we do indeed have **some confidence that some values are more credible than others**, while still not making any overly specific statements about the exact true value of the parameter. ... In most applied cases, it seems reasonable to assume that the true effect size we want to estimate must lie somewhere between, for example, Cohen's $d=-2.0$ and $d=2.0$, but will unlikely be hovering around $d=50$. A good starting point for our $\mu$ prior may therefore be a normal distribution with mean $0$ and variance $1$. This means that we grant a 95% prior probability that the true pooled effect size $\mu$ lies between $d=-2.0$ and $d=2.0$:

$$ \mu \sim \mathcal{N}(0,1)$$

**For  $\tau^2$**

- must be non-negative, but might be very close to zero.

- Recommended distribution for this case (for variances in general):  *Half-Cauchy prior* (a censored Cauchy)

$\mathcal{HC}(x_0,s)$

- with  *location parameter* $x_0$ (peak on x-axis)
- and  $s$, scaling parameter 'how heavy-tailed'

\

Half-Cauchy distribution for varying $s$, with $x_0=0$:

```{r, echo=FALSE, fig.width=6, fig.align='center'}
library(png)
library(grid)
img <- readPNG(here("images", "half_cauchy.png"))
grid.raster(img)
```


HC is 'heavy-tailed;... gives some probability to very high values but low values are still more likely.

One might consider $s=0.3$ 

<div class="marginnote">
$s$ corresponds to the std deviation here? ... so an SD of the effect size about 1/3 of it's mean size?
</div>


Checking the share of this distribution below 0.3...
```{r, message=F, warning=F}
phcauchy(0.3, sigma = 0.3) #cumulative share of distribution below 0.3 ... is 1/2 ... with sigma=0.3
```

\

... But they go for the 'more conservative' $s=0.5$.

> In general, it is advised to always conduct sensitivity analyses with different prior specifications to check if they affect the results substantially

\

Complete model:

$$ \hat\theta_k \sim \mathcal{N}(\theta_k,\sigma_k^2) $$
$$ \theta_k \sim \mathcal{N}(\mu,\tau^2) $$
$$ \mu \sim \mathcal{N}(0,1)$$
$$ \tau \sim \mathcal{HC}(0,0.5)$$

#### Bayesian Meta-Analysis in R using the `brms` package



You specify the priors as a vector of elements, each of which invokes the 'prior' function, which makes some sort of data frame. The priors function takes a distribution function with parameters, and a 'class'. 

```{r priors}
priors <- c(prior(normal(0,1), class = Intercept), prior(cauchy(0,0.5), class = sd))
```



A quick look at the data we're using here: 

```{r}

str(ThirdWave[,1:3])
```


\

To actually run the model, he uses the following code:

(it seems to require Xcode to run on my mac)

```{r}

m.brm <- brm(TE|se(seTE) ~ 1 + (1|Author),
             data = ThirdWave,
             prior = priors,
             iter = 400)

```


- The *formula for the model* is specified using 'regression formula notation'

- As there is no 'predictor variable' here (unless it's meta-regression), `x` is replaced with `1`

- But we want to  give studies with greater precision of the effect size estimate a greather weight. 
  - Done using `y|se(se_y)`
  
- For  the *random effects terms* he adds  `(1|study)` to the predictor part.

-  `prior`: Plugs in the priors created above plug in the `priors` object we created previously here.

- `iter`:  Number of iterations of MCMC algorithm... the more complex your model, the higher this number should be. [DR: but what's a rule of thumb here?]





<!--chapter:end:meta_anal_and_open_science/metaanalysis.Rmd-->

# Bayesian approaches


```{block2,  type='note'}
I take notes on several different resources/texts below. Ultimately I'll try to integrate these into a single set of notes.
```  

## My (David Reinstein's) uses for Bayesian approaches (brainstorm)


### Meta-analysis of previous evidence

- Of prior work, especially on motivators of (effective) charitable giving and responses to effectiveness information

- Of my own series' of experiments (potentially joint with prior work)

### Inference, particularly about 'null effects'

When/what can we say about the 'absence of an effect'

How to integrate into inferences from diagnostic testing (e.g., common-trend assumption)?  

### 'Policy' and business implications and recommendations

- E.g., for 'which pages to seed'

### Theory-driven inference about optimizing agents, esp. in strategic settings

- Especially in 'predicted contributions to public goods' settings and 2nd order beliefs

### Experimental design

- Optimal treatment assignment, with previous observables and a track record

- Sequential designs

- Bayesian Power calculation
\

```{r}
#sessionInfo()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Package loadings from Kurtz:
```{r, echo = F, message = F, warning = F, results = "hide"}

pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")

library(tidyverse) #adds in next chapter

```


## 'Statistical thinking' (McElreath) and [AJ Kurtz 'recoded' (bookdown)](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded): highlights and notes 


```{block2,  type='note'}

McElreath's course and text looks great. I'm taking selective notes here; I'll try to incorporate content from both text and [youtube video lectures](https://www.youtube.com/watch?v=4WVelCswXo4&list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI).

[AJ Kurtz has re-written the code](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded) using the `brms` package, which he finds superior. More crucially for me, he redoes the code using ggplot and tidyverse?
  
I'm planning to through this here, adding my own notes, questions, and considerations and (hopefully) incorporating some of my own work.
```  

<div class="marginnote">
I've also forked Kurtz's repo [here](https://github.com/daaronr/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse), which I may play with.
</div>
 
### The Golem of Prague (Chapter 1)

Don't let your model or approach turn into a Golem you can't control. Don't 'believe the model'; continuously validate it. The map is not the territory.

'Statistical decision trees' lend a false sense of security... and almost never fit the actual case we are dealing with. (fig 1.1)

\

Statistical models are non-unique maps to 'Process models' which are non-unique maps to Hypotheses. ('Nuetral evolutionary selection' example.) This makes strict falsification impossible ... How can you falsify a hypothesis/theory if it corresponds to a wide set of process models and statistical models, many of which overlap other hypotheses? 

\

But this warning is at least as relevant for Bayesian analyses, which must be based on specifically defined (term) models of the DGP etc. Thus he recommends caution and continuous (?) interplay between the model and the data. (See next chapter ... 'small worlds and large worlds'.)
\

He also suggests we refer not to 'Confidence intervals' or even 'Credible intervals', but to 'Consistent intervals' ... as in 'these intervals are consistent with the model and data'.

\

And... 
> [so you should] '...Explicitly compare predictions of more than one model'


#### Rethinking: Is NHST falsificationist? 

```{r failure_of_falsification.png, fig.cap='From McElreath video lecture 1', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("images/failure_of_falsification.png")

```

> Null hypothesis significance testing, NHST, is often identified with the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify a null hypothesis, not the actual research hypothesis. So the falsification is being done to something other than the explanatory model. This seems the reverse from Karl Popper’s philosophy.

<div class="marginnote">
 
I.e., scientists have turned things upside down; originally the idea was that you had substitute of hypotheses that you would want to falsify and now we try to falsify silly null hypotheses that “nothing is going on”. You should try to really build a hypothesis and test it not just reject that nothing is going on.

</div>


#### Book's foci

1. Bayesian data analysis
2. Multilevel modeling
3. Model comparison using information criteria

\

### Small Worlds and Large Worlds (Ch 2)

> ... The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20)

\

We imagine a bag filled with four marbles, each of which is blue or white. 

"So, if we're willing to code the marbles as 0 = "white" 1 = "blue", we can arrange the possibility data in a tibble as follows." I.e., we can consider the five possible worlds, in each of which the bag has a different number of white and blue marbles, and represent each of these worlds as a column vector:

```{r, warning = F, message = F}
d <-
  tibble(p_1 = 0,
         p_2 = rep(1:0, times = c(1, 3)),
         p_3 = rep(1:0, times = c(2, 2)),
         p_4 = rep(1:0, times = c(3, 1)),
         p_5 = 1)

d

```

\

We visualize this in the plot below, where each column is one 'world':

```{r,  out.width='50%'}
d %>% 
  gather() %>% #make it long, with an ket variable for the possibility 'world'
  mutate(x = rep(1:4, times = 5), #an index for 'which ball'
         possibility = rep(1:5, each = 4)) %>% #distributing the 'which world' index
  ggplot(aes(x = x, y = possibility, 
             fill = value %>% as.character())) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) +
  theme(legend.position = "none")
```

\

Simple combinatorics (permutations rule) tells us how many 'ways' we can draw 1, 2, and 3 marbles... Here we think about 'which' marble is drawn, and not just 'which color' it is. We can draw marble 1-4, the first time, then 1-4 the second time, and then 1-4 the third time... so `possibilities=marbles ^ draw`.

```{r}

tibble(draw    = 1:3,
       marbles = 4) %>% 
  mutate(possibilities = marbles ^ draw) %>% 
  knitr::kable()

```

```{block2,  type='note'}
Next, there is a huge amount of code explaining how to make the 'garden of forking paths' diagrams. I'm basically going to skip all that code, and paste in a few images. You can find  all the code [HERE](https://bookdown.org/content/3890/small-worlds-and-large-worlds.html#the-garden-of-forking-data)

```  
\

Suppose there is only one blue ball and three white balls, possibility '2' above. For this world, we see the full 'garden of forking paths' --- the number of ways to select 1, 2, and 3 balls (with replacement) --- below. 


Every path starting from the center is a possible (sequence of) draws. 

```{r, fig.cap='All possible draws of three balls', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-13-1.png")

```

\

Now the inferential exercise: we want to know (the likelihood) of each of the five possible 'worlds'. As we draw data we know we are  proceeding along one of some subset of the forking paths.
\
For example, under possible world 2,  if we draw Blue, then White, then Blue, this could have occured with any of the following paths (consider a draw of each of the white balls as distinct):

```{r, fig.cap='All possible draws of three balls', out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-14-1.png")

```

We see that under World 2 there are 3 ways of getting this sequence. 3 out of $4^3$ equally likely paths under World 2, or a $3/64$ chance  (about 5%).

\

We can do similar for the other possible worlds; multiplying the 'ways to produce each draw' in the path yields the 'total ways to produce the path', under each world.  

```{r}
# if we make two custom functions, here, it will simplify the code within `mutate()`, below
n_blue <- function(x) {
  rowSums(x == "b")
}

n_white <- function(x) {
  rowSums(x == "w")
}

t <-
  # for the first four columns, `p_` indexes position
  tibble(p_1 = rep(c("w", "b"), times = c(1, 4)),
         p_2 = rep(c("w", "b"), times = c(2, 3)),
         p_3 = rep(c("w", "b"), times = c(3, 2)),
         p_4 = rep(c("w", "b"), times = c(4, 1))) %>% 
  mutate(`draw 1: blue`  = n_blue(.),
         `draw 2: white` = n_white(.),
         `draw 3: blue`  = n_blue(.)) %>% 
  mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`)

t %>% 
  knitr::kable()
```

Among of all possible worlds, we see the most number of ways to get B-W-B in a world 4; with three blues and one white -- here there are 9 ways in total to get B-W-B. Under world 4 this sequence occurs 9/64, or roughly 14% of the time. 


\

We can see this in the following plot. (We leave out the worlds with only one color ball, as these will have no paths that produce B-W-B). Below, each partitioned section represents one world, and the paths in that world that could produce B-W-B are shown.

\ 

```{r, fig.cap='All possible draws of three balls',  out.width='50%', fig.asp=.5, fig.align='center', echo = FALSE}

knitr::include_graphics("https://bookdown.org/content/3890/02_files/figure-gfm/unnamed-chunk-20-1.png")

```

Three paths, versus 8 paths, versus nine paths...

Does this reveal world 4 to be the most likely contents of the present bag? Not necessarily. Suppose we knew ex-ante, from the factory, that '99 bags out of 100 have equal numbers of whites and blues.' Then, it would be much more likely that this bag was from an equal-color bag (world 3), even though this *draw* is more likely conditional on the bag being from world 4. We need will to consider the base-rate probabilities as well. 

<div class="marginnote">
This in turn motivates the standard 'false positive/false negative HIV test' example.
</div>
 

### Using prior information 

> We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25)

This seems to easy to be true, but our garden illustration helps us understand why it is the case. 


#### "Multiply in" new information

First consider, what if we had another draw from the bag, how would this adjust the 'number of paths' for each world? Remember, each draw is independent (replacement). We simply record the number of ways (permutations) that could lead to this draw in each world, and we *multiply* the previous count by this number. You can consider this visually in seeing how 'adding an additional fork at the end of each path' changes the count. 

\

This is given in the table below:

```{r}
t <-
  t %>% 
  rename(`previous counts` = `ways to produce`,
         `ways to produce` = `draw 1: blue`) %>% 
  select(p_1:p_4, `ways to produce`, `previous counts`) %>% 
  mutate(`new count` = `ways to produce` * `previous counts`)

t %>% 
  knitr::kable()
```
\
How to incorporate *prior* information about the probability of each world? 
\

Suppose your friend in the factory tells you (reliably) that 'we produce 3 bags with (just) 1 blue for every 2 bags with equal counts, for every 1 bag with 3 blues.

We can think of the 'factory choosing which bag to produce' as another draw, thus another path.

\
Here the *sequence* in which the information is recieved shouldn't matter. The draws are independent (we presume).  

We can thus multiply the number of paths for each marbles-in-bag world by the (relative) frequency with which the factory 'draws' that bag... as shown below:


```{r}
t <- t %>% 
  select(p_1:p_4, `new count`) %>% 
  rename(`prior count` = `new count`) %>% 
  mutate(`factory count` = c(0, 3:0)) %>% 
  mutate(`new count` = `prior count` * `factory count`)

t %>% 
  knitr::kable()
```

### From counts to probability.


## Title: "Introduction to Bayesian analysis in R and Stata - Katz, Qstep"

*Content from notes from this lecture*

### Why and when use Bayesian (MCMC) methods?

#### Pros
1. No need for asymptotics ... good when sample sizes are small

2. Incorporate previous information

You can consider the 'robustness to other priors'

3. Fit complex nonstandard models
... e.g., with difficult functional forms or likelihood settings (more computation, less thinking)

4. Easy to make predictions (e.g., simulate scenarios) after estimation

5. Incorporate evidence, results, expert judgement

('restrictions' with some lee-way?)

(ISn't this the same as number 2?)

6. Cleaner treatment/imputation of missing values ... these are just parameters

#### Cons

1. Must specify prior distributions ... allows subjective judgement

2. Different way of thinking about stats and inference; probability distributions and simulations, not much about p-values, point estimates and standard errors ... path dependence

3. Computational cost

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


#### Why more popular today?

- Starting from around 2005 in Political Science and Sociology

Computational revolution comes from Markov chain Monte Carlo (MCMC) methods ... don't need analytical solutions

Software implementations -- many in R, specialised software like EWinBugs, JAGS, STAN; also increasingly in Stata


### Theory

Bayes theorem ... inverting conditional probability thing ... 'inversion' to make inferences about the parameters

- In Bayesian stats the *parameters* (and sometimes missing values) are random variables, we make probability statements about them

$$P(A|B)=P(B|A)P(A)/P(B)$$


Frequentist: Point estimates, unknown fixed parameters, data from a hyol repeataable random sample

Bayesian: Fixed data (from the experiment), parameters are random variables ... results based on probability distributions about rthese

Classical statistics: likelihood of data given parameter: $p(y|\theta)$

Bayes we want, $p(\theta|y) = p(y|\theta)p(\theta)/p(y)$

$p(y)$  is a 'constant' in our estimation ... the data is fixed.

So it's proportional to $p(\theta|y) = p(y|\theta)\times p(\theta)$

$p(y|\theta)$ is what we max when we do ML

$ p(\theta)$: prior distribution capturing beliefs about $\theta$

#### So how do we estimate it?

1.  Specify a probability model, a distribution for Y (likelihood function) and the priors for $\theta$

2. Solve (find) the posterior distribution $p(\theta|Y)$ and summarise the parameters of interest

In practice, step 2 is usually done via MCMC simulation rather than analytically.

... via simulations, I approach the 'true' value on $\theta$

(Given 'regularity conditions')

#### Linear regression model example

$$Y = x'\beta+\epsilon$$ with n obs

only random term is epsilon ... natural candidate is a normal distribution, so $Y \sim N(x'\beta,\sigma^2_e)$


So we want to find $p(\beta, \sigma^2_\epsilon|Y,X)$. This depends on the choices of $p(\beta)$ and $p(\epsilon)$. Could choose conjugate priors, leading to a particular joint posterior, you can solve it analytically.

Can yield a joint posterior.

Instead, let's assume that the latter (variance) parameter is known, you can show that the posterior for $\beta$ is also normally distributed. (Conjugate)

Similarly, if we assume $\beta$ is known, if the variance term had an inverse gamma distribution (prior), so will the posterior.

In these conjugate priors, the posterior mean will be a weighted average of the priors and the data.

#### Gibbs

Needs closed form conditional posterior for every parameter.

What Gibbs sampler does is break the parameter space into sets of parameters

1. Choose starting values, $\theta^0_1,...\theta^0_k$

2. sample from the first parameter's distribution given the others
... the second one, ... the k'th one .

3. Repeat step 2 ... thousands of times (starting with the parameters from the previous iteration)
Eventually 'we obtain samples of $p(\theta|y)$'

*But if we don't have a closed form, we cannot simply sample from known distributions in each step*

E.g., in case of Logit distribution.

#### Metropolis Hastings

1. Choose 'proposal distribution' to sample parameter values (a candidate like normal, uniform)
1. Start w a prelim guess for parameter values $\theta_0$
1. At iteration t sample a proposal $\theta_t$  from $p(\theta_t|\theta_{t-1})$ ?? what does this come from?
1. If $p(\theta_t|y)>p(\theta_{t-1}|y)$ accept it as the new value of $\theta$. ??? how is this computed if we don't have conjugate closed-form posteriors?
1. Otherwise flip a coin  with probability  r = (ratio of those probabilities)
- if coin tosses heads, accept as new theta, otherwise stay at previous theta
-  allows algorithm to avoid getting stuck at local maxima

Commonly used proposal: random walk sample: $\theta_t=\theta_{t-1}+z_t$, $z_t \sim f$

?? I do this because there is no analytical way to derive this, unlike in the conjugate case, where we might use the Gibbs

- can combine Gibbs with Metropolis steps; relevant to some problems

#### Assessing convergence

- previous ... 'eyeballing'
- formal:
  - single-chain tests (Geweke/Heidel) ... is the last part of the chain stable (stationary)... compare simulation at middle and end, is there much variation?
  - multiple-chain test... (starting from different values), do they end similar ... Gelman-Rubin diagnosting $\hat{R}$
  - typically either a very long chain and use GH convergence, or multiple shorter chains and use $\hat{R}$

Gabriel: Gelman-Rubin is probably preferred; more conservative

?? What am I iterating towards? Converging on what?

#### Assesing 'fit' in Bayesian

- No r-squared
- Typical measure is 'posterior predictive comparisons'

$p(y_{replicated}|y_{observed}= ...$

1. Simulate data from estimated parameters
2. Compare to observed data
3. Use an overall fit measure to assess model fit

E.g.,   percent correct predictions (binary), whether the true data is within the 95\% CI of the replicates, deviance

For each replicate Choose statistic D, compare the replicated $D(y_s_{replicated})$ against $D(y_s_{observed})

Quantify the discrepancy ... percent of correct predictions, proportion of times replicated y is below true y ... compute 'bayesian p-value's'

Systematic differences between replicate and actual data indicate model limitations

(?? what are reasonable values here??)

### Comparing models ... Equivalent of 'likelihood'

'Deviance Information Criterion' (most used); specific for MCMC simulations: compares expected LL of the model (of the data given the estimated parameters; average here across much of the later points in the chain) against the llhd at the posterior parameter mean.   Always select model with lowest DIC.

Bayes Factor (less used): Ratio of llhd of the models; higher BF means model is more supported; BF>10 seen to provide strong evidence for model w higher value

### On choosing priors

Most social scientists use non-informative or vague priors; i.e., large variance... e.g., $\beta \sim N(0,1000)$

But its often useful to incorporate information into your priors

Small pilot to test, $\rightarrow$ data $Y_1$, another study gives data $Y_2$; repeated application of Bayes theorem gives the posterior.

Same result whether you obtained these together, or whether you did one and then updated (e.g., via an MCMC, starting with the first one as a prior)

Conjugate priors (mentioned before)

- Jeffrey's priors (??)

### Implementation

If you don't need to do fancy things, and don't want to (?) generate the full posterior distribution (or something)

Some Stata/R commands that make Bayesian look frequentist.

In Jags and Winbugs, we only have to specify the prior... rest is done for us

Jags is great ... you only need to do self-coding with lots of data and super complicated models as it can freeze up

We went through it the fancy way in Probit.R

Then the easy way with 'script probit Jags.R'

### Generate predictions from a WinBUGS model

You can just generate these outcomes ...

Prediction: generate a new observation #note, he is doing one per iteration, but since these are convergent it would be basically the same if you just chose a random iteration and did all the draws from that one

### Missing data case

One solution -- multiple imputation

- choose imputation model to predict missings,
- generate many copies of orig data set, imputing missibg value for each
- 2 more steps here

Need a model for X|alpha, because missing variables are random variables

### Stata

Has some rather simple implementations; e.g., just using commands like ```bayes: regress y x```

### R mcmc pac

Also simple code; great for standard use

Speedup with parallelization; see "script for parallel probit.R" and "parallelprobit.R"

More advanced: C++; can integrate it with Rcpp, or even use Exeter's ISCA cluster


```{r cars}
summary(cars)
```


<!--chapter:end:bayesian/bayes_notes.Rmd-->

