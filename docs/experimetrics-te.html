<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>15 ‘Experimetrics’ and measurement of treatment effects from RCTs | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus</title>
  <meta name="description" content="15 ‘Experimetrics’ and measurement of treatment effects from RCTs | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="15 ‘Experimetrics’ and measurement of treatment effects from RCTs | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="daaronr/metrics_discussion_work" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="15 ‘Experimetrics’ and measurement of treatment effects from RCTs | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus" />
  
  
  

<meta name="author" content="Dr. David Reinstein," />


<meta name="date" content="2021-02-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="power.html"/>
<link rel="next" href="metaanalysis.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- <script src="js/hideOutput.js"></script> -->

<!-- Mathjax -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>




<script async defer src="https://hypothes.is/embed.js"></script>

<!-- Folding text box javascript thing -->

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold" ? "Unfold" : "Fold");  // if the text equals "Fold", change it to "Unfold"or else to "Fold"
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>


<script type="text/javascript">

// toggle visibility of R source blocks in R Markdown output
function toggle_R() {
  var x = document.getElementsByClassName('r');
  if (x.length == 0) return;
  function toggle_vis(o) {
    var d = o.style.display;
    o.style.display = (d == 'block' || d == '') ? 'none':'block';
  }

  for (i = 0; i < x.length; i++) {
    var y = x[i];
    if (y.tagName.toLowerCase() === 'pre') toggle_vis(y);
  }

    var elem = document.getElementById("myButton1");
    if (elem.value === "Hide Global") elem.value = "Show Global";
    else elem.value = "Hide Global";
}

document.write('<input onclick="toggle_R();" type="button" value="Hide Global" id="myButton1" style="position: absolute; top: 10%; right: 2%; z-index: 200"></input>')

</script>

<!-- Global site tag (gtag.js) - Google Analytics
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-148137970-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-148137970-3');
</script>
-->


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="support/tufte_plus.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li><a href="introduction.html#conceptual-approaches-to-statisticsinference-and-causality"><span>Conceptual: approaches to statistics/inference and causality</span></a></li>
<li><a href="introduction.html#getting-cleaning-and-using-data-project-management-and-coding"><span>Getting, cleaning and using data; project management and coding</span></a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#basic-regression-and-statistical-inference-common-mistakes-and-issues"><i class="fa fa-check"></i>Basic regression and statistical inference: Common mistakes and issues</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#ldv-and-discrete-choice-modeling"><i class="fa fa-check"></i>LDV and discrete choice modeling</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#robustness-and-diagnostics-with-integrity"><i class="fa fa-check"></i>Robustness and diagnostics, with integrity</a></li>
<li><a href="introduction.html#control-strategies-and-prediction-machine-learning-approaches"><span>Control strategies and prediction; Machine Learning approaches</span></a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#iv-and-its-many-issues"><i class="fa fa-check"></i>IV and its many issues</a></li>
<li><a href="introduction.html#other-paths-to-observational-identification"><span>Other paths to observational identification</span></a></li>
<li><a href="introduction.html#causal-pathways-mediation-modeling-and-its-massive-limitations">Causal pathways: <span>Mediation modeling and its massive limitations</span></a></li>
<li><a href="introduction.html#causal-pathways-selection-corners-hurdles-and-conditional-on-estimates">Causal pathways: <span>selection, corners, hurdles, and ‘conditional on’ estimates</span></a></li>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#survey-design-and-implementation-analysis-of-survey-data"><i class="fa fa-check"></i><b>1.1</b> <span>Survey design and implementation; analysis of survey data</span></a></li>
<li><a href="introduction.html#experimental-study-design-identifying-meaningful-and-useful-causal-relationships-and-parameters"><span>(Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters</span></a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#experimental-study-design-background-and-quantitative-issues"><i class="fa fa-check"></i>(Experimental) Study design: Background and quantitative issues</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#experimental-study-design-ex-ante-power-calculations"><i class="fa fa-check"></i><b>1.2</b> (Experimental) Study design: (Ex-ante) Power calculations</a></li>
<li><a href="introduction.html#experimetrics-and-measurement-of-treatment-effects-from-rcts"><span>‘Experimetrics’ and measurement of treatment effects from RCTs</span></a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#making-inferences-from-previous-work-meta-analysis-combining-studies"><i class="fa fa-check"></i><b>1.3</b> <span>Making inferences from previous work; Meta-analysis, combining studies</span></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#publication-bias"><i class="fa fa-check"></i><b>1.3.1</b> Publication bias</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#combining-a-few-your-own-studiesestimates"><i class="fa fa-check"></i><b>1.3.2</b> Combining a few (your own) studies/estimates</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#full-meta-analyses"><i class="fa fa-check"></i><b>1.3.3</b> Full meta-analyses</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#the-bayesian-approach"><i class="fa fa-check"></i><b>1.4</b> The Bayesian approach</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#some-key-resources-and-references"><i class="fa fa-check"></i><b>1.5</b> Some key resources and references</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="conceptual.html"><a href="conceptual.html"><i class="fa fa-check"></i><b>2</b> Conceptual: approaches to statistics/inference and causality</a>
<ul>
<li class="chapter" data-level="2.1" data-path="conceptual.html"><a href="conceptual.html#bayesian-vs.-frequentist-approaches"><i class="fa fa-check"></i><b>2.1</b> Bayesian vs. frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="conceptual.html"><a href="conceptual.html#interpretation-of-cis-aside"><i class="fa fa-check"></i><b>2.1.1</b> Interpretation of CI’s (aside)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="conceptual.html"><a href="conceptual.html#causal-vs.-descriptive-treatment-effects-and-the-potential-outcomes-causal-model"><i class="fa fa-check"></i><b>2.2</b> Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="conceptual.html"><a href="conceptual.html#dags-and-potential-outcomes"><i class="fa fa-check"></i><b>2.2.1</b> DAGs and Potential outcomes</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="conceptual.html"><a href="conceptual.html#theory-restrictions-and-structural-vs-reduced-form"><i class="fa fa-check"></i><b>2.3</b> Theory, restrictions, and ‘structural vs reduced form’</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-sci.html"><a href="data-sci.html"><i class="fa fa-check"></i><b>3</b> Getting, cleaning and using data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-sci.html"><a href="data-sci.html#data-whatwhywherehow"><i class="fa fa-check"></i><b>3.1</b> Data: What/why/where/how</a></li>
<li class="chapter" data-level="3.2" data-path="data-sci.html"><a href="data-sci.html#organizing-a-project"><i class="fa fa-check"></i><b>3.2</b> Organizing a project</a></li>
<li class="chapter" data-level="3.3" data-path="data-sci.html"><a href="data-sci.html#dynamic-documents-esp-rmdbookdown"><i class="fa fa-check"></i><b>3.3</b> Dynamic documents (esp Rmd/bookdown)</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="data-sci.html"><a href="data-sci.html#managing-referencescitations"><i class="fa fa-check"></i><b>3.3.1</b> Managing references/citations</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-sci.html"><a href="data-sci.html#an-example-of-dynamic-code"><i class="fa fa-check"></i><b>3.3.2</b> An example of dynamic code</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data-sci.html"><a href="data-sci.html#project-management-tools-esp.-gitgithub"><i class="fa fa-check"></i><b>3.4</b> Project management tools, esp. Git/Github</a></li>
<li class="chapter" data-level="3.5" data-path="data-sci.html"><a href="data-sci.html#good-coding-practices"><i class="fa fa-check"></i><b>3.5</b> Good coding practices</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="data-sci.html"><a href="data-sci.html#new-tools-and-approaches-to-data-esp-tidyverse"><i class="fa fa-check"></i><b>3.5.1</b> New tools and approaches to data (esp ‘tidyverse’)</a></li>
<li class="chapter" data-level="3.5.2" data-path="data-sci.html"><a href="data-sci.html#style-and-consistency"><i class="fa fa-check"></i><b>3.5.2</b> Style and consistency</a></li>
<li class="chapter" data-level="3.5.3" data-path="data-sci.html"><a href="data-sci.html#using-functions-variable-lists-etc.-for-clean-concise-readable-code"><i class="fa fa-check"></i><b>3.5.3</b> Using functions, variable lists, etc., for clean, concise, readable code</a></li>
<li class="chapter" data-level="3.5.4" data-path="data-sci.html"><a href="data-sci.html#mapping-over-lists-to-produce-results"><i class="fa fa-check"></i><b>3.5.4</b> Mapping over lists to produce results</a></li>
<li class="chapter" data-level="3.5.5" data-path="data-sci.html"><a href="data-sci.html#building-results-based-on-lists-of-filters-of-the-data-set"><i class="fa fa-check"></i><b>3.5.5</b> Building results based on ‘lists of filters’ of the data set</a></li>
<li class="chapter" data-level="3.5.6" data-path="data-sci.html"><a href="data-sci.html#coding-style-and-indenting-in-stata-one-approach"><i class="fa fa-check"></i><b>3.5.6</b> Coding style and indenting in Stata (one approach)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="data-sci.html"><a href="data-sci.html#additional-tips-integrate"><i class="fa fa-check"></i><b>3.6</b> Additional tips (integrate)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reg-follies.html"><a href="reg-follies.html"><i class="fa fa-check"></i><b>4</b> Basic statistical inference and regressions: Common mistakes and issues</a>
<ul>
<li class="chapter" data-level="4.1" data-path="reg-follies.html"><a href="reg-follies.html#basic-regression-and-statistical-inference-common-mistakes-and-issues-briefly-listed"><i class="fa fa-check"></i><b>4.1</b> Basic regression and statistical inference: Common mistakes and issues briefly listed</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="reg-follies.html"><a href="reg-follies.html#bad-control"><i class="fa fa-check"></i><b>4.1.1</b> Bad control</a></li>
<li class="chapter" data-level="4.1.2" data-path="reg-follies.html"><a href="reg-follies.html#bad-control-colliders"><i class="fa fa-check"></i><b>4.1.2</b> “Bad control” (“colliders”)</a></li>
<li class="chapter" data-level="4.1.3" data-path="reg-follies.html"><a href="reg-follies.html#choices-of-lhs-and-rhs-variables"><i class="fa fa-check"></i><b>4.1.3</b> Choices of lhs and rhs variables</a></li>
<li class="chapter" data-level="4.1.4" data-path="reg-follies.html"><a href="reg-follies.html#functional-form"><i class="fa fa-check"></i><b>4.1.4</b> Functional form</a></li>
<li class="chapter" data-level="4.1.5" data-path="reg-follies.html"><a href="reg-follies.html#ols-and-heterogeneity"><i class="fa fa-check"></i><b>4.1.5</b> OLS and heterogeneity</a></li>
<li class="chapter" data-level="4.1.6" data-path="reg-follies.html"><a href="reg-follies.html#null-effects"><i class="fa fa-check"></i><b>4.1.6</b> “Null effects”</a></li>
<li class="chapter" data-level="4.1.7" data-path="reg-follies.html"><a href="reg-follies.html#mht"><i class="fa fa-check"></i><b>4.1.7</b> Multiple hypothesis testing (MHT)</a></li>
<li class="chapter" data-level="4.1.8" data-path="reg-follies.html"><a href="reg-follies.html#interaction-terms-and-pitfalls"><i class="fa fa-check"></i><b>4.1.8</b> Interaction terms and pitfalls</a></li>
<li class="chapter" data-level="4.1.9" data-path="reg-follies.html"><a href="reg-follies.html#choice-of-test-statistics-including-nonparametric"><i class="fa fa-check"></i><b>4.1.9</b> Choice of test statistics (including nonparametric)</a></li>
<li class="chapter" data-level="4.1.10" data-path="reg-follies.html"><a href="reg-follies.html#how-to-display-and-write-about-regression-results-and-tests"><i class="fa fa-check"></i><b>4.1.10</b> How to display and write about regression results and tests</a></li>
<li class="chapter" data-level="4.1.11" data-path="reg-follies.html"><a href="reg-follies.html#bayesian-interpretations-of-results"><i class="fa fa-check"></i><b>4.1.11</b> Bayesian interpretations of results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="robust-diag.html"><a href="robust-diag.html"><i class="fa fa-check"></i><b>5</b> Robustness and diagnostics, with integrity; Open Science resources</a>
<ul>
<li class="chapter" data-level="5.1" data-path="robust-diag.html"><a href="robust-diag.html#how-can-diagnostic-tests-make-sense-where-is-the-burden-of-proof"><i class="fa fa-check"></i><b>5.1</b> (How) can diagnostic tests make sense? Where is the burden of proof?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="robust-diag.html"><a href="robust-diag.html#further-discussion-the-did-approach-and-parallel-trends"><i class="fa fa-check"></i><b>5.1.1</b> Further discussion: the DiD approach and ‘parallel trends’</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="robust-diag.html"><a href="robust-diag.html#estimating-standard-errors"><i class="fa fa-check"></i><b>5.2</b> Estimating standard errors</a></li>
<li class="chapter" data-level="5.3" data-path="robust-diag.html"><a href="robust-diag.html#sensitivity-analysis-interactive-presentation"><i class="fa fa-check"></i><b>5.3</b> Sensitivity analysis: Interactive presentation</a></li>
<li class="chapter" data-level="5.4" data-path="robust-diag.html"><a href="robust-diag.html#supplement-open-science-resources-tools-and-considerations"><i class="fa fa-check"></i><b>5.4</b> Supplement: open science resources, tools and considerations</a></li>
<li class="chapter" data-level="5.5" data-path="robust-diag.html"><a href="robust-diag.html#diagnosing-p-hacking-and-publication-bias-see-also-meta-analysis"><i class="fa fa-check"></i><b>5.5</b> Diagnosing p-hacking and publication bias (see also <span>meta-analysis</span>)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="robust-diag.html"><a href="robust-diag.html#publication-bias-see-also-considering-publication-bias-in-meta-analysis"><i class="fa fa-check"></i><b>5.5.1</b> Publication bias – see also <span>considering publication bias in meta-analysis</span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="robust-diag.html"><a href="robust-diag.html#multiple-hypothesis-testing---see-above"><i class="fa fa-check"></i><b>5.6</b> <span>Multiple hypothesis testing - see above</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="control-ml.html"><a href="control-ml.html"><i class="fa fa-check"></i><b>6</b> Control strategies and prediction, Machine Learning (Statistical Learning) approaches</a>
<ul>
<li class="chapter" data-level="6.1" data-path="control-ml.html"><a href="control-ml.html#see-also-notes-on-data-science-for-business"><i class="fa fa-check"></i><b>6.1</b> See also <span>“notes on Data Science for Business”</span></a></li>
<li class="chapter" data-level="6.2" data-path="control-ml.html"><a href="control-ml.html#machine-learning-statistical-learning-lasso-ridge-and-more"><i class="fa fa-check"></i><b>6.2</b> Machine Learning (statistical learning): Lasso, Ridge, and more</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="control-ml.html"><a href="control-ml.html#limitations-to-inference-from-learning-approaches"><i class="fa fa-check"></i><b>6.2.1</b> Limitations to inference from learning approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="control-ml.html"><a href="control-ml.html#notes-hastie-statistical-learning-with-sparsity"><i class="fa fa-check"></i><b>6.3</b> Notes Hastie: Statistical Learning with Sparsity</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="control-ml.html"><a href="control-ml.html#introduction-1"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="control-ml.html"><a href="control-ml.html#ch2-lasso-for-linear-models"><i class="fa fa-check"></i><b>6.3.2</b> Ch2: Lasso for linear models</a></li>
<li class="chapter" data-level="6.3.3" data-path="control-ml.html"><a href="control-ml.html#chapter-3-generalized-linear-models"><i class="fa fa-check"></i><b>6.3.3</b> Chapter 3: Generalized linear models</a></li>
<li class="chapter" data-level="6.3.4" data-path="control-ml.html"><a href="control-ml.html#chapter-4-generalizations-of-the-lasso-penalty"><i class="fa fa-check"></i><b>6.3.4</b> Chapter 4: Generalizations of the Lasso penalty</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="control-ml.html"><a href="control-ml.html#notes-mullainathan"><i class="fa fa-check"></i><b>6.4</b> Notes: Mullainathan</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html"><i class="fa fa-check"></i><b>7</b> IV and its many issues</a>
<ul>
<li class="chapter" data-level="" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#some-casual-discussion"><i class="fa fa-check"></i>Some casual discussion</a></li>
<li class="chapter" data-level="7.1" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#instrument-validity"><i class="fa fa-check"></i><b>7.1</b> Instrument validity</a></li>
<li class="chapter" data-level="7.2" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#heterogeneity-and-late"><i class="fa fa-check"></i><b>7.2</b> Heterogeneity and LATE</a></li>
<li class="chapter" data-level="7.3" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#weak-instruments-other-issues"><i class="fa fa-check"></i><b>7.3</b> Weak instruments, other issues</a></li>
<li class="chapter" data-level="7.4" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#instrumenting-interactions"><i class="fa fa-check"></i><b>7.4</b> Instrumenting Interactions</a></li>
<li class="chapter" data-level="7.5" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#reference-to-the-use-of-iv-in-experimentsmediation"><i class="fa fa-check"></i><b>7.5</b> Reference to the use of IV in experiments/mediation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html"><i class="fa fa-check"></i><b>8</b> <span id="other_paths">Other paths to observational identification</span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#fixed-effects-and-differencing"><i class="fa fa-check"></i><b>8.1</b> Fixed effects and differencing</a></li>
<li class="chapter" data-level="8.2" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#did"><i class="fa fa-check"></i><b>8.2</b> DiD</a></li>
<li class="chapter" data-level="8.3" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#rd"><i class="fa fa-check"></i><b>8.3</b> RD</a></li>
<li class="chapter" data-level="8.4" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#time-series-ish-panel-approaches-to-micro"><i class="fa fa-check"></i><b>8.4</b> Time-series-ish panel approaches to micro</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#lagged-dependent-variable-and-fixed-effects-nickel-bias"><i class="fa fa-check"></i><b>8.4.1</b> Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mediators.html"><a href="mediators.html"><i class="fa fa-check"></i><b>9</b> Causal pathways - mediators</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mediators.html"><a href="mediators.html#mediators-and-selection-and-roy-models-a-review-considering-two-research-applications"><i class="fa fa-check"></i><b>9.1</b> Mediators (and selection and Roy models): a review, considering two research applications</a></li>
<li class="chapter" data-level="9.2" data-path="mediators.html"><a href="mediators.html#dr-initial-thoughts-for-nl-education-paper"><i class="fa fa-check"></i><b>9.2</b> DR initial thoughts (for NL education paper)</a></li>
<li class="chapter" data-level="9.3" data-path="mediators.html"><a href="mediators.html#econometric-mediation-analyses-heckman-and-pinto"><i class="fa fa-check"></i><b>9.3</b> Econometric Mediation Analyses (Heckman and Pinto)</a>
<ul>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#relevance-to-parey-et-al"><i class="fa fa-check"></i>Relevance to Parey et al</a></li>
<li class="chapter" data-level="9.3.1" data-path="mediators.html"><a href="mediators.html#summary-and-key-modeling"><i class="fa fa-check"></i><b>9.3.1</b> Summary and key modeling</a></li>
<li class="chapter" data-level="9.3.2" data-path="mediators.html"><a href="mediators.html#common-assumptions-and-their-implications"><i class="fa fa-check"></i><b>9.3.2</b> Common assumptions and their implications</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="mediators.html"><a href="mediators.html#pinto-2015-selection-bias-in-a-controlled-experiment-the-case-of-moving-to-opportunity"><i class="fa fa-check"></i><b>9.4</b> Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity</a>
<ul>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#summary"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#relevance-to-parey-et-al-1"><i class="fa fa-check"></i>Relevance to Parey et al</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#introduction-2"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#identification-strategy-brief"><i class="fa fa-check"></i>Identification strategy brief</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#results-in-brief"><i class="fa fa-check"></i>Results in brief</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#framework-first-for-binarybinary-simplification"><i class="fa fa-check"></i>Framework: first for binary/binary (simplification)</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#framework-for-mto-multiple-treatment-groups-multiple-choices"><i class="fa fa-check"></i>Framework for MTO multiple treatment groups, multiple choices</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="mediators.html"><a href="mediators.html#antonakis-approaches"><i class="fa fa-check"></i><b>9.5</b> Antonakis approaches</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="selection-cop.html"><a href="selection-cop.html"><i class="fa fa-check"></i><b>10</b> Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates</a>
<ul>
<li class="chapter" data-level="10.1" data-path="selection-cop.html"><a href="selection-cop.html#corner-solution-or-hurdle-variables-and-conditional-on-positive"><i class="fa fa-check"></i><b>10.1</b> ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’</a></li>
<li class="chapter" data-level="10.2" data-path="selection-cop.html"><a href="selection-cop.html#bounding-approaches-lee-manski-etc"><i class="fa fa-check"></i><b>10.2</b> Bounding approaches (Lee, Manski, etc)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="selection-cop.html"><a href="selection-cop.html#notes-training-wages-and-sample-selection-estimating-sharp-bounds-on-treatment-effects-david-lee-2009-restud"><i class="fa fa-check"></i><b>10.2.1</b> Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="surveys.html"><a href="surveys.html"><i class="fa fa-check"></i><b>11</b> Survey design and implementation; analysis of survey data</a>
<ul>
<li class="chapter" data-level="11.1" data-path="surveys.html"><a href="surveys.html#survey-samplingintake"><i class="fa fa-check"></i><b>11.1</b> Survey sampling/intake</a>
<ul>
<li class="chapter" data-level="" data-path="surveys.html"><a href="surveys.html#probability-sampling"><i class="fa fa-check"></i>Probability sampling</a></li>
<li class="chapter" data-level="" data-path="surveys.html"><a href="surveys.html#np-sampling"><i class="fa fa-check"></i>Non-probability sampling</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="surveys.html"><a href="surveys.html#jazz-case"><i class="fa fa-check"></i><b>11.2</b> Case: Surveying an unmeasured and rare population surrounding a ‘social movement’</a>
<ul>
<li class="chapter" data-level="" data-path="surveys.html"><a href="surveys.html#background-and-setup"><i class="fa fa-check"></i>Background and setup</a></li>
<li class="chapter" data-level="" data-path="surveys.html"><a href="surveys.html#our-convenience-method-issues-alternatives"><i class="fa fa-check"></i>Our ‘convenience’ method; issues, alternatives</a></li>
<li class="chapter" data-level="" data-path="surveys.html"><a href="surveys.html#our-methodological-questions"><i class="fa fa-check"></i>Our methodological questions</a></li>
<li class="chapter" data-level="11.2.1" data-path="surveys.html"><a href="surveys.html#sketched-model-and-approach-bayesian-inferenceupdating-for-estimating-demographics-and-attitudes-of-an-rarehidden-population"><i class="fa fa-check"></i><b>11.2.1</b> Sketched model and approach: Bayesian inference/updating for estimating demographics and attitudes of an rare/hidden population</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="why-experiment-design.html"><a href="why-experiment-design.html"><i class="fa fa-check"></i><b>12</b> Experimental design: Identifying meaningful and useful (causal) relationships and parameters</a>
<ul>
<li class="chapter" data-level="12.1" data-path="why-experiment-design.html"><a href="why-experiment-design.html#why-run-an-experiment-or-study"><i class="fa fa-check"></i><b>12.1</b> Why run an experiment or study?</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="why-experiment-design.html"><a href="why-experiment-design.html#sitzia-and-sugden-on-what-theoretically-driven-experiments-can-and-should-do"><i class="fa fa-check"></i><b>12.1.1</b> Sitzia and Sugden on what theoretically driven experiments can and should do</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="why-experiment-design.html"><a href="why-experiment-design.html#causal-channels-and-identification"><i class="fa fa-check"></i><b>12.2</b> Causal channels and identification</a></li>
<li class="chapter" data-level="12.3" data-path="why-experiment-design.html"><a href="why-experiment-design.html#artifacts"><i class="fa fa-check"></i><b>12.3</b> Types of experiments, ‘demand effects’ and more artifacts of artificial setups</a></li>
<li class="chapter" data-level="12.4" data-path="why-experiment-design.html"><a href="why-experiment-design.html#ws-bs"><i class="fa fa-check"></i><b>12.4</b> Within vs between-subject designs</a></li>
<li class="chapter" data-level="12.5" data-path="why-experiment-design.html"><a href="why-experiment-design.html#generalizability-and-heterogeneity"><i class="fa fa-check"></i><b>12.5</b> Generalizability (and heterogeneity)</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="quant-design-power.html"><a href="quant-design-power.html"><i class="fa fa-check"></i><b>13</b> Robust experimental design: pre-registration and efficient assignment of treatments</a>
<ul>
<li class="chapter" data-level="13.1" data-path="quant-design-power.html"><a href="quant-design-power.html#pre-reg-pap"><i class="fa fa-check"></i><b>13.1</b> Pre-registration and Pre-analysis plans</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="quant-design-power.html"><a href="quant-design-power.html#the-benefits-and-costs-of-pre-registration-a-typical-discussion"><i class="fa fa-check"></i><b>13.1.1</b> The benefits and costs of pre-registration: a typical discussion</a></li>
<li class="chapter" data-level="13.1.2" data-path="quant-design-power.html"><a href="quant-design-power.html#the-hazards-of-specification-searching"><i class="fa fa-check"></i><b>13.1.2</b> The hazards of specification-searching</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="quant-design-power.html"><a href="quant-design-power.html#designs-for-decision-making"><i class="fa fa-check"></i><b>13.2</b> Designs for <em>decision-making</em></a></li>
<li class="chapter" data-level="13.3" data-path="quant-design-power.html"><a href="quant-design-power.html#sequential"><i class="fa fa-check"></i><b>13.3</b> Sequential and adaptive designs</a>
<ul>
<li class="chapter" data-level="" data-path="quant-design-power.html"><a href="quant-design-power.html#sequential-1"><i class="fa fa-check"></i>Sequential</a></li>
<li class="chapter" data-level="13.3.1" data-path="quant-design-power.html"><a href="quant-design-power.html#adaptive"><i class="fa fa-check"></i><b>13.3.1</b> Adaptive</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="quant-design-power.html"><a href="quant-design-power.html#efficient-assignment-of-treatments"><i class="fa fa-check"></i><b>13.4</b> Efficient assignment of treatments</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="quant-design-power.html"><a href="quant-design-power.html#see-also-multiple-hypothesis-testing"><i class="fa fa-check"></i><b>13.4.1</b> See also <span>multiple hypothesis testing</span></a></li>
<li class="chapter" data-level="13.4.2" data-path="quant-design-power.html"><a href="quant-design-power.html#how-many-treatment-arms-can-you-afford"><i class="fa fa-check"></i><b>13.4.2</b> How many treatment arms can you ‘afford’?</a></li>
<li class="chapter" data-level="13.4.3" data-path="quant-design-power.html"><a href="quant-design-power.html#other-notes-and-resources"><i class="fa fa-check"></i><b>13.4.3</b> Other notes and resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="power.html"><a href="power.html"><i class="fa fa-check"></i><b>14</b> (Ex-ante) Power calculations for (Experimental) study design</a>
<ul>
<li class="chapter" data-level="14.1" data-path="power.html"><a href="power.html#what-is-the-point-of-doing-a-power-analysis-or-power-calculations"><i class="fa fa-check"></i><b>14.1</b> What is the point of doing a ‘power analysis’ or ‘power calculations’?</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="power.html"><a href="power.html#practical-power"><i class="fa fa-check"></i><b>14.1.1</b> What are the practical benefits of doing a power analysis</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="power.html"><a href="power.html#power-ingredients"><i class="fa fa-check"></i><b>14.2</b> Key ingredients for doing a power analysis (and designing an experimental study in light of this)</a></li>
<li class="chapter" data-level="14.3" data-path="power.html"><a href="power.html#underpowered"><i class="fa fa-check"></i><b>14.3</b> The ‘harm to science’ from running underpowered studies</a></li>
<li class="chapter" data-level="14.4" data-path="power.html"><a href="power.html#power-calculations-without-real-data"><i class="fa fa-check"></i><b>14.4</b> Power calculations without real data</a></li>
<li class="chapter" data-level="14.5" data-path="power.html"><a href="power.html#power-calculations-using-prior-data"><i class="fa fa-check"></i><b>14.5</b> Power calculations using prior data</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="power.html"><a href="power.html#from-reinstein-upcoming-experiment-preregistration"><i class="fa fa-check"></i><b>14.5.1</b> From Reinstein upcoming experiment preregistration</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="power.html"><a href="power.html#survey-power-likert"><i class="fa fa-check"></i><b>14.6</b> Survey design digression: sample size for a “precise estimate of a ‘population parameter’” (focus: mean of a Likert scale response)</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="power.html"><a href="power.html#how-to-measure-and-consider-the-precision-of-likert-item-responses"><i class="fa fa-check"></i><b>14.6.1</b> How to measure and consider the precision of Likert-item responses</a></li>
<li class="chapter" data-level="14.6.2" data-path="power.html"><a href="power.html#computing-sample-size-to-achieve-this-precision"><i class="fa fa-check"></i><b>14.6.2</b> Computing sample size to achieve this precision</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="power.html"><a href="power.html#digression-power-calculationsoptimal-sample-size-for-lift-in-a-ranking-case"><i class="fa fa-check"></i><b>14.7</b> Digression: Power calculations/optimal sample size for ‘lift’ in a ranking case</a>
<ul>
<li class="chapter" data-level="14.7.1" data-path="power.html"><a href="power.html#design-which-questions-to-ask-the-audience-about-the-proposed-titles-and-in-what-order"><i class="fa fa-check"></i><b>14.7.1</b> Design: Which questions to ask the audience about the proposed titles, and in what order</a></li>
<li class="chapter" data-level="" data-path="power.html"><a href="power.html#which-statistical-testsanalyses-to-run-if-any-and-what-measures-to-report"><i class="fa fa-check"></i>Which statistical test(s)/analyses to run (if any) and what measures to report?</a></li>
<li class="chapter" data-level="" data-path="power.html"><a href="power.html#how-to-assign-the-treatments-and-how-large-a-sample-is-optimal-considering-power-or-lift"><i class="fa fa-check"></i>How to assign the ‘treatments’, and how large a sample is optimal, considering ‘power’ (or ‘lift’)?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="experimetrics-te.html"><a href="experimetrics-te.html"><i class="fa fa-check"></i><b>15</b> ‘Experimetrics’ and measurement of treatment effects from RCTs</a>
<ul>
<li class="chapter" data-level="15.1" data-path="experimetrics-te.html"><a href="experimetrics-te.html#which-error-structure-random-effects"><i class="fa fa-check"></i><b>15.1</b> Which error structure? Random effects?</a></li>
<li class="chapter" data-level="15.2" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomization-inference"><i class="fa fa-check"></i><b>15.2</b> Randomization inference?</a></li>
<li class="chapter" data-level="15.3" data-path="experimetrics-te.html"><a href="experimetrics-te.html#parametric-and-nonparametric-tests-of-simple-hypotheses"><i class="fa fa-check"></i><b>15.3</b> Parametric and nonparametric tests of simple hypotheses</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="experimetrics-te.html"><a href="experimetrics-te.html#parametric-tests"><i class="fa fa-check"></i><b>15.3.1</b> Parametric tests</a></li>
<li class="chapter" data-level="15.3.2" data-path="experimetrics-te.html"><a href="experimetrics-te.html#non-parametric-tests"><i class="fa fa-check"></i><b>15.3.2</b> Non-parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="experimetrics-te.html"><a href="experimetrics-te.html#adjustments-for-exogenous-but-non-random-treatment-assignment"><i class="fa fa-check"></i><b>15.4</b> Adjustments for exogenous (but non-random) treatment assignment</a></li>
<li class="chapter" data-level="15.5" data-path="experimetrics-te.html"><a href="experimetrics-te.html#iv-in-an-experimental-context-to-get-at-mediators"><i class="fa fa-check"></i><b>15.5</b> IV in an experimental context to get at ‘mediators’?</a></li>
<li class="chapter" data-level="15.6" data-path="experimetrics-te.html"><a href="experimetrics-te.html#heterogeneity-in-an-experimental-context"><i class="fa fa-check"></i><b>15.6</b> Heterogeneity in an experimental context</a></li>
<li class="chapter" data-level="15.7" data-path="experimetrics-te.html"><a href="experimetrics-te.html#incorporate-above-notes-on-the-econometrics-of-randomised-experiments-athey-and-imbens"><i class="fa fa-check"></i><b>15.7</b> Incorporate above: Notes on “The econometrics of randomised experiments” (Athey and Imbens)</a>
<ul>
<li class="chapter" data-level="15.7.1" data-path="experimetrics-te.html"><a href="experimetrics-te.html#abstract-and-intro"><i class="fa fa-check"></i><b>15.7.1</b> Abstract and intro</a></li>
<li class="chapter" data-level="15.7.2" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomised-experiments-and-validity"><i class="fa fa-check"></i><b>15.7.2</b> Randomised experiments and validity</a></li>
<li class="chapter" data-level="15.7.3" data-path="experimetrics-te.html"><a href="experimetrics-te.html#potential-outcomes-rubin-causal-model-framework-covered-earlier"><i class="fa fa-check"></i><b>15.7.3</b> Potential outcomes/ Rubin causal model framework (covered earlier)</a></li>
<li class="chapter" data-level="15.7.4" data-path="experimetrics-te.html"><a href="experimetrics-te.html#classification-of-assignment-mechanisms"><i class="fa fa-check"></i><b>15.7.4</b> 3.2 Classification of assignment mechanisms</a></li>
<li class="chapter" data-level="15.7.5" data-path="experimetrics-te.html"><a href="experimetrics-te.html#the-analysis-of-completely-randomized-experiments"><i class="fa fa-check"></i><b>15.7.5</b> The analysis of Completely randomized experiments</a></li>
<li class="chapter" data-level="15.7.6" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomization-inference-for-average-treatment-effects"><i class="fa fa-check"></i><b>15.7.6</b> Randomization inference for Average treatment effects</a></li>
<li class="chapter" data-level="15.7.7" data-path="experimetrics-te.html"><a href="experimetrics-te.html#quantile-treatment-effect-infinite-population-context"><i class="fa fa-check"></i><b>15.7.7</b> Quantile treatment effect (Infinite population context)</a></li>
<li class="chapter" data-level="15.7.8" data-path="experimetrics-te.html"><a href="experimetrics-te.html#covariates-if-not-stratified-in-completely-randomized-experiments"><i class="fa fa-check"></i><b>15.7.8</b> Covariates (if not stratified) in completely randomized experiments</a></li>
<li class="chapter" data-level="15.7.9" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomization-inference-and-regression-estimators"><i class="fa fa-check"></i><b>15.7.9</b> Randomization inference and regression estimators</a></li>
<li class="chapter" data-level="15.7.10" data-path="experimetrics-te.html"><a href="experimetrics-te.html#regression-estimators-with-additional-covariates-dr-seems-important"><i class="fa fa-check"></i><b>15.7.10</b> Regression Estimators with Additional Covariates [DR: seems important]</a></li>
<li class="chapter" data-level="15.7.11" data-path="experimetrics-te.html"><a href="experimetrics-te.html#stratified-randomized-experiments-analysis"><i class="fa fa-check"></i><b>15.7.11</b> Stratified randomized experiments: analysis</a></li>
<li class="chapter" data-level="15.7.12" data-path="experimetrics-te.html"><a href="experimetrics-te.html#the-design-of-randomised-experiments-and-the-benefits-of-stratification"><i class="fa fa-check"></i><b>15.7.12</b> 7 The Design of randomised experiments and the benefits of stratification</a></li>
<li class="chapter" data-level="15.7.13" data-path="experimetrics-te.html"><a href="experimetrics-te.html#power-calculations"><i class="fa fa-check"></i><b>15.7.13</b> 7.1 Power calculations</a></li>
<li class="chapter" data-level="15.7.14" data-path="experimetrics-te.html"><a href="experimetrics-te.html#stratified-randomized-experiments-benefits"><i class="fa fa-check"></i><b>15.7.14</b> Stratified randomized experiments: Benefits</a></li>
<li class="chapter" data-level="15.7.15" data-path="experimetrics-te.html"><a href="experimetrics-te.html#re-randomization"><i class="fa fa-check"></i><b>15.7.15</b> Re-randomization</a></li>
<li class="chapter" data-level="15.7.16" data-path="experimetrics-te.html"><a href="experimetrics-te.html#analysis-of-clustered-randomised-experiments"><i class="fa fa-check"></i><b>15.7.16</b> Analysis of Clustered Randomised Experiments</a></li>
<li class="chapter" data-level="15.7.17" data-path="experimetrics-te.html"><a href="experimetrics-te.html#noncompliance-in-randomized-experiments-dr-relevant-to-nl-lottery-not-to-charity-experiments"><i class="fa fa-check"></i><b>15.7.17</b> Noncompliance in randomized experiments (DR: Relevant to NL lottery, not to charity experiments)</a></li>
<li class="chapter" data-level="15.7.18" data-path="experimetrics-te.html"><a href="experimetrics-te.html#heterogenous-treatment-effects-and-pretreatment-variables"><i class="fa fa-check"></i><b>15.7.18</b> Heterogenous Treatment Effects and Pretreatment Variables</a></li>
<li class="chapter" data-level="15.7.19" data-path="experimetrics-te.html"><a href="experimetrics-te.html#data-driven-subgroup-analysis-recursive-partitioning-for-treatment-effects"><i class="fa fa-check"></i><b>15.7.19</b> 10.3.1 Data-driven Subgroup Analysis: Recursive Partitioning for Treatment Effects</a></li>
<li class="chapter" data-level="15.7.20" data-path="experimetrics-te.html"><a href="experimetrics-te.html#non-parametric-estimation-of-treatment-effect-heterogeneity"><i class="fa fa-check"></i><b>15.7.20</b> 10.3.2 Non-Parametric Estimation of Treatment Effect Heterogeneity</a></li>
<li class="chapter" data-level="15.7.21" data-path="experimetrics-te.html"><a href="experimetrics-te.html#treatment-effect-heterogeneity-using-regularized-regression"><i class="fa fa-check"></i><b>15.7.21</b> 10.3.3 Treatment Effect Heterogeneity Using Regularized Regression</a></li>
<li class="chapter" data-level="15.7.22" data-path="experimetrics-te.html"><a href="experimetrics-te.html#comparison-of-methods"><i class="fa fa-check"></i><b>15.7.22</b> 10.3.4 Comparison of Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="metaanalysis.html"><a href="metaanalysis.html"><i class="fa fa-check"></i><b>16</b> Meta-analysis and combining studies: Making inferences from previous work</a>
<ul>
<li class="chapter" data-level="16.1" data-path="metaanalysis.html"><a href="metaanalysis.html#notes-christensen-et-al-2019-ch-5-using-all-evidence-registration-and-meta-analysis"><i class="fa fa-check"></i><b>16.1</b> Notes: Christensen et al 2019, ch 5, ’Using all evidence, registration and meta-analysis</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="metaanalysis.html"><a href="metaanalysis.html#the-origins-and-importance-of-study-pre-registration"><i class="fa fa-check"></i><b>16.1.1</b> The origins [and importance] of study [pre-]registration</a></li>
<li class="chapter" data-level="16.1.2" data-path="metaanalysis.html"><a href="metaanalysis.html#social-science-study-registries"><i class="fa fa-check"></i><b>16.1.2</b> Social science study registries</a></li>
<li class="chapter" data-level="16.1.3" data-path="metaanalysis.html"><a href="metaanalysis.html#meta-analysis"><i class="fa fa-check"></i><b>16.1.3</b> Meta-analysis</a></li>
<li class="chapter" data-level="16.1.4" data-path="metaanalysis.html"><a href="metaanalysis.html#combining-estimates"><i class="fa fa-check"></i><b>16.1.4</b> Combining estimates</a></li>
<li class="chapter" data-level="16.1.5" data-path="metaanalysis.html"><a href="metaanalysis.html#heterogeneous-estimates"><i class="fa fa-check"></i><b>16.1.5</b> Heterogeneous estimates…</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="metaanalysis.html"><a href="metaanalysis.html#doing-meta"><i class="fa fa-check"></i><b>16.2</b> Excerpts and notes from ‘Doing Meta-Analysis in R: A Hands-on Guide’ (Harrer et al)</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="metaanalysis.html"><a href="metaanalysis.html#pooling-effect-sizes"><i class="fa fa-check"></i><b>16.2.1</b> Pooling effect sizes</a></li>
<li class="chapter" data-level="16.2.2" data-path="metaanalysis.html"><a href="metaanalysis.html#doing-bayes-meta"><i class="fa fa-check"></i><b>16.2.2</b> Bayesian Meta-analysis</a></li>
<li class="chapter" data-level="16.2.3" data-path="metaanalysis.html"><a href="metaanalysis.html#forest-plots"><i class="fa fa-check"></i><b>16.2.3</b> Forest plots</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="metaanalysis.html"><a href="metaanalysis.html#pubbias"><i class="fa fa-check"></i><b>16.3</b> Dealing with publication bias</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="metaanalysis.html"><a href="metaanalysis.html#diagnosis-and-responses-p-curves-funnel-plots-adjustments"><i class="fa fa-check"></i><b>16.3.1</b> Diagnosis and responses: P-curves, funnel plots, adjustments</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="metaanalysis.html"><a href="metaanalysis.html#other-notes-links-and-commentary"><i class="fa fa-check"></i><b>16.4</b> Other notes, links, and commentary</a></li>
<li class="chapter" data-level="16.5" data-path="metaanalysis.html"><a href="metaanalysis.html#other-resources-and-tools"><i class="fa fa-check"></i><b>16.5</b> Other resources and tools</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="metaanalysis.html"><a href="metaanalysis.html#institutional-and-systematic-guidelines"><i class="fa fa-check"></i><b>16.5.1</b> Institutional and systematic guidelines</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="metaanalysis.html"><a href="metaanalysis.html#example-discussion-of-meta-analyses-of-the-paleolithic-diet-below"><i class="fa fa-check"></i><b>16.6</b> Example: discussion of meta-analyses of the Paleolithic diet <span>BELOW</span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>17</b> Bayesian approaches</a>
<ul>
<li class="chapter" data-level="17.1" data-path="bayes.html"><a href="bayes.html#my-david-reinsteins-uses-for-bayesian-approaches-brainstorm"><i class="fa fa-check"></i><b>17.1</b> My (David Reinstein’s) uses for Bayesian approaches (brainstorm)</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="bayes.html"><a href="bayes.html#meta-analysis-of-previous-evidence"><i class="fa fa-check"></i><b>17.1.1</b> Meta-analysis of previous evidence</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayes.html"><a href="bayes.html#inference-particularly-about-null-effects"><i class="fa fa-check"></i><b>17.1.2</b> Inference, particularly about ‘null effects’</a></li>
<li class="chapter" data-level="17.1.3" data-path="bayes.html"><a href="bayes.html#policy-and-business-implications-and-recommendations"><i class="fa fa-check"></i><b>17.1.3</b> ‘Policy’ and business implications and recommendations</a></li>
<li class="chapter" data-level="17.1.4" data-path="bayes.html"><a href="bayes.html#theory-driven-inference-about-optimizing-agents-esp.-in-strategic-settings"><i class="fa fa-check"></i><b>17.1.4</b> Theory-driven inference about optimizing agents, esp. in strategic settings</a></li>
<li class="chapter" data-level="17.1.5" data-path="bayes.html"><a href="bayes.html#experimental-design"><i class="fa fa-check"></i><b>17.1.5</b> Experimental design</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="bayes.html"><a href="bayes.html#statistical-thinking-mcelreath-and-aj-kurtz-recoded-bookdown-highlights-and-notes"><i class="fa fa-check"></i><b>17.2</b> ‘Statistical thinking’ (McElreath) and <span>AJ Kurtz ‘recoded’ (bookdown)</span>: highlights and notes</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="bayes.html"><a href="bayes.html#the-golem-of-prague-chapter-1"><i class="fa fa-check"></i><b>17.2.1</b> The Golem of Prague (Chapter 1)</a></li>
<li class="chapter" data-level="17.2.2" data-path="bayes.html"><a href="bayes.html#small-worlds-and-large-worlds-ch-2"><i class="fa fa-check"></i><b>17.2.2</b> Small Worlds and Large Worlds (Ch 2)</a></li>
<li class="chapter" data-level="17.2.3" data-path="bayes.html"><a href="bayes.html#using-prior-information"><i class="fa fa-check"></i><b>17.2.3</b> Using prior information</a></li>
<li class="chapter" data-level="17.2.4" data-path="bayes.html"><a href="bayes.html#from-counts-to-probability."><i class="fa fa-check"></i><b>17.2.4</b> From counts to probability.</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="bayes.html"><a href="bayes.html#third-videochapter"><i class="fa fa-check"></i><b>17.3</b> Third video/chapter</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="bayes.html"><a href="bayes.html#normal-distributions"><i class="fa fa-check"></i><b>17.3.1</b> Normal distributions</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="bayes.html"><a href="bayes.html#title-introduction-to-bayesian-analysis-in-r-and-stata---katz-qstep"><i class="fa fa-check"></i><b>17.4</b> Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep”</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="bayes.html"><a href="bayes.html#why-and-when-use-bayesian-mcmc-methods"><i class="fa fa-check"></i><b>17.4.1</b> Why and when use Bayesian (MCMC) methods?</a></li>
<li class="chapter" data-level="17.4.2" data-path="bayes.html"><a href="bayes.html#theory"><i class="fa fa-check"></i><b>17.4.2</b> Theory</a></li>
<li class="chapter" data-level="17.4.3" data-path="bayes.html"><a href="bayes.html#comparing-models-equivalent-of-likelihood"><i class="fa fa-check"></i><b>17.4.3</b> Comparing models … Equivalent of ‘likelihood’</a></li>
<li class="chapter" data-level="17.4.4" data-path="bayes.html"><a href="bayes.html#on-choosing-priors"><i class="fa fa-check"></i><b>17.4.4</b> On choosing priors</a></li>
<li class="chapter" data-level="17.4.5" data-path="bayes.html"><a href="bayes.html#implementation"><i class="fa fa-check"></i><b>17.4.5</b> Implementation</a></li>
<li class="chapter" data-level="17.4.6" data-path="bayes.html"><a href="bayes.html#generate-predictions-from-a-winbugs-model"><i class="fa fa-check"></i><b>17.4.6</b> Generate predictions from a WinBUGS model</a></li>
<li class="chapter" data-level="17.4.7" data-path="bayes.html"><a href="bayes.html#missing-data-case"><i class="fa fa-check"></i><b>17.4.7</b> Missing data case</a></li>
<li class="chapter" data-level="17.4.8" data-path="bayes.html"><a href="bayes.html#stata"><i class="fa fa-check"></i><b>17.4.8</b> Stata</a></li>
<li class="chapter" data-level="17.4.9" data-path="bayes.html"><a href="bayes.html#r-mcmc-pac"><i class="fa fa-check"></i><b>17.4.9</b> R mcmc pac</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="bayes.html"><a href="bayes.html#other-resources-and-notes-to-integrate"><i class="fa fa-check"></i><b>17.5</b> Other resources and notes to integrate</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="n-ds4bs.html"><a href="n-ds4bs.html"><i class="fa fa-check"></i><b>18</b> Notes on Data Science for Business by Foster Provost and Tom Fawcett (2013)</a>
<ul>
<li class="chapter" data-level="18.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#evaluation-of-this-resource"><i class="fa fa-check"></i><b>18.1</b> Evaluation of this resource</a></li>
<li class="chapter" data-level="" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ch-1-introduction-data-analytic-thinking"><i class="fa fa-check"></i>Ch 1 Introduction: Data-Analytic Thinking</a>
<ul>
<li class="chapter" data-level="" data-path="n-ds4bs.html"><a href="n-ds4bs.html#example-during-hurricane-frances-predicting-demand-to-gear-inventory-and-avoid-shortages-lead-to-huge-profit-for-wal-mart"><i class="fa fa-check"></i>Example: During Hurricane Frances… predicting demand to gear inventory and avoid shortages … lead to huge profit for Wal-Mart</a></li>
<li class="chapter" data-level="" data-path="n-ds4bs.html"><a href="n-ds4bs.html#example-predicting-customer-churn"><i class="fa fa-check"></i>Example: Predicting Customer Churn</a></li>
<li class="chapter" data-level="18.1.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#data-science-engineering-and-data-driven-decision-making"><i class="fa fa-check"></i><b>18.1.1</b> Data Science, Engineering, and Data-Driven Decision Making</a></li>
<li class="chapter" data-level="18.1.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#data-processing-and-big-data"><i class="fa fa-check"></i><b>18.1.2</b> Data Processing and “Big Data”</a></li>
<li class="chapter" data-level="18.1.3" data-path="n-ds4bs.html"><a href="n-ds4bs.html#data-asset"><i class="fa fa-check"></i><b>18.1.3</b> Data and Data Science Capability as a <strong>Strategic Asset</strong></a></li>
<li class="chapter" data-level="18.1.4" data-path="n-ds4bs.html"><a href="n-ds4bs.html#da-thinking"><i class="fa fa-check"></i><b>18.1.4</b> Data-Analytic Thinking</a></li>
<li class="chapter" data-level="18.1.5" data-path="n-ds4bs.html"><a href="n-ds4bs.html#data-mining-and-data-science-revisited"><i class="fa fa-check"></i><b>18.1.5</b> Data Mining and Data Science, Revisited</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ds4bs-ch2"><i class="fa fa-check"></i><b>18.2</b> Ch 2 Business Problems and Data Science Solutions</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#types-of-problems-and-approaches"><i class="fa fa-check"></i><b>18.2.1</b> Types of problems and approaches</a></li>
<li class="chapter" data-level="18.2.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#data-mining-process"><i class="fa fa-check"></i><b>18.2.2</b> The Data Mining Process</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ch-3-introduction-to-predictive-modeling-from-correlation-to-supervised-segmentation"><i class="fa fa-check"></i><b>18.3</b> Ch 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#models-induction-and-prediction"><i class="fa fa-check"></i><b>18.3.1</b> Models, Induction, and Prediction</a></li>
<li class="chapter" data-level="18.3.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#supervised-segmentation"><i class="fa fa-check"></i><b>18.3.2</b> Supervised Segmentation</a></li>
<li class="chapter" data-level="18.3.3" data-path="n-ds4bs.html"><a href="n-ds4bs.html#summary-1"><i class="fa fa-check"></i><b>18.3.3</b> Summary</a></li>
<li class="chapter" data-level="18.3.4" data-path="n-ds4bs.html"><a href="n-ds4bs.html#note-check-if-there-is-a-gap-here"><i class="fa fa-check"></i><b>18.3.4</b> NOTE – check if there is a gap here</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ds4bs-model-to-data"><i class="fa fa-check"></i><b>18.4</b> Ch. 4: Fitting a Model to Data</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#classification-via-mathematical-functions"><i class="fa fa-check"></i><b>18.4.1</b> Classification via Mathematical Functions</a></li>
<li class="chapter" data-level="18.4.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#regression-via-mathematical-functions"><i class="fa fa-check"></i><b>18.4.2</b> Regression via Mathematical Functions</a></li>
<li class="chapter" data-level="18.4.3" data-path="n-ds4bs.html"><a href="n-ds4bs.html#class-probability-estimation-and-logistic-regression"><i class="fa fa-check"></i><b>18.4.3</b> Class Probability Estimation and Logistic Regression</a></li>
<li class="chapter" data-level="18.4.4" data-path="n-ds4bs.html"><a href="n-ds4bs.html#logistic-regression-some-technical-details"><i class="fa fa-check"></i><b>18.4.4</b> Logistic Regression: Some Technical Details</a></li>
<li class="chapter" data-level="18.4.5" data-path="n-ds4bs.html"><a href="n-ds4bs.html#example-logistic-regression-versus-tree-induction"><i class="fa fa-check"></i><b>18.4.5</b> Example: Logistic Regression versus Tree Induction</a></li>
<li class="chapter" data-level="18.4.6" data-path="n-ds4bs.html"><a href="n-ds4bs.html#nonlinear-functions-support-vector-machines-and-neural-networksthe-two-most-common-families-of-techniques-that-are-based-on-fitting-the-parameters-of-complex-nonlinear-functions-are-nonlinear-supportvector-machines-and-neural-networks."><i class="fa fa-check"></i><b>18.4.6</b> Nonlinear Functions, Support Vector Machines, and Neural NetworksThe two most common families of techniques that are based on fitting the parameters of complex, nonlinear functions are nonlinear supportvector machines and neural networks.</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ds4bs-overfitting"><i class="fa fa-check"></i><b>18.5</b> Ch 5: Overfitting and its avoidance</a>
<ul>
<li class="chapter" data-level="18.5.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#generalization"><i class="fa fa-check"></i><b>18.5.1</b> Generalization</a></li>
<li class="chapter" data-level="18.5.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#holdout-data-and-fitting-graphs"><i class="fa fa-check"></i><b>18.5.2</b> Holdout Data and Fitting Graphs</a></li>
<li class="chapter" data-level="18.5.3" data-path="n-ds4bs.html"><a href="n-ds4bs.html#example-overfitting-linear-functions"><i class="fa fa-check"></i><b>18.5.3</b> Example: Overfitting Linear Functions</a></li>
<li class="chapter" data-level="18.5.4" data-path="n-ds4bs.html"><a href="n-ds4bs.html#example-why-is-overfitting-bad"><i class="fa fa-check"></i><b>18.5.4</b> Example: Why Is Overfitting Bad?</a></li>
<li class="chapter" data-level="18.5.5" data-path="n-ds4bs.html"><a href="n-ds4bs.html#from-holdout-evaluation-to-cross-validation"><i class="fa fa-check"></i><b>18.5.5</b> From Holdout Evaluation to Cross-Validation</a></li>
<li class="chapter" data-level="18.5.6" data-path="n-ds4bs.html"><a href="n-ds4bs.html#learning-curves"><i class="fa fa-check"></i><b>18.5.6</b> Learning Curves</a></li>
<li class="chapter" data-level="18.5.7" data-path="n-ds4bs.html"><a href="n-ds4bs.html#avoiding-overfitting-with-tree-induction"><i class="fa fa-check"></i><b>18.5.7</b> Avoiding Overfitting with Tree Induction</a></li>
<li class="chapter" data-level="18.5.8" data-path="n-ds4bs.html"><a href="n-ds4bs.html#a-general-method-for-avoiding-overfitting"><i class="fa fa-check"></i><b>18.5.8</b> A General Method for Avoiding Overfitting</a></li>
<li class="chapter" data-level="18.5.9" data-path="n-ds4bs.html"><a href="n-ds4bs.html#a-general-method-for-avoiding-overfitting-1"><i class="fa fa-check"></i><b>18.5.9</b> A General Method for Avoiding Overfitting</a></li>
<li class="chapter" data-level="18.5.10" data-path="n-ds4bs.html"><a href="n-ds4bs.html#avoiding-overfitting-for-parameter-optimization"><i class="fa fa-check"></i><b>18.5.10</b> Avoiding Overfitting for Parameter Optimization</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ds4bs-similarity"><i class="fa fa-check"></i><b>18.6</b> Ch 6.: Similarity, Neighbors, and Clusters</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#similarity-and-distance"><i class="fa fa-check"></i><b>18.6.1</b> Similarity and Distance</a></li>
<li class="chapter" data-level="18.6.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#similarity-and-distance-1"><i class="fa fa-check"></i><b>18.6.2</b> Similarity and Distance</a></li>
<li class="chapter" data-level="18.6.3" data-path="n-ds4bs.html"><a href="n-ds4bs.html#example-whiskey-analytics"><i class="fa fa-check"></i><b>18.6.3</b> Example: Whiskey Analytics</a></li>
<li class="chapter" data-level="18.6.4" data-path="n-ds4bs.html"><a href="n-ds4bs.html#nearest-neighbors-for-predictive-modeling"><i class="fa fa-check"></i><b>18.6.4</b> Nearest Neighbors for Predictive Modeling</a></li>
<li class="chapter" data-level="18.6.5" data-path="n-ds4bs.html"><a href="n-ds4bs.html#how-many-neighbors-and-how-much-influence"><i class="fa fa-check"></i><b>18.6.5</b> How Many Neighbors and How Much Influence?</a></li>
<li class="chapter" data-level="18.6.6" data-path="n-ds4bs.html"><a href="n-ds4bs.html#geometric-interpretation-overfitting-and-complexity-control"><i class="fa fa-check"></i><b>18.6.6</b> Geometric Interpretation, Overfitting, and Complexity Control</a></li>
<li class="chapter" data-level="18.6.7" data-path="n-ds4bs.html"><a href="n-ds4bs.html#issues-with-nearest-neighbor-methods"><i class="fa fa-check"></i><b>18.6.7</b> Issues with Nearest-Neighbor Methods</a></li>
<li class="chapter" data-level="18.6.8" data-path="n-ds4bs.html"><a href="n-ds4bs.html#other-distance-functions"><i class="fa fa-check"></i><b>18.6.8</b> Other Distance Functions</a></li>
<li class="chapter" data-level="18.6.9" data-path="n-ds4bs.html"><a href="n-ds4bs.html#stepping-back-solving-a-business-problem-versus-data-exploration"><i class="fa fa-check"></i><b>18.6.9</b> Stepping Back: Solving a Business Problem Versus Data Exploration</a></li>
<li class="chapter" data-level="18.6.10" data-path="n-ds4bs.html"><a href="n-ds4bs.html#summary-2"><i class="fa fa-check"></i><b>18.6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ds4bs-decision-thinking"><i class="fa fa-check"></i><b>18.7</b> Ch. 7. Decision Analytic Thinking I: What Is a Good Model?</a>
<ul>
<li class="chapter" data-level="18.7.1" data-path="n-ds4bs.html"><a href="n-ds4bs.html#evaluating-classifier"><i class="fa fa-check"></i><b>18.7.1</b> Evaluating Classifier</a></li>
<li class="chapter" data-level="18.7.2" data-path="n-ds4bs.html"><a href="n-ds4bs.html#the-confusion-matrix"><i class="fa fa-check"></i><b>18.7.2</b> The Confusion Matrix</a></li>
<li class="chapter" data-level="18.7.3" data-path="n-ds4bs.html"><a href="n-ds4bs.html#problems-with-unbalanced-classes"><i class="fa fa-check"></i><b>18.7.3</b> Problems with Unbalanced Classes</a></li>
<li class="chapter" data-level="18.7.4" data-path="n-ds4bs.html"><a href="n-ds4bs.html#generalizing-beyond-classification"><i class="fa fa-check"></i><b>18.7.4</b> Generalizing Beyond Classification</a></li>
<li class="chapter" data-level="18.7.5" data-path="n-ds4bs.html"><a href="n-ds4bs.html#a-key-analytical-framework-expected-value"><i class="fa fa-check"></i><b>18.7.5</b> A Key Analytical Framework: Expected Value</a></li>
<li class="chapter" data-level="18.7.6" data-path="n-ds4bs.html"><a href="n-ds4bs.html#using-expected-value-to-frame-classifier-use"><i class="fa fa-check"></i><b>18.7.6</b> Using Expected Value to Frame Classifier Use</a></li>
<li class="chapter" data-level="18.7.7" data-path="n-ds4bs.html"><a href="n-ds4bs.html#using-expected-value-to-frame-classifier-evaluation"><i class="fa fa-check"></i><b>18.7.7</b> Using Expected Value to Frame Classifier Evaluation</a></li>
<li class="chapter" data-level="18.7.8" data-path="n-ds4bs.html"><a href="n-ds4bs.html#evaluation-baseline-performance-and-implications-for-investments-in-data"><i class="fa fa-check"></i><b>18.7.8</b> Evaluation, Baseline Performance, and Implications for Investments in Data</a></li>
<li class="chapter" data-level="18.7.9" data-path="n-ds4bs.html"><a href="n-ds4bs.html#summary-3"><i class="fa fa-check"></i><b>18.7.9</b> Summary</a></li>
<li class="chapter" data-level="18.7.10" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ranking-instead-of-classifying"><i class="fa fa-check"></i><b>18.7.10</b> Ranking Instead of Classifying</a></li>
<li class="chapter" data-level="18.7.11" data-path="n-ds4bs.html"><a href="n-ds4bs.html#profit-curves"><i class="fa fa-check"></i><b>18.7.11</b> Profit Curves</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="n-ds4bs.html"><a href="n-ds4bs.html#ds4bs-contents"><i class="fa fa-check"></i><b>18.8</b> Contents and consideration</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="paleo-example.html"><a href="paleo-example.html"><i class="fa fa-check"></i><b>19</b> Meta-analysis arbitrary example: the ‘Paleo diet’</a>
<ul>
<li class="chapter" data-level="19.1" data-path="conceptual.html"><a href="conceptual.html#conceptual"><i class="fa fa-check"></i><b>19.1</b> Conceptual: Thoughts on nutritional studies and meta-analysis issues</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="paleo-example.html"><a href="paleo-example.html#compliance"><i class="fa fa-check"></i><b>19.1.1</b> Limited compliance; ‘what are we aiming to measure and why?’</a></li>
<li class="chapter" data-level="19.1.2" data-path="paleo-example.html"><a href="paleo-example.html#control-group-what-is-being-measured"><i class="fa fa-check"></i><b>19.1.2</b> Control group: what is being measured?</a></li>
<li class="chapter" data-level="19.1.3" data-path="paleo-example.html"><a href="paleo-example.html#what-is-being-tested-and-how-broadly-should-we-interpret-the-results"><i class="fa fa-check"></i><b>19.1.3</b> What is being tested and how broadly should we interpret the results?</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="paleo-example.html"><a href="paleo-example.html#manheimer"><i class="fa fa-check"></i><b>19.2</b> Manheimer et al</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="paleo-example.html"><a href="paleo-example.html#strengths-and-limitations"><i class="fa fa-check"></i><b>19.2.1</b> Strengths and limitations</a></li>
<li class="chapter" data-level="19.2.2" data-path="paleo-example.html"><a href="paleo-example.html#overall-results-interpretation-consideration-of-evidence-presented-in-manheimerpaleolithicnutritionmetabolic2015"><i class="fa fa-check"></i><b>19.2.2</b> Overall results, interpretation, consideration of evidence presented in <span class="citation">Manheimer et al. (<span>2015</span>)</span></a></li>
<li class="chapter" data-level="19.2.3" data-path="paleo-example.html"><a href="paleo-example.html#my-rough-conclusions-from-manheimer-et-al"><i class="fa fa-check"></i><b>19.2.3</b> My rough conclusions from Manheimer et al</a></li>
<li class="chapter" data-level="19.2.4" data-path="paleo-example.html"><a href="paleo-example.html#critiques"><i class="fa fa-check"></i><b>19.2.4</b> External critiques and evaluations of Manheimer et al, (esp Fenton) authors’ response</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="paleo-example.html"><a href="paleo-example.html#other-meta-analyses-and-consideration-of-the-paleo-diet"><i class="fa fa-check"></i><b>19.3</b> Other meta-analyses and consideration of the Paleo diet</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="paleo-example.html"><a href="paleo-example.html#process-of-finding-relevant-work-informal"><i class="fa fa-check"></i><b>19.3.1</b> Process of finding relevant work (informal)</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="paleo-example.html"><a href="paleo-example.html#boers"><i class="fa fa-check"></i><b>19.4</b> Focus: Boers et al</a></li>
<li class="chapter" data-level="19.5" data-path="paleo-example.html"><a href="paleo-example.html#overall-analysis"><i class="fa fa-check"></i><b>19.5</b> Overall analysis</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="paleo-example.html"><a href="paleo-example.html#limitations-p"><i class="fa fa-check"></i><b>19.5.1</b> Limitations and uncertainties to my own analysis; proposed future steps</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>20</b> List of references</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experimetrics_te" class="section level1" number="15">
<h1><span class="header-section-number">15</span> ‘Experimetrics’ and measurement of treatment effects from RCTs</h1>
<div id="which-error-structure-random-effects" class="section level2" number="15.1">
<h2><span class="header-section-number">15.1</span> Which error structure? Random effects?</h2>
</div>
<div id="randomization-inference" class="section level2" number="15.2">
<h2><span class="header-section-number">15.2</span> Randomization inference?</h2>
</div>
<div id="parametric-and-nonparametric-tests-of-simple-hypotheses" class="section level2" number="15.3">
<h2><span class="header-section-number">15.3</span> Parametric and nonparametric tests of simple hypotheses</h2>
<div id="parametric-tests" class="section level3" number="15.3.1">
<h3><span class="header-section-number">15.3.1</span> Parametric tests</h3>
</div>
<div id="non-parametric-tests" class="section level3" number="15.3.2">
<h3><span class="header-section-number">15.3.2</span> Non-parametric tests</h3>
<div id="fligner-policello-test" class="section level4" number="15.3.2.1">
<h4><span class="header-section-number">15.3.2.1</span> Fligner-Policello test</h4>
<p><span class="citation">Randles et al. (<a href="#ref-randlesAsymptoticallyDistributionfreeTest1980" role="doc-biblioref">1980</a>)</span></p>
<p>Null-hypothesis that the medians in the two groups (samples) are the same.</p>
<blockquote>
<p>Note on assumptions: Under the null, it assumes that the the distributions are symmetric It does not assume that the shape of the distribution is similar in two groups, contrary to the Mann-Whitney-Wilcoxon test.</p>
</blockquote>
</div>
</div>
</div>
<div id="adjustments-for-exogenous-but-non-random-treatment-assignment" class="section level2" number="15.4">
<h2><span class="header-section-number">15.4</span> Adjustments for exogenous (but non-random) treatment assignment</h2>
</div>
<div id="iv-in-an-experimental-context-to-get-at-mediators" class="section level2" number="15.5">
<h2><span class="header-section-number">15.5</span> IV in an experimental context to get at ‘mediators’?</h2>
</div>
<div id="heterogeneity-in-an-experimental-context" class="section level2" number="15.6">
<h2><span class="header-section-number">15.6</span> Heterogeneity in an experimental context</h2>
</div>
<div id="incorporate-above-notes-on-the-econometrics-of-randomised-experiments-athey-and-imbens" class="section level2" number="15.7">
<h2><span class="header-section-number">15.7</span> Incorporate above: Notes on “The econometrics of randomised experiments” (Athey and Imbens)</h2>
<p>(with an eye towards giving experiments_</p>
<p>Page 7</p>
<blockquote>
<p>Fundamentally, most concerns with external validity are related to treatment effect heterogeneity …
[ considering extrapolation between settings A and B] Units in the two settings may differ in observed or unobserved characteristics, or treatments may differ in some aspect.
to assess these issues it is helpful to have … randomized experiments, in multiple settings [varying] in the distribution of characteristics of the units, and possibly … the nature of the treatments or the treatment rate, in order to assess the credibility of generalizing to other settings</p>
</blockquote>
<ul>
<li><p>Shall we do Fisher’s test based on computing the distribution of differences in means randomly reassigning the “treatment”?</p></li>
<li><p>F-tests to consider multiple outcomes for any cases?</p></li>
<li><p>(When) shall we use covariates (esp those for interactions) in the ‘deviations from mean’ form?</p></li>
<li><p>Considering when to use controls (and interactions?)</p></li>
</ul>
<blockquote>
<p>the asymptotic variance for <span class="math inline">\(\hat{\tau}}\)</span> is less than that of the simple difference estimator by a factor equal to <span class="math inline">\(1-R^2\)</span> from including the covariates relative to not including the covariates
If… the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial</p>
</blockquote>
<ul>
<li><p>DR: Could there not ever be a loss from doing interactions dividing up the sample too fine in doing this interactive estimation? This should depend on the true <span class="math inline">\(R^2\)</span> I think. Try to remember what is the real tradeoff?</p></li>
<li><p>Statistics adjusted for stratification:</p></li>
</ul>
<blockquote>
<p>One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance</p>
</blockquote>
<div class="marginnote">
DR: Is it valid to simply say “we choose the lower value of the estimated variances”? Are they advocating this? Such a procedure seems like it would have a bias.
</div>
<p>** Things to potential incorporate in NL HE lottery paper(s) **</p>
<ul>
<li>(When) shall we use covariates (esp those for interactions) in the ‘deviations from mean’ form?</li>
</ul>
<p>“randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received”</p>
<ul>
<li><p>Consider a “partial identification or bounds analysis” to deal with noncompliance at each margin</p></li>
<li><p>Look up “randomization-based approach to IV” (Imbens and Rosenbaum, 2005)</p></li>
</ul>
<div id="abstract-and-intro" class="section level3" number="15.7.1">
<h3><span class="header-section-number">15.7.1</span> Abstract and intro</h3>
<ol style="list-style-type: decimal">
<li>randomisation-based inference as opposed to sampling based inference</li>
</ol>
<ul>
<li>DR: I Disagree as the object of interest is ultimately not the experimental sample, particularly not in the lab</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>efficiency gains from stratification into small strata, adjust se to capture these gains (We have only done this to a limited extent)</li>
<li>(Non-compliance, intention-to-treat, and IV)</li>
<li>Estimation and inference for <strong>heterogeneous treatment with covariates… </strong> subpopulations… Maintaining the ability to construct valid confidence (mht etc). “Conditional average treatment effects”</li>
<li>(interaction between units)</li>
</ol>
<p>Why careful statistics are important even for randomised experiments</p>
<p>“Randomisation approach”: potential outcomes fixed, Assignment to treatments random</p>
<p>Example of why the randomisation-based inference approach matters:</p>
<p>“in the conventional sampling paradigm… Controlling for observable heterogeneity using a regression model” required for the assumptions to be justified with this approach. With randomisation approach it makes more sense to put data into strata by covariates, analyse within-group experiments and average results.</p>
<p>Recommend small strata but not too small as variances cannot be estimated within pairs</p>
<ul>
<li><p>DR: Section 10 on heterogeneity is particularly relevant for us</p></li>
<li><p>Other experimetrics methodology surveys mentioned (Duflo et al ’06, Glennerster and T ’13, Glennerster ’16); present one is more theoretical</p></li>
</ul>
</div>
<div id="randomised-experiments-and-validity" class="section level3" number="15.7.2">
<h3><span class="header-section-number">15.7.2</span> Randomised experiments and validity</h3>
<p>Defined as settings “where the assignment mechanism does not depend on characteristics of the units”
- That seems to be “pure randomisation”</p>
<p>… Debate about the supremacy of randomised experiments</p>
<p>Definition of internal validity (DR:: it is a bit imprecise here).</p>
<p><br />
</p>
<p>Typical argument about how external validity is no more guaranteed in observational studies then and randomised experiments “there is nothing in non-experimental methods which made some superior randomised experiments with the same population and sample size in this regard.”</p>
<p>-DR: I think this is a bit of a strawman and a weak argument here
- GR: This is the Deaton argument, very strange</p>
<p>They argue for experiments in multiple settings varying in the characteristics of the units and perhaps the treatments to assess the credibility of generalizing to other settings.</p>
<p><br />
</p>
<p>(?Graphical methods to deal with external validity issues?)</p>
<p><br />
</p>
<p><strong>Finite population versus random sample from super-population</strong></p>
<p>We can interpret the uncertainty as unobserved potential outcomes rather than sampling uncertainty.</p>
<ul>
<li>DR: I don’t see why these are mutually exclusive.</li>
<li>GR: agreed</li>
<li>DR: Viewing the sample of the full population of interest may not even work I’m considering some experiments such as those using within subject treatments</li>
</ul>
<p><br />
</p>
<p>The differences in these approaches matter in some settings but not others.
Sometimes “…conventional sampling-based standard errors will be unnecessarily conservative”</p>
<ul>
<li>DR: This could be helpful</li>
</ul>
</div>
<div id="potential-outcomes-rubin-causal-model-framework-covered-earlier" class="section level3" number="15.7.3">
<h3><span class="header-section-number">15.7.3</span> Potential outcomes/ Rubin causal model framework (covered earlier)</h3>
<p>(This is somewhat familiar by now)</p>
<p><br />
</p>
<p><strong>Potential outcomes</strong></p>
<p>If we do not impose limitations on interactions between units (like SUTVA) there will be a dimensionality problem</p>
<p><span class="math display">\[p\:\{0,1\}^N \times Y^{2N}\times X^N \rightarrow [0,1]\]</span></p>
<ul>
<li>DR: I am not sure I understand this notation, particularly the <span class="math inline">\(\{0,1\}\)</span> bit</li>
</ul>
<p><br />
</p>
<p>" For randomized experiments we disallow dependence on the potential outcomes, and we assume that the functional form of the assignment mechanism is known"
(this goes further than “completely randomized”)</p>
</div>
<div id="classification-of-assignment-mechanisms" class="section level3" number="15.7.4">
<h3><span class="header-section-number">15.7.4</span> 3.2 Classification of assignment mechanisms</h3>
<p>(Formula for probabilities of each assignment combination of control and treatment given for each one)</p>
<ol style="list-style-type: decimal">
<li><p>Completely Randomized: <span class="math inline">\(N_t\)</span> Units drawn at random from a population of <span class="math inline">\(N\)</span> to receive the treatment, remaining <span class="math inline">\(N_c\)</span> get control.</p></li>
<li><p>Stratified randomized: Partition into <span class="math inline">\(G\)</span> strata based on covariates values, “Disallowing assignments that are likely to be uninformative about the treatment effects of interest”</p></li>
<li><p>Paired randomized (Extreme stratification)</p></li>
<li><p>Cluster randomized: Treatments assigned randomly to entire clusters. Maybe cheaper to implement and more valid in the presence of interactions between units within but not across clusters.</p></li>
</ol>
</div>
<div id="the-analysis-of-completely-randomized-experiments" class="section level3" number="15.7.5">
<h3><span class="header-section-number">15.7.5</span> The analysis of Completely randomized experiments</h3>

<div class="note">
Aside: see the limitations of the Fisher test when not considering a lady tasting tea (known shares of outcomes). One response: <a href="https://statmodeling.stat.columbia.edu/2009/10/13/what_is_the_bay/">a Bayesian approach</a>
</div>
<p><strong>Exact p-values for sharp null hypotheses (Fisher etc)</strong></p>
<p>“Sharp”: “Under which we can an for all the missing potential outcomes from the observed data” … So we can infer the distribution of any statistics under the Null.</p>
<p>E.g., <span class="math inline">\(H0\)</span>, The treatment has no effect <span class="math inline">\(Y_i(0)=Y_i(1)\forall i\)</span>, vs <span class="math inline">\(Ha\)</span>, At least one unit i has <span class="math inline">\(Y_i(0)\neq Y_i(1)\)</span></p>
<p><br />
</p>
<p>Difference in means by treatment status: Calculate the probability over the randomization distribution of a value with as large an absolute value as the one observed given the actual assignments.</p>
<p>This is done by reassigning what we call the “treatments” to all possible combinations (keeping the number of treated units constant) and calculating the “placebo” treatment effect. Calculate the fraction of assignment vectors with statistic at least as large (in absolute value) as the observed one.</p>
<ul>
<li><p>DR: What is the statistic called and is there preprogrammed code? Is it the Fisher’s exact test?</p></li>
<li><p>Can do for means or means of the ranks by treatment status (rank sum?) or any stat.</p>
<ul>
<li>Latter is less sensitive to outliers and thick-tailed distributions</li>
</ul></li>
</ul>
<p><br />
</p>
<p>With multiple outcomes, multiple comparisons issues</p>
<ul>
<li><p>Use statistics it takes into account all the outcomes (e.g., F-stat, calculate exact P value using the ‘Fisher randomization distribution’ as in Young, ’16)</p></li>
<li><p>Or use adjustments to P values e.g., Bonferroni or tighter bounds (Which are still more conservative than the Fisher thing); Romano ea survey (2010)</p></li>
<li><p>Rosenbaum ’92 on estimating treatment effects based on rank statistics (DR: I don’t get this at all)</p></li>
</ul>
</div>
<div id="randomization-inference-for-average-treatment-effects" class="section level3" number="15.7.6">
<h3><span class="header-section-number">15.7.6</span> Randomization inference for Average treatment effects</h3>
<p>Neyman wanted to estimate the ATE for the sample at hand</p>
<p><span class="math display">\[\tau=\frac{1}{N}\sum_{i=1..N}{(Y_i(1)-Y_i(0)i)}=\bar{Y}(1)-\bar{Y}(0)\]</span></p>
<ul>
<li><p>DR: Again, this is really not what we care about particularly not in a small-scale experiment.</p>
<p><br />
</p></li>
</ul>
<p>Proposed the estimator “Difference in average outcomes by treatment status” (DR: Same as in last section)</p>
<p><br />
</p>
<p>Defining <span class="math inline">\(D_i\)</span>, a term representing “assignment minus the average assignment”</p>
<p>Allows a restatement of the estimator which makes it clear that this is unbiased for the average treatment effect <span class="math inline">\(\tau\)</span></p>
<p><span class="math display">\[\hat{i\tau}=\tau+\frac{1}{N}\sum_{i=1..N}{(D_i(\frac{N}{N_t}Y_i(1)+\frac{N}{N_c}Y_i(0)}\]</span></p>
<p>Sampling variance of <span class="math inline">\(\hat{\tau}\)</span> over the randomization distribution decomposed as</p>
<p><span class="math display">\[V({\hat{\tau})=\frac{S_c^2}{N_c}+\frac{S_t^2}{N_t}-\frac{S_{tc}^2}{N}\]</span></p>
<p>Where <span class="math inline">\(S_c^2\)</span> and <span class="math inline">\(S_t^2\)</span> Are the variances of the control and treated outcomes,
and <span class="math inline">\(S_{tc}^2\)</span> is the variance of the unit level treatment effect (DR: This must be related to the covariance)</p>
<p>We can estimate rhe first two terms but not the latter term as we have no observations with both a control and a treatment.</p>
<p><br />
</p>
<p>In practice researchers ignore the third term, which leads to an upward bias for the <em>sample</em> treatment effect but an unbiased estimator of the population ATE</p>
<ul>
<li>DR: Any intuition for this?</li>
</ul>
<p><br />
</p>
<p>We still need to make large sample approximations to construct confidence intervals for the ATE.</p>
<ul>
<li>DR: Does this yield any practical strategy for us to use?</li>
</ul>
</div>
<div id="quantile-treatment-effect-infinite-population-context" class="section level3" number="15.7.7">
<h3><span class="header-section-number">15.7.7</span> Quantile treatment effect (Infinite population context)</h3>
<p>Usefulness:</p>
<ol style="list-style-type: decimal">
<li><p>Uncover “Treatment effects in the tails”</p></li>
<li><p>Results robust to thick tails</p></li>
</ol>
<p>S-th quantile treatment effect defined as the difference in quantiles between the <span class="math inline">\(Y_i(1)\)</span> and<span class="math inline">\(Y_i(0)\)</span> distributions:</p>
<p><span class="math display">\[\tau_s=q_{Y(1)}(s)-q_{Y(0)}(s)\]</span> …</p>
<ul>
<li>this is distinct from the “quantile of the differences”: <span class="math inline">\(q_{Y(1)-Y(0)}(s)\)</span>, which is in general not identified
<ul>
<li>DR: the letter is truly more interesting; we care about the distribution of the <em>impact of the treatment</em> and not so much about the impact of the treatment on the distribution of outcomes.</li>
</ul></li>
<li>the two are equal if there is “perfect rank correlation between the two potential outcomes” (DR: I think this simply means that the unit ranked n’th if not treated would also be the unit ranked n’th if treated … no crossing over).</li>
</ul>
<p><br />
</p>
<p>Making lemonade: they argue here that the (identifiable) difference in quantiles would be more interesting to a policymaker considering exposing all units to the treatment … (DR: presumably because she should not care <em>who</em> get the particular outcome but only about the distribution of outcomes, a common axiom for social
welfare functions).</p>
<p><br />
</p>
<p>Estimates and tests: use the difference in quantiles as a statistic in and exact P value computation … results for such exact tests are quite different than those based on estimated effects and standard errors because “Quantile estimates are far from normally distributed.”</p>
</div>
<div id="covariates-if-not-stratified-in-completely-randomized-experiments" class="section level3" number="15.7.8">
<h3><span class="header-section-number">15.7.8</span> Covariates (if not stratified) in completely randomized experiments</h3>
<p>(They strongly recommend stratifying instead of ex post controls.)</p>
<p>Why use controls if a simple difference in means is unbiased for the ATE?</p>
<ol style="list-style-type: decimal">
<li>“incorporating covariates may make analyses more informative” (greater precision)</li>
</ol>
<ul>
<li><p>Can incorporate covariates in exact P value analysis, or estimate average treatment effects within subpopulations and average these up appropriately</p></li>
<li><p>DR: How to do these things in practice?</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>correcting for compromised randomization … which may occur because of missing data and selective attrition</li>
</ol>
<p><br />
They give an example with the data from Lalonde where they estimate the average treatment effect for two groups those with and without prior earnings. They then add these up weighted by the estimated probability of being in each group.</p>
<ul>
<li>DR: I understand this correctly, as they do not define all variables
<ul>
<li>also, how do they compute the standard error of the combined estimator here?!</li>
<li>this seems more like an interaction than a standard control here, which would allow a different intercept (control outcome) but not a different treatment effect.</li>
</ul></li>
</ul>
<p>? se of
<span class="math display">\[\hat{p}(\bar{Y}_t|Y^{t-1}=0-\bar{Y}_c|Y^{t-1}=0)+(1-\hat{p})(\bar{Y}_t|Y^{t-1}=1-\bar{Y}_c|Y^{t-1}=1)\]</span></p>
<ul>
<li><p>DR: I think there is a simple formula for difference in means that could be applied to this</p>
<p><br />
</p></li>
</ul>
</div>
<div id="randomization-inference-and-regression-estimators" class="section level3" number="15.7.9">
<h3><span class="header-section-number">15.7.9</span> Randomization inference and regression estimators</h3>
<p>They urge caution in using reg. “Since randomization does not justify the models, almost anything can happen” (Freedman 08)</p>
<p>But using only “indicator variables based on partitioning the covariate space” preserves many of the finite simple properties of simple comparisons of means.<br />
</p>
<p><strong>Regression estimators for average treatment effects</strong></p>
<p>With a single variable, the least-squares estimate of <span class="math inline">\(\tau\)</span> is identical to the simple difference in means:</p>
<p><span class="math display">\[\hat{tau}_{ols} = \bar{Y^o}_t - \bar{Y}^o_c\]</span></p>
<p>The intercept is the control value of course: <span class="math inline">\(\hat{\alpha}_{ols}=\bar{Y}^o-\hat{\tau_{ols}}\bar{W}=\bar{Y^0}_c\)</span>.</p>
<p><em>Conceptually important:</em></p>
<blockquote>
<p>the unbiasedness claim in the Neyman analysis is conceptually different from the one in conventional regression analysis: in the first case the repeated sampling paradigm keeps the potential outcomes fixed and varies the assignments, whereas in the latter the realized outcomes and assignments are fixed but different units with different residuals, but the same treatment status, are sampled.</p>
</blockquote>
<p><br />
</p>
<p>Redefining the residual in randomisation-based inference terms</p>
<blockquote>
<p>Now the error term has a clear meaning as the difference between potential outcomes and their population expectation
[DR: I think they mean the expectation conditional on treatment]</p>
</blockquote>
<blockquote>
<p>The randomization implies that the average residuals for treated and control units are zero …</p>
</blockquote>
<p>DR: They mean it implies mean independence (?) but not full independence, heteroskedasticity still likely</p>
<blockquote>
<p>Because the general robust variance estimator has no natural degrees-of-freedom adjustment [DR: ??], these standard [Randomisation-based?] robust variance estimators differs slightly from the Neyman unbiased variance estimator <span class="math inline">\(\hat{V}_{neyman}\)</span></p>
</blockquote>
<p><br />
</p>
<p><span class="math inline">\(\hat{V}_{robust} =\frac{s^2_c}{N_c}\frac{N_c-1}{N_c}+\frac{s^2_t}{N_t}\frac{N_t-1}{N_t}\)</span></p>
<p>Compared to the previously stated estimator for the TE variance for the sample (which we argued overstates the true sample TE variance)</p>
<p><br />
</p>
<p><span class="math inline">\(\hat{V}_{neyman} =\frac{s^2_c}{N_c}+\frac{s^2_t}{N_t}\)</span></p>
<p><br />
</p>
<blockquote>
<p>The Eicker-Huber-White variance estimator is not unbiased, and in settings where one of the treatment arms is rare, the difference may matter</p>
</blockquote>
<p>They give an example where it does not matter.</p>
<ul>
<li>DR: This point seems ignorable for most of our designs, as we intentionally avoid such rare arms (but in NL lottery maybe)</li>
</ul>
</div>
<div id="regression-estimators-with-additional-covariates-dr-seems-important" class="section level3" number="15.7.10">
<h3><span class="header-section-number">15.7.10</span> Regression Estimators with Additional Covariates [DR: seems important]</h3>
<p>For now they continue to focus on ‘pure randomisation’, not stratified nor merely exogenous conditional on observables</p>
<p><br />
</p>
<ol style="list-style-type: decimal">
<li>Can include these additively:</li>
</ol>
<p><span class="math display">\[Y^{obs}_i=\alpha+\tau W_i + \beta&#39;\dot{X}_i +  \epsilon_i\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Can allow a ‘full set of interactions’</li>
</ol>
<p><br />
</p>
<p><span class="math inline">\(Y^{obs}_i=\alpha+\tau W_i + \beta&#39;\dot{X}_i + \gamma&#39;\dot{X}_i W_i + \epsilon_i\)</span></p>
<p><br />
</p>
<ul>
<li>DR: They do not do much discussion here of whether to do additive or full interactions; maybe it comes later (causal trees etc)</li>
</ul>
<p><br />
</p>
<blockquote>
<p>In general the least squares estimates based on these regression functions are not unbiased for the average treatment effects over the randomization distribution given the finite population.</p>
</blockquote>
<ul>
<li><p>DR: Why not? Intuition? Which regression function of the ones above is referred to here?</p></li>
<li><p>DR: The discussion below suggests it will <em>still</em> be consistent (asymptotically unbiased)</p></li>
</ul>
<p><br />
</p>
<blockquote>
<p>There is one exception. If the covariates are all indicators and they partition the population, and we estimate the model with a full set of interactions, Equation (5.4), then the least squares estimate of <span class="math inline">\(\tau\)</span> is unbiased for the average treatment effect</p>
</blockquote>
<p><br />
</p>
<p>If <span class="math inline">\(\bar{X}\)</span> is the average value of <span class="math inline">\(X_i\)</span> in the sample, then =_1{X}+_0(1−{X})$, and <span class="math inline">\(\hat{\gamma}=\hat{\tau}_1-\hat{\tau}_0\)</span></p>
<p><br />
</p>
<p>With large sample approximations we can ‘say something about the case with multivalued covariates’ … “<span class="math inline">\(\tau\)</span> [DR: estimated how?] is asymptotically unbiased for the average treatment effect …”</p>
<blockquote>
<p>the asymptotic variance for <span class="math inline">\(\hat{\tau}}\)</span> is less than that of the simple difference estimator by a factor equal to <span class="math inline">\(1-R^2\)</span> from including the covariates relative to not including the covariates</p>
</blockquote>
<ul>
<li>DR: This motivates the use of covariates even in a randomized design, and even if we don’t take the ‘model of the covariates’ seriously.
<ul>
<li>“results do not rely on the regression model being true in the sense that the conditional expectation of Y obs i is actually linear in the covariates and the treatment indicator in the population”</li>
</ul></li>
<li>DR: Is this for the linear controls model or for the full interactions model?</li>
</ul>
<p>However, …</p>
<blockquote>
<p>If… the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial</p>
</blockquote>
<ul>
<li>DR: Intuition?</li>
</ul>
<p>The presence of non-zero values for γ imply treatment effect heterogeneity.</p>
<p><em>Best argument for using only binary/categorical interactions: interpretation</em></p>
<p>“Only if the covariates partition the population do these <span class="math inline">\(\gamma\)</span> have a clear interpretation as differences in average treatment effects.”</p>
<p><br />
</p>
<ul>
<li>DR: Could there not ever be a loss from including interactions and dividing up the sample too fine in doing this interactive estimation? This should depend on the true <span class="math inline">\(R^2\)</span> I think. Try to remember what is the real tradeoff.</li>
</ul>
<p>** 6 The Analysis of Stratified and paired randomized experiments **</p>
</div>
<div id="stratified-randomized-experiments-analysis" class="section level3" number="15.7.11">
<h3><span class="header-section-number">15.7.11</span> Stratified randomized experiments: analysis</h3>
<p><em>Case for stratification</em></p>
<blockquote>
<p>capture the gains from ex post regression adjustment without the potential costs of linear regression, and the potential costs of linear regression, and therefore stratification is generally preferable over regression adjustment</p>
</blockquote>
<blockquote>
<p>Within this stratum we can estimate the average effect as the difference in average outcomes for treated and control units: <span class="math inline">\(\tauˆg = \bar{Y}^{obs}_{t,g} − \bar{Y}^{obs}_{c,g}\)</span>,</p>
</blockquote>
<blockquote>
<p>and we can estimate the within-stratum variance, using the Neyman results, as</p>
</blockquote>
<p><span class="math inline">\(\hat{V}(\hat{\tau}) =\frac{s^2_{t,g}}{N_{t,g}} + \frac{s^2_{c,g}}{N_{c,g}}\)</span></p>
<blockquote>
<p>where the g-subscript indexes the stratum [They wrote ‘j’ but I think its a typo]</p>
</blockquote>
<p>Next just average weighted by stratum shares:</p>
<p><span class="math inline">\(\hat{\tau} = \sum_{g=1..G}{\hat{tau}_g \frac{N_g}{N}\)</span></p>
<p>with estimated variance <span class="math inline">\(\sum_{g=1..G}\)</span>(_g)()^2$</p>
<ul>
<li>DR: Presumably they mean the above mentioned Neyman variance
<ul>
<li>Also note the squared term in the variance estimation, this may be how they computed the variance in the above empirical example</li>
</ul></li>
</ul>
<p>“Special case”: proportion treat units the same in all strata <span class="math inline">\(\rightarrow\)</span> ATE estimator equals difference in means by treatment status:</p>
<p><span class="math display">\[\hat{\tau} = \sum_{g=1..G}{\hat{tau}_g \frac{N_g}{N}}=\bar{Y}^{obs}_t-\bar{Y}^{obs}_c\]</span></p>
<p>… same as estimator for completely randomized experiment</p>
<p>But the estimated variance for the latter will be overly conservative.</p>
<ul>
<li>DR: But I thought stratifying sometime ends up yielding a larger <em>estimated</em> variance?</li>
</ul>
<p>##Paired randomized experiments: analysis</p>
<p>(Skipping note-taking for now)</p>
</div>
<div id="the-design-of-randomised-experiments-and-the-benefits-of-stratification" class="section level3" number="15.7.12">
<h3><span class="header-section-number">15.7.12</span> 7 The Design of randomised experiments and the benefits of stratification</h3>
<blockquote>
<p>… Our recommendation is that one should always stratify as much as possible, up to the point that each stratum contains at least two treated and two control units</p>
</blockquote>
</div>
<div id="power-calculations" class="section level3" number="15.7.13">
<h3><span class="header-section-number">15.7.13</span> 7.1 Power calculations</h3>
<ul>
<li>DR: This section is fairly basic and trivial, largely what we already know</li>
</ul>
<blockquote>
<p>we largely focus on the formulation where the output is the minimum sample size required to find treatment effects of a pre-specified size with a pre-specified probability</p>
</blockquote>
<ul>
<li><p>DR: My usual formulation</p></li>
<li><p>DR: Why are they doing these calculations based on the t-statistic, when they recommend using other measures?</p></li>
<li><p>DR: They claim equal sample sizes is “typically close to optimal” in cases without homoskedasticity. I think this is pure speculation.</p></li>
</ul>
</div>
<div id="stratified-randomized-experiments-benefits" class="section level3" number="15.7.14">
<h3><span class="header-section-number">15.7.14</span> Stratified randomized experiments: Benefits</h3>
<blockquote>
<p>Stratifying does not remove any bias, it simply leads more precise inferences than complete randomization</p>
</blockquote>
<blockquote>
<p>confusion in the literature concerning the benefits of stratification in small samples if this correlation is weak [between the stratifying variables and the outcome]</p>
</blockquote>
<blockquote>
<p>in fact there is no tradeoff. We present formal results that show that in terms of expected-squared-error, stratification (with the same treatment probabilities in each stratum) cannot be worse than complete randomization.</p>
</blockquote>
<blockquote>
<p>if one stratifies on a covariate that is independent of all other variables, then stratification is obviously equivalent to complete randomization.</p>
</blockquote>
<blockquote>
<p>Ex ante, committing to stratification can only improve precision, not lower it</p>
</blockquote>
<p><em>Qualifications to this:</em></p>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<blockquote>
<p>Ex-post, given the joint distribution of the covariates in the sample, a particular stratification may be inferior to complete randomization.</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li></li>
</ol>
<blockquote>
<p>… Second, the result requires that the sample can be viewed as a (stratified) random sample from an infinitely large population… guarantees that outcomes within strata cannot be negatively correlated.</p>
</blockquote>
<p>(Note)</p>
<blockquote>
<p>The lack of any finite sample cost … contrasts with … regression adjustment. [which] may increase the finite sample variance, and in fact it will strictly increase the variance for any sample size, if the covariates have no predictive power at all.</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li><blockquote>
<p>Although there is no cost to stratification in terms of the variance, there is a cost in terms of estimation of the variance.</p>
</blockquote></li>
</ol>
<p><em>Still</em></p>
<blockquote>
<p>One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance</p>
</blockquote>
<ul>
<li>DR: Is it valid to simply say “we choose the lower value of the estimated variances”? Are they advocating this? Such a procedure seems like it would have a bias.</li>
</ul>
<blockquote>
<p>exact variance for a completely randomized experiment can be written as … variance for the corresponding stratified randomized experiment is… the difference in the two variances is <span class="math inline">\(V_C − V_S =... \geq 0\)</span></p>
</blockquote>
<ul>
<li>DR: I am curious how these terms are derived and compared</li>
</ul>
<blockquote>
<p>if the strata we draw from are small, say litters of puppies, it may well be that the within-stratum correlation is negative, but that is not possible if all the strata are large: in that case the correlation has to be non-negative</p>
</blockquote>
<ul>
<li>DR: unless sutva violated perhaps (?)</li>
</ul>
<blockquote>
<p>consider two estimators for the variance [both unbiased]</p>
</blockquote>
<p><span class="math inline">\(\hat{V}_C=\frac{s^2_{t,g}}{N_{t,g}} + \frac{s^2_{c,g}}{N_{c,g}}\)</span>
&gt; the natural estimator for the variance under the completely randomized experiment is: <span class="math inline">\(\hat{V}_c=\frac{s^2_{t}}{N_{t}} + \frac{s^2_{c,g}}{N_{c,g}}\)</span></p>
<blockquote>
<p>or a stratified randomized experiment the natural variance estimator, taking into account the stratification, is:
<span class="math inline">\(\hat{V}_S=\frac{N_f}{N_f+N_m}\Big(\frac{s^2_{fc}}{N_{fc}}\frac{s^2_{ft}}{N_{ft}}\Big)+\frac{N_f}{N_f+N_m}\Big(\frac{s^2_{mc}}{N_{mc}}\frac{s^2_{mt}}{N_{mt}}\Big)\)</span></p>
</blockquote>
<blockquote>
<p>Hence, <span class="math inline">\(E\hat{V}_S\leq\hat{V}_C\)</span>.</p>
</blockquote>
<ul>
<li>DR: Because we know both are unbiased and we know the true variance of <span class="math inline">\(\hat{V}_C\)</span> is larger.</li>
</ul>
<p>Nevertheless, the reverse may hold in a particular sample</p>
<blockquote>
<p>where the stratification is not related to the potential outcomes … the two variances are identical in expectation</p>
</blockquote>
<p>but the <span class="math inline">\(var\Big(hat{V}_S\Big) &lt; var\Big(hat{V}_C\Big)\)</span></p>
<ul>
<li>DR: This seems contradictory at first but I think it’s correct. The expectation of the estimated variance can be smaller or identical, while the <em>variance of the estimated variance</em> can still be larger.</li>
</ul>
</div>
<div id="re-randomization" class="section level3" number="15.7.15">
<h3><span class="header-section-number">15.7.15</span> Re-randomization</h3>
<p>Basically, they argue that if the first pre-implementation experiment comes out very unbalanced, you can randomize again – this will be an indirect method of stratifying.</p>
<p>P-values could/should be adjusted to take into account that you are basically stratifying imprecisely.</p>
</div>
<div id="analysis-of-clustered-randomised-experiments" class="section level3" number="15.7.16">
<h3><span class="header-section-number">15.7.16</span> Analysis of Clustered Randomised Experiments</h3>
<blockquote>
<p>our main recommendation is to include analyses that are based on the cluster as the unit of analysis. Although more sophisticated analyses may be more informative than simple analyses using the clusters as units, it is rare that these differences in precision are substantial, and a cluster-based analysis has the virtue of great transparency</p>
</blockquote>
<p><em>DR: skipping most of this section for now</em></p>
</div>
<div id="noncompliance-in-randomized-experiments-dr-relevant-to-nl-lottery-not-to-charity-experiments" class="section level3" number="15.7.17">
<h3><span class="header-section-number">15.7.17</span> Noncompliance in randomized experiments (DR: Relevant to NL lottery, not to charity experiments)</h3>
<blockquote>
<p>randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received.</p>
</blockquote>
<ul>
<li>DR: good quote for Nlmed</li>
</ul>
<p>Responses to noncompliance:</p>
<ol style="list-style-type: decimal">
<li>ITT</li>
<li>LATE</li>
<li>Partial identification or bounds analysis</li>
</ol>
<ul>
<li>Latter: “to obtain the range of values for the average causal effect of the receipt of treatment for the full population.”</li>
</ul>
<blockquote>
<p>Another approach, not further discussed here, is the randomization-based approach to instrumental variables developed in Imbens and Rosenbaum (2005).
[check into that]</p>
</blockquote>
<p>They recommend against:
&gt; The first of these is an as-treated analysis, where units are compared by the treatment received; this relies on an unconfoundedness or selectionon-observables assumption. A second type of analysis is a per protocol analysis, where units are dropped who do not receive the treatment they were assigned to. We need some additional notation in this section.</p>
<ul>
<li>DR: Skipping full note-taking on this for now but <em>COME BACK TO IT</em> as it is very relevant to NL Med; the bounds analysis could be particularly interesting</li>
</ul>
</div>
<div id="heterogenous-treatment-effects-and-pretreatment-variables" class="section level3" number="15.7.18">
<h3><span class="header-section-number">15.7.18</span> Heterogenous Treatment Effects and Pretreatment Variables</h3>
<ul>
<li>Crump et al setup (?)</li>
</ul>
<p>Multiple splits and tests may lead to overstated statistical significance for differences in TE’s.</p>
<ul>
<li>Bonferroni “overly conservative in an environment where many covariates are correlated with one another”
<ul>
<li>List, Shaikh, and Xu (2016) propose an approach accounting for this; it uses bootstrapping, and requires pre-specifying list of tests to conduct</li>
</ul></li>
</ul>
<p>** 10.3 Estimating Treatment Effect Heterogeneity **</p>
<ul>
<li>Parametric estimators, ‘all interactions’ (presumably with a correction as noted above)</li>
<li>Nonparametric estimator of <span class="math inline">\(\tau(x)\)</span></li>
</ul>
<blockquote>
<p>The approach of List, Shaikh, and Xu (2016) works for an arbitrary set of null hypotheses, so the researcher could generate a long list of hypotheses using the causal tree approach restricted to different subsets of covariates, and then test them with a correction for multiple testing. Since in datasets with many covariates, there are often many ways to describe what are essentially the same sub-groups, we expect a lot of correlation in test statistics, reducing the magnitude of the correction for multiple hypothesis testing.</p>
</blockquote>
</div>
<div id="data-driven-subgroup-analysis-recursive-partitioning-for-treatment-effects" class="section level3" number="15.7.19">
<h3><span class="header-section-number">15.7.19</span> 10.3.1 Data-driven Subgroup Analysis: Recursive Partitioning for Treatment Effects</h3>
<ul>
<li>Partition sample by “region of covariate space”</li>
<li>Determine which partition produces subgroups that differ the most in terms of treatment effects.</li>
<li>The method avoids introducing biases in the estimated average treatment effects and allows for valid confidence intervals using “sample splitting,” or “honest” estimation</li>
<li>Output of the method … is a set of subgroups, selected to optimize for treatment effect heterogeneity (to minimize expected mean-squared error of treatment effects), together with treatment effect estimates and standard errors for each subgroup.</li>
</ul>
<p><br />
</p>
<p>If instead…
&gt; we estimate the average treatment effect on the two subsamples using the same sample, the fact that this particular split led to a high value of the criterion would often imply that the average treatment effect estimate is biased.</p>
<p><br />
</p>
<p>But here ,,,
&gt; The treatment effect estimates are unbiased on the two subsamples, and the corresponding confidence intervals are valid, even in settings with a large number of pretreatment variables or covariates.</p>
<p><br />
</p>
<p>Because unit level TE is not observed, it is difficult to use standard protocols<br />
</p>
<p>… suggest transforming outcome from Y_i^{obs} to <span class="math inline">\(Y_i^\ast=Y_i^{obs}\frac{W_i−p}{p(1−p)}\)</span></p>
<p>… “so that standard methods for recursive partitioning based on prediction apply”</p>
<p>Which implies <span class="math inline">\(E[Y_i^\ast|X_i=x]=\tau(x)=E[Y_i(1)−Y_i(0)|X_i = x]\)</span></p>
<ul>
<li>DR: Are these p’s conditional on the x’s? Probably it doesn’t matter here as they are assuming pure randomisation.</li>
</ul>
<p>AI criterion</p>
<blockquote>
<p>focuses directly on the expected squared error of the treatment effect estimator … which turns out to depend both on the t-statistic and on the fit measures.
… further modified to anticipate … that the treatment effects will be re-estimated on an independent sample after the subgroups are selected</p>
</blockquote>
<ul>
<li>This penalises too small groups and too much variance,</li>
<li>(in general) rewards explain outcomes but not treatment effect heterogeneity…enables a lower-variance estimate of the treatment effect.</li>
</ul>
<p>Wager and W argue for inflating SE’s rather than partitioning
- DR: I see an advantage there, as the AI approach throws away data</p>
</div>
<div id="non-parametric-estimation-of-treatment-effect-heterogeneity" class="section level3" number="15.7.20">
<h3><span class="header-section-number">15.7.20</span> 10.3.2 Non-Parametric Estimation of Treatment Effect Heterogeneity</h3>
<ul>
<li><p>Many allow descriptive evidence and prediction, but few methods available that allow for confidence intervals</p></li>
<li><p>K-nearest neighbors, hurdle methods</p>
<ul>
<li>Do not prioritise ‘more important’ covariates</li>
</ul></li>
</ul>
<blockquote>
<p>… can work well and provide satisfactory coverage of confidence intervals with one or two covariates, but performance deteriorates quickly after that.
The output of the nonparametric estimator is a treatment effect for an arbitrary x. The estimates generally must be further summarized or visualized since the model produces a distinct prediction for each x.</p>
</blockquote>
<blockquote>
<p>A key problem with kernels and nearest neighbor matching is that all covariates are treated symmetrically; if one unit is close to another in 20 dimensions, the units are probably not particularly similar in any given dimension. We would ideally like to prioritize dimensions that are most important for heterogeneous treatment effects, as is done in many machine learning methods, including the highly successful random forest algorithm.</p>
</blockquote>
<p>But these are often “bias-dominated asymptotically” … except the ones proposed by Wager and Athey (2015) :)</p>
<blockquote>
<p>asymptotically normal and centered on the true value of the treatment effect,… consistent estimator for the asymptotic variance.</p>
</blockquote>
<blockquote>
<p>Averages over the many “trees” of the form developed in Athey and Imbens (2016)</p>
</blockquote>
<blockquote>
<p>… different subsamples are used for each tree [plus some randomness]
Each tree is “honest,” in that one subsample is used to determine a partition and [another] to estimate treatment effects within the leaves.
Unlike the case of a single tree, no data is “wasted” because each observation is used to determine the partition in some trees and used to estimate treatment effects in other trees, and subsampling is already an inherent part of the method.</p>
</blockquote>
<p>What does this mean?:</p>
<blockquote>
<p>can obtain nominal coverage with more covariates than K-Nearest Neighbour matching or kernel methods,</p>
</blockquote>
<p>(but still “eventually becomes bias-dominated when the number of covariates grows” … but “much more robust to irrelevant covariates than kernels or nearest neighbor matching.”)</p>
<ul>
<li><p>Also, approaches fitting separately for treatment and control</p></li>
<li><p>Also, Bayesian perspectives on this: Green and Kern (2011), Hill (2012), others … but unknown asymptotic properties (DR: do we care?)</p></li>
</ul>
</div>
<div id="treatment-effect-heterogeneity-using-regularized-regression" class="section level3" number="15.7.21">
<h3><span class="header-section-number">15.7.21</span> 10.3.3 Treatment Effect Heterogeneity Using Regularized Regression</h3>
<ul>
<li><p>Lasso-like (Imai and Ratkovic (2013), etc.)</p></li>
<li><p>With few important covariates (a ‘sparse’ model), can derive valid CI’s w/o sample-splitting</p></li>
<li><p>Some proposed modeling heterogeneity separately for treatment and control;… can be inefficient if the covariates that affect the level of outcomes are distinct from those that affect treatment effect heterogeneity.</p>
<ul>
<li>alternative … incorporate interactions … as covariates, and then allow LASSO to select which covariates are important.</li>
</ul></li>
</ul>
</div>
<div id="comparison-of-methods" class="section level3" number="15.7.22">
<h3><span class="header-section-number">15.7.22</span> 10.3.4 Comparison of Methods</h3>
<ul>
<li>Lasso: more sparsity restrictions, better handle linear or polynomial relationships between covariates and outcomes;
<ul>
<li>outputs a regression; but CI’s justified only under strict conditions</li>
</ul></li>
<li>Random forest methods … are more localized, … capture complex, multi-dimensional interactions among covariates, or highly nonlinear interactions.
<ul>
<li>Less sensitive to sparsity, CI’s do not ‘deteriorate’ as covariates grow (but MSE of predictions suffer)</li>
<li>Inference more justifiable by random assignment (Lasso requires stronger assumptions)</li>
</ul></li>
</ul>

</div>
</div>
</div>
<h3> List of references</h3>
<div id="refs" class="references">
<div id="ref-randlesAsymptoticallyDistributionfreeTest1980">
<p>Randles, Ronald H., Michael A. Fligner, George E. Policello, and Douglas A. Wolfe. 1980. “An Asymptotically Distribution-Free Test for Symmetry Versus Asymmetry.” <em>Journal of the American Statistical Association</em> 75 (369): 168–72. <a href="https://doi.org/10/ggx9q9">https://doi.org/10/ggx9q9</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="power.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="metaanalysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
