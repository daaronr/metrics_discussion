<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus</title>
  <meta name="description" content="6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="daaronr/metrics_discussion_work" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus" />
  
  
  

<meta name="author" content="Dr. David Reinstein," />


<meta name="date" content="2020-06-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="robust-diag.html"/>
<link rel="next" href="iv-and-its-many-issues-1.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- <script src="js/hideOutput.js"></script> -->

<!-- Mathjax -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>



<!-- open review block, in case we want it ever

<script async defer src="https://hypothes.is/embed.js"></script>
-->

<!-- Folding text box javascript thing -->

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold" ? "Unfold" : "Fold");  // if the text equals "Fold", change it to "Unfold"or else to "Fold"
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>


<script type="text/javascript">

// toggle visibility of R source blocks in R Markdown output
function toggle_R() {
  var x = document.getElementsByClassName('r');
  if (x.length == 0) return;
  function toggle_vis(o) {
    var d = o.style.display;
    o.style.display = (d == 'block' || d == '') ? 'none':'block';
  }

  for (i = 0; i < x.length; i++) {
    var y = x[i];
    if (y.tagName.toLowerCase() === 'pre') toggle_vis(y);
  }

    var elem = document.getElementById("myButton1");
    if (elem.value === "Hide Global") elem.value = "Show Global";
    else elem.value = "Hide Global";
}

document.write('<input onclick="toggle_R();" type="button" value="Hide Global" id="myButton1" style="position: absolute; top: 10%; right: 2%; z-index: 200"></input>')

</script>

<!-- Global site tag (gtag.js) - Google Analytics
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-148137970-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-148137970-3');
</script>
-->


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="support/tufte_plus.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#conceptual-approaches-to-statisticsinference-and-causalityconceptual"><i class="fa fa-check"></i><b>1.1</b> (Conceptual: approaches to statistics/inference and causality)[#conceptual]</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#bayesian-vs.-frequentist-approaches"><i class="fa fa-check"></i>Bayesian vs. frequentist approaches</a></li>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#causal-vs.-descriptive-treatment-effects-and-the-potential-outcomes-causal-model"><i class="fa fa-check"></i><b>1.1.1</b> Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#theory-restrictions-and-structural-vs-reduced-form"><i class="fa fa-check"></i><b>1.1.2</b> Theory, restrictions, and ‘structural vs reduced form’</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#getting-cleaning-and-using-data-project-management-and-coding"><i class="fa fa-check"></i><b>1.2</b> <span>Getting, cleaning and using data; project management and coding</span></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#data-whatwhywherehow"><i class="fa fa-check"></i><b>1.2.1</b> Data: What/why/where/how</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#organizing-a-project"><i class="fa fa-check"></i><b>1.2.2</b> Organizing a project</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#dynamic-documents-esp-rmdbookdown"><i class="fa fa-check"></i><b>1.2.3</b> Dynamic documents (esp Rmd/bookdown)</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#good-coding-practices"><i class="fa fa-check"></i><b>1.2.4</b> Good coding practices</a></li>
<li class="chapter" data-level="1.2.5" data-path="introduction.html"><a href="introduction.html#data-sharing-and-integrity"><i class="fa fa-check"></i><b>1.2.5</b> Data sharing and integrity</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#basic-regression-and-statistical-inference-common-mistakes-and-issues"><i class="fa fa-check"></i><b>1.3</b> Basic regression and statistical inference: Common mistakes and issues</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#bad-control-colliders"><i class="fa fa-check"></i><b>1.3.1</b> “Bad control” (“colliders”)</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#choices-of-lhs-and-rhs-variables"><i class="fa fa-check"></i><b>1.3.2</b> Choices of lhs and rhs variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#functional-form"><i class="fa fa-check"></i><b>1.3.3</b> Functional form</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#ols-and-heterogeneity"><i class="fa fa-check"></i><b>1.3.4</b> OLS and heterogeneity</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#null-effects"><i class="fa fa-check"></i><b>1.3.5</b> “Null effects”</a></li>
<li class="chapter" data-level="1.3.6" data-path="introduction.html"><a href="introduction.html#multiple-hypothesis-testing-mht"><i class="fa fa-check"></i><b>1.3.6</b> Multiple hypothesis testing (MHT)</a></li>
<li class="chapter" data-level="1.3.7" data-path="introduction.html"><a href="introduction.html#interaction-terms-and-pitfalls"><i class="fa fa-check"></i><b>1.3.7</b> Interaction terms and pitfalls</a></li>
<li class="chapter" data-level="1.3.8" data-path="introduction.html"><a href="introduction.html#choice-of-test-statistics-including-nonparametric"><i class="fa fa-check"></i><b>1.3.8</b> Choice of test statistics (including nonparametric)</a></li>
<li class="chapter" data-level="1.3.9" data-path="introduction.html"><a href="introduction.html#how-to-display-and-write-about-regression-results-and-tests"><i class="fa fa-check"></i><b>1.3.9</b> How to display and write about regression results and tests</a></li>
<li class="chapter" data-level="1.3.10" data-path="introduction.html"><a href="introduction.html#bayesian-interpretations-of-results"><i class="fa fa-check"></i><b>1.3.10</b> Bayesian interpretations of results</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#ldv-and-discrete-choice-modeling"><i class="fa fa-check"></i><b>1.4</b> LDV and discrete choice modeling</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#robustness-and-diagnostics-with-integrity"><i class="fa fa-check"></i><b>1.5</b> Robustness and diagnostics, with integrity</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#how-can-diagnostic-tests-make-sense-where-is-the-burden-of-proof"><i class="fa fa-check"></i><b>1.5.1</b> (How) can diagnostic tests make sense? Where is the burden of proof?</a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction.html"><a href="introduction.html#estimating-standard-errors"><i class="fa fa-check"></i><b>1.5.2</b> Estimating standard errors</a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction.html"><a href="introduction.html#sensitivity-analysis-interactive-presentation"><i class="fa fa-check"></i><b>1.5.3</b> Sensitivity analysis: Interactive presentation</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#control-strategies-and-prediction-machine-learning-approaches"><i class="fa fa-check"></i><b>1.6</b> <span>Control strategies and prediction; Machine Learning approaches</span></a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#machine-learning-statistical-learning-lasso-ridge-and-more"><i class="fa fa-check"></i><b>1.6.1</b> Machine Learning (statistical learning): Lasso, Ridge, and more</a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#limitations-to-inference-from-learning-approaches"><i class="fa fa-check"></i><b>1.6.2</b> Limitations to inference from learning approaches</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#iv-and-its-many-issues"><i class="fa fa-check"></i><b>1.7</b> IV and its many issues</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="introduction.html"><a href="introduction.html#instrument-validity"><i class="fa fa-check"></i><b>1.7.1</b> Instrument validity</a></li>
<li class="chapter" data-level="1.7.2" data-path="introduction.html"><a href="introduction.html#heterogeneity-and-late"><i class="fa fa-check"></i><b>1.7.2</b> Heterogeneity and LATE</a></li>
<li class="chapter" data-level="1.7.3" data-path="introduction.html"><a href="introduction.html#weak-instruments-other-issues"><i class="fa fa-check"></i><b>1.7.3</b> Weak instruments, other issues</a></li>
<li class="chapter" data-level="1.7.4" data-path="introduction.html"><a href="introduction.html#reference-to-the-use-of-iv-in-experimentsmediation"><i class="fa fa-check"></i><b>1.7.4</b> Reference to the use of IV in experiments/mediation</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#other-paths-to-observational-identification"><i class="fa fa-check"></i><b>1.8</b> <span>Other paths to observational identification</span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="introduction.html"><a href="introduction.html#fixed-effects-and-differencing"><i class="fa fa-check"></i><b>1.8.1</b> Fixed effects and differencing</a></li>
<li class="chapter" data-level="1.8.2" data-path="introduction.html"><a href="introduction.html#did"><i class="fa fa-check"></i><b>1.8.2</b> DiD</a></li>
<li class="chapter" data-level="1.8.3" data-path="introduction.html"><a href="introduction.html#rd"><i class="fa fa-check"></i><b>1.8.3</b> RD</a></li>
<li class="chapter" data-level="1.8.4" data-path="introduction.html"><a href="introduction.html#time-series-ish-panel-approaches-to-micro"><i class="fa fa-check"></i><b>1.8.4</b> Time-series-ish panel approaches to micro</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#causal-pathways-mediation-modeling-and-its-massive-limitations"><i class="fa fa-check"></i><b>1.9</b> Causal pathways: <span>Mediation modeling and its massive limitations</span></a></li>
<li class="chapter" data-level="1.10" data-path="introduction.html"><a href="introduction.html#causal-pathways-selection-corners-hurdles-and-conditional-on-estimates"><i class="fa fa-check"></i><b>1.10</b> Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="introduction.html"><a href="introduction.html#corner-solution-or-hurdle-variables-and-conditional-on-positive"><i class="fa fa-check"></i><b>1.10.1</b> ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="introduction.html"><a href="introduction.html#experimental-study-design-identifying-meaningful-and-useful-causal-relationships-and-parameters"><i class="fa fa-check"></i><b>1.11</b> <span>(Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters</span></a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="introduction.html"><a href="introduction.html#why-run-an-experiment-or-study"><i class="fa fa-check"></i><b>1.11.1</b> Why run an experiment or study?</a></li>
<li class="chapter" data-level="1.11.2" data-path="introduction.html"><a href="introduction.html#causal-channels-and-identification"><i class="fa fa-check"></i><b>1.11.2</b> Causal channels and identification</a></li>
<li class="chapter" data-level="1.11.3" data-path="introduction.html"><a href="introduction.html#types-of-experiments-demand-effects-and-more-artifacts-of-artifical-setups"><i class="fa fa-check"></i><b>1.11.3</b> Types of experiments, ‘demand effects’ and more artifacts of artifical setups</a></li>
<li class="chapter" data-level="1.11.4" data-path="introduction.html"><a href="introduction.html#generalizability-and-heterogeneity"><i class="fa fa-check"></i><b>1.11.4</b> Generalizability (and heterogeneity)</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="introduction.html"><a href="introduction.html#experimental-study-design-background-and-quantitative-issues"><i class="fa fa-check"></i><b>1.12</b> (Experimental) Study design: Background and quantitative issues</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="introduction.html"><a href="introduction.html#pre-registration-and-pre-analysis-plans"><i class="fa fa-check"></i><b>1.12.1</b> Pre-registration and Pre-analysis plans</a></li>
<li class="chapter" data-level="1.12.2" data-path="introduction.html"><a href="introduction.html#sequential-and-adaptive-designs"><i class="fa fa-check"></i><b>1.12.2</b> Sequential and adaptive designs</a></li>
<li class="chapter" data-level="1.12.3" data-path="introduction.html"><a href="introduction.html#efficient-assignment-of-treatments"><i class="fa fa-check"></i><b>1.12.3</b> Efficient assignment of treatments</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="introduction.html"><a href="introduction.html#experimental-study-design-ex-ante-power-calculations"><i class="fa fa-check"></i><b>1.13</b> (Experimental) Study design: (Ex-ante) Power calculations</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="introduction.html"><a href="introduction.html#what-sort-of-power-calculations-make-sense-and-what-is-the-point"><i class="fa fa-check"></i><b>1.13.1</b> What sort of ‘power calculations’ make sense, and what is the point?</a></li>
<li class="chapter" data-level="1.13.2" data-path="introduction.html"><a href="introduction.html#power-calculations-without-real-data"><i class="fa fa-check"></i><b>1.13.2</b> Power calculations without real data</a></li>
<li class="chapter" data-level="1.13.3" data-path="introduction.html"><a href="introduction.html#power-calculations-using-prior-data"><i class="fa fa-check"></i><b>1.13.3</b> Power calculations using prior data</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="introduction.html"><a href="introduction.html#experimetrics-and-measurement-of-treatment-effects-from-rcts"><i class="fa fa-check"></i><b>1.14</b> <span>‘Experimetrics’ and measurement of treatment effects from RCTs</span></a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="introduction.html"><a href="introduction.html#which-error-structure-random-effects"><i class="fa fa-check"></i><b>1.14.1</b> Which error structure? Random effects?</a></li>
<li class="chapter" data-level="1.14.2" data-path="introduction.html"><a href="introduction.html#randomization-inference"><i class="fa fa-check"></i><b>1.14.2</b> Randomization inference?</a></li>
<li class="chapter" data-level="1.14.3" data-path="introduction.html"><a href="introduction.html#parametric-and-nonparametric-tests-of-simple-hypotheses"><i class="fa fa-check"></i><b>1.14.3</b> Parametric and nonparametric tests of simple hypotheses</a></li>
<li class="chapter" data-level="1.14.4" data-path="introduction.html"><a href="introduction.html#adjustments-for-exogenous-but-non-random-treatment-assignment"><i class="fa fa-check"></i><b>1.14.4</b> Adjustments for exogenous (but non-random) treatment assignment</a></li>
<li class="chapter" data-level="1.14.5" data-path="introduction.html"><a href="introduction.html#iv-in-an-experimental-context-to-get-at-mediators"><i class="fa fa-check"></i><b>1.14.5</b> IV in an experimental context to get at ‘mediators’?</a></li>
<li class="chapter" data-level="1.14.6" data-path="introduction.html"><a href="introduction.html#heterogeneity-in-an-experimental-context"><i class="fa fa-check"></i><b>1.14.6</b> Heterogeneity in an experimental context</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="introduction.html"><a href="introduction.html#making-inferences-from-previous-work-meta-analysis-combining-studies"><i class="fa fa-check"></i><b>1.15</b> <span>Making inferences from previous work; Meta-analysis, combining studies</span></a>
<ul>
<li class="chapter" data-level="1.15.1" data-path="introduction.html"><a href="introduction.html#publication-bias"><i class="fa fa-check"></i><b>1.15.1</b> Publication bias</a></li>
<li class="chapter" data-level="1.15.2" data-path="introduction.html"><a href="introduction.html#combining-a-few-your-own-studiesestimates"><i class="fa fa-check"></i><b>1.15.2</b> Combining a few (your own) studies/estimates</a></li>
<li class="chapter" data-level="1.15.3" data-path="introduction.html"><a href="introduction.html#full-meta-analyses"><i class="fa fa-check"></i><b>1.15.3</b> Full meta-analyses</a></li>
</ul></li>
<li class="chapter" data-level="1.16" data-path="introduction.html"><a href="introduction.html#the-bayesian-approach"><i class="fa fa-check"></i><b>1.16</b> The Bayesian approach</a></li>
<li class="chapter" data-level="1.17" data-path="introduction.html"><a href="introduction.html#some-key-resources-and-references"><i class="fa fa-check"></i><b>1.17</b> Some key resources and references</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="introduction.html"><a href="introduction.html#consider"><i class="fa fa-check"></i><b>1.17.1</b> Consider:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="conceptual.html"><a href="conceptual.html"><i class="fa fa-check"></i><b>2</b> Conceptual: approaches to statistics/inference and causality</a>
<ul>
<li class="chapter" data-level="2.1" data-path="conceptual.html"><a href="conceptual.html#bayesian-vs.-frequentist-approaches-1"><i class="fa fa-check"></i><b>2.1</b> Bayesian vs. frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="conceptual.html"><a href="conceptual.html#interpretation-of-cis-aside"><i class="fa fa-check"></i><b>2.1.1</b> Interpretation of CI’s (aside)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="conceptual.html"><a href="conceptual.html#causal-vs.-descriptive-treatment-effects-and-the-potential-outcomes-causal-model-1"><i class="fa fa-check"></i><b>2.2</b> Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="conceptual.html"><a href="conceptual.html#dags-and-potential-outcomes-1"><i class="fa fa-check"></i><b>2.2.1</b> DAGs and Potential outcomes</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="conceptual.html"><a href="conceptual.html#theory-restrictions-and-structural-vs-reduced-form-1"><i class="fa fa-check"></i><b>2.3</b> Theory, restrictions, and ‘structural vs reduced form’</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-sci.html"><a href="data-sci.html"><i class="fa fa-check"></i><b>3</b> Getting, cleaning and using data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-sci.html"><a href="data-sci.html#data-whatwhywherehow-1"><i class="fa fa-check"></i><b>3.1</b> Data: What/why/where/how</a></li>
<li class="chapter" data-level="3.2" data-path="data-sci.html"><a href="data-sci.html#organizing-a-project-1"><i class="fa fa-check"></i><b>3.2</b> Organizing a project</a></li>
<li class="chapter" data-level="3.3" data-path="data-sci.html"><a href="data-sci.html#dynamic-documents-esp-rmdbookdown-1"><i class="fa fa-check"></i><b>3.3</b> Dynamic documents (esp Rmd/bookdown)</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="data-sci.html"><a href="data-sci.html#managing-referencescitations"><i class="fa fa-check"></i><b>3.3.1</b> Managing references/citations</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-sci.html"><a href="data-sci.html#an-example-of-dynamic-code"><i class="fa fa-check"></i><b>3.3.2</b> An example of dynamic code</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data-sci.html"><a href="data-sci.html#project-management-tools-esp.-gitgithub"><i class="fa fa-check"></i><b>3.4</b> Project management tools, esp. Git/Github</a></li>
<li class="chapter" data-level="3.5" data-path="data-sci.html"><a href="data-sci.html#good-coding-practices-1"><i class="fa fa-check"></i><b>3.5</b> Good coding practices</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="data-sci.html"><a href="data-sci.html#new-tools-and-approaches-to-data-esp-tidyverse-1"><i class="fa fa-check"></i><b>3.5.1</b> New tools and approaches to data (esp ‘tidyverse’)</a></li>
<li class="chapter" data-level="3.5.2" data-path="data-sci.html"><a href="data-sci.html#style-and-consistency-1"><i class="fa fa-check"></i><b>3.5.2</b> Style and consistency</a></li>
<li class="chapter" data-level="3.5.3" data-path="data-sci.html"><a href="data-sci.html#using-functions-variable-lists-etc.-for-clean-concise-readable-code-1"><i class="fa fa-check"></i><b>3.5.3</b> Using functions, variable lists, etc., for clean, concise, readable code</a></li>
<li class="chapter" data-level="3.5.4" data-path="data-sci.html"><a href="data-sci.html#mapping-over-lists-to-produce-results"><i class="fa fa-check"></i><b>3.5.4</b> Mapping over lists to produce results</a></li>
<li class="chapter" data-level="3.5.5" data-path="data-sci.html"><a href="data-sci.html#building-results-based-on-lists-of-filters-of-the-data-set"><i class="fa fa-check"></i><b>3.5.5</b> Building results based on ‘lists of filters’ of the data set</a></li>
<li class="chapter" data-level="3.5.6" data-path="data-sci.html"><a href="data-sci.html#coding-style-and-indenting-in-stata-one-approach"><i class="fa fa-check"></i><b>3.5.6</b> Coding style and indenting in Stata (one approach)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="data-sci.html"><a href="data-sci.html#additional-tips-integrate"><i class="fa fa-check"></i><b>3.6</b> Additional tips (integrate)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reg-follies.html"><a href="reg-follies.html"><i class="fa fa-check"></i><b>4</b> Basic statistical inference and regressions: Common mistakes and issues</a>
<ul>
<li class="chapter" data-level="4.1" data-path="reg-follies.html"><a href="reg-follies.html#basic-regression-and-statistical-inference-common-mistakes-and-issues-briefly-listed"><i class="fa fa-check"></i><b>4.1</b> Basic regression and statistical inference: Common mistakes and issues briefly listed</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="reg-follies.html"><a href="reg-follies.html#bad-control"><i class="fa fa-check"></i><b>4.1.1</b> Bad control</a></li>
<li class="chapter" data-level="4.1.2" data-path="reg-follies.html"><a href="reg-follies.html#bad-control-colliders-1"><i class="fa fa-check"></i><b>4.1.2</b> “Bad control” (“colliders”)</a></li>
<li class="chapter" data-level="4.1.3" data-path="reg-follies.html"><a href="reg-follies.html#choices-of-lhs-and-rhs-variables-1"><i class="fa fa-check"></i><b>4.1.3</b> Choices of lhs and rhs variables</a></li>
<li class="chapter" data-level="4.1.4" data-path="reg-follies.html"><a href="reg-follies.html#functional-form-1"><i class="fa fa-check"></i><b>4.1.4</b> Functional form</a></li>
<li class="chapter" data-level="4.1.5" data-path="reg-follies.html"><a href="reg-follies.html#ols-and-heterogeneity-1"><i class="fa fa-check"></i><b>4.1.5</b> OLS and heterogeneity</a></li>
<li class="chapter" data-level="4.1.6" data-path="reg-follies.html"><a href="reg-follies.html#null-effects-1"><i class="fa fa-check"></i><b>4.1.6</b> “Null effects”</a></li>
<li class="chapter" data-level="4.1.7" data-path="reg-follies.html"><a href="reg-follies.html#mht"><i class="fa fa-check"></i><b>4.1.7</b> Multiple hypothesis testing (MHT)</a></li>
<li class="chapter" data-level="4.1.8" data-path="reg-follies.html"><a href="reg-follies.html#interaction-terms-and-pitfalls-1"><i class="fa fa-check"></i><b>4.1.8</b> Interaction terms and pitfalls</a></li>
<li class="chapter" data-level="4.1.9" data-path="reg-follies.html"><a href="reg-follies.html#choice-of-test-statistics-including-nonparametric-1"><i class="fa fa-check"></i><b>4.1.9</b> Choice of test statistics (including nonparametric)</a></li>
<li class="chapter" data-level="4.1.10" data-path="reg-follies.html"><a href="reg-follies.html#how-to-display-and-write-about-regression-results-and-tests-1"><i class="fa fa-check"></i><b>4.1.10</b> How to display and write about regression results and tests</a></li>
<li class="chapter" data-level="4.1.11" data-path="reg-follies.html"><a href="reg-follies.html#bayesian-interpretations-of-results-1"><i class="fa fa-check"></i><b>4.1.11</b> Bayesian interpretations of results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="robust-diag.html"><a href="robust-diag.html"><i class="fa fa-check"></i><b>5</b> Robustness and diagnostics, with integrity; Open Science resources</a>
<ul>
<li class="chapter" data-level="5.1" data-path="robust-diag.html"><a href="robust-diag.html#how-can-diagnostic-tests-make-sense-where-is-the-burden-of-proof-1"><i class="fa fa-check"></i><b>5.1</b> (How) can diagnostic tests make sense? Where is the burden of proof?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="robust-diag.html"><a href="robust-diag.html#further-discussion-the-did-approach-and-parallel-trends"><i class="fa fa-check"></i><b>5.1.1</b> Further discussion: the DiD approach and ‘parallel trends’</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="robust-diag.html"><a href="robust-diag.html#estimating-standard-errors-1"><i class="fa fa-check"></i><b>5.2</b> Estimating standard errors</a></li>
<li class="chapter" data-level="5.3" data-path="robust-diag.html"><a href="robust-diag.html#sensitivity-analysis-interactive-presentation-1"><i class="fa fa-check"></i><b>5.3</b> Sensitivity analysis: Interactive presentation</a></li>
<li class="chapter" data-level="5.4" data-path="robust-diag.html"><a href="robust-diag.html#supplement-open-science-resources-tools-and-considerations"><i class="fa fa-check"></i><b>5.4</b> Supplement: open science resources, tools and considerations</a></li>
<li class="chapter" data-level="5.5" data-path="robust-diag.html"><a href="robust-diag.html#diagnosing-p-hacking-and-publication-bias-see-also-meta-analysis"><i class="fa fa-check"></i><b>5.5</b> Diagnosing p-hacking and publication bias (see also <span>meta-analysis</span>)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="robust-diag.html"><a href="robust-diag.html#publication-bias-see-also-considering-publication-bias-in-meta-analysis"><i class="fa fa-check"></i><b>5.5.1</b> Publication bias – see also <span>considering publication bias in meta-analysis</span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="robust-diag.html"><a href="robust-diag.html#multiple-hypothesis-testing---see-above"><i class="fa fa-check"></i><b>5.6</b> <span>Multiple hypothesis testing - see above</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="control-ml.html"><a href="control-ml.html"><i class="fa fa-check"></i><b>6</b> Control strategies and prediction, Machine Learning (Statistical Learning) approaches</a>
<ul>
<li class="chapter" data-level="6.1" data-path="control-ml.html"><a href="control-ml.html#machine-learning-statistical-learning-lasso-ridge-and-more-1"><i class="fa fa-check"></i><b>6.1</b> Machine Learning (statistical learning): Lasso, Ridge, and more</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="control-ml.html"><a href="control-ml.html#limitations-to-inference-from-learning-approaches-1"><i class="fa fa-check"></i><b>6.1.1</b> Limitations to inference from learning approaches</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="control-ml.html"><a href="control-ml.html#notes-hastie-statistical-learning-with-sparsity"><i class="fa fa-check"></i><b>6.2</b> Notes Hastie: Statistical Learning with Sparsity</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="control-ml.html"><a href="control-ml.html#introduction-1"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="control-ml.html"><a href="control-ml.html#ch2-lasso-for-linear-models"><i class="fa fa-check"></i><b>6.2.2</b> Ch2: Lasso for linear models</a></li>
<li class="chapter" data-level="6.2.3" data-path="control-ml.html"><a href="control-ml.html#chapter-3-generalized-linear-models"><i class="fa fa-check"></i><b>6.2.3</b> Chapter 3: Generalized linear models</a></li>
<li class="chapter" data-level="6.2.4" data-path="control-ml.html"><a href="control-ml.html#chapter-4-generalizations-of-the-lasso-penalty"><i class="fa fa-check"></i><b>6.2.4</b> Chapter 4: Generalizations of the Lasso penalty</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="control-ml.html"><a href="control-ml.html#notes-mullainathan"><i class="fa fa-check"></i><b>6.3</b> Notes: Mullainathan</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html"><i class="fa fa-check"></i><b>7</b> IV and its many issues</a>
<ul>
<li class="chapter" data-level="" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#some-casual-discussion"><i class="fa fa-check"></i>Some casual discussion</a></li>
<li class="chapter" data-level="7.1" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#instrument-validity-1"><i class="fa fa-check"></i><b>7.1</b> Instrument validity</a></li>
<li class="chapter" data-level="7.2" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#heterogeneity-and-late-1"><i class="fa fa-check"></i><b>7.2</b> Heterogeneity and LATE</a></li>
<li class="chapter" data-level="7.3" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#weak-instruments-other-issues-1"><i class="fa fa-check"></i><b>7.3</b> Weak instruments, other issues</a></li>
<li class="chapter" data-level="7.4" data-path="iv-and-its-many-issues-1.html"><a href="iv-and-its-many-issues-1.html#reference-to-the-use-of-iv-in-experimentsmediation-1"><i class="fa fa-check"></i><b>7.4</b> Reference to the use of IV in experiments/mediation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html"><i class="fa fa-check"></i><b>8</b> <span id="other_paths">Other paths to observational identification</span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#fixed-effects-and-differencing-1"><i class="fa fa-check"></i><b>8.1</b> Fixed effects and differencing</a></li>
<li class="chapter" data-level="8.2" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#did-1"><i class="fa fa-check"></i><b>8.2</b> DiD</a></li>
<li class="chapter" data-level="8.3" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#rd-1"><i class="fa fa-check"></i><b>8.3</b> RD</a></li>
<li class="chapter" data-level="8.4" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#time-series-ish-panel-approaches-to-micro-1"><i class="fa fa-check"></i><b>8.4</b> Time-series-ish panel approaches to micro</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="other-paths-to-observational-identification-1.html"><a href="other-paths-to-observational-identification-1.html#lagged-dependent-variable-and-fixed-effects-nickel-bias-1"><i class="fa fa-check"></i><b>8.4.1</b> Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mediators.html"><a href="mediators.html"><i class="fa fa-check"></i><b>9</b> Causal pathways - mediators</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mediators.html"><a href="mediators.html#mediators-and-selection-and-roy-models-a-review-considering-two-research-applications"><i class="fa fa-check"></i><b>9.1</b> Mediators (and selection and Roy models): a review, considering two research applications</a></li>
<li class="chapter" data-level="9.2" data-path="mediators.html"><a href="mediators.html#dr-initial-thoughts-for-nl-education-paper"><i class="fa fa-check"></i><b>9.2</b> DR initial thoughts (for NL education paper)</a></li>
<li class="chapter" data-level="9.3" data-path="mediators.html"><a href="mediators.html#econometric-mediation-analyses-heckman-and-pinto"><i class="fa fa-check"></i><b>9.3</b> Econometric Mediation Analyses (Heckman and Pinto)</a>
<ul>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#relevance-to-parey-et-al"><i class="fa fa-check"></i>Relevance to Parey et al</a></li>
<li class="chapter" data-level="9.3.1" data-path="mediators.html"><a href="mediators.html#summary-and-key-modeling"><i class="fa fa-check"></i><b>9.3.1</b> Summary and key modeling</a></li>
<li class="chapter" data-level="9.3.2" data-path="mediators.html"><a href="mediators.html#common-assumptions-and-their-implications"><i class="fa fa-check"></i><b>9.3.2</b> Common assumptions and their implications</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="mediators.html"><a href="mediators.html#pinto-2015-selection-bias-in-a-controlled-experiment-the-case-of-moving-to-opportunity"><i class="fa fa-check"></i><b>9.4</b> Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity</a>
<ul>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#summary"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#relevance-to-parey-et-al-1"><i class="fa fa-check"></i>Relevance to Parey et al</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#introduction-2"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#identification-strategy-brief"><i class="fa fa-check"></i>Identification strategy brief</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#results-in-brief"><i class="fa fa-check"></i>Results in brief</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#framework-first-for-binarybinary-simplification"><i class="fa fa-check"></i>Framework: first for binary/binary (simplification)</a></li>
<li class="chapter" data-level="" data-path="mediators.html"><a href="mediators.html#framework-for-mto-multiple-treatment-groups-multiple-choices"><i class="fa fa-check"></i>Framework for MTO multiple treatment groups, multiple choices</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="mediators.html"><a href="mediators.html#antonakis-approaches"><i class="fa fa-check"></i><b>9.5</b> Antonakis approaches</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="selection-cop.html"><a href="selection-cop.html"><i class="fa fa-check"></i><b>10</b> Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates</a>
<ul>
<li class="chapter" data-level="10.1" data-path="selection-cop.html"><a href="selection-cop.html#corner-solution-or-hurdle-variables-and-conditional-on-positive-1"><i class="fa fa-check"></i><b>10.1</b> ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’</a></li>
<li class="chapter" data-level="10.2" data-path="selection-cop.html"><a href="selection-cop.html#bounding-approaches-lee-manski-etc-1"><i class="fa fa-check"></i><b>10.2</b> Bounding approaches (Lee, Manski, etc)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="selection-cop.html"><a href="selection-cop.html#notes-training-wages-and-sample-selection-estimating-sharp-bounds-on-treatment-effects-david-lee-2009-restud"><i class="fa fa-check"></i><b>10.2.1</b> Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="why-experiment-design.html"><a href="why-experiment-design.html"><i class="fa fa-check"></i><b>11</b> (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters</a>
<ul>
<li class="chapter" data-level="11.1" data-path="why-experiment-design.html"><a href="why-experiment-design.html#why-run-an-experiment-or-study-1"><i class="fa fa-check"></i><b>11.1</b> Why run an experiment or study?</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="why-experiment-design.html"><a href="why-experiment-design.html#sitzia-and-sugden-on-what-theoretically-driven-experiments-can-and-should-do"><i class="fa fa-check"></i><b>11.1.1</b> Sitzia and Sugden on what theoretically driven experiments can and should do</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="why-experiment-design.html"><a href="why-experiment-design.html#causal-channels-and-identification-1"><i class="fa fa-check"></i><b>11.2</b> Causal channels and identification</a></li>
<li class="chapter" data-level="11.3" data-path="why-experiment-design.html"><a href="why-experiment-design.html#types-of-experiments-demand-effects-and-more-artifacts-of-artifical-setups-1"><i class="fa fa-check"></i><b>11.3</b> Types of experiments, ‘demand effects’ and more artifacts of artifical setups</a></li>
<li class="chapter" data-level="11.4" data-path="why-experiment-design.html"><a href="why-experiment-design.html#generalizability-and-heterogeneity-1"><i class="fa fa-check"></i><b>11.4</b> Generalizability (and heterogeneity)</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="quant-design-power.html"><a href="quant-design-power.html"><i class="fa fa-check"></i><b>12</b> (Experimental) Study design: Background and quantitative issues</a>
<ul>
<li class="chapter" data-level="12.1" data-path="quant-design-power.html"><a href="quant-design-power.html#pre-reg-pap"><i class="fa fa-check"></i><b>12.1</b> Pre-registration and Pre-analysis plans</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="quant-design-power.html"><a href="quant-design-power.html#the-benefits-and-costs-of-pre-registration-a-typical-discussion"><i class="fa fa-check"></i><b>12.1.1</b> The benefits and costs of pre-registration: a typical discussion</a></li>
<li class="chapter" data-level="12.1.2" data-path="quant-design-power.html"><a href="quant-design-power.html#the-hazards-of-specification-searching-1"><i class="fa fa-check"></i><b>12.1.2</b> The hazards of specification-searching</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="quant-design-power.html"><a href="quant-design-power.html#sequential-and-adaptive-designs-1"><i class="fa fa-check"></i><b>12.2</b> Sequential and adaptive designs</a></li>
<li class="chapter" data-level="12.3" data-path="quant-design-power.html"><a href="quant-design-power.html#efficient-assignment-of-treatments-1"><i class="fa fa-check"></i><b>12.3</b> Efficient assignment of treatments</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="quant-design-power.html"><a href="quant-design-power.html#see-also-multiple-hypothesis-testing"><i class="fa fa-check"></i><b>12.3.1</b> See also <span>multiple hypothesis testing</span></a></li>
<li class="chapter" data-level="12.3.2" data-path="quant-design-power.html"><a href="quant-design-power.html#how-many-treatment-arms-can-you-afford"><i class="fa fa-check"></i><b>12.3.2</b> How many treatment arms can you ‘afford’?</a></li>
<li class="chapter" data-level="12.3.3" data-path="quant-design-power.html"><a href="quant-design-power.html#other-notes-and-resources"><i class="fa fa-check"></i><b>12.3.3</b> Other notes and resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="power.html"><a href="power.html"><i class="fa fa-check"></i><b>13</b> (Experimental) Study design: (Ex-ante) Power calculations</a>
<ul>
<li class="chapter" data-level="13.1" data-path="power.html"><a href="power.html#what-sort-of-power-calculations-make-sense-and-what-is-the-point-1"><i class="fa fa-check"></i><b>13.1</b> What sort of ‘power calculations’ make sense, and what is the point?</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="power.html"><a href="power.html#the-harm-to-science-from-running-underpowered-studies-1"><i class="fa fa-check"></i><b>13.1.1</b> The ‘harm to science’ from running underpowered studies</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="power.html"><a href="power.html#power-calculations-without-real-data-1"><i class="fa fa-check"></i><b>13.2</b> Power calculations without real data</a></li>
<li class="chapter" data-level="13.3" data-path="power.html"><a href="power.html#power-calculations-using-prior-data-1"><i class="fa fa-check"></i><b>13.3</b> Power calculations using prior data</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="experimetrics-te.html"><a href="experimetrics-te.html"><i class="fa fa-check"></i><b>14</b> ‘Experimetrics’ and measurement of treatment effects from RCTs</a>
<ul>
<li class="chapter" data-level="14.1" data-path="experimetrics-te.html"><a href="experimetrics-te.html#which-error-structure-random-effects-1"><i class="fa fa-check"></i><b>14.1</b> Which error structure? Random effects?</a></li>
<li class="chapter" data-level="14.2" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomization-inference-1"><i class="fa fa-check"></i><b>14.2</b> Randomization inference?</a></li>
<li class="chapter" data-level="14.3" data-path="experimetrics-te.html"><a href="experimetrics-te.html#parametric-and-nonparametric-tests-of-simple-hypotheses-1"><i class="fa fa-check"></i><b>14.3</b> Parametric and nonparametric tests of simple hypotheses</a></li>
<li class="chapter" data-level="14.4" data-path="experimetrics-te.html"><a href="experimetrics-te.html#adjustments-for-exogenous-but-non-random-treatment-assignment-1"><i class="fa fa-check"></i><b>14.4</b> Adjustments for exogenous (but non-random) treatment assignment</a></li>
<li class="chapter" data-level="14.5" data-path="experimetrics-te.html"><a href="experimetrics-te.html#iv-in-an-experimental-context-to-get-at-mediators-1"><i class="fa fa-check"></i><b>14.5</b> IV in an experimental context to get at ‘mediators’?</a></li>
<li class="chapter" data-level="14.6" data-path="experimetrics-te.html"><a href="experimetrics-te.html#heterogeneity-in-an-experimental-context-1"><i class="fa fa-check"></i><b>14.6</b> Heterogeneity in an experimental context</a></li>
<li class="chapter" data-level="14.7" data-path="experimetrics-te.html"><a href="experimetrics-te.html#incorporate-above-notes-on-the-econometrics-of-randomised-experiments-athey-and-imbens"><i class="fa fa-check"></i><b>14.7</b> Incorporate above: Notes on “The econometrics of randomised experiments” (Athey and Imbens)</a>
<ul>
<li class="chapter" data-level="14.7.1" data-path="experimetrics-te.html"><a href="experimetrics-te.html#abstract-and-intro"><i class="fa fa-check"></i><b>14.7.1</b> Abstract and intro</a></li>
<li class="chapter" data-level="14.7.2" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomised-experiments-and-validity"><i class="fa fa-check"></i><b>14.7.2</b> randomised experiments and validity</a></li>
<li class="chapter" data-level="14.7.3" data-path="experimetrics-te.html"><a href="experimetrics-te.html#potential-outcomes-rubin-causal-model-framework-covered-earlier"><i class="fa fa-check"></i><b>14.7.3</b> Potential outcomes/ Rubin causal model framework (covered earlier)</a></li>
<li class="chapter" data-level="14.7.4" data-path="experimetrics-te.html"><a href="experimetrics-te.html#classification-of-assignment-mechanisms"><i class="fa fa-check"></i><b>14.7.4</b> 3.2 Classification of assignment mechanisms</a></li>
<li class="chapter" data-level="14.7.5" data-path="experimetrics-te.html"><a href="experimetrics-te.html#the-analysis-of-completely-randomized-experiments"><i class="fa fa-check"></i><b>14.7.5</b> The analysis of Completely randomized experiments</a></li>
<li class="chapter" data-level="14.7.6" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomization-inference-for-average-treatment-effects"><i class="fa fa-check"></i><b>14.7.6</b> Randomization inference for Average treatment effects</a></li>
<li class="chapter" data-level="14.7.7" data-path="experimetrics-te.html"><a href="experimetrics-te.html#quantile-treatment-effect-infinite-population-context"><i class="fa fa-check"></i><b>14.7.7</b> Quantile treatment effect (Infinite population context)</a></li>
<li class="chapter" data-level="14.7.8" data-path="experimetrics-te.html"><a href="experimetrics-te.html#covariates-if-not-stratified-in-completely-randomized-experiments"><i class="fa fa-check"></i><b>14.7.8</b> Covariates (if not stratified) in completely randomized experiments</a></li>
<li class="chapter" data-level="14.7.9" data-path="experimetrics-te.html"><a href="experimetrics-te.html#randomization-inference-and-regression-estimators"><i class="fa fa-check"></i><b>14.7.9</b> Randomization inference and regression estimators</a></li>
<li class="chapter" data-level="14.7.10" data-path="experimetrics-te.html"><a href="experimetrics-te.html#regression-estimators-with-additional-covariates-dr-seems-important"><i class="fa fa-check"></i><b>14.7.10</b> Regression Estimators with Additional Covariates [DR: seems important]</a></li>
<li class="chapter" data-level="14.7.11" data-path="experimetrics-te.html"><a href="experimetrics-te.html#stratified-randomized-experiments-analysis"><i class="fa fa-check"></i><b>14.7.11</b> Stratified randomized experiments: analysis</a></li>
<li class="chapter" data-level="14.7.12" data-path="experimetrics-te.html"><a href="experimetrics-te.html#the-design-of-randomised-experiments-and-the-benefits-of-stratification"><i class="fa fa-check"></i><b>14.7.12</b> 7 The Design of randomised experiments and the benefits of stratification</a></li>
<li class="chapter" data-level="14.7.13" data-path="experimetrics-te.html"><a href="experimetrics-te.html#power-calculations"><i class="fa fa-check"></i><b>14.7.13</b> 7.1 Power calculations</a></li>
<li class="chapter" data-level="14.7.14" data-path="experimetrics-te.html"><a href="experimetrics-te.html#stratified-randomized-experiments-benefits"><i class="fa fa-check"></i><b>14.7.14</b> Stratified randomized experiments: Benefits</a></li>
<li class="chapter" data-level="14.7.15" data-path="experimetrics-te.html"><a href="experimetrics-te.html#re-randomization"><i class="fa fa-check"></i><b>14.7.15</b> Re-randomization</a></li>
<li class="chapter" data-level="14.7.16" data-path="experimetrics-te.html"><a href="experimetrics-te.html#analysis-of-clustered-randomised-experiments"><i class="fa fa-check"></i><b>14.7.16</b> Analysis of Clustered Randomised Experiments</a></li>
<li class="chapter" data-level="14.7.17" data-path="experimetrics-te.html"><a href="experimetrics-te.html#noncompliance-in-randomized-experiments-dr-relevant-to-nl-lottery-not-to-charity-experiments"><i class="fa fa-check"></i><b>14.7.17</b> Noncompliance in randomized experiments (DR: Relevant to NL lottery, not to charity experiments)</a></li>
<li class="chapter" data-level="14.7.18" data-path="experimetrics-te.html"><a href="experimetrics-te.html#heterogenous-treatment-effects-and-pretreatment-variables"><i class="fa fa-check"></i><b>14.7.18</b> Heterogenous Treatment Effects and Pretreatment Variables</a></li>
<li class="chapter" data-level="14.7.19" data-path="experimetrics-te.html"><a href="experimetrics-te.html#data-driven-subgroup-analysis-recursive-partitioning-for-treatment-effects"><i class="fa fa-check"></i><b>14.7.19</b> 10.3.1 Data-driven Subgroup Analysis: Recursive Partitioning for Treatment Effects</a></li>
<li class="chapter" data-level="14.7.20" data-path="experimetrics-te.html"><a href="experimetrics-te.html#non-parametric-estimation-of-treatment-effect-heterogeneity"><i class="fa fa-check"></i><b>14.7.20</b> 10.3.2 Non-Parametric Estimation of Treatment Effect Heterogeneity</a></li>
<li class="chapter" data-level="14.7.21" data-path="experimetrics-te.html"><a href="experimetrics-te.html#treatment-effect-heterogeneity-using-regularized-regression"><i class="fa fa-check"></i><b>14.7.21</b> 10.3.3 Treatment Effect Heterogeneity Using Regularized Regression</a></li>
<li class="chapter" data-level="14.7.22" data-path="experimetrics-te.html"><a href="experimetrics-te.html#comparison-of-methods"><i class="fa fa-check"></i><b>14.7.22</b> 10.3.4 Comparison of Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="metaanalysis.html"><a href="metaanalysis.html"><i class="fa fa-check"></i><b>15</b> Meta-analysis and combining studies: Making inferences from previous work</a>
<ul>
<li class="chapter" data-level="15.1" data-path="metaanalysis.html"><a href="metaanalysis.html#notes-christensen-et-al-2019-ch-5-using-all-evidence-registration-and-meta-analysis"><i class="fa fa-check"></i><b>15.1</b> Notes: Christensen et al 2019, ch 5, ’Using all evidence, registration and meta-analysis</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="metaanalysis.html"><a href="metaanalysis.html#the-origins-and-importance-of-study-pre-registration"><i class="fa fa-check"></i><b>15.1.1</b> The origins [and importance] of study [pre-]registration</a></li>
<li class="chapter" data-level="15.1.2" data-path="metaanalysis.html"><a href="metaanalysis.html#social-science-study-registries"><i class="fa fa-check"></i><b>15.1.2</b> Social science study registries</a></li>
<li class="chapter" data-level="15.1.3" data-path="metaanalysis.html"><a href="metaanalysis.html#meta-analysis"><i class="fa fa-check"></i><b>15.1.3</b> Meta-analysis</a></li>
<li class="chapter" data-level="15.1.4" data-path="metaanalysis.html"><a href="metaanalysis.html#combining-estimates"><i class="fa fa-check"></i><b>15.1.4</b> Combining estimates</a></li>
<li class="chapter" data-level="15.1.5" data-path="metaanalysis.html"><a href="metaanalysis.html#heterogeneous-estimates"><i class="fa fa-check"></i><b>15.1.5</b> Heterogeneous estimates…</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="metaanalysis.html"><a href="metaanalysis.html#doing-meta"><i class="fa fa-check"></i><b>15.2</b> Excerpts and notes from ‘Doing Meta-Analysis in R: A Hands-on Guide’ (Harrer et al)</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="metaanalysis.html"><a href="metaanalysis.html#pooling-effect-sizes"><i class="fa fa-check"></i><b>15.2.1</b> Pooling effect sizes</a></li>
<li class="chapter" data-level="15.2.2" data-path="metaanalysis.html"><a href="metaanalysis.html#doing-bayes-meta"><i class="fa fa-check"></i><b>15.2.2</b> Bayesian Meta-analysis</a></li>
<li class="chapter" data-level="15.2.3" data-path="metaanalysis.html"><a href="metaanalysis.html#forest-plots"><i class="fa fa-check"></i><b>15.2.3</b> Forest plots</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="metaanalysis.html"><a href="metaanalysis.html#pubbias"><i class="fa fa-check"></i><b>15.3</b> Dealing with publication bias</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="metaanalysis.html"><a href="metaanalysis.html#diagnosis-and-responses-p-curves-funnel-plots-adjustments"><i class="fa fa-check"></i><b>15.3.1</b> Diagnosis and responses: P-curves, funnel plots, adjustments</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="metaanalysis.html"><a href="metaanalysis.html#other-notes-links-and-commentary"><i class="fa fa-check"></i><b>15.4</b> Other notes, links, and commentary</a></li>
<li class="chapter" data-level="15.5" data-path="metaanalysis.html"><a href="metaanalysis.html#other-resources-and-tools"><i class="fa fa-check"></i><b>15.5</b> Other resources and tools</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html"><i class="fa fa-check"></i><b>16</b> Bayesian approaches</a>
<ul>
<li class="chapter" data-level="16.1" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#my-david-reinsteins-uses-for-bayesian-approaches-brainstorm"><i class="fa fa-check"></i><b>16.1</b> My (David Reinstein’s) uses for Bayesian approaches (brainstorm)</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#meta-analysis-of-previous-evidence"><i class="fa fa-check"></i><b>16.1.1</b> Meta-analysis of previous evidence</a></li>
<li class="chapter" data-level="16.1.2" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#inference-particularly-about-null-effects"><i class="fa fa-check"></i><b>16.1.2</b> Inference, particularly about ‘null effects’</a></li>
<li class="chapter" data-level="16.1.3" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#policy-and-business-implications-and-recommendations"><i class="fa fa-check"></i><b>16.1.3</b> ‘Policy’ and business implications and recommendations</a></li>
<li class="chapter" data-level="16.1.4" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#theory-driven-inference-about-optimizing-agents-esp.-in-strategic-settings"><i class="fa fa-check"></i><b>16.1.4</b> Theory-driven inference about optimizing agents, esp. in strategic settings</a></li>
<li class="chapter" data-level="16.1.5" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#experimental-design"><i class="fa fa-check"></i><b>16.1.5</b> Experimental design</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#statistical-thinking-mcelreath-and-aj-kurtz-recoded-bookdown-highlights-and-notes"><i class="fa fa-check"></i><b>16.2</b> ‘Statistical thinking’ (McElreath) and <span>AJ Kurtz ‘recoded’ (bookdown)</span>: highlights and notes</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#the-golem-of-prague-chapter-1"><i class="fa fa-check"></i><b>16.2.1</b> The Golem of Prague (Chapter 1)</a></li>
<li class="chapter" data-level="16.2.2" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#small-worlds-and-large-worlds-ch-2"><i class="fa fa-check"></i><b>16.2.2</b> Small Worlds and Large Worlds (Ch 2)</a></li>
<li class="chapter" data-level="16.2.3" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#using-prior-information"><i class="fa fa-check"></i><b>16.2.3</b> Using prior information</a></li>
<li class="chapter" data-level="16.2.4" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#from-counts-to-probability."><i class="fa fa-check"></i><b>16.2.4</b> From counts to probability.</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#title-introduction-to-bayesian-analysis-in-r-and-stata---katz-qstep"><i class="fa fa-check"></i><b>16.3</b> Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep”</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#why-and-when-use-bayesian-mcmc-methods"><i class="fa fa-check"></i><b>16.3.1</b> Why and when use Bayesian (MCMC) methods?</a></li>
<li class="chapter" data-level="16.3.2" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#theory"><i class="fa fa-check"></i><b>16.3.2</b> Theory</a></li>
<li class="chapter" data-level="16.3.3" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#comparing-models-equivalent-of-likelihood"><i class="fa fa-check"></i><b>16.3.3</b> Comparing models … Equivalent of ‘likelihood’</a></li>
<li class="chapter" data-level="16.3.4" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#on-choosing-priors"><i class="fa fa-check"></i><b>16.3.4</b> On choosing priors</a></li>
<li class="chapter" data-level="16.3.5" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#implementation"><i class="fa fa-check"></i><b>16.3.5</b> Implementation</a></li>
<li class="chapter" data-level="16.3.6" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#generate-predictions-from-a-winbugs-model"><i class="fa fa-check"></i><b>16.3.6</b> Generate predictions from a WinBUGS model</a></li>
<li class="chapter" data-level="16.3.7" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#missing-data-case"><i class="fa fa-check"></i><b>16.3.7</b> Missing data case</a></li>
<li class="chapter" data-level="16.3.8" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#stata"><i class="fa fa-check"></i><b>16.3.8</b> Stata</a></li>
<li class="chapter" data-level="16.3.9" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#r-mcmc-pac"><i class="fa fa-check"></i><b>16.3.9</b> R mcmc pac</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="bayesian-approaches.html"><a href="bayesian-approaches.html#other-resources-and-notes-to-integrate"><i class="fa fa-check"></i><b>16.4</b> Other resources and notes to integrate</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>17</b> List of references</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="control-ml" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Control strategies and prediction, Machine Learning (Statistical Learning) approaches</h1>
<blockquote>
<p>‘Identification’ of causal effects with a control strategy not credible</p>
</blockquote>
<p>Essentially a ‘control strategy’ is “control for all or most of the reasonable determinants of the independent variable so as to make the remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest”. All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., <span class="citation">(Oster <a href="#ref-osterUnobservableSelectionCoefficient2019" role="doc-biblioref">2019</a>)</span>.</p>
<div id="machine-learning-statistical-learning-lasso-ridge-and-more-1" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Machine Learning (statistical learning): Lasso, Ridge, and more</h2>
<div id="limitations-to-inference-from-learning-approaches-1" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Limitations to inference from learning approaches</h3>
</div>
</div>
<div id="notes-hastie-statistical-learning-with-sparsity" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Notes Hastie: Statistical Learning with Sparsity</h2>
<p><a href="https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=f-A_CQAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;ots=G4RMC-gZU-&amp;sig=u_EoI3-ZcX0phwsZgLt2Ux7DXyc#v=onepage&amp;q&amp;f=true">google books link</a></p>
<div id="introduction-1" class="section level3" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Introduction</h3>
<blockquote>
<p>One form of simplicity is sparsity, the central theme of this book. Loosely speaking, a sparse statistical model is one in which only a relatively small number of parameters (or predictors) play an important role.</p>
</blockquote>
<p>“the <span class="math inline">\(\ell_1\)</span> norm is special” (abs value). Other norms yield nonconvex problems, hard to minimize.</p>
<blockquote>
<p>“bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems.</p>
</blockquote>
<ul>
<li>Examples from gene mapping</li>
</ul>
<div id="book-roadmap" class="section level4" number="6.2.1.1">
<h4><span class="header-section-number">6.2.1.1</span> Book roadmap</h4>
<ul>
<li><p>Chapter 2 … lasso for linear regression, and a simple coordinate descent algorithm for its computation.</p></li>
<li><p>Chapter 3 application of <span class="math inline">\(\ell_1\)</span> [lasso-type] penalties to generalized linear models such as multinomial and survival models, as well as support vector machines. [?]</p></li>
<li><p>Chapter 4: Generalized penalties such as the elastic net and group lasso are discussed in Chapter 4.</p></li>
<li><p>Chapter 5: numerical methods for optimization (skip for now]</p></li>
<li><p>Chapter 6: statistical inference for fitted (lasso) models, including the bootstrap, Bayesian methods and more recent stuff</p></li>
<li><p>Chapter 7: Sparse matrix decomposition [?] (Skip?)</p></li>
<li><p>Ch 8: sparse multivariate analysis of that (Skip?)</p></li>
<li><p>Ch 9: Graphical models and their selection (Skip?)</p></li>
<li><p>Ch 10: compressed sensing (Skip?)</p></li>
<li><p>Ch 11: a survey of theoretical results for the lasso (Skip?)</p></li>
</ul>
</div>
</div>
<div id="ch2-lasso-for-linear-models" class="section level3" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Ch2: Lasso for linear models</h3>
<ul>
<li>N samples (?N observations), want to approx the response variable using a linear combination of the predoctors</li>
</ul>
<hr />
<p><strong>OLS</strong> minimizes squared-error loss but</p>
<ol style="list-style-type: decimal">
<li>Prediction accuracy</li>
</ol>
<ul>
<li>OLS unbiased but ‘often has large variance’</li>
<li>prediction accuracy can be improved by shrinking coefficients (even to zero)
<ul>
<li>yielding biased but perhaps better predictive estimators</li>
</ul></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Interpretation: too many predictors hard to interpret</li>
</ol>
<ul>
<li>DR: I do not care about this for fitting background noise in experiments</li>
</ul>
<div id="the-lasso-estimator" class="section level4" number="6.2.2.1">
<h4><span class="header-section-number">6.2.2.1</span> 2.2 The Lasso Estimator</h4>
<p>Lasso bounds the sum of the abs values of coefficients, an "$_1" constraint.</p>
<p>Lasso is OLS subject to</p>
<p><br />
</p>
<p><span class="math inline">\(\sum_{j=1..p}{\abs(\beta_j)}\leq t\)</span></p>
<p><br />
</p>
<p>“compactly” <span class="math inline">\(||\beta||_1\leq t\)</span></p>
<p>with notation for the “<span class="math inline">\(\ell_1\)</span> norm”</p>
<p><br />
</p>
<ul>
<li><p>Bound <span class="math inline">\(t\)</span> acts as a ‘budget’, must be specified by an ‘external procedure’ such as cross-validation</p></li>
<li><p>typically we must <em>standardize the predictors</em> $** so that each column is centered with unit variance … as well as the outcome variables (?) … can ignore intercept</p>
<ul>
<li>DR: Not clear here whether standardisation is necessary for the procedure to be valid or just convenient for explaining and deriving its properties.</li>
</ul></li>
</ul>
<p><br />
</p>
<div class="marginnote">
<p>Aside: Can re-write Lasso minimization st constraint as a Lagrangian. <span class="math inline">\(\lambda\)</span> plays the same role as <span class="math inline">\(t\)</span> in the constraint. Thus we can speak of the solution to the Lagrangian minimisation problem <span class="math inline">\(\hat{\beta)_{\lambda}\)</span> which also solves the bound problem with <span class="math inline">\(t=||\hat_{\lambda}||_1\)</span>.</p>
</div>
<div class="marginnote">
<p>Aside: We often remove the <span class="math inline">\(1/2n\)</span> term at the beginning of the minimization problem. Same minimization, minimizing sum of squared deviations rather than something like an average of this.</p>
</div>
<p><br />
</p>
<p>Express (Karush-Kuhn-Tucker) optimisation conditions for this …</p>
<hr />
<p>Example from Thomas (1990) on crime data</p>
<blockquote>
<p>Typically … lasso is most useful for much larger problems, including “wide” data for which <span class="math inline">\(p&gt;&gt;N\)</span></p>
</blockquote>
<p><br />
</p>
<p>Fig 2.1: Lasso vs ridge regression; coefficients of each for a set of considered variables plotted against their respective norms (as shares of maximal bound on coefficient sum measure, i.e., ols, for each)</p>
<ul>
<li>Note ridge regression penalises <em>squared</em> sums of betas
<ul>
<li>Fig 2.2., in <span class="math inline">\(\beta_1,\beta_2\)</span> space illustrates the difference well: contour lines of Resid SS elipses, ‘budget constraint’ for each (disc vs diamond)</li>
</ul></li>
</ul>
<p>(Note: lasso bound was chosen via cross-validation)</p>
<ul>
<li>No analytical statistical inference after lasso (some being developed?), bootstrap is common</li>
</ul>
<blockquote>
<p>lasso sets two of the five coefficients to zero, and tends to shrink the coefficients of the others toward zero relative to the full least-squares estimate.</p>
</blockquote>
<ul>
<li>DR: analytically and intuitively, I do not yet understand why lasso should shrink coefficients but not all the way to zero.
<ul>
<li>The penalty is linear in the coefficient size, so I would think the solution would be bang-bang, either drop a coeficient or leave it unchanged. But it is not.</li>
<li>Adding an increment to a <span class="math inline">\(\hat{\beta}\)</span> when it is below the OLS estimate should have a linear effect on the RSS (according to my memory and according to Sebastian).</li>
<li>But that would mean that shrinking one parameter always yields a better benefit to cost ratio. Thus I should shrink each parameter to zero before beginning to shrink any others. This cannot be right!</li>
</ul></li>
</ul>
<p>I looked up this derivative wrt the beta vector (one needs to set this to 0 to get the ols estimates</p>
<p><a href="https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression">stackexchange</a></p>
<p><span class="math inline">\(\frac{d RSS}{d \beta}=-2X^{T}(y-X\beta}\)</span></p>
<p>or</p>
<p><span class="math inline">\(−\frac{d e&#39;e}{d b}=2X′y+2X′Xb\)</span></p>
<p>The answer to this question: while the impact of changing each coefficient on SSR is in fact constant (a constant own-derivative), there is <em>also</em> an impact of changing one coefficient on the <em>other</em> derivatives. As one coefficient shrinks to zero the marginal impact of the other coefficients on the SSR may (will?) increase.</p>
<pre><code>- At the same time, we need that the effect of increasing it from zero need not be infinite, so it might not outweigh the linear penalty, thus some coefficients might be set to zero</code></pre>
<p><strong>Relaxed lasso</strong></p>
<blockquote>
<p>the least-squares fit on the subset of the three predictors tends to expand the lasso estimates away from zero. The nonzero estimates from the lasso tend to be biased toward zero, so the debiasing in the right panel can often improve the prediction error of the model. This two-stage process is also known as the relaxed lasso (Meinshausen 2007).</p>
</blockquote>
<ul>
<li><p>DR: When is this likely to help/hurt relative to pure lasso?</p></li>
<li><p><a href="https://stats.stackexchange.com/questions/285501/why-do-we-use-ols-to-estimate-the-final-model-chosen-by-lars/285518#285518">Stackexchange discussion</a> Contrasts a ‘relaxed-lasso’ from a ‘lars-ols’</p></li>
</ul>
<hr />
<p>Aside: which seems better for <em>Control variable selection for prediction/reducing noise to enable better inference of treatment effects</em>?</p>
<p>Ridge? better than Lasso here? We do not care about <em>interpreting</em> the predictors here… so if we allow <span class="math inline">\(\beta\)</span>‘s to be shrunk towards zero for each coefficient maybe that should yield better prediction than making them exactly zero?</p>
<p><br />
</p>
<p>On the other hand if we know the true model is ‘parsimonious’ (as in the genes problem) it might boost efficiency to allow inference about coefficients that should be exactly zero (edited)</p>
<hr />
</div>
<div id="cross-validation-and-inference" class="section level4" number="6.2.2.2">
<h4><span class="header-section-number">6.2.2.2</span> 2.3 Cross-Validation and Inference</h4>
<dl>
<dt>Generalization ability</dt>
<dd>accuracy for predicting independent test data from the same population
</dd>
</dl>
<p>… find the value of t that does best</p>
<p>**Cross-validation procedure*</p>
<ol style="list-style-type: decimal">
<li>randomly divide … dataset into K groups.</li>
</ol>
<p>“Typical choices … might be 5 or 10, and sometimes N.”</p>
<ol start="2" style="list-style-type: decimal">
<li><p>One ‘test’, remaining K-1 ‘training’</p></li>
<li><p>Apply lasso to training data for a range of t values,</p>
<ul>
<li>use each fitted model to predict the responses in the test set, recording mean-squared prediction errors for each value of t.</li>
</ul></li>
<li><p>Repeat the previous step K times</p>
<ul>
<li>each time, one of the K groups is the test data, remaining K − 1 are training data.</li>
<li>yields K different estimates of the prediction error over a range of t values.</li>
</ul></li>
<li><p>Average K estimates of prediction error for each value of t <span class="math inline">\(\rightarrow\)</span> cross-validation error curve.</p></li>
</ol>
<p>Fig 2.3 plots an example with K=10 splits for cross validation</p>
<ul>
<li>… of the estimated MS prediction error vs the relative bound <span class="math inline">\(\tilde{t}\)</span>(summed absolute value of Lasso betas divided by summed abs value of OLS betas).</li>
<li>Also draw dotted line at the 1-std-error rule choice of <span class="math inline">\(\tilde{t\)</span>}</li>
<li>Number of nonzero coefficients plotted at top</li>
</ul>
</div>
<div id="computation-of-the-lasso-solution" class="section level4" number="6.2.2.3">
<h4><span class="header-section-number">6.2.2.3</span> 2.4 Computation of the Lasso solution</h4>
<p>DR: I think I will skip this for now</p>
<p>least angle/LARS is mentioned at the bottom as a ‘homotopy method’ which “produce the entire path of solutions in a sequential fashion, starting at zero”</p>
</div>
<div id="degrees-of-freedom" class="section level4" number="6.2.2.4">
<h4><span class="header-section-number">6.2.2.4</span> 2.5 Degrees of freedom</h4>
<p>…</p>
<p>Jumping to</p>
</div>
<div id="some-perspective" class="section level4" number="6.2.2.5">
<h4><span class="header-section-number">6.2.2.5</span> 2.10 Some perspective</h4>
<p><strong>Good properties of the Lasso (<span class="math inline">\(\ell_1\)</span> penalty)</strong></p>
<ul>
<li><p>Natural interpretation (enforce sparsity and simplicity)</p></li>
<li><p>Statistical efficiency … if the underlying true signal is sparse (but if it is not sparse “no method can do well relative to the Bayes error”)</p></li>
<li><p>Computational efficiency, as <span class="math inline">\(\ell_1\)</span> penalties are convex</p></li>
</ul>
</div>
</div>
<div id="chapter-3-generalized-linear-models" class="section level3" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Chapter 3: Generalized linear models</h3>
</div>
<div id="chapter-4-generalizations-of-the-lasso-penalty" class="section level3" number="6.2.4">
<h3><span class="header-section-number">6.2.4</span> Chapter 4: Generalizations of the Lasso penalty</h3>
<blockquote>
<p>lasso does not handle highly correlated variables very well; the coefficient paths tend to be erratic and can sometimes show wild behavior.</p>
</blockquote>
<p>The elastic net makes a compromise between the ridge and the lasso penalties (Zou and Hastie 2005)1] is a parameter that can be varied.</p>
<p>For an individual coefficient the penalty is
<span class="math inline">\(\frac{1}{2} (1-\alpha)\beta_j^2 + \alpha|\beta_j|\)</span></p>
<p>(a convex combo of the lasso and ridge penalties)</p>
<p>multiplied by a ‘regularization weight’ <span class="math inline">\(\lambda&gt;0\)</span> which plays the same role (I think) as in lasso</p>
<ul>
<li>elastic net is also <em>strictly convex</em></li>
</ul>
</div>
</div>
<div id="notes-mullainathan" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Notes: Mullainathan</h2>
<blockquote>
<p>The fundamental insight behind these breakthroughs is as much statistical as computational. Machine intelligence became possible once researchers
stopped approaching intelligence tasks procedurally and began tackling them
empirically. Face recognition algorithms, for example, do not consist of hard-wired
rules to scan for certain pixel combinations, based on human understanding of
what constitutes a face. Instead, these algorithms use a large dataset of photos
labeled as having a face or not to estimate a function f (x) that predicts the presence y of a face from pixels x</p>
</blockquote>
<p>(p2)
&gt; supervised- machine learning, the focus of this article) revolves around the problem of prediction: produce predictions of y from x</p>
<p>…</p>
<blockquote>
<p>manages to fit complex and very flexible functional forms to the data without simply overfitting; it finds functions that work well out-of-sample</p>
</blockquote>
<blockquote>
<p>danger in using these tools is taking an algorithm built for [predicting <span class="math inline">\(y\)</span>-] and presuming their [parameters <span class="math inline">\(\beta\)</span>] - have the properties we typically associate with estimation output</p>
</blockquote>
<blockquote>
<p>One category of such applications appears when using new kinds of data for traditional questions; for example, in measuring economic activity using satellite images or in classifying industries using corporate 10-K filings.
Making sense of complex data such as images and text often involves a prediction
pre-processing step</p>
</blockquote>
<div class="marginnote">
<p>This middle category is most relevant for me</p>
</div>
<blockquote>
<p>In another category of applications, the key object of interest is actually a parameter … but the inference procedures (often implicitly) contain a prediction task. For example, the first stage of a linear instrumental variables regression is effectively prediction. The same is true when estimating heterogeneous treatment effects, testing for effects on multiple outcomes in experiments, and
flexibly controlling for observed confounders</p>
</blockquote>
<blockquote>
<p>A final category is in direct policy applications. Deciding which teacher to hire implicitly involves a prediction task (what added value will a given teacher have?), one that is intimately tied to the causal question of the value of an additional teacher.</p>
</blockquote>
<p>(p3)</p>
<p><strong>A useful (interactive?) example:</strong></p>
<blockquote>
<p>We consider 10,000 randomly selected owner-occupied units from the 2011 metropolitan sample of the American Housing Survey. In addition to the values of each unit, we also include 150 variables that contain information about the unit and its location, such as the number of rooms, the base area, and the census region within the United States. To compare different prediction techniques, we evaluate how well each approach predicts (log) unit value on a separate hold-out set of 41,808 units from the same sample. All details on the sample and our empirical exercise can be found in an online appendix available with this paper at <a href="http://e-jep.org" class="uri">http://e-jep.org</a></p>
</blockquote>
<blockquote>
<p>In-sample performance may overstate performance; this is especially true for certain machine learning algorithms like random forests that have a strong tendency to overfit. Second, on out-of-sample performance, machine learning algorithms such as random forests can do significantly better than ordinary least squares, even at moderate sample sizes and with a limited number of covariates</p>
</blockquote>
<p>(p4)</p>
<blockquote>
<p>algorithms are fitted on the same, randomly drawn training sample of 10,000 units and evaluated on the 41,808 remaining held-out units.</p>
</blockquote>
<blockquote>
<p>Simply including all pairwise interactions would be infeasible as it produces more regressors than data points (especially considering that some variables are categorical</p>
</blockquote>
<blockquote>
<p>Machine learning searches for these interactions automatically</p>
</blockquote>
<p>(p5)</p>
<blockquote>
<p>Shallow Regression Tree Predicting House Values</p>
</blockquote>
<div class="marginnote">
<p>not sure what’s going on here. is this the random forest thing?</p>
</div>
<blockquote>
<p>The prediction function takes the form of a tree that splits in two at every
node. At each node of the tree, the value of a single variable (say, number of bathrooms) determines whether the left (less than two bathrooms) or the right (two or
more) child node is considered next. When a terminal node-a leaf—is reached, a
prediction is returned. An</p>
</blockquote>
<p>So how
does machine learning manage to do out-of-sample prediction?
The first part of the solution is regularization. In the tree case, instead of
choosing the -best” overall tree, we could choose the best tree among those of a
certain depth.</p>
<p>(p5)
Tree depth is an example of a regularizer. It measures the complexity of a
function. As we regularize less, we do a better job at approximating the in-sample
variation, but for the same reason, the wedge between in-sample and out-of-sample</p>
<p>(p6)
how do we choose the level of regularization (-tune the algorithm”)? This is the
second key insight: empirical tuning.</p>
<p>(p6)
-tuning within the training sample
In
empirical tuning, we create an out-of-sample experiment inside the original sample.
We fit on one part of the data and ask which level of regularization leads to the best
performance on the other part of the data.4
We can increase the efficiency of this
procedure through cross-validation: we randomly partition the sample into equally
sized subsamples (-folds”). The estimation process then involves successively holding
out one of the folds for evaluation while fitting the prediction function for a range
of regularization parameters on all remaining folds. Finally, we pick the parameter
with the best estimated average performance.5
The</p>
<p>(p6)
-!
This procedure works because prediction quality is observable: both predictions y- and outcomes y are observed. Contrast this with parameter estimation, where
typically we must rely on assumptions about the data-generating process to ensure
consistency</p>
<p>(p7)
Some Machine Learning Algorithms
Function class - (and its parametrization) Regularizer R( f )
Global/parametric predictors
Linear -′x (and generalizations) Subset selection|</p>
<p>(p7)
-very useful table
Some Machine Learning Algorithms
Function class - (and its parametrization) Regularizer R( f )
Global/parametric predictors
Linear -′x (and generalizations) Subset selection||β|</p>
<p>(p7)
Random forest (linear combination of
trees</p>
<p>(p7)
-kernel in an ml framework!
Kernel regression</p>
<p>(p6)
-but can we make inferences about the structure? hypothesis testing?
Regularization combines with the observability of prediction quality to allow us to fit flexible functional forms and still find generalizable
structure.</p>
<p>(p7)
Picking the prediction function then involves two steps: The first step is, conditional on a level of complexity,
to pick the best in-sample loss-minimizing function.8
The second step is to estimate
the optimal level of complexity using empirical tuning (as we saw in cross-validating
the depth of the tree).</p>
<p>(p8)
-but they forgot to mention that others are shrunk
linear regression in which only a small number of predictors from all possible variables are chosen to have nonzero values: the absolute-value regularizer encourages
a coefficient vector where many are exactly zero.</p>
<p>(p4)
-why no ridge or elastic net?
LASSO</p>
<p>(p8)
-ensembles usually win contests
While it may be unsurprising that such ensembles perform well on average-
after all, they can cover a wider array of functional forms-it may be more surprising
that they come on top in virtually every prediction competition</p>
<p>(p8)
-neural nets broadly explained
neural nets are popular prediction algorithms for image recognition tasks. For one standard implementation in binary prediction, the underlying
function class is that of nested logistic regressions: The final prediction is a logistic
transformation of a linear combination of variables (-neurons”) that are themselves
such logistic transformations, creating a layered hierarchy of logit regressions. The
complexity of these functions is controlled by the number of layers, the number of
neurons per layer, and their connectivity (that is, how many variables from one level
enter each logistic regression on the next)</p>
<p>(p9)
These choices about how to represent the features will interact with the regularizer
and function class: A linear model can reproduce the log base area per room from
log base area and log room number easily, while a regression tree would require
many splits to do so.</p>
<p>(p9)
In a traditional estimator, replacing one set of variables by a set
of transformed variables from which it could be reconstructed would not change the
predictions, because the set of functions being chosen from has not changed. But
with regularization, including these variables can improve predictions because-at
any given level of regularization-the set of functions might change</p>
<p>(p9)
-!!
Economic theory and content expertise play a crucial role in guiding where
the algorithm looks for structure first. This is the sense in which -simply throw it
all in- is an unreasonable way to understand or run these machine learning algorithms</p>
<p>(p9)
-I need hear of using adjusted r square for this
Should out-ofsample performance be estimated using some known correction for overfitting
(such as an adjusted R2
when it is available) or using cross-validation</p>
<p>(p9)
-big unknowns
available
finite-sample guidance on its implementation-such as heuristics for the number
of folds (usually five to ten) or the -one standard-error rule” for tuning the LASSO
(Hastie, Tibshirani, and Friedman 2009)-has a more ad-hoc flavor</p>
<p>(p9)
firewall principle:
none of the data involved in fitting the prediction function-which includes crossvalidation to tune the algorithm—is used to evaluate the prediction function that
is produced</p>
<p>(p10)
-how?
First, econometrics can guide
design choices, such as the number of folds or the function class</p>
<p>(p10)
with the fitted function. Why not also use it to learn
something about the -underlying model</p>
<p>(p10)
-!!
the lack of standard errors on the coefficients. Even when machine-learning predictors produce
familiar output like linear functions, forming these standard errors can be more
complicated than seems at first glance as they would have to account for the model
selection itself. In fact, Leeb and P-tscher (2006, 2008) develop conditions under
which it is impossible to obtain (uniformly) consistent estimates of the distribution
of model parameters after data-driven selection</p>
<p>(p11)
-lasso chosen variables are unstable because of multicollinearity. a problem for making inferences from estimated coefficients
the variables are correlated with each other (say the number of rooms of a house and
its square-footage), then such variables are substitutes in predicting house prices.
Similar predictions can be produced using very different variables. Which variables
are actually chosen depends on the specific finite sample</p>
<p>(p11)
this creates an Achilles-
heel: more functions mean a greater chance that two functions with very different</p>
<p>(p12)
coefficients can produce similar prediction quality</p>
<p>(p12)
In econometric terms, while the lack of standard errors illustrates the limitations to making inference after model selection, the challenge here is (uniform)
model selection consistency itself</p>
<p>(p12)
-is this equally a problem for non sparsity based procedures like ridge?
First, it encourages the choice
of less complex, but wrong models. Even if the best model uses interactions of
number of bathrooms with number of rooms, regularization may lead to a choice
of a simpler (but worse) model that uses only number of fireplaces. Second, it can
bring with it a cousin of omitted variable bias, where we are typically concerned with
correlations between observed variables and unobserved ones. Here, when regularization excludes some variables, even a correlation between observed variables and
other observed (but excluded) ones can create bias in the estimated coefficients</p>
<p>(p12)
Some econometric results also show the converse: when there is structure,
it will be recovered at least asymptotically (for example, for prediction consistency
of LASSO-type estimators in an approximately sparse linear framework, see Belloni,
Chernozhukov, and Hansen 2011).</p>
<p>(p12)
-unrealistic for micro economic applications
Zhao and Yu (2006) who establish asymptotic model-selection consistency for the
LASSO. Besides assuming that the true model is -sparse”—only a few variables are
relevant-they also require the “irrepresentable condition” between observables:
loosely put, none of the irrelevant covariates can be even moderately related to the
set of relevant ones.
In practice, these assumptions are strong.</p>
<p>(p13)
Machine learning can
deal with unconventional data that is too high-dimensional for standard estimation
methods, including image and language information that we conventionally had
not even thought of as data we can work with, let alone include in a regression</p>
<p>(p13)
satellite data</p>
<p>(p13)
they provide us with a large x vector of image-based
data; these images are then matched (in what we hope is a representative sample)
to yield data which form the y variable. This translation of satellite images to yield
measures is a prediction problem</p>
<p>(p13)
particularly relevant where reliable data on
economic outcomes are missing, such as in tracking and targeting poverty in developing countries (Blumenstock 2016</p>
<p>(p13)
cell-phone data to measure
wealth</p>
<p>(p13)
Google Street View to
measure block-level income in New York City and Boston</p>
<p>(p13)
online posts can be made meaningful by labeling them with machine
learning</p>
<p>(p14)
extract similarity of firms from their
10-K business description texts, generating new time-varying industry classifications
for these firms</p>
<p>(p14)
and imputing even in traditional datasets. In this vein, Feigenbaum (2015a, b) applies machine-learning
classifiers to match individuals in historical records</p>
<p>(p13)
-the first prediction applications
New Data</p>
<p>(p14)
Prediction in the Service of Estimation</p>
<p>(p14)
linear instrumental variables understood as a two-stage procedure</p>
<p>(p14)
The first stage is typically handled as an estimation step. But this is effectively a
prediction task: only the predictions x- enter the second stage; the coefficients in the
first stage are merely a means to these fitted values.
Understood this way, the finite-sample biases in instrumental variables are a
consequence of overfitting</p>
<p>(p14)
-ll
overfitting. Overfitting means that the in-sample fitted values x- pick
up not only the signal -′z, but also the noise δ. As a consequence, xˆ is biased towards
x, and the second-stage instrumental variable estimate -
- is thus biased towards the
ordinary least squares estimate of y on x. Since overfit will be larger when sample
size is low, the number of instruments is high, or the instruments are weak, we can
see why biases arise in these cases</p>
<p>(p14)
same techniques applied here result in split-sample instrumental variables
(Angrist and Krueger 1995) and -jackknife” instrumental variables (Angrist,
Imbens, and Krueger 1999)</p>
<p>(p15)
-worth referencing
In particular, a set of papers has already introduced regularization into the first stage in a high-dimensional setting, including the LASSO
(Belloni, Chen, Chernozhukov, and Hansen 2012) and ridge regression (Carrasco
2012; Hansen and Kozbur 2014). More recent extensions include nonlinear functional forms, all the way to neural nets (Hartford, Leyton-Brown, and Taddy 2016</p>
<p>(p15)
Practically, even when there appears to be only a few instruments, the problem
is effectively high-dimensional because there are many degrees of freedom in how
instruments are actually constructed</p>
<p>(p15)
-a note of caution
It allows us to let the data
explicitly pick effective specifications, and thus allows us to recover more of the variation and construct stronger instruments, provided that predictions are constructed
and used in a way that preserves the exclusion restriction</p>
<p>(p15)
-this seems similar to my idea of regularising on a subset
Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey
(2016) take care of high-dimensional controls in treatment effect estimation by
solving two simultaneous prediction problems, one in the outcome and one in the
treatment equation</p>
<p>(p15)
the problem of verifying balance between treatment and control groups
(such as when there is attrition</p>
<p>(p15)
-!
Or consider the seemingly different problem of
testing for effects on many outcomes. Both can be viewed as prediction problems
(Ludwig, Mullainathan, and Spiess 2017). If treatment assignment can be predicted
better than chance from pretreatment covariates, this is a sign of imbalance. If treatment assignment can be predicted from a set of outcomes, the treatment must have
had an effect</p>
<p>(p15)
prediction task of mapping
unit-level attributes to individual effect estimates</p>
<p>(p15)
Athey
and Imbens (2016) use sample-splitting to obtain valid (conditional) inference on</p>
<p>(p16)
treatment effects that are estimated using decision trees,</p>
<p>(p16)
-look into the implication for treatment assignment with heterogeneity
heterogenous treatment effects can be used to assign treatments;
Misra and Dub- (2016) illustrate this on the problem of price targeting, applying
Bayesian regularized methods to a large-scale experiment where prices were
randomly assigned</p>
<p>(p16)
-caveat
Suppose the algorithm chooses a tree that splits on
education but not on age. Conditional on this tree, the estimated coefficients are
consistent. But that does not imply that treatment effects do not also vary by age,
as education may well covary with age; on other draws of the data, in fact, the same
procedure could have chosen a tree that split on age instead</p>
<p>(p16)
Prediction in Policy</p>
<p>(p16)
-no .. can we predict who will gain most from admission? but even if we can what can we report?
Prediction in Policy</p>

</div>
</div>
<h3> List of references</h3>
<div id="refs" class="references">
<div id="ref-osterUnobservableSelectionCoefficient2019">
<p>Oster, Emily. 2019. “Unobservable Selection and Coefficient Stability: Theory and Evidence.” <em>Journal of Business &amp; Economic Statistics</em> 37 (2): 187–204. <a href="https://doi.org/10.1080/07350015.2016.1227711">https://doi.org/10.1080/07350015.2016.1227711</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="robust-diag.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="iv-and-its-many-issues-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": true,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
