<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Section 3: identification of bounds on treatment effects; the main meat of the model | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus</title>
  <meta name="description" content="5 Section 3: identification of bounds on treatment effects; the main meat of the model | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Section 3: identification of bounds on treatment effects; the main meat of the model | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="daaronr/metrics_discussion_work" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Section 3: identification of bounds on treatment effects; the main meat of the model | Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus" />
  
  
  

<meta name="author" content="Dr. David Reinstein," />


<meta name="date" content="2020-04-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html"/>
<link rel="next" href="my-uses-for-bayesian-approaches-brainstorm.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- <script src="js/hideOutput.js"></script> -->

<!-- Mathjax -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>



<!-- open review block, in case we want it ever

<script async defer src="https://hypothes.is/embed.js"></script>
-->

<!-- Folding text box javascript thing -->

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold" ? "Unfold" : "Fold");  // if the text equals "Fold", change it to "Unfold"or else to "Fold"
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>


<script type="text/javascript">

// toggle visibility of R source blocks in R Markdown output
function toggle_R() {
  var x = document.getElementsByClassName('r');
  if (x.length == 0) return;
  function toggle_vis(o) {
    var d = o.style.display;
    o.style.display = (d == 'block' || d == '') ? 'none':'block';
  }

  for (i = 0; i < x.length; i++) {
    var y = x[i];
    if (y.tagName.toLowerCase() === 'pre') toggle_vis(y);
  }

    var elem = document.getElementById("myButton1");
    if (elem.value === "Hide Global") elem.value = "Show Global";
    else elem.value = "Hide Global";
}

document.write('<input onclick="toggle_R();" type="button" value="Hide Global" id="myButton1" style="position: absolute; top: 10%; right: 2%; z-index: 200"></input>')

</script>

<!-- Global site tag (gtag.js) - Google Analytics
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-148137970-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-148137970-3');
</script>
-->


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="econometrics-statistics-and-data-science-reinstein-notes-with-a-micro-behavioural-and-experimental-focus.html"><a href="econometrics-statistics-and-data-science-reinstein-notes-with-a-micro-behavioural-and-experimental-focus.html"><i class="fa fa-check"></i><b>1</b> Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioural, and Experimental focus</a></li>
<li class="chapter" data-level="2" data-path="notes-introduction.html"><a href="notes-introduction.html"><i class="fa fa-check"></i><b>2</b> Notes introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="notes-introduction.html"><a href="notes-introduction.html#conceptual"><i class="fa fa-check"></i><b>2.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="notes-introduction.html"><a href="notes-introduction.html#bayesian-vs.-frequentist-approaches"><i class="fa fa-check"></i><b>2.1.1</b> Bayesian vs. frequentist approaches</a></li>
<li class="chapter" data-level="2.1.2" data-path="notes-introduction.html"><a href="notes-introduction.html#causal-vs.-descriptive-treatment-effects-and-the-potential-outcomes-causal-model"><i class="fa fa-check"></i><b>2.1.2</b> Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model</a></li>
<li class="chapter" data-level="2.1.3" data-path="notes-introduction.html"><a href="notes-introduction.html#theory-restrictions-and-structural-vs-reduced-form"><i class="fa fa-check"></i><b>2.1.3</b> Theory, restrictions, and ‘structural vs reduced form’</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="notes-introduction.html"><a href="notes-introduction.html#getting-cleaning-and-using-data"><i class="fa fa-check"></i><b>2.2</b> Getting, cleaning and using data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="notes-introduction.html"><a href="notes-introduction.html#data-whatwhywherehow"><i class="fa fa-check"></i><b>2.2.1</b> Data: What/why/where/how</a></li>
<li class="chapter" data-level="2.2.2" data-path="notes-introduction.html"><a href="notes-introduction.html#good-coding-practices"><i class="fa fa-check"></i><b>2.2.2</b> Good coding practices</a></li>
<li class="chapter" data-level="2.2.3" data-path="notes-introduction.html"><a href="notes-introduction.html#new-tools-and-approaches-to-data-esp-tidyverse"><i class="fa fa-check"></i><b>2.2.3</b> New tools and approaches to data (esp ‘tidyverse’)</a></li>
<li class="chapter" data-level="2.2.4" data-path="notes-introduction.html"><a href="notes-introduction.html#data-sharing-and-integrity"><i class="fa fa-check"></i><b>2.2.4</b> Data sharing and integrity</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="notes-introduction.html"><a href="notes-introduction.html#control-strategies-and-prediction-machine-learning-approaches"><i class="fa fa-check"></i><b>2.3</b> Control strategies and prediction; Machine Learning approaches</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="notes-introduction.html"><a href="notes-introduction.html#machine-learning-statistical-learning-lasso-ridge-and-more"><i class="fa fa-check"></i><b>2.3.1</b> Machine Learning (statistical learning): Lasso, Ridge, and more</a></li>
<li class="chapter" data-level="2.3.2" data-path="notes-introduction.html"><a href="notes-introduction.html#limitations-to-inference-from-learning-approaches"><i class="fa fa-check"></i><b>2.3.2</b> Limitations to inference from learning approaches</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="notes-introduction.html"><a href="notes-introduction.html#basic-regression-and-statistical-inference-common-mistakes-and-issues"><i class="fa fa-check"></i><b>2.4</b> Basic regression and statistical inference: Common mistakes and issues</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="notes-introduction.html"><a href="notes-introduction.html#bad-control-colliders"><i class="fa fa-check"></i><b>2.4.1</b> “Bad control” (“colliders”)</a></li>
<li class="chapter" data-level="2.4.2" data-path="notes-introduction.html"><a href="notes-introduction.html#choices-of-lhs-and-rhs-variables"><i class="fa fa-check"></i><b>2.4.2</b> Choices of lhs and rhs variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="notes-introduction.html"><a href="notes-introduction.html#functional-form"><i class="fa fa-check"></i><b>2.4.3</b> Functional form</a></li>
<li class="chapter" data-level="2.4.4" data-path="notes-introduction.html"><a href="notes-introduction.html#ols-and-heterogeneity"><i class="fa fa-check"></i><b>2.4.4</b> OLS and heterogeneity</a></li>
<li class="chapter" data-level="2.4.5" data-path="notes-introduction.html"><a href="notes-introduction.html#null-effects"><i class="fa fa-check"></i><b>2.4.5</b> “Null effects”</a></li>
<li class="chapter" data-level="2.4.6" data-path="notes-introduction.html"><a href="notes-introduction.html#multiple-hypothesis-testing-mht"><i class="fa fa-check"></i><b>2.4.6</b> Multiple hypothesis testing (MHT)</a></li>
<li class="chapter" data-level="2.4.7" data-path="notes-introduction.html"><a href="notes-introduction.html#interaction-terms-and-pitfalls"><i class="fa fa-check"></i><b>2.4.7</b> Interaction terms and pitfalls</a></li>
<li class="chapter" data-level="2.4.8" data-path="notes-introduction.html"><a href="notes-introduction.html#choice-of-test-statistics-including-nonparametric"><i class="fa fa-check"></i><b>2.4.8</b> Choice of test statistics (including nonparametric)</a></li>
<li class="chapter" data-level="2.4.9" data-path="notes-introduction.html"><a href="notes-introduction.html#how-to-display-and-write-about-regression-results-and-tests"><i class="fa fa-check"></i><b>2.4.9</b> How to display and write about regression results and tests</a></li>
<li class="chapter" data-level="2.4.10" data-path="notes-introduction.html"><a href="notes-introduction.html#bayesian-interpretations-of-results"><i class="fa fa-check"></i><b>2.4.10</b> Bayesian interpretations of results</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="notes-introduction.html"><a href="notes-introduction.html#ldv-and-discrete-choice-modeling"><i class="fa fa-check"></i><b>2.5</b> LDV and discrete choice modeling</a></li>
<li class="chapter" data-level="2.6" data-path="notes-introduction.html"><a href="notes-introduction.html#robustness-and-diagnostics-with-integrity"><i class="fa fa-check"></i><b>2.6</b> Robustness and diagnostics, with integrity</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="notes-introduction.html"><a href="notes-introduction.html#how-can-diagnostic-tests-make-sense-where-is-the-burden-of-proof"><i class="fa fa-check"></i><b>2.6.1</b> (How) can diagnostic tests make sense? Where is the burden of proof?</a></li>
<li class="chapter" data-level="2.6.2" data-path="notes-introduction.html"><a href="notes-introduction.html#estimating-standard-errors"><i class="fa fa-check"></i><b>2.6.2</b> Estimating standard errors</a></li>
<li class="chapter" data-level="2.6.3" data-path="notes-introduction.html"><a href="notes-introduction.html#sensitivity-analysis-interactive-presentation"><i class="fa fa-check"></i><b>2.6.3</b> Sensitivity analysis: Interactive presentation</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="notes-introduction.html"><a href="notes-introduction.html#iv-and-its-many-issues"><i class="fa fa-check"></i><b>2.7</b> IV and its many issues</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="notes-introduction.html"><a href="notes-introduction.html#instrument-validity"><i class="fa fa-check"></i><b>2.7.1</b> Instrument validity</a></li>
<li class="chapter" data-level="2.7.2" data-path="notes-introduction.html"><a href="notes-introduction.html#heterogeneity-and-late"><i class="fa fa-check"></i><b>2.7.2</b> Heterogeneity and LATE</a></li>
<li class="chapter" data-level="2.7.3" data-path="notes-introduction.html"><a href="notes-introduction.html#weak-instruments-other-issues"><i class="fa fa-check"></i><b>2.7.3</b> Weak instruments, other issues</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="notes-introduction.html"><a href="notes-introduction.html#causal-pathways-mediation-modeling-and-its-massive-limitations"><i class="fa fa-check"></i><b>2.8</b> Causal pathways: <span>Mediation modeling and its massive limitations</span></a></li>
<li class="chapter" data-level="2.9" data-path="notes-introduction.html"><a href="notes-introduction.html#causal-pathways-selection-corners-hurdles-and-conditional-on-estimates"><i class="fa fa-check"></i><b>2.9</b> Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates</a></li>
<li class="chapter" data-level="2.10" data-path="notes-introduction.html"><a href="notes-introduction.html#other-paths-to-observational-identification"><i class="fa fa-check"></i><b>2.10</b> Other paths to observational identification</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="notes-introduction.html"><a href="notes-introduction.html#fixed-effects-and-differencing"><i class="fa fa-check"></i><b>2.10.1</b> Fixed effects and differencing</a></li>
<li class="chapter" data-level="2.10.2" data-path="notes-introduction.html"><a href="notes-introduction.html#did"><i class="fa fa-check"></i><b>2.10.2</b> DiD</a></li>
<li class="chapter" data-level="2.10.3" data-path="notes-introduction.html"><a href="notes-introduction.html#rd"><i class="fa fa-check"></i><b>2.10.3</b> RD</a></li>
<li class="chapter" data-level="2.10.4" data-path="notes-introduction.html"><a href="notes-introduction.html#time-series-ish-panel-approaches-to-micro"><i class="fa fa-check"></i><b>2.10.4</b> Time-series-ish panel approaches to micro</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="notes-introduction.html"><a href="notes-introduction.html#ex-ante-power-calculations"><i class="fa fa-check"></i><b>2.11</b> (Ex-ante) Power calculations</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="notes-introduction.html"><a href="notes-introduction.html#what-sort-of-power-calculations-make-sense-and-what-is-the-point"><i class="fa fa-check"></i><b>2.11.1</b> What sort of ‘power calculations’ make sense, and what is the point?</a></li>
<li class="chapter" data-level="2.11.2" data-path="notes-introduction.html"><a href="notes-introduction.html#power-calculations-without-real-data"><i class="fa fa-check"></i><b>2.11.2</b> Power calculations without real data</a></li>
<li class="chapter" data-level="2.11.3" data-path="notes-introduction.html"><a href="notes-introduction.html#power-calculations-using-prior-data"><i class="fa fa-check"></i><b>2.11.3</b> Power calculations using prior data</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="notes-introduction.html"><a href="notes-introduction.html#experimental-study-design-identifying-meaningful-and-useful-causal-relationships-and-parameters"><i class="fa fa-check"></i><b>2.12</b> (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="notes-introduction.html"><a href="notes-introduction.html#why-run-an-experiment-or-study"><i class="fa fa-check"></i><b>2.12.1</b> Why run an experiment or study?</a></li>
<li class="chapter" data-level="2.12.2" data-path="notes-introduction.html"><a href="notes-introduction.html#causal-channels-and-identification"><i class="fa fa-check"></i><b>2.12.2</b> Causal channels and identification</a></li>
<li class="chapter" data-level="2.12.3" data-path="notes-introduction.html"><a href="notes-introduction.html#types-of-experiments-demand-effects-and-more-artifacts-of-artifical-setups"><i class="fa fa-check"></i><b>2.12.3</b> Types of experiments, ‘demand effects’ and more artifacts of artifical setups</a></li>
<li class="chapter" data-level="2.12.4" data-path="notes-introduction.html"><a href="notes-introduction.html#generalizability-and-heterogeneity"><i class="fa fa-check"></i><b>2.12.4</b> Generalizability (and heterogeneity)</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="notes-introduction.html"><a href="notes-introduction.html#experimental-study-design-background-and-quantitative-issues"><i class="fa fa-check"></i><b>2.13</b> (Experimental) Study design: Background and quantitative issues</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="notes-introduction.html"><a href="notes-introduction.html#pre-registration-and-pre-analysis-plans"><i class="fa fa-check"></i><b>2.13.1</b> Pre-registration and Pre-analysis plans</a></li>
<li class="chapter" data-level="2.13.2" data-path="notes-introduction.html"><a href="notes-introduction.html#sequential-and-adaptive-designs"><i class="fa fa-check"></i><b>2.13.2</b> Sequential and adaptive designs</a></li>
<li class="chapter" data-level="2.13.3" data-path="notes-introduction.html"><a href="notes-introduction.html#efficient-assignment-of-treatments"><i class="fa fa-check"></i><b>2.13.3</b> Efficient assignment of treatments</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="notes-introduction.html"><a href="notes-introduction.html#experimetrics-and-measurement-of-treatment-effects-from-rcts"><i class="fa fa-check"></i><b>2.14</b> ‘Experimetrics’ and measurement of treatment effects from RCTs</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="notes-introduction.html"><a href="notes-introduction.html#which-error-structure-random-effects"><i class="fa fa-check"></i><b>2.14.1</b> Which error structure? Random effects?</a></li>
<li class="chapter" data-level="2.14.2" data-path="notes-introduction.html"><a href="notes-introduction.html#randomization-inference"><i class="fa fa-check"></i><b>2.14.2</b> Randomization inference?</a></li>
<li class="chapter" data-level="2.14.3" data-path="notes-introduction.html"><a href="notes-introduction.html#parametric-and-nonparametric-tests-of-simple-hypotheses"><i class="fa fa-check"></i><b>2.14.3</b> Parametric and nonparametric tests of simple hypotheses</a></li>
<li class="chapter" data-level="2.14.4" data-path="notes-introduction.html"><a href="notes-introduction.html#adjustments-for-exogenous-but-non-random-treatment-assignment"><i class="fa fa-check"></i><b>2.14.4</b> Adjustments for exogenous (but non-random) treatment assignment</a></li>
<li class="chapter" data-level="2.14.5" data-path="notes-introduction.html"><a href="notes-introduction.html#iv-in-an-experimental-context-to-get-at-mediators"><i class="fa fa-check"></i><b>2.14.5</b> IV in an experimental context to get at ‘mediators’?</a></li>
<li class="chapter" data-level="2.14.6" data-path="notes-introduction.html"><a href="notes-introduction.html#heterogeneity-in-an-experimental-context"><i class="fa fa-check"></i><b>2.14.6</b> Heterogeneity in an experimental context</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="notes-introduction.html"><a href="notes-introduction.html#the-bayesian-approach"><i class="fa fa-check"></i><b>2.15</b> The Bayesian approach</a></li>
<li class="chapter" data-level="2.16" data-path="notes-introduction.html"><a href="notes-introduction.html#making-inferences-from-previous-work-meta-analysis-combining-studies"><i class="fa fa-check"></i><b>2.16</b> Making inferences from previous work; Meta-analysis, combining studies</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="notes-introduction.html"><a href="notes-introduction.html#publication-bias"><i class="fa fa-check"></i><b>2.16.1</b> Publication bias</a></li>
<li class="chapter" data-level="2.16.2" data-path="notes-introduction.html"><a href="notes-introduction.html#combining-a-few-your-own-studiesestimates"><i class="fa fa-check"></i><b>2.16.2</b> Combining a few (your own) studies/estimates</a></li>
<li class="chapter" data-level="2.16.3" data-path="notes-introduction.html"><a href="notes-introduction.html#full-meta-analyses"><i class="fa fa-check"></i><b>2.16.3</b> Full meta-analyses</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="notes-introduction.html"><a href="notes-introduction.html#some-key-resources-and-references"><i class="fa fa-check"></i><b>2.17</b> Some key resources and references</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="notes-introduction.html"><a href="notes-introduction.html#consider"><i class="fa fa-check"></i><b>2.17.1</b> Consider:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><i class="fa fa-check"></i><b>3</b> Causal pathways: <span>Mediation modeling and its massive limitations</span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#mediators-and-selection-and-roy-models-a-review-considering-two-research-applications"><i class="fa fa-check"></i><b>3.1</b> Mediators (and selection and Roy models): a review, considering two research applications</a></li>
<li class="chapter" data-level="3.2" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#dr-initial-thoughts-for-nl-education-paper"><i class="fa fa-check"></i><b>3.2</b> DR initial thoughts (for NL education paper)</a></li>
<li class="chapter" data-level="3.3" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#econometric-mediation-analyses-heckman-and-pinto"><i class="fa fa-check"></i><b>3.3</b> Econometric Mediation Analyses (Heckman and Pinto)</a>
<ul>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#relevance-to-parey-et-al"><i class="fa fa-check"></i>Relevance to Parey et al</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#summary-and-key-modeling"><i class="fa fa-check"></i><b>3.4</b> Summary and key modeling</a></li>
<li class="chapter" data-level="3.5" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#pinto-2015-selection-bias-in-a-controlled-experiment-the-case-of-moving-to-opportunity"><i class="fa fa-check"></i><b>3.5</b> Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity</a></li>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#summary"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#relevance-to-parey-et-al-1"><i class="fa fa-check"></i>Relevance to Parey et al</a></li>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#introduction"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#identification-strategy-brief"><i class="fa fa-check"></i>Identification strategy brief</a></li>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#results-in-brief"><i class="fa fa-check"></i>Results in brief</a></li>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#framework-first-for-binarybinary-simplification"><i class="fa fa-check"></i>Framework: first for binary/binary (simplification)</a></li>
<li class="chapter" data-level="" data-path="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html"><a href="causal-pathways-mediation-modeling-and-its-massive-limitations-1.html#framework-for-mto-multiple-treatment-groups-multiple-choices"><i class="fa fa-check"></i>Framework for MTO multiple treatment groups, multiple choices</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html"><a href="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html"><i class="fa fa-check"></i><b>4</b> Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates</a>
<ul>
<li class="chapter" data-level="4.1" data-path="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html"><a href="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html#corner-solution-or-hurdle-variables-and-conditional-on-positive"><i class="fa fa-check"></i><b>4.1</b> ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’</a></li>
<li class="chapter" data-level="4.2" data-path="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html"><a href="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html#bounding-approaches-lee-manski-etc-1"><i class="fa fa-check"></i><b>4.2</b> Bounding approaches (Lee, Manski, etc)</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html"><a href="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html#notes-training-wages-and-sample-selection-estimating-sharp-bounds-on-treatment-effects-david-lee-2009-restud"><i class="fa fa-check"></i><b>4.2.1</b> Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-3-identification-of-bounds-on-treatment-effects-the-main-meat-of-the-model.html"><a href="section-3-identification-of-bounds-on-treatment-effects-the-main-meat-of-the-model.html"><i class="fa fa-check"></i><b>5</b> Section 3: identification of bounds on treatment effects; the main meat of the model</a></li>
<li class="chapter" data-level="6" data-path="my-uses-for-bayesian-approaches-brainstorm.html"><a href="my-uses-for-bayesian-approaches-brainstorm.html"><i class="fa fa-check"></i><b>6</b> My uses for Bayesian approaches (brainstorm)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="my-uses-for-bayesian-approaches-brainstorm.html"><a href="my-uses-for-bayesian-approaches-brainstorm.html#meta-analysis-of-previous-evidence"><i class="fa fa-check"></i><b>6.1</b> Meta-analysis of previous evidence</a></li>
<li class="chapter" data-level="6.2" data-path="my-uses-for-bayesian-approaches-brainstorm.html"><a href="my-uses-for-bayesian-approaches-brainstorm.html#inference-particularly-about-null-effects"><i class="fa fa-check"></i><b>6.2</b> Inference, particularly about ‘null effects’</a></li>
<li class="chapter" data-level="6.3" data-path="my-uses-for-bayesian-approaches-brainstorm.html"><a href="my-uses-for-bayesian-approaches-brainstorm.html#policy-and-business-implications-and-recommendations"><i class="fa fa-check"></i><b>6.3</b> ‘Policy’ and business implications and recommendations</a></li>
<li class="chapter" data-level="6.4" data-path="my-uses-for-bayesian-approaches-brainstorm.html"><a href="my-uses-for-bayesian-approaches-brainstorm.html#theory-driven-inference-about-optimizing-agents-esp.-in-strategic-settings"><i class="fa fa-check"></i><b>6.4</b> Theory-driven inference about optimizing agents, esp. in strategic settings</a></li>
<li class="chapter" data-level="6.5" data-path="my-uses-for-bayesian-approaches-brainstorm.html"><a href="my-uses-for-bayesian-approaches-brainstorm.html#experimental-design"><i class="fa fa-check"></i><b>6.5</b> Experimental design</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="title-introduction-to-bayesian-analysis-in-r-and-stata-katz-qstep.html"><a href="title-introduction-to-bayesian-analysis-in-r-and-stata-katz-qstep.html"><i class="fa fa-check"></i><b>7</b> Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep”</a></li>
<li class="chapter" data-level="8" data-path="why-and-when-use-bayesian-mcmc-methods.html"><a href="why-and-when-use-bayesian-mcmc-methods.html"><i class="fa fa-check"></i><b>8</b> Why and when use Bayesian (MCMC) methods?</a>
<ul>
<li class="chapter" data-level="8.1" data-path="why-and-when-use-bayesian-mcmc-methods.html"><a href="why-and-when-use-bayesian-mcmc-methods.html#pros"><i class="fa fa-check"></i><b>8.1</b> Pros</a></li>
<li class="chapter" data-level="8.2" data-path="why-and-when-use-bayesian-mcmc-methods.html"><a href="why-and-when-use-bayesian-mcmc-methods.html#cons"><i class="fa fa-check"></i><b>8.2</b> Cons</a></li>
<li class="chapter" data-level="8.3" data-path="why-and-when-use-bayesian-mcmc-methods.html"><a href="why-and-when-use-bayesian-mcmc-methods.html#why-more-popular-today"><i class="fa fa-check"></i><b>8.3</b> Why more popular today?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>9</b> Theory</a>
<ul>
<li class="chapter" data-level="9.1" data-path="theory.html"><a href="theory.html#so-how-do-we-estimate-it"><i class="fa fa-check"></i><b>9.1</b> So how do we estimate it?</a></li>
<li class="chapter" data-level="9.2" data-path="theory.html"><a href="theory.html#linear-regression-model-example"><i class="fa fa-check"></i><b>9.2</b> Linear regression model example</a></li>
<li class="chapter" data-level="9.3" data-path="theory.html"><a href="theory.html#gibbs"><i class="fa fa-check"></i><b>9.3</b> Gibbs</a></li>
<li class="chapter" data-level="9.4" data-path="theory.html"><a href="theory.html#metropolis-hastings"><i class="fa fa-check"></i><b>9.4</b> Metropolis Hastings</a></li>
<li class="chapter" data-level="9.5" data-path="theory.html"><a href="theory.html#assessing-convergence"><i class="fa fa-check"></i><b>9.5</b> Assessing convergence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="comparing-models-equivalent-of-likelihood.html"><a href="comparing-models-equivalent-of-likelihood.html"><i class="fa fa-check"></i><b>10</b> Comparing models … Equivalent of ‘likelihood’</a></li>
<li class="chapter" data-level="11" data-path="on-choosing-priors.html"><a href="on-choosing-priors.html"><i class="fa fa-check"></i><b>11</b> On choosing priors</a></li>
<li class="chapter" data-level="12" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i><b>12</b> Implementation</a></li>
<li class="chapter" data-level="13" data-path="generate-predictions-from-a-winbugs-model.html"><a href="generate-predictions-from-a-winbugs-model.html"><i class="fa fa-check"></i><b>13</b> Generate predictions from a WinBUGS model</a></li>
<li class="chapter" data-level="14" data-path="missing-data-case.html"><a href="missing-data-case.html"><i class="fa fa-check"></i><b>14</b> Missing data case</a></li>
<li class="chapter" data-level="15" data-path="stata.html"><a href="stata.html"><i class="fa fa-check"></i><b>15</b> Stata</a></li>
<li class="chapter" data-level="16" data-path="r-mcmc-pac.html"><a href="r-mcmc-pac.html"><i class="fa fa-check"></i><b>16</b> R mcmc pac</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-3-identification-of-bounds-on-treatment-effects-the-main-meat-of-the-model" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Section 3: identification of bounds on treatment effects; the main meat of the model</h1>
<p>He starts with a simple example. He begins with a model with a treatment indicator and no other covariates, and a continuous outcome variable, but notes that this will clearly apply to discrete outcome variables and will also apply conditional on controls.</p>
<p>Nest, he brings forward the statement… from the earlier selection models. In each case the latent variable must overcome a hurdle for the outcome to be observed and in fact <em>the hurdle differs depending on the impact of the treatment itself</em>. In general <em>when the errors in the selection and outcome equations are correlated the difference in these means differs from the actual treatment effect</em>. In other words through a slightly complicated story, when those who have unobservables that make them more likely to work also tend to have unobservables that would make them likely to earn more the standard difference in outcomes between control and treatment will <em>not</em> describe the true treatment effect.</p>
<p><br> </p>
<p><em>A key insight</em> seems to be that we could identify the treatment effect if we could estimate the expected outcome given treatment <em>and</em> given that the unobservable component in the selection equation would lead to an observable outcome had the person <em>not</em> been given treatment. If so, we could subtract the observed mean control outcome from the above to yield the true treatment effect (for those who would be observed always). However, we obviously do not observe this because we only observe the outcomes for those who are treated where the selection equation <em>is</em> in fact positive and not “where the selection equation <em>would have</em> been positive had they not been treated.”</p>
<p>However, the insight here is that this term can in fact be bounded. We <em>do</em> observe these outcomes for the treated people (note we are assuming without loss of generality that the treatment raises the probability of selection for this discussion) but we don’t know exactly which ones they are. In other words, we observe outcomes for more people in the treatment group than we need; we wish we could figure out what is the subset of these that would have <em>also</em> been observed had they not been treated, so we could compare like-to-like. The observed treatment mean is a weighted average of the thing we are seeking (to difference from the control) and “the mean for a subpopulation of marginal individuals… that are induced to be selected into the sample <em>because</em> of the treatment”</p>
<p>This then gets us the upper bound for the term expressing the treatment outcome for those who would have been observed even if they had been in the control. The upper bound for this is the expected outcome for those in the treated group (who are observed of course) and who are in quantile-p or above of the outcome, where this <span class="math inline">\(p\)</span> is the share of the treated population that are in the marginal group we referred to that were only induced to be selected into the sample because of the treatment.</p>
<p>In other words the worst case scenario is that the smallest share <span class="math inline">\(p\)</span> values of <span class="math inline">\(Y\)</span> are in the marginal group and the largest one (which is share 1-pone are in the inframarginal group. We don’t know which observations are inframarginal and which ones are marginal.</p>
<p><span class="math inline">\(p\)</span>: the share of marginal individuals and (1-p) the share of inframarginal individuals (the latter is group we want the average outcome for). The highest could be would be the average outcomefor the largest (1-p) share of this group. We are looking for the expectation given that they are at or above at will at or above percentile p within this group.</p>
<p>In other words we trim the lower tail of the Y distribution by the portion <span class="math inline">\(p\)</span>, (so what remains is the 1-p share) to get the upper bound for the inframarginal groups mean. We can then subtract the mean for the control group to get an upper bound for the treatment effect.</p>
<p>To compute this “trimming proportion p”: this p is equal to the share of the treated group whose outcome is observed minus the share of the control group whose outcome is deserved is observed, divided by the share of the treatment group where the outcome is observed.
Something like the <em>increased likelihood of observation that is driven by the treatment, as a share of the total number as a share of the probability of observation in the treatment group</em>.</p>
<p>The average observed outcome for the treatment group is including too many observations; we need to difference out the share of observations that are observed only because the treatment caused them to be observed; this share is certainly no larger than the increased probability of observation in the treatment group as a share of the probability of observations the treatment group.</p>
<p>Another much simpler way of saying this is “trimming the data by the known proportion of excess individuals” in the treatment group. (To gain bounds on the mean for the inframarginal group which we can then difference from the control-group mean get the treatment effect).</p>
<p>Perhaps some intuition for why this improves on the Horwitz model: we don’t need to assume that those observed in the treatment group that wouldn’t have been observed in the control would’ve had the highest possible outcomes. No, we only need to assume (to get the upper bound) that these came from the highest <em>distribution</em> because they had to come from somewhere. These were the people in the upper tail of the relevant group but they couldn’t <em>all</em> have been the individual highest achiever.</p>
<hr />
<p>The model is extended to heterogeneity and heteroscedasticity. This begins with the independence of treatment assignment the “potential sample selection indicators” for either treatment or control, in other words whether that individual will have an observed outcome under treatment and whether that the individual would have an observed outcome under control, and the latent potential outcomes.</p>
<p>Experimental or random assignment ensures that each of the potential outcomes (and the correspondence to observability under each treatment) is independent of the actual assigned treatment.</p>
<p>The second assumption is monotonicity: treatment assignment can only affect sample selection in one direction.</p>
<p>– DR: For our (substitution) experiments, it is in fact not clear to me whether this should necessarily be the case, as some (less generous?) people may be induced to leave because of having been asked to donate, while potentially other (more generous people) might be induced to return given that they were asked to donate. (This proposed nonmonotonicity implies that the ‘asked twice’ sample tends to weed out the less generous, which would lead to a bias <em>against</em> substitution, strengthening the case for our result.)
- DR, aside: However, even though the paper doesn’t say it, I suspect this assumption could be weakened and you would still get some similar bounds.
To put it another way, I would imagine that these bounds could be adjusted based on some reasonable ad hoc assumptions about the share of the population who is affected in either direction.</p>
<p>– @NL: I’m coming to think that our Dutch data problems are more things involving “hurdle models”. Can this technique also be applied to such hurdle models?</p>
<p>Next proposition 1a states that given these assumptions we can derive sharp lower and upper bounds for the average treatment effect (conditional on ‘would be observed in both states’). Note that for this estimator if the probability of observation is greater under the treatment we need to trim the treatment groups outcome distribution and if the probability of observation is greater under the control we need to trim the control group’s outcome distribution.</p>
<ul>
<li>DR, aside comment: we seem to be throwing out a bit of the data in these estimates, which would suggest that something more efficient could be generated.</li>
</ul>
<p>(The stated bounds you can estimate are exactly the same as the bounds from the previous specification, at least as I had interpreted the way they would be produced.)</p>
<p>Their remark 2 notes that an implication is that as <span class="math inline">\(P_0\)</span>, that as the “difference between the relative probability of observation of an outcome under treatment versus control” tends to zero,
i.e., as the probability of having an observed outcome (or the conditional probability of this) is the same for treatment and control) then there is no sample selection bias.</p>
<p>Their estimate convergences to the estimate he calls an estimate for the “always takers subpopulation… except that taking… is selection into the [outcome-observed] sample.”</p>
<p><em>So, a very vanilla estimator is acceptable if we find the same conditional probability of selection for each group, under monotonicity, which, for this case, we can test (see Remark 4 below).</em></p>
<p>– (DR: To me this suggests that there might be something wrong going on here. Intuitively, If I simply observe the same rate of attrition in the treatment and control groups this <em>shouldn’t</em> be enough to tell me that attrition did not matter, as it could occur differentially for both groups, but it seems to be a result here; this is probably due to the assumption of monotonicity of the selection/observation term, as well as the random/exogenous assignment to each group.)</p>
<hr />
<p>Remark 3 discusses the importance of monotonicity for the bounds, saying this assumption is “minimally sufficient” (I think it would be better to say minimally sufficient for these particular bounds that he computed). To demonstrate this he gives an extreme example. Without monotonicity it could be (note: this would seem like a very unlikely outcome!) that every observation in the control group comes from the population in the treatment group that would <em>not</em> have been observed had they been treated and every observation in the treatment group happens to come from the set of people that would <em>not</em> have been observed had they been in the control group. These two “subpopulations do not overlap, so the difference in the means could not be interpreted as a causal effect.”</p>
<p>– DR, aside : there must be some way to impose some restrictions on this even allowing for this non-monotonicity. (He notes that this can be improved upon somewhat by thinking about the total the idea that the total masses of unobserved that would’ve been observed in the other group can’t be greater than the share that is not observed in the other treatment group, but this doesn’t seem like a particularly fruitful route as it in most reasonable cases will still allow for very wide bounds.)</p>
<hr />
<p>Remark 4 suggests that if we can assume (or somehow observe?) that the conditional probabilities of selection are the same for treatment and control, we can <em>test whether monotonicity in fact holds</em> and the simple difference in means will be an appropriate estimate of the treatment effect. Here, the assumption implies that everyone in the treatment or control group would have been observed under the opposite treatment as well. This in fact implies that the distribution of the exogenous variables should be the same in the treatment and control groups conditional on being selected. This seems fairly intuitive, we look at whether selection seems to be occurring in different ways are on different margins for the two groups treatment versus control.</p>
<p>Apparently for this test to have <em>power</em> we need that the subpopulations of “noncompliant errors in opposite directions” (quotation mine) must have <em>distinct</em> distributions of baselines exogenous characteristics. If these were the same then whether or not monotonicity holds the test doesn’t tell us anything.</p>
<p>– DR: <em>I wonder if anyone uses this test for Monotonicity under non-differential selection?</em></p>
<p>Another relevant note that he bundles in this remark is that the technique here only yields estimates <em>for those who would be with an observed outcome for either treatment or control.</em> One could <em>additionally</em> try to bound this as an estimate for the entire population using the Horwitz and Manski bounds for this latter thing. However, in many contexts there are reasons that the bounded estimates they mainly use are the relevant ones, such as “the impact of the program on wage rates for those whose employment status was not affected by the program.”</p>
<ul>
<li>DR: In our substitution experiment case, the substitution patterns for those for whom attrition was not affected by the first-round-charity treatment</li>
<li>@NL: E.g., the impact of an institution on income for those whose choice to remain in the course was not affected by their institutional assignment</li>
</ul>
<hr />
<p>“Narrowing bounds using covariates”</p>
<p>All of the above could be done conditional on a particular set of baseline characteristics such as gender or race. The average treatment effect could be estimated separately for each. (Note: and perhaps combined in a fruitful way?)</p>
<p>One can alternately use covariates to reduce the width of these bounds. To give intuition, we can imagine a baseline covariate that perfectly predicts an individual’s wage. Because treatments are randomly assigned the maintained assumptions will still hold conditionally on this X. The results the methods can be applied separately for each value of this covariate, and for each such value the trimming procedure will actually have no impact on the estimate.</p>
<p>DR: I think this is the “estimate and sum things up in a weighted way” procedure I thought about a moment ago.</p>
<p><br> </p>
<p>Proposition 1B gives the balance estimator for a model involving exogenous variables. Essentially, this computes the corresponding bounds estimator at each X, where the differential selection probability is computed for that particular X, the upper quantile value of the outcome is given conditional on the same X and on being in the treatment group. These are then integrated (or summed up) weighted by the distribution or the cdf of this covariate in the control group. These bounds will necessarily be sharper than the balance without controls.</p>
<p>#Section 4: estimation and inference</p>
<p>The asymptotic variance depends on components reflecting the variance of the trimmed distribution, the variance of the estimated trimming threshold, and the variance in the estimate of “how much of the distribution to trim” (the relative selection probability differential).</p>
<p>Equation 6 formally defines the estimator</p>
<p>Estimated bounds consistent for ‘true bounds’ under standard conditions</p>
<p>Two ways to compute CI’s – CI’s for the ‘true bounds’ or CI’s for the TE itself. A 95% CI for the former will contain the latter with even greater probability.</p>
<p>Imbens and Manski ‘04 can be used to derive the latter which are ’more apppropriate here’ since the object of interest is the TA and not the ’region of all rationalizable treatment effects.
These are built off of a transformation of the estimate UB and LB and max estimated sd of each of these.</p>
<ul>
<li>the latter are reported by the ‘cie’ option in ‘leebounds’</li>
</ul>
<p>Generalisation to monotonicity (without knowing direction of impact of treatment on selection)…</p>
<blockquote>
<p>As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative… [do similar]</p>
</blockquote>
<blockquote>
<p>though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small and p0 is “close” to zero
… A simple, conservative approach to combining the trimmed and untrimmed intervals is to compute their union</p>
</blockquote>
<p>#Section 5: Empirical Results</p>
<p>Table 4 gives a step-by-step that is a good way of seeing and understanding the construction of the estimator, and where the ‘action’ is, in treimming, in components of the SE, etc.</p>
<p>Intervals are 1/14 the width of the equivalent Horowitz/Manski bounds</p>
<p>##5.2 using covariates to narrow bounds</p>
<blockquote>
<p>Any baseline covariate will do, as will any function of all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to be a proxy for the predicted wage potential for each individual—is constructed from a linear combination of all observed baseline characteristics. This single covariate is then discretized, so that effectively five groups are formed according to whether the predicted wage is within intervals defined by $6·75, $7, $7·50, and $8·50.</p>
</blockquote>
<ul>
<li><span class="citation">(<span class="citeproc-not-found" data-reference-id="Substitution"><strong>???</strong></span>)</span>: this is essentially what I propose we do, but using Ridge Regressions or something similar</li>
</ul>
<blockquote>
<p>To compute the bounds for the overall average…the group-specific bounds must be averaged, weighted by the proportion (sPr Group J|S0=1,S1=1)</p>
</blockquote>
<blockquote>
<p>The estimated asymptotic variance for these overall averages is the sum of (1) a weighted average of the group-specific variances and (2) the (weighted-) mean squared deviation of the group-specific estimates from the overall mean. This second term takes into account the sampling variability of the weights</p>
</blockquote>
<p><span class="math inline">\(\rightarrow\)</span> result: 11% narrower bounds</p>
<p><br> </p>
<p><em>Interesting; possibly do similar for @NL-ed</em>:</p>
<blockquote>
<p>By statistically ruling out any effect more negative than −0·037, this suggests that after 4 years, the Job Corps enabled program group members to offset at least 35% (and perhaps more) of the potential 0·058 loss in wages due to lost labour market experience that could have been caused by the program</p>
</blockquote>
<p>#Section 6: Conclusions: implications and applications</p>
<p>Interesting intuitive argument:</p>
<blockquote>
<p>Another reason to interpret the evidence as pointing to positive wage effects is that the lower bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly negatively correlated with the propensity to be employed. From a purely theoretical standpoint, a simple labour supply model suggests that, all other things equal, those on the margin of being employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be the smallest wage that draws the individual into the labour force). In addition, the empirical evidence in Table 2 suggests that there is positive selection into employment: those who are predicted to have higher wages are more likely to be employed (i.e., U and V are positively correlated). If this is true, it seems relatively more plausible to trim the lower rather than the upper tail of the distribution to get an estimate of the treatment effect.</p>
</blockquote>

<p># Notes on Bayesian approaches – David Reinstein</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="my-uses-for-bayesian-approaches-brainstorm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": true,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
