[
["introduction.html", "Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus 1 Introduction 1.1 (Conceptual: approaches to statistics/inference and causality)[#conceptual] 1.2 Getting, cleaning and using data; project management and coding 1.3 Basic regression and statistical inference: Common mistakes and issues 1.4 LDV and discrete choice modeling 1.5 Robustness and diagnostics, with integrity 1.6 Control strategies and prediction; Machine Learning approaches 1.7 IV and its many issues 1.8 Other paths to observational identification 1.9 Causal pathways: Mediation modeling and its massive limitations 1.10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 1.11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 1.12 (Experimental) Study design: Background and quantitative issues 1.13 (Experimental) Study design: (Ex-ante) Power calculations 1.14 ‘Experimetrics’ and measurement of treatment effects from RCTs (#experimetrics_te) 1.15 Making inferences from previous work; Meta-analysis, combining studies 1.16 The Bayesian approach 1.17 Some key resources and references", " Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus Dr. David Reinstein, 2020-05-25 Abstract This ‘book’ organizes my notes and helps others understand it and learn from it 1 Introduction My goal in putting this resource is to focus on the practical tools I use and the challenges I (David Reinstein) face. But I am open to collaboration with others on this. My focus: Microeconomics, behavioral economics, focus on charitable giving and ‘returns to education’ type of straightforward problems. (Minimal focus on structural approaches.) What I care about: Where we can add value to real econometric (and statistical and experimental design) practice? The data I focus on: Observational (esp. web-scraped and API data and national surveys/admin data) Experimental: esp. where with multiple crossed arms, and where the ‘cleanest design’ may not be possible I will assume familiarity with most basic statistical concepts like ‘bias’, ‘consistency’, and ‘null hypothesis testing.’ However, I will focus on some concepts that seem to often be misunderstood and mis-applied. If you are involved with this project, you can find a brief guide (somewhat WIP) on how to add content (HERE)[https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html]. This is from a different project but the setup is basically the same. A note to potential research assistants and student collaborators (unfold): 1.1 (Conceptual: approaches to statistics/inference and causality)[#conceptual] Bayesian vs. frequentist approaches Folder: bayesian Notes: bayes_notes 1.1.1 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 1.1.1.1 DAGs and Potential outcomes 1.1.2 Theory, restrictions, and ‘structural vs reduced form’ 1.2 Getting, cleaning and using data; project management and coding This will build on my content here, and integrate with it. 1.2.1 Data: What/why/where/how 1.2.2 Organizing a project 1.2.3 Dynamic documents (esp Rmd/bookdown) 1.2.4 Good coding practices 1.2.4.1 New tools and approaches to data (esp ‘tidyverse’) 1.2.4.2 Style and consistency Indenting, snake-case, etc 1.2.4.3 Using functions, variable lists, etc., for clean, concise, readable code 1.2.5 Data sharing and integrity 1.3 Basic regression and statistical inference: Common mistakes and issues 1.3.1 “Bad control” (“colliders”) Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita) 1.3.2 Choices of lhs and rhs variables Missing data Choice of control variables and interactions Which outcome variable/variables 1.3.3 Functional form Logs and exponentials Nonlinear modeling (and interpreting coefficients) 1.3.3.1 ‘Testing for nonlinear terms’ Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18 http://datacolada.org/62 1.3.4 OLS and heterogeneity OLS does not identify the ATE http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT Modeling heterogeneity: the limits of Quantile re regression 1.3.5 “Null effects” “While the classical statistical framework is not terribly clear about when one should”“accept”\" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a \"“zero or minimal effect”\" one would want to find these closely bounded around zero under a variety of conditions for generalizability. In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks”.\" Harms-lakens-18 1.3.5.1 Confidence intervals and Bayesian credible intervals 1.3.5.2 Comparing relative parameters E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men”. This doesn’t cut it: we need to see a statistical test for the difference in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing). 1.3.6 Multiple hypothesis testing (MHT) See (???) 1.3.7 Interaction terms and pitfalls 1.3.7.1 ‘Moderators’ Confusion with nonlinearity Moderators: Heterogeneity mixed with nonlinearity/corners In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in’. Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative 1.3.7.2 MHT 1.3.8 Choice of test statistics (including nonparametric) 1.3.9 How to display and write about regression results and tests 1.3.10 Bayesian interpretations of results (see ‘the Bayesian Approach’) 1.4 LDV and discrete choice modeling 1.5 Robustness and diagnostics, with integrity 1.5.1 (How) can diagnostic tests make sense? Where is the burden of proof? 1.5.2 Estimating standard errors 1.5.3 Sensitivity analysis: Interactive presentation 1.6 Control strategies and prediction; Machine Learning approaches 1.6.1 Machine Learning (statistical learning): Lasso, Ridge, and more 1.6.2 Limitations to inference from learning approaches 1.7 IV and its many issues 1.7.1 Instrument validity Exogeneity vs. exclusion Very hard to ‘powerfully test’ 1.7.2 Heterogeneity and LATE Basic consideration: what does IV identify and when:? Focusing on a binary endogenous ‘treatment’ variable With heterogeneity With imperfect compliance With one-way compliance 1.7.3 Weak instruments, other issues With a ‘weak instrument’ … why does that matter? 1.7.4 Reference to the use of IV in experiments/mediation 1.8 Other paths to observational identification 1.8.1 Fixed effects and differencing 1.8.2 DiD FE/DiD does not rule out a correlated dynamic unobservable, causing a bias 1.8.3 RD 1.8.4 Time-series-ish panel approaches to micro 1.8.4.1 Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ 1.9 Causal pathways: Mediation modeling and its massive limitations An applied review 1.10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 1.10.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ “Conditional on positive”/“intensive margin” analysis ignores selection “Conditional on positive”/“intensive margin” analysis ignores selection identification issue See Angrist and Pischke on “Good CoP, bad CoP”. See also bounding approaches such as (???) AngristJ.D.2008a, 1.10.1.1 Bounding approaches (Lee, Manski, etc) See Notes on Lee bounds 1.11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 1.11.1 Why run an experiment or study? Sugden and Sitzia critique here, give more motivation 1.11.2 Causal channels and identification Ruling out alternative hypotheses, etc 1.11.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 1.11.4 Generalizability (and heterogeneity) 1.12 (Experimental) Study design: Background and quantitative issues 1.12.1 Pre-registration and Pre-analysis plans 1.12.1.1 The hazards of specification-searching 1.12.2 Sequential and adaptive designs Needs to adjust significance tests for augmenting data/sequential analysis/peeking Statistics/econometrics new-statistics sagarin_2014 http://www.paugmented.com/ resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/ Yet … P_augmented may overstate type-1 error rate Statistics/econometrics response to referees, new-statistics \" A process involving stopping \"“whenever the nominal \\(p.0.5\\)”\" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5%. Even if the subsequent data suggested a \"“one in a million chance of arising under the null”\" the overall process yields a 5%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the \"“likelihood of the null hypothesis”\" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate. Considering the calculations in , it is clear that \\(p_{augmented}\\) should the type-1 error of the process if there is a positive probability that after an initial experiment attains p\\(&lt;0.05\\), more data is collected. A headline \\(p&lt;0.05\\) does imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded \\(p&gt;0.05\\), thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though \\(p&lt;0.05\\). Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine’s experiment). Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p&lt;0.05, more data is collected, implying that \\(p_{augmented}\\) as calculated above overstates the type I error rate that would arise from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below \\(p=0.05\\), in order to attain a type-1 error rate of 5% or less in our published corpus.\" 1.12.3 Efficient assignment of treatments (Links back to power analyses) 1.13 (Experimental) Study design: (Ex-ante) Power calculations 1.13.1 What sort of ‘power calculations’ make sense, and what is the point? 1.13.1.1 The ‘harm to science’ from running underpowered studies \"One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results. Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.\" verkaik2016, metzger2015 1.13.2 Power calculations without real data 1.13.3 Power calculations using prior data 1.14 ‘Experimetrics’ and measurement of treatment effects from RCTs (#experimetrics_te) 1.14.1 Which error structure? Random effects? 1.14.2 Randomization inference? 1.14.3 Parametric and nonparametric tests of simple hypotheses 1.14.4 Adjustments for exogenous (but non-random) treatment assignment 1.14.5 IV in an experimental context to get at ‘mediators’? 1.14.6 Heterogeneity in an experimental context 1.15 Making inferences from previous work; Meta-analysis, combining studies 1.15.1 Publication bias 1.15.2 Combining a few (your own) studies/estimates 1.15.3 Full meta-analyses Models to address publication biases 1.16 The Bayesian approach 1.17 Some key resources and references (Angrist J. D. and Pischke 2008) ‘The Mixtape’ (Cunningham) (Kennedy 2003) (Tibshirani, n.d.) OSF guides Christensen ea “Transparent and Reproducable Social Science Research” (Gentzkow 2013; Wooldridge 2002, 2008) An Introduction to Statistical Learning with Applications in R R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org Statistical Rethinking: A Bayesian Course with Examples in R and Stan 1.17.1 Consider: Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017 Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction . Cambridge University Press, 2015 Judea Pearl Imbens: Potential Outcomes versus DAGs List of references "],
["conceptual.html", "2 Conceptual: approaches to statistics/inference and causality 2.1 Bayesian vs. frequentist approaches 2.2 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 2.3 Theory, restrictions, and ‘structural vs reduced form’", " 2 Conceptual: approaches to statistics/inference and causality 2.1 Bayesian vs. frequentist approaches Folder: bayesian Notes: bayes_notes 2.1.1 Interpretation of CI’s (aside) The fact that 95% of all (correct) CIs contain the true value does not mean that 95% of those that exclude zero do so correctly. You could have (say) 59% correct coverage for 10% excluding zero and 99% for 90% including zero. — Stephen John Senn ((???)) January 21, 2020 2.2 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 2.2.1 DAGs and Potential outcomes See especially Imbens, 2019: “Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics” 2.3 Theory, restrictions, and ‘structural vs reduced form’ "],
["data-sci.html", "3 Getting, cleaning and using data 3.1 Data: What/why/where/how 3.2 Organizing a project 3.3 Dynamic documents (esp Rmd/bookdown) 3.4 Project management tools, esp. Git/Github 3.5 Good coding practices 3.6 Additional tips (integrate)", " 3 Getting, cleaning and using data This will build on my content here, and integrate with it. Some key resources are in a continually updated airtable HERE See especially: R for data science Advanced R bookdown: Authoring Books and Technical Documents with R Markdown: [OSF: ‘PhD Toolkit on Transparent, Open, Reproducible Research’ https://osf.io/g8yjz/](OSF: ‘PhD Toolkit on Transparent, Open, Reproducible Research’ https://osf.io/g8yjz/) Happy Git and GitHub for the useR “Data science for business” “Code and Data for the Social Sciences” (Gentzkow/Shapiro) 3.1 Data: What/why/where/how 3.2 Organizing a project 3.3 Dynamic documents (esp Rmd/bookdown) Some guidelines from a particular project: Appendix: Tech for creating, editing and collaborating on this ‘Bookdown’ web book/project (and starting your own) 3.3.1 Managing references/citations A letter to my co-authors… Hi all. Hope you are doing well. I’ve just invited you to a shared Zotero group managing my bibliography/references. I think this should be useful. (I prefer Zotero to Mendeley because it’s open source and… I forgot the other reason.) On my computer it synchronizes with a .bib (bibtex) file in a dropbox folder … For latex files we just refer to this as normal. In the Rmd files/bookdown (producing output like EA barriers or Metrics notes (present book) this is referenced in the YAML header to the index.Rmd file as bibliography: [reinstein_bibtex.bib] Then, to keep this file, I have a “download block” included in that same file (the first line with ‘dropbox’ is the key one). The download code follows (remove the ‘eval=FALSE’ to get it to actually run)… tryCatch( #trycatch lets us &#39;try&#39; to execute and if there is an error, it does the thing *after* the braces, rather than crashing { download.file(url = &quot;https://www.dropbox.com/s/3i8bjrgo8u08v5w/reinstein_bibtex.bib?raw=1&quot;, des tfile = &quot;reinstein_bibtex.bib&quot;) #download the bibtex database download.file(url = &quot;https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css&quot;, destfile = here(&quot;support&quot;, &quot;tufte_plus.css&quot;)) #this downloads the style file }, error = function(e) { print(&quot;you are not online, so we can&#39;t download&quot;) } ) A fairly comprehensive discussion of tools for citation in R-markdown: A Roundup of R Tools for Handling BibTeX 3.3.2 An example of dynamic code Shapiro Wilk test for normality; professor salaries at some US university from the built in Cars data… By the way, if anyone wants me to offer me a job at that university, it looks like a great deal! prof_sal_shapiro_test &lt;- shapiro.test(carData::Salaries$salary) # ShapiroTest &lt;- map_df(list(SXDonShapiroTest, EXDonShapiroTest), tidy) # (ShapiroTest &lt;- kable(ShapiroTest) %&gt;% kable_styling()) The results from the Shapiro Wilk normality test … The p-values are 6.08^{-9} suggesting this data is not normal 3.4 Project management tools, esp. Git/Github (More to be added/linked here) See ‘Git and GitHub’ here… watch this space For students and research assistants, I've been sending first time git users/developers to this:https://t.co/P6KQpXHCWI+ https://t.co/q4R4Ei5Biw — Nathan Lane ((???)) September 14, 2019 3.5 Good coding practices 3.5.1 New tools and approaches to data (esp ‘tidyverse’) From Kurtz: If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in section 5.6.1 from Grolemund and Wickham’s R for Data Science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2. 3.5.2 Style and consistency lower_snake_case Use lower_snake_case to name all objects (that’s my preference anyways) unless there’s a strong reason to do otherwise. This includes: file_names.txt folder_names function_names (with few exceptions) names_of_data_objects_like_vectors names_of_data_output_objects_like_correlation_coefficients ex_df1 In R you probably should keep data frame names short to avoid excessive typing And by all that is holy, never put spaces or slashes in file or object names! This can make it very hard to process across systems… there are various ways of referring to spaces and other white space. 3.5.2.1 Indenting and spacing 3.5.3 Using functions, variable lists, etc., for clean, concise, readable code 3.5.4 Mapping over lists to produce results 3.5.5 Building results based on ‘lists of filters’ of the data set We can store a filter as a character vector and then apply it selection_statement &lt;- &quot;Species == &#39;setosa&#39; &amp; Petal.Width&gt;0.3&quot; iris %&gt;% as.tibble() %&gt;% filter(rlang::eval_tidy(rlang::parse_expr(selection_statement))) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.4 3.9 1.7 0.4 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.7 1.5 0.4 setosa 5.1 3.3 1.7 0.5 setosa 5&nbsp;&nbsp; 3.4 1.6 0.4 setosa 5.4 3.4 1.5 0.4 setosa 5&nbsp;&nbsp; 3.5 1.6 0.6 setosa 5.1 3.8 1.9 0.4 setosa Making this a function for later use: selection_statement &lt;- &quot;Species == &#39;setosa&#39; &amp; Petal.Width&gt;0.3&quot; filter_parse = function(df, x) { {{df}} %&gt;% filter(rlang::eval_tidy(rlang::parse_expr({{x}}))) } iris %&gt;% as.tibble() %&gt;% filter_parse(selection_statement) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.4 3.9 1.7 0.4 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.7 1.5 0.4 setosa 5.1 3.3 1.7 0.5 setosa 5&nbsp;&nbsp; 3.4 1.6 0.4 setosa 5.4 3.4 1.5 0.4 setosa 5&nbsp;&nbsp; 3.5 1.6 0.6 setosa 5.1 3.8 1.9 0.4 setosa We can do the same for a list of character vectors of filter statements, and apply each filter from the list to the dataframe, and then the output function: sel_st &lt;- c(&quot;Species == &#39;setosa&#39; &amp; Petal.Width&gt;0.3&quot;, &quot;Species == &#39;virginica&#39; &amp; Petal.Width&gt;2.4&quot;) map(iris, selection_statement) ## $Sepal.Length ## NULL ## ## $Sepal.Width ## NULL ## ## $Petal.Length ## NULL ## ## $Petal.Width ## NULL ## ## $Species ## NULL sel_st %&gt;% map(~ filter_parse(iris, .x)) ## [[1]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.4 3.9 1.7 0.4 setosa ## 2 5.7 4.4 1.5 0.4 setosa ## 3 5.4 3.9 1.3 0.4 setosa ## 4 5.1 3.7 1.5 0.4 setosa ## 5 5.1 3.3 1.7 0.5 setosa ## 6 5.0 3.4 1.6 0.4 setosa ## 7 5.4 3.4 1.5 0.4 setosa ## 8 5.0 3.5 1.6 0.6 setosa ## 9 5.1 3.8 1.9 0.4 setosa ## ## [[2]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.3 3.3 6.0 2.5 virginica ## 2 7.2 3.6 6.1 2.5 virginica ## 3 6.7 3.3 5.7 2.5 virginica It works nicely if you have a list of filters aligned with a list of names or other objects ‘specific to each filter’ (Code below: adapt to public data and explain) bal_sx &lt;- map2(subsets_sx_dur, subsets_sx_dur_name, function(x, y) { filter_parse(sa, x) %&gt;% dplyr::filter(stage == &quot;2&quot;) %&gt;% tabyl(treat_1, treat_2, show_missing_levels = FALSE) %&gt;% adornme_not(cap = paste(y, &quot;; 1st and 2nd stage treatments&quot;)) } ) 3.5.6 Coding style and indenting in Stata (one approach) I indent every line except clear, import, save, merge (‘file operations’) except where these occur as part of a loop, in which case I put in an ‘important comment’ noting these operations lines that call other do files important comments/flags/to-do’s I only put ‘small todo’ elements having to do with code in a code file itself (and even then there may be better places). If we are going to put todos I suggest we include #todo to search for these later (and R has a utility to collect these in a nice way… maybe Stata does too. Whenever there are more than 20 lines of something prosaic that cannot/has not been put into a loop or function, I suggest we put it in a separate ‘do’ file and call that do file (with no indent). That’s what I do here, giving a brief description and a ‘back link’. Sometimes I put all those do files into an separate folder. 3.6 Additional tips (integrate) When you have to upgrade R on Mac, how to preserve package installations - twitter thread This worked well for me. Thanks (???) ! Are you teaching/learning #r #rstats &amp; want to teach/learn the latest #tidyverse #tidyr tools? e.g. bind_rows, pivot_wider/pivot_longer, the join family (full_join, inner_join…) Check out my slides on “Advanced data manipulation” here 😺https://t.co/dr9VNx7MFf — Amy Willis ((???)) November 16, 2019 Tools for structuring your workflow for reproducable code with Rmd and Git: The workflowr package “This paper does a thorough job setting out the rationale, design, and implementation of the workflowr package” says (???) (???) in his review of this #softwaretool article introducing workflowr by (???) and co-authors https://t.co/ZXmQkDhFuD #OpenScience pic.twitter.com/e77SVo8PhO — F1000Research ((???)) November 17, 2019 Switching from Latex to markdown/R-markdown? These tips from Colin Bousige look pretty good, although I prefer the bookdown/gitbook format "],
["reg-follies.html", "4 Basic regression and statistical inference: Common mistakes and issues 4.1 Basic regression and statistical inference: Common mistakes and issues briefly listed", " 4 Basic regression and statistical inference: Common mistakes and issues 4.1 Basic regression and statistical inference: Common mistakes and issues briefly listed Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Identification Random effects estimators show a lack of robustness Specification Clustering SE is more standard practice OLS/IV estimators not ‘mean effect’ in presence of heterogeneity Power calculations/underpowered Selection bias due to attrition Selection bias due to missing variables – impute these as a solution Signs of p-hacking and specification-hunting Weak diagnostic/identification tests Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness -(Some notes on multi-level modeling) and RE linked in this Twitter thread With heterogeneity the simple OLS estimator is not the ‘mean effect’ P_augmented may overstate type-1 error rate Impact size from regression of “log 1+gift amount” Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Weak IV bias Bias from selecting instruments and estimating using the same data 4.1.1 Bad control From MHE: some variables are bad controls and should not be included in a regression model even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notational experiment at hand. That is, bad controls might just as well be dependent variables too.\" – They could also be interpreted as endogenous variables. Example of looking at a regression of wages in schooling, controlling for college degree completion: Once we acknowledge the fact that college affects occupation, comparison of wages by college degree status within occupation are no longer apples to apples, even if college degree completion is randomly assigned.\" – The question here was whether to control for the category of occupation, not the college degree. It is also incorrect to say that the conditional comparison captures the part of the effect of college that is ‘not explained by occupation’ … so we would do better to control only for variables that are not themselves caused by education.\" 4.1.2 “Bad control” (“colliders”) Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita) 4.1.3 Choices of lhs and rhs variables Missing data Choice of control variables and interactions Which outcome variable/variables 4.1.4 Functional form Logs and exponentials Nonlinear modeling (and interpreting coefficients) 4.1.4.1 ‘Testing for nonlinear terms’ Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18 http://datacolada.org/62 4.1.5 OLS and heterogeneity 4.1.5.1 OLS does not identify the ATE (-) In general, with heterogeneity, OLS does not identify the ATE. It weights observations from different parts of the sample differently. Parts with greater residual variation in the treatment (outcome) variable are more (less) heavily weighted. E.g., if the treatment is binary, the estimator will most heavily weight those parts of the sample where the probability of treatment is closest to 1/2. The formula is … Some intuition Why is this the case? The OLS type estimators we are taught in Econometrics are ‘BLUE’ under the assumption of a single homogenous ‘effect’ (the ‘slope’… although the discussion itself is often agnostic as to whether this represents a causal effect). It is ‘best’ in a minimizing MSE sense under certain assumptions; in particular, we must also know the true functional form and the set of variables to be included. See ‘overfitting’ issues. In order to have the estimate of the true slope that minimizes the squared errors, OLS (and related estimators like FGLS; as well as 2SLS in a more complicated sense) weights some observations more than others. The ‘influence’ of an observation on the estimated slope depends on the nature of the variation in the dependent and independent variables in the region that observation is drawn from. Think of drawing a line through a set of points that were drawn with some noise from the true distribution. If you drew it based on a bunch of points (from a region where) the treatment varies very little and the outcomes have a lot of noise, the line you draw will be very sensitive to the latter noise and thus unreliable. So, would optimally ‘down-weight’ these observations in drawing the line. However, if the actual slope varies by region, this also means you are under-representing certain regions, and thus getting a biased estimate of the average slope. How can we deal with this? If we think that the treatment effect varies with observable variables, we could include ‘interactions’; essentially making separate estimates of the slope for each share of the population (but potentially allowing other control variables to have a homogenous effects, and pooled or clustered estimation of underlying variance.) …Although we may want to consider both overfitting here and the idea that there may be some shared component, so the fully-interacted model may be sub-optimal. See mixed modeling (?) However, this does not tell us how to recover the average of these slopes (approximately, the ATE). Should we weight each of the slopes by the share of the population that this group represents? Mechanically, the standard way of estimating and representing these interactions and economics has been with simple dummies (0,1) for each compared group. This yields a ‘base group’ (e.g., males aged 35-60) – this obviously does not recover the average slope– as well as the ‘adjustment’ coefficients. Another way of expressing interactions, particularly helpful with multi-level interactions is called ‘effect coding’: each group is coded as a ‘difference from 0’ (e.g,. males are -1/2 and females +1/2), before doing the interactions. This could allow for a more straightforward interpretation: at each level, the uninteracted term represents the average treatment effects, and the interacted terms represent adjustments relative to this average. But under which conditions is this in fact the case? [insert here]. WB blog - your-go-to regression-specification is -biased-here-s-simple-way-fix-it A key paper: http://www.jcsuarez.com/Files/Suarez_Serrato-BFE.pdf In particular, we compare treatment effect estimates using a fixed effects estimator (FE) to the average treatment effect (ATE) by replicating eight influential papers from the American Economic Review published between 2004 and 2009.1 Using these examples, we consider a randomized experiment in Section 1 as a case study and, in Section 3, we show generally that heterogeneous treatment effects are common and that the FE and ATE are often different in statistically and economically significant degrees. In all but one paper, there is at least one statistically significant source of treatment effect heterogeneity. In five papers, this heterogeneity induces the ATE to be statistically different from the FE estimate at the 5% level (7 of 8 are statistically different at the 10% level). Five of these differences are economically significant, which we define as an absolute difference exceeding 10%. Based upon these results, we conclude that methods that consistently estimate the ATE offer more interpretable results than standard FE models By “FE” here I think they mean group dummies; they are focused on cross-sectional and not panel data! While fixed effects permit different mean outcomesamong groups, the estimates of treatment effects are typically required to be the same; in more colloquial terms, the intercepts of the conditional expectation functions may differ, but not the slopes DGP \\[y_i = x_i \\beta_{g(i)} + \\mathbf{z_i}&#39; \\gamma + \\epsilon_i\\] where $y_i is the outcome for observation i among N [N what?], \\(x_i\\) is treatment or another variable of interest, and \\(z_i\\) contains control variables, including group-specific fixed effects. The treatment effects aregroup-specific for each of the \\(g=1,...,G\\) groups, where group membership is known for each observation. Defining ATE \\[\\beta^{ATE}=\\sum_g \\pi_g \\beta_g \\] where the \\(\\pi\\) terms are population frequencies The use of interaction terms is delicate… Modeling heterogeneity: the limits of Quantile re regression 4.1.6 “Null effects” “While the classical statistical framework is not terribly clear about when one should”“accept”\" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a \"“zero or minimal effect”\" one would want to find these closely bounded around zero under a variety of conditions for generalizability. In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks”.\" Harms-lakens-18 4.1.6.1 Confidence intervals and Bayesian credible intervals 4.1.6.2 Comparing relative parameters E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men”. This doesn’t cut it: we need to see a statistical test for the difference in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing). 4.1.7 Multiple hypothesis testing (MHT) See (???) 4.1.8 Interaction terms and pitfalls See also ‘effect coding’ 4.1.8.1 ‘Moderators’ Confusion with nonlinearity Moderators: Heterogeneity mixed with nonlinearity/corners In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in’. Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative 4.1.8.2 MHT 4.1.9 Choice of test statistics (including nonparametric) (Or get to this in the experimetrics section) 4.1.10 How to display and write about regression results and tests 4.1.11 Bayesian interpretations of results "],
["robust-diag.html", "5 Robustness and diagnostics, with integrity; Open Science resources 5.1 (How) can diagnostic tests make sense? Where is the burden of proof? 5.2 Estimating standard errors 5.3 Sensitivity analysis: Interactive presentation 5.4 Supplement: open science resources, tools and considerations 5.5 Diagnosing p-hacking (see also meta-analysis)", " 5 Robustness and diagnostics, with integrity; Open Science resources 5.1 (How) can diagnostic tests make sense? Where is the burden of proof? Where a particular assumption is critical to identification and inference …Failure to reject the violation of an assumptionis not sufficient to give us confidence that it is satisfied and the results are credible. At several points the authors cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles). I am concerned with the interpretation of diagnostic testing, both in model selection, and in the defense of the exclusion restrictions or identification assumptions. It is problematic, when the basic consistency of the estimator (or a main finding of the paper) critically depends on such tests failing to reject a null hypothesis, to merely state that the ‘test failed to reject, therefore we maintain the null hypothesis’. How powerful are these tests? I.e. what is the probability of a false negative Type II error? How large a bias would be compatible with reasonable confidence intervals for these tests? 5.2 Estimating standard errors 5.3 Sensitivity analysis: Interactive presentation 5.4 Supplement: open science resources, tools and considerations I'm in psychology research (just finished PhD) and I want to get into #OpenScience to make sure I'm following best practices. But this is something that wasn't explicitly taught to me. What are some good resources? Thanks! (???) (???) (???) (???) — Alessa Teunisse ((???)) April 21, 2020 A couple of months ago I made a guide on how to use Binder to make our #RStats code #reproducible. E.g., Binder will make your code runnable using the versions of R and r-packages used when you analyzed your data. https://t.co/srYNazwy0q #reproducibility #openscience pic.twitter.com/gcTlVFpaY5 — Erik Marsja ((???)) December 17, 2019 5.5 Diagnosing p-hacking (see also meta-analysis) Ever wonder “Were those results p-hacked?” Brodeur et al. propose a useful new check (“speccheck” on Stata. R/etc. coming soon). #ASSA2020 pic.twitter.com/NCZ1jZTaO5 — Eva Vivalt ((???)) January 4, 2020 "],
["control-ml.html", "6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches 6.1 Machine Learning (statistical learning): Lasso, Ridge, and more 6.2 Notes Hastie: Statistical Learning with Sparsity 6.3 Notes: Mullainathan", " 6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches ‘Identification’ of causal effects with a control strategy not credible Essentially a ‘control strategy’ is “control for all or most of the reasonable determinants of the independent variable so as to make the remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest”. All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., (???). 6.1 Machine Learning (statistical learning): Lasso, Ridge, and more 6.1.1 Limitations to inference from learning approaches 6.2 Notes Hastie: Statistical Learning with Sparsity google books link 6.2.1 Introduction One form of simplicity is sparsity, the central theme of this book. Loosely speaking, a sparse statistical model is one in which only a relatively small number of parameters (or predictors) play an important role. “the \\(\\ell_1\\) norm is special” (abs value). Other norms yield nonconvex problems, hard to minimize. “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems. Examples from gene mapping 6.2.1.1 Book roadmap Chapter 2 … lasso for linear regression, and a simple coordinate descent algorithm for its computation. Chapter 3 application of \\(\\ell_1\\) [lasso-type] penalties to generalized linear models such as multinomial and survival models, as well as support vector machines. [?] Chapter 4: Generalized penalties such as the elastic net and group lasso are discussed in Chapter 4. Chapter 5: numerical methods for optimization (skip for now] Chapter 6: statistical inference for fitted (lasso) models, including the bootstrap, Bayesian methods and more recent stuff Chapter 7: Sparse matrix decomposition [?] (Skip?) Ch 8: sparse multivariate analysis of that (Skip?) Ch 9: Graphical models and their selection (Skip?) Ch 10: compressed sensing (Skip?) Ch 11: a survey of theoretical results for the lasso (Skip?) 6.2.2 Ch2: Lasso for linear models N samples (?N observations), want to approx the response variable using a linear combination of the predoctors OLS minimizes squared-error loss but Prediction accuracy OLS unbiased but ‘often has large variance’ prediction accuracy can be improved by shrinking coefficients (even to zero) yielding biased but perhaps better predictive estimators Interpretation: too many predictors hard to interpret DR: I do not care about this for fitting background noise in experiments 6.2.2.1 2.2 The Lasso Estimator Lasso bounds the sum of the abs values of coefficients, an \"$_1\" constraint. Lasso is OLS subject to \\(\\sum_{j=1..p}{\\abs(\\beta_j)}\\leq t\\) “compactly” \\(||\\beta||_1\\leq t\\) with notation for the “\\(\\ell_1\\) norm” Bound \\(t\\) acts as a ‘budget’, must be specified by an ‘external procedure’ such as cross-validation typically we must standardize the predictors $** so that each column is centered with unit variance … as well as the outcome variables (?) … can ignore intercept DR: Not clear here whether standardisation is necessary for the procedure to be valid or just convenient for explaining and deriving its properties. Aside: Can re-write Lasso minimization st constraint as a Lagrangian. \\(\\lambda\\) plays the same role as \\(t\\) in the constraint. Thus we can speak of the solution to the Lagrangian minimisation problem \\(\\hat{\\beta)_{\\lambda}\\) which also solves the bound problem with \\(t=||\\hat_{\\lambda}||_1\\). Aside: We often remove the \\(1/2n\\) term at the beginning of the minimization problem. Same minimization, minimizing sum of squared deviations rather than something like an average of this. Express (Karush-Kuhn-Tucker) optimisation conditions for this … Example from Thomas (1990) on crime data Typically … lasso is most useful for much larger problems, including “wide” data for which \\(p&gt;&gt;N\\) Fig 2.1: Lasso vs ridge regression; coefficients of each for a set of considered variables plotted against their respective norms (as shares of maximal bound on coefficient sum measure, i.e., ols, for each) Note ridge regression penalises squared sums of betas Fig 2.2., in \\(\\beta_1,\\beta_2\\) space illustrates the difference well: contour lines of Resid SS elipses, ‘budget constraint’ for each (disc vs diamond) (Note: lasso bound was chosen via cross-validation) No analytical statistical inference after lasso (some being developed?), bootstrap is common lasso sets two of the five coefficients to zero, and tends to shrink the coefficients of the others toward zero relative to the full least-squares estimate. DR: analytically and intuitively, I do not yet understand why lasso should shrink coefficients but not all the way to zero. The penalty is linear in the coefficient size, so I would think the solution would be bang-bang, either drop a coeficient or leave it unchanged. But it is not. Adding an increment to a \\(\\hat{\\beta}\\) when it is below the OLS estimate should have a linear effect on the RSS (according to my memory and according to Sebastian). But that would mean that shrinking one parameter always yields a better benefit to cost ratio. Thus I should shrink each parameter to zero before beginning to shrink any others. This cannot be right! I looked up this derivative wrt the beta vector (one needs to set this to 0 to get the ols estimates stackexchange \\(\\frac{d RSS}{d \\beta}=-2X^{T}(y-X\\beta}\\) or \\(−\\frac{d e&#39;e}{d b}=2X′y+2X′Xb\\) The answer to this question: while the impact of changing each coefficient on SSR is in fact constant (a constant own-derivative), there is also an impact of changing one coefficient on the other derivatives. As one coefficient shrinks to zero the marginal impact of the other coefficients on the SSR may (will?) increase. - At the same time, we need that the effect of increasing it from zero need not be infinite, so it might not outweigh the linear penalty, thus some coefficients might be set to zero Relaxed lasso the least-squares fit on the subset of the three predictors tends to expand the lasso estimates away from zero. The nonzero estimates from the lasso tend to be biased toward zero, so the debiasing in the right panel can often improve the prediction error of the model. This two-stage process is also known as the relaxed lasso (Meinshausen 2007). DR: When is this likely to help/hurt relative to pure lasso? Stackexchange discussion Contrasts a ‘relaxed-lasso’ from a ‘lars-ols’ Aside: which seems better for Control variable selection for prediction/reducing noise to enable better inference of treatment effects? Ridge? better than Lasso here? We do not care about interpreting the predictors here… so if we allow \\(\\beta\\)‘s to be shrunk towards zero for each coefficient maybe that should yield better prediction than making them exactly zero? On the other hand if we know the true model is ‘parsimonious’ (as in the genes problem) it might boost efficiency to allow inference about coefficients that should be exactly zero (edited) 6.2.2.2 2.3 Cross-Validation and Inference Generalization ability accuracy for predicting independent test data from the same population … find the value of t that does best **Cross-validation procedure* randomly divide … dataset into K groups. “Typical choices … might be 5 or 10, and sometimes N.” One ‘test’, remaining K-1 ‘training’ Apply lasso to training data for a range of t values, use each fitted model to predict the responses in the test set, recording mean-squared prediction errors for each value of t. Repeat the previous step K times each time, one of the K groups is the test data, remaining K − 1 are training data. yields K different estimates of the prediction error over a range of t values. Average K estimates of prediction error for each value of t \\(\\rightarrow\\) cross-validation error curve. Fig 2.3 plots an example with K=10 splits for cross validation … of the estimated MS prediction error vs the relative bound \\(\\tilde{t}\\)(summed absolute value of Lasso betas divided by summed abs value of OLS betas). Also draw dotted line at the 1-std-error rule choice of \\(\\tilde{t\\)} Number of nonzero coefficients plotted at top 6.2.2.3 2.4 Computation of the Lasso solution DR: I think I will skip this for now least angle/LARS is mentioned at the bottom as a ‘homotopy method’ which “produce the entire path of solutions in a sequential fashion, starting at zero” 6.2.2.4 2.5 Degrees of freedom … Jumping to 6.2.2.5 2.10 Some perspective Good properties of the Lasso (\\(\\ell_1\\) penalty) Natural interpretation (enforce sparsity and simplicity) Statistical efficiency … if the underlying true signal is sparse (but if it is not sparse “no method can do well relative to the Bayes error”) Computational efficiency, as \\(\\ell_1\\) penalties are convex 6.2.3 Chapter 3: Generalized linear models 6.2.4 Chapter 4: Generalizations of the Lasso penalty lasso does not handle highly correlated variables very well; the coefficient paths tend to be erratic and can sometimes show wild behavior. The elastic net makes a compromise between the ridge and the lasso penalties (Zou and Hastie 2005)1] is a parameter that can be varied. For an individual coefficient the penalty is \\(\\frac{1}{2} (1-\\alpha)\\beta_j^2 + \\alpha|\\beta_j|\\) (a convex combo of the lasso and ridge penalties) multiplied by a ‘regularization weight’ \\(\\lambda&gt;0\\) which plays the same role (I think) as in lasso elastic net is also strictly convex 6.3 Notes: Mullainathan The fundamental insight behind these breakthroughs is as much statistical as computational. Machine intelligence became possible once researchers stopped approaching intelligence tasks procedurally and began tackling them empirically. Face recognition algorithms, for example, do not consist of hard-wired rules to scan for certain pixel combinations, based on human understanding of what constitutes a face. Instead, these algorithms use a large dataset of photos labeled as having a face or not to estimate a function f (x) that predicts the presence y of a face from pixels x (p2) &gt; supervised- machine learning, the focus of this article) revolves around the problem of prediction: produce predictions of y from x … manages to fit complex and very flexible functional forms to the data without simply overfitting; it finds functions that work well out-of-sample danger in using these tools is taking an algorithm built for [predicting \\(y\\)-] and presuming their [parameters \\(\\beta\\)] - have the properties we typically associate with estimation output One category of such applications appears when using new kinds of data for traditional questions; for example, in measuring economic activity using satellite images or in classifying industries using corporate 10-K filings. Making sense of complex data such as images and text often involves a prediction pre-processing step This middle category is most relevant for me In another category of applications, the key object of interest is actually a parameter … but the inference procedures (often implicitly) contain a prediction task. For example, the first stage of a linear instrumental variables regression is effectively prediction. The same is true when estimating heterogeneous treatment effects, testing for effects on multiple outcomes in experiments, and flexibly controlling for observed confounders A final category is in direct policy applications. Deciding which teacher to hire implicitly involves a prediction task (what added value will a given teacher have?), one that is intimately tied to the causal question of the value of an additional teacher. (p3) A useful (interactive?) example: We consider 10,000 randomly selected owner-occupied units from the 2011 metropolitan sample of the American Housing Survey. In addition to the values of each unit, we also include 150 variables that contain information about the unit and its location, such as the number of rooms, the base area, and the census region within the United States. To compare different prediction techniques, we evaluate how well each approach predicts (log) unit value on a separate hold-out set of 41,808 units from the same sample. All details on the sample and our empirical exercise can be found in an online appendix available with this paper at http://e-jep.org In-sample performance may overstate performance; this is especially true for certain machine learning algorithms like random forests that have a strong tendency to overfit. Second, on out-of-sample performance, machine learning algorithms such as random forests can do significantly better than ordinary least squares, even at moderate sample sizes and with a limited number of covariates (p4) algorithms are fitted on the same, randomly drawn training sample of 10,000 units and evaluated on the 41,808 remaining held-out units. Simply including all pairwise interactions would be infeasible as it produces more regressors than data points (especially considering that some variables are categorical Machine learning searches for these interactions automatically (p5) Shallow Regression Tree Predicting House Values not sure what’s going on here. is this the random forest thing? The prediction function takes the form of a tree that splits in two at every node. At each node of the tree, the value of a single variable (say, number of bathrooms) determines whether the left (less than two bathrooms) or the right (two or more) child node is considered next. When a terminal node-a leaf—is reached, a prediction is returned. An So how does machine learning manage to do out-of-sample prediction? The first part of the solution is regularization. In the tree case, instead of choosing the -best” overall tree, we could choose the best tree among those of a certain depth. (p5) Tree depth is an example of a regularizer. It measures the complexity of a function. As we regularize less, we do a better job at approximating the in-sample variation, but for the same reason, the wedge between in-sample and out-of-sample (p6) how do we choose the level of regularization (-tune the algorithm”)? This is the second key insight: empirical tuning. (p6) -tuning within the training sample In empirical tuning, we create an out-of-sample experiment inside the original sample. We fit on one part of the data and ask which level of regularization leads to the best performance on the other part of the data.4 We can increase the efficiency of this procedure through cross-validation: we randomly partition the sample into equally sized subsamples (-folds”). The estimation process then involves successively holding out one of the folds for evaluation while fitting the prediction function for a range of regularization parameters on all remaining folds. Finally, we pick the parameter with the best estimated average performance.5 The (p6) -! This procedure works because prediction quality is observable: both predictions y- and outcomes y are observed. Contrast this with parameter estimation, where typically we must rely on assumptions about the data-generating process to ensure consistency (p7) Some Machine Learning Algorithms Function class - (and its parametrization) Regularizer R( f ) Global/parametric predictors Linear -′x (and generalizations) Subset selection| (p7) -very useful table Some Machine Learning Algorithms Function class - (and its parametrization) Regularizer R( f ) Global/parametric predictors Linear -′x (and generalizations) Subset selection||β| (p7) Random forest (linear combination of trees (p7) -kernel in an ml framework! Kernel regression (p6) -but can we make inferences about the structure? hypothesis testing? Regularization combines with the observability of prediction quality to allow us to fit flexible functional forms and still find generalizable structure. (p7) Picking the prediction function then involves two steps: The first step is, conditional on a level of complexity, to pick the best in-sample loss-minimizing function.8 The second step is to estimate the optimal level of complexity using empirical tuning (as we saw in cross-validating the depth of the tree). (p8) -but they forgot to mention that others are shrunk linear regression in which only a small number of predictors from all possible variables are chosen to have nonzero values: the absolute-value regularizer encourages a coefficient vector where many are exactly zero. (p4) -why no ridge or elastic net? LASSO (p8) -ensembles usually win contests While it may be unsurprising that such ensembles perform well on average- after all, they can cover a wider array of functional forms-it may be more surprising that they come on top in virtually every prediction competition (p8) -neural nets broadly explained neural nets are popular prediction algorithms for image recognition tasks. For one standard implementation in binary prediction, the underlying function class is that of nested logistic regressions: The final prediction is a logistic transformation of a linear combination of variables (-neurons”) that are themselves such logistic transformations, creating a layered hierarchy of logit regressions. The complexity of these functions is controlled by the number of layers, the number of neurons per layer, and their connectivity (that is, how many variables from one level enter each logistic regression on the next) (p9) These choices about how to represent the features will interact with the regularizer and function class: A linear model can reproduce the log base area per room from log base area and log room number easily, while a regression tree would require many splits to do so. (p9) In a traditional estimator, replacing one set of variables by a set of transformed variables from which it could be reconstructed would not change the predictions, because the set of functions being chosen from has not changed. But with regularization, including these variables can improve predictions because-at any given level of regularization-the set of functions might change (p9) -!! Economic theory and content expertise play a crucial role in guiding where the algorithm looks for structure first. This is the sense in which -simply throw it all in- is an unreasonable way to understand or run these machine learning algorithms (p9) -I need hear of using adjusted r square for this Should out-ofsample performance be estimated using some known correction for overfitting (such as an adjusted R2 when it is available) or using cross-validation (p9) -big unknowns available finite-sample guidance on its implementation-such as heuristics for the number of folds (usually five to ten) or the -one standard-error rule” for tuning the LASSO (Hastie, Tibshirani, and Friedman 2009)-has a more ad-hoc flavor (p9) firewall principle: none of the data involved in fitting the prediction function-which includes crossvalidation to tune the algorithm—is used to evaluate the prediction function that is produced (p10) -how? First, econometrics can guide design choices, such as the number of folds or the function class (p10) with the fitted function. Why not also use it to learn something about the -underlying model (p10) -!! the lack of standard errors on the coefficients. Even when machine-learning predictors produce familiar output like linear functions, forming these standard errors can be more complicated than seems at first glance as they would have to account for the model selection itself. In fact, Leeb and P-tscher (2006, 2008) develop conditions under which it is impossible to obtain (uniformly) consistent estimates of the distribution of model parameters after data-driven selection (p11) -lasso chosen variables are unstable because of multicollinearity. a problem for making inferences from estimated coefficients the variables are correlated with each other (say the number of rooms of a house and its square-footage), then such variables are substitutes in predicting house prices. Similar predictions can be produced using very different variables. Which variables are actually chosen depends on the specific finite sample (p11) this creates an Achilles- heel: more functions mean a greater chance that two functions with very different (p12) coefficients can produce similar prediction quality (p12) In econometric terms, while the lack of standard errors illustrates the limitations to making inference after model selection, the challenge here is (uniform) model selection consistency itself (p12) -is this equally a problem for non sparsity based procedures like ridge? First, it encourages the choice of less complex, but wrong models. Even if the best model uses interactions of number of bathrooms with number of rooms, regularization may lead to a choice of a simpler (but worse) model that uses only number of fireplaces. Second, it can bring with it a cousin of omitted variable bias, where we are typically concerned with correlations between observed variables and unobserved ones. Here, when regularization excludes some variables, even a correlation between observed variables and other observed (but excluded) ones can create bias in the estimated coefficients (p12) Some econometric results also show the converse: when there is structure, it will be recovered at least asymptotically (for example, for prediction consistency of LASSO-type estimators in an approximately sparse linear framework, see Belloni, Chernozhukov, and Hansen 2011). (p12) -unrealistic for micro economic applications Zhao and Yu (2006) who establish asymptotic model-selection consistency for the LASSO. Besides assuming that the true model is -sparse”—only a few variables are relevant-they also require the “irrepresentable condition” between observables: loosely put, none of the irrelevant covariates can be even moderately related to the set of relevant ones. In practice, these assumptions are strong. (p13) Machine learning can deal with unconventional data that is too high-dimensional for standard estimation methods, including image and language information that we conventionally had not even thought of as data we can work with, let alone include in a regression (p13) satellite data (p13) they provide us with a large x vector of image-based data; these images are then matched (in what we hope is a representative sample) to yield data which form the y variable. This translation of satellite images to yield measures is a prediction problem (p13) particularly relevant where reliable data on economic outcomes are missing, such as in tracking and targeting poverty in developing countries (Blumenstock 2016 (p13) cell-phone data to measure wealth (p13) Google Street View to measure block-level income in New York City and Boston (p13) online posts can be made meaningful by labeling them with machine learning (p14) extract similarity of firms from their 10-K business description texts, generating new time-varying industry classifications for these firms (p14) and imputing even in traditional datasets. In this vein, Feigenbaum (2015a, b) applies machine-learning classifiers to match individuals in historical records (p13) -the first prediction applications New Data (p14) Prediction in the Service of Estimation (p14) linear instrumental variables understood as a two-stage procedure (p14) The first stage is typically handled as an estimation step. But this is effectively a prediction task: only the predictions x- enter the second stage; the coefficients in the first stage are merely a means to these fitted values. Understood this way, the finite-sample biases in instrumental variables are a consequence of overfitting (p14) -ll overfitting. Overfitting means that the in-sample fitted values x- pick up not only the signal -′z, but also the noise δ. As a consequence, xˆ is biased towards x, and the second-stage instrumental variable estimate - - is thus biased towards the ordinary least squares estimate of y on x. Since overfit will be larger when sample size is low, the number of instruments is high, or the instruments are weak, we can see why biases arise in these cases (p14) same techniques applied here result in split-sample instrumental variables (Angrist and Krueger 1995) and -jackknife” instrumental variables (Angrist, Imbens, and Krueger 1999) (p15) -worth referencing In particular, a set of papers has already introduced regularization into the first stage in a high-dimensional setting, including the LASSO (Belloni, Chen, Chernozhukov, and Hansen 2012) and ridge regression (Carrasco 2012; Hansen and Kozbur 2014). More recent extensions include nonlinear functional forms, all the way to neural nets (Hartford, Leyton-Brown, and Taddy 2016 (p15) Practically, even when there appears to be only a few instruments, the problem is effectively high-dimensional because there are many degrees of freedom in how instruments are actually constructed (p15) -a note of caution It allows us to let the data explicitly pick effective specifications, and thus allows us to recover more of the variation and construct stronger instruments, provided that predictions are constructed and used in a way that preserves the exclusion restriction (p15) -this seems similar to my idea of regularising on a subset Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016) take care of high-dimensional controls in treatment effect estimation by solving two simultaneous prediction problems, one in the outcome and one in the treatment equation (p15) the problem of verifying balance between treatment and control groups (such as when there is attrition (p15) -! Or consider the seemingly different problem of testing for effects on many outcomes. Both can be viewed as prediction problems (Ludwig, Mullainathan, and Spiess 2017). If treatment assignment can be predicted better than chance from pretreatment covariates, this is a sign of imbalance. If treatment assignment can be predicted from a set of outcomes, the treatment must have had an effect (p15) prediction task of mapping unit-level attributes to individual effect estimates (p15) Athey and Imbens (2016) use sample-splitting to obtain valid (conditional) inference on (p16) treatment effects that are estimated using decision trees, (p16) -look into the implication for treatment assignment with heterogeneity heterogenous treatment effects can be used to assign treatments; Misra and Dub- (2016) illustrate this on the problem of price targeting, applying Bayesian regularized methods to a large-scale experiment where prices were randomly assigned (p16) -caveat Suppose the algorithm chooses a tree that splits on education but not on age. Conditional on this tree, the estimated coefficients are consistent. But that does not imply that treatment effects do not also vary by age, as education may well covary with age; on other draws of the data, in fact, the same procedure could have chosen a tree that split on age instead (p16) Prediction in Policy (p16) -no .. can we predict who will gain most from admission? but even if we can what can we report? Prediction in Policy "],
["iv-and-its-many-issues-1.html", "7 IV and its many issues 7.1 Instrument validity 7.2 Heterogeneity and LATE 7.3 Weak instruments, other issues 7.4 Reference to the use of IV in experiments/mediation", " 7 IV and its many issues 7.1 Instrument validity Exogeneity vs. exclusion Very hard to ‘powerfully test’ IV not credible? Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a direct effect on the outcome (only an indirect effect through the endogenous variable 7.2 Heterogeneity and LATE Basic consideration: what does IV identify and when:? Focusing on a binary endogenous ‘treatment’ variable With heterogeneity With imperfect compliance With one-way compliance 7.3 Weak instruments, other issues With a ‘weak instrument’ … why does that matter? 7.4 Reference to the use of IV in experiments/mediation "],
["other-paths-to-observational-identification-1.html", "8 Other paths to observational identification 8.1 Fixed effects and differencing 8.2 DiD 8.3 RD 8.4 Time-series-ish panel approaches to micro", " 8 Other paths to observational identification 8.1 Fixed effects and differencing 8.2 DiD Key issue: FE/DiD does not rule out a correlated dynamic unobservable, causing a bias Helpful links from a Twitter thread: Ok, let’s say you’ve neither written nor refereed a Diff-in-Diff paper in the last two yeasts. What are the key methodological papers I need to brush up on? #EconTwitterDidn’t someone put together a list? — Damon Jones ((???)) May 20, 2020 8.3 RD 8.4 Time-series-ish panel approaches to micro 8.4.1 Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ "],
["mediators.html", "9 Causal pathways - mediators 9.1 Mediators (and selection and Roy models): a review, considering two research applications 9.2 DR initial thoughts (for NL education paper) 9.3 Econometric Mediation Analyses (Heckman and Pinto) 9.4 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity 9.5 Antonakis approaches", " 9 Causal pathways - mediators 9.1 Mediators (and selection and Roy models): a review, considering two research applications Originally focused on issues relevant to Parey et al project on ‘returns to HE institution’ using data from the Netherlands (flagged as @NL); also relevant to Reinstein et al work on substitution in charitable giving (flagged as @subst). 9.2 DR initial thoughts (for NL education paper) Here were my initial thoughts as pertaining to our paper on the returns to university. Suppose we observe treatment \\(T\\) (e.g., ‘allowed to enter first-choice institution and course’), intermediate outcome \\(M\\) (e.g., completion of degree in first-choice course and institution), and final outcome \\(Y\\) (e.g., lifetime income.) Alternately, in the “substitution between charities” (@subst) context… (unfold) The treatment \\(T\\) is ‘asked to donate in the first round’ (in Reinstein, Riener and Vance-McMullen, henceforth ‘RRV’ experiments, and perhaps in Schmitz 2019)’, a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?), the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others), the intermediate outcome \\(M\\) is the amount given to that (first-round) charity, and the final outcome \\(Y\\) is the amount given to that charity (or other charities) in round 2 (experiments “3”: other charities in that round ). The treatment \\(T\\) (may) directly affect the final outcome \\(Y\\). Do: show a diagram here \\[T\\rightarrow Y\\] \\(T\\) also may affect an intermediate outcome \\(M\\). \\[T \\rightarrow M\\] The intermediate outcome also may affect the final outcome \\(Y\\). \\[M \\rightarrow Y\\] With exogenous variation in \\(T\\) and \\(M\\) (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions. With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of \\(T\\) on \\(Y\\). We could also compute the share of this effect that occurs via the intermediate effect, i.e., \\(T \\rightarrow M\\rightarrow Y\\). This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients. However, there are two major challenges to this estimation. We (may) have a valid instrument for (exogenous variation in) \\(T\\) only, and \\(M\\) may arise through a process involving selection on unobserved variables. Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects. Thus we consult the relevant literature, discussed below. The most influential paper in Economics has been (Heckman, Pinto, and Heckman 2013). It is cited in more recent applied work such as Fagereng, 2018 (unfold). … We follow Heckman et al. (2013) and Heckman and Pinto (2015) in using mediation analysis. The goal of this analysis is to disentangle the average causal effect on outcomes that operate through two channels: a) Indirect effects arising from the effect of treatment on measured mediators, and b) direct effects that operate through channels other than changes in the measured mediators (including changes in mediators that are not observed by the analyst and changes in the mapping between mediators and outcomes). It is therefore necessary to assume that the mediators we do not observe are uncorrelated with both \\(\\mathbf{X}\\) and the measured mediators for all values of \\(D\\). Antonakis, coming from the Psychology and Leadership disciplines, considers the mediation question in a much simpler set of models. 9.3 Econometric Mediation Analyses (Heckman and Pinto) Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs Relevance to Parey et al We have an instrument for admission to one’s first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these ‘intermediate outcomes’, including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these. a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well 9.3.1 Summary and key modeling There is a ‘production function’ cf income as a function of human capital, opportunities, etc. cf donation as a function of income, prices, mood, framing, etc. Treatments (e.g., RCTs) may affect outcomes through the following channels: observable or proxied inputs Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood unobservable/unmeasured inputs cf human capital, social connections cf unobservable generosity, wealth, or temporary mood the production function itself, the ‘map between inputs and outputs for treatment group members’ Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital ‘matter more’ at some institutions? Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else? If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs. RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs ... [nor distinguish effects from changes in production functions]. Here “we can test some of the strong assumptions implicitly invoked”. “Direct effects” as commonly stated refer to the impact of both channels 2 and 3 above. DR: Channel 2 isn’t really a direct effect imho (what was this?) Standard potential outcomes framework: \\[Y=DY_{1}+(1-D)Y_{0}\\] \\[ATE=E(Y_{1}-Y_{0})\\] Production function \\[Y_{d}=f_{d}(\\mathbf{\\mathbf{{\\theta}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] ... the function under treatment \\(d\\); of proxied and unobserved inputs that occur under state \\(d\\), and baseline variables. The production function implies: \\[ATE=E\\Big(f_{1}(\\mathbf{\\mathbf{{\\theta}}}_{1}^{p},\\mathbf{\\mathbf{{\\theta}}}_{1}^{u},\\mathbf{{X}})-f_{0}(\\mathbf{\\mathbf{{\\theta}}}_{0}^{p},\\mathbf{\\mathbf{{\\theta}}}_{0}^{u},\\mathbf{{X}})\\Big)\\] We also consider counterfactual outputs, fixing treatment status and proxied inputs: \\[Y_{d,\\bar{\\theta_{d}}^{p}}=f_{d}(\\mathbf{\\mathbf{{\\bar{{\\theta}}}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] This allows us to decompose (‘as in the mediation literature’): \\[ATE(d)=IE(d)+DE(d)\\] IE, Indirect effect: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment statuses) DE, Direct effect: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed at one of the two treatment statuses) HP further decompose the direct effect into: \\(DE&#39;(d,d&#39;)\\): The impact of letting the treatment vary the map only (fixing the rest at one of the two appropriate values) \\(DE&#39;&#39;(d,d&#39;)\\): The impact of letting the treatment vary the unmeasured inputs only (fixing the rest at one of the two appropriate values) They use this to give two further ways of decomposing the ATE. 9.3.2 Common assumptions and their implications “The standard literature on mediation analysis in psychology regresses outputs on mediator inputs” ... often adopts the strong assumptions of: no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy) and Cf ‘winning institution’ impacts human capital, social networks, etc identically for everyone; e.g., not a greater effect for men then for women, nor a greater effect for those entering particular specializations. full invariance of the production function: \\(f_{1}=f_{0}\\). ... which implies \\(Y_{d}=f(\\mathbf{\\theta}_{d}^{p},d,\\mathbf{X})\\). Sequential ignorability (Imai et al, 10, ’11): Essentially, independent randomization of both treatment status and measured inputs. Cf ‘winning institution’ does not effect the specialization entered nor the location of residence, nor are both determined by a third factor. This sentence is hard to follow: In other words, input \\(\\theta_{d&#39;}^{p}\\) is statistically independent of potential outputs when treatment is fixed at \\(D=d\\) and measured inputs are fixed at \\(\\bar{\\theta_{d&#39;}^{p}}\\) conditional on treatment assignment \\(D\\) and same preprogram characteristics \\(X\\). This assumption yields the ‘mediation formulas’: \\begin{aligned} E(IE(d)|X)= &amp; E(Y|^{p}=t,D=d,X){{}} &amp; (9)\\ E(DE(d)|X)= &amp; {}expe_{} &amp; (10) \\end{aligned} (??F is presumably the distribution over the observables; where did the unobservables go? They are in the expectations, I guess.) Difference from RCT What RCT doesn’t do: [sequential ignorability] translates into ... no confounding effects on both treatments and measured inputs ... does not follow from a randomized assignment of treatment ...[which] ensures independence between treatment status and counterfactual inputs/outputs ... [but not] between proxied inputs \\(\\theta_{d}^{p}\\) and unmeasured inputs \\(\\theta_{d}^{u}\\). [Thus not between counterfactual outputs and measured inputs is assumed in condition (ii).] Cf, randomizing ‘win first-choice institution’ does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution. What RCT does do: RCT ensures “independence between treatment status and counterfactual inputs/outputs”, thus identifying ’treatment effects for proxied inputs and for outputs. CF, we can identify the impact of the treatment ‘win first chosen institution’ on proxied input like ‘enters a specialization’ and on outputs like ‘income in observed years.’ 9.4 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity Summary ... 4000+ families targeted, incentive to relocate from projects to better neighbourhoods. Easy to identify impact of vouchers Challenge (here) is to assess impact of neighborhoods on outcomes. Method here to decompose the TEOT into unambiguously interpreted effects. Method applicable to ‘unordered choice models with categorical instrumental variables and multiple treatments’ Finds significant causal effect on labour market outcomes Relevance to Parey et al We also have an instrument (DUO lottery numbers) cleanly identifying the effect of the ‘opportunity to do something’ (in our case, to enter the course at your preferred institution). However, we also want to measure the impact of choices ‘encouraged’ by the instrument, such as (i) attending the first choice course and institution and (ii) completing this course. We also deal with unordered choices (i. enter course and institution, enter course at other institution, enter other course at institution, enter neither) (ii. choice of medical specialisation) The geographic outcome is relevant to our second paper (impact on ‘lives close to home’) Introduction The causal link between neighborhood characteristics and resident’s outcomes has seldom been assessed. Treatments: Control (no voucher) Experimental: could use voucher to lease in low-poverty neighborhood Section 8: Could use voucher in any () neighborhood Many papers evaluate the ITT or TOT effects of MTO. ITT: effect of being offered voucher estimated as difference in average outcome of experimental vs control families TOT: effect for ‘voucher compliers’ (assuming no effect of simply being offered voucher on those who don’t use it) estimated as ITT/compliance rate [ITT and TOT] are the most useful parameters to investigate the effects of offering [EA] rent subsidising vouchers to families. Identification strategy brief Vouchers as IVs for choice among 3 neighborhood alternatives (no relocation, relocate bad, relocate good) Cf @NL: enter course and fp-institution, enter course at other institution, do not enter course Neighborhood causal effects as difference in counterfactual outcomes among 3 categories Challenge: “MTO vouchers are insufficient to identify the expected outcomes for all possible counterfactual relocation decisions” ... “compliance with the terms of the program was highly selective [Clampet-Lundquist and M, 08]” Solution: Uses theory and ‘tools of causal inference. Invokes SARP to identify ’set of counterfactual relocation choices that are economically justifiable’ Identifying assumption: “the overall quality of the neighborhood is not directly caused by the unobserved family variables even though neighborhood quality correlates with these unobserved family variables due to network sorting” ‘Partition sample ... into unobserved subsets associated with economically justified counterfactual relocation choices and estimate the causal effect of neighborhood relocation conditioned on these partition sets.’ [what does this mean?] Results in brief “Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes ... 65% higher than the TOT effect for adult earnings.” Framework: first for binary/binary (simplification) First, for binary outcomes (simplified) \\(Z_{\\omega}\\): whether family \\(\\omega\\) receives a voucher (cf institution-winning lottery number) \\(T_{\\omega}\\): whether family \\(\\omega\\) relocates (cf enters first choice institution and course) Counterfactuals \\(T_{\\omega}(z)\\): relocation decision \\(\\omega\\) would choose if it had been assigned voucher \\(z\\in{0,1}\\)’: vector of potential relocation decisions (cf education choices) for each voucher assignment (cf lottery number) Can partition into never-takers, compliers, always takers, and defiers \\((Y_{\\omega}(0);Y_{\\omega}(1\\))): (Potential counterfactual) outcomes (cf income, residence, etc) when relocation decision is fixed at 0 and 1, respectively Key ( standard) identification assumption: instrument independent of counterfactual variables \\[(Y_{\\omega}(0),Y_{\\omega}(1),T_{\\omega}(0),T_{\\omega}(1))\\perp Z_{\\omega}\\] Standard result 1: ITT \\[\\begin{aligned} ITT=E(Y_{\\omega}|Z_{\\omega}=1)-(Y_{\\omega}|Z_{\\omega}=0)\\\\ =E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)P(S_{\\omega}=[0,1])+E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[1,0]&#39;)P(S_{\\omega}=[0,1])\\end{aligned}\\] i.e., ITT computation yields the sum of the ‘causal effect for compliers’ and the ’causal effect for defiers, weighted by the probability of each. Standard result 2: LATE \\[\\begin{aligned} LATE=\\frac{{ITT}}{P(T_{\\omega}=1|Z_{\\omega}=1)-P(T_{\\omega}=1|Z_{\\omega}=0)}= &amp; &amp; E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)\\\\ if\\:P(S_{\\omega}=[0,1])=0\\end{aligned}\\] i.e., the LATE, computed as the ITT divided by the ‘first stage’ impact of the instrument, is the causal effect for compliers if there are no defiers. Framework for MTO multiple treatment groups, multiple choices \\(Z_{\\omega}\\in\\{z_{1,}z_{2,}z_{3}\\}\\) for no voucher, experimental voucher, and section 8 voucher, respectively \\(T_{\\omega}\\in\\{1,2,3\\}\\) ... no relocation, low poverty neighborhood relocation, high poverty relocation \\(T_{\\omega}(z)\\): relocation decision for family \\(\\omega\\) if assigned voucher \\(z\\) \\(\\rightarrow\\) Response type for each family \\(\\omega\\) is a three-dimensional vector: \\[S_{\\omega}=[T_{\\omega}(z_{1}),T_{\\omega}(z_{2}),T_{\\omega}(z_{3})]\\]. \\(\\rightarrow\\) ITT computation now measures a weighted sum of effects across a subset of those response types whose responses vary between the assignments being compared. Cf: Considering the ‘treatments’: ‘1: enter other course at fp-inst, ’2: enter course at fp-inst’, ‘3: enter course at non-fp inst’ (I ignore other course at other institution for now) Looking among those who won the course lottery (so we have a binary instrument: wininst \\(Z_{\\omega}\\in{0,1\\}}\\) Our reduced-form estimates (regressions on the ‘lottery number wins institution’ dummy) measures the probablility-weighted sum of: impact of institution within course ($T_{}=$2 versus 3); for those who would ‘fully comply’ (enter course at institution if \\(Z_{\\omega}=1\\), enter course at other institution if 0) impact of the course at fp-institution versus second-best course at fp-institution for ‘institution-loving’ noncompliers; those who would enter the course only if they get the fp-institution and otherwise another course at the same institution effects for perverse defiers 9.5 Antonakis approaches Insert notes here List of references "],
["selection-cop.html", "10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 10.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ 10.2 Bounding approaches (Lee, Manski, etc)", " 10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 10.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ “Conditional on positive”/“intensive margin” analysis ignores selection “Conditional on positive”/“intensive margin” analysis ignores selection identification issue See Angrist and Pischke on “Good CoP, bad CoP”. See also bounding approaches such as (???) AngristJ.D.2008a, 10.2 Bounding approaches (Lee, Manski, etc) See Notes on Lee bounds 10.2.1 Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD Notes David Reinstein 10.2.1.0.1 Introduction even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research Intuitive trimming procedure for bounding average treatment effects in the presence of sample selection… Requires neither exclusion restrictions nor a bounded support for the outcome of interest.\" (Also) applicable to “nonrandom sample selection/attrition”, as well as to the ‘conditional on positive’/hurdle/mediation effect discussed here analyses and evaluations typically focus on \"reduced form impacts on total earnings, a first-order issue for cost-benefit analysis. Unfortunately, exclusively studying the effect on total earnings leaves open the question of whether any earnings gains are achieved through raising individuals hypothesis wage rates (price affects or hours of work (quantity effects). Important methodological point to constantly bring up: “even a randomized experiment cannot guarantee the treatment and control individuals will be comparable conditional on being employed.” Claims that standard “parametric or semi-parametric methods for correcting sample selection require exclusion restrictions that have little justification in this case.” Notes that most of the baseline variables could affect employment probabilities or have a direct impact on wage rates. Summary of the method: “…amounts to first identifying the excess number of individuals who were induced to be selected (employed) because of the treatment and then trimming the upper and lower tails of the outcome… distribution by this number, yielding a worst-case scenario bound.” Uses same assumptions as in “conventional models for sample selection” regressor of interest is independent of the errors in the outcome and selection models selection equations – this is ensured by random assignment. “the selection equation can be written as a standard latent variable binary response model” – what meaningful restriction does this impose? He proves this procedure “yields the tightest bounds for the average treatment effect that are consistent with the observed data.” The bounds estimator is shown to be \\(\\sqrt(n)\\) consistent and asymptotically normal with an intuitive expression for its asymptotic variance which depends on the variance of the trimmed outcome and the trimming threshold, an estimated quantifiable; (and an added term accounting for the estimation of which quantile to trim on) Note for charity experiment (unfold) (@subst) – DR, Note, charity data: We can make confidence statements over the bounds themselves. Will this procedure be easy to bring into our code? – In our (charity) experiment we in fact do have upper bounds on the outcome variable. Could this yield even greater efficiency? Note for the Netherlands data: (unfold, @NL) it is not immediately clear how this could be adapted to instrumental variables; we shall see. Can we recover something meaningful from the reduced form model they are? Can it be applied to the (instrumental variables) estimates to disentangle the impact of changing courses from the impact of the institution itself? In Lee’s paper, the estimate seems to give very narrow and informative bounds even though they have a great many people who do not earn any wages as a share of the population, about 54%. These are much narrower than the bounds proposed by Horowitz and M then what those bounds produce. &lt;!- ask (???) whether his Horowitz/Manski estimator incorporated the natural bounds on the outcome. –&gt; 10.2.1.0.2 The National Job Corps Study and Sample Selection [prior approaches] In the experiment discussed here those in the control group were embargoed from the program for three years but could join afterwards, thus “when I use the phrase ‘effect of the program’ I am referring to this reduced-form treatment effect”, i.e., the intent to treat effect. – “some subpopulations were randomized into the program group with differing, but known probabilities. Thus analyzing the data requires the use of design weights.” Note: (\\(???)) this bears some resemblance to our Dutch data situation, and we can probably use examples from analyses of these programs. We can check their code against ours. – Note also that they impute means of the baseline variables with their means; this seems to be an accepted practice. Lee notes that he focuses exclusively on the “sample selection on wages caused by employment” and not the attrition/nonresponse problem, to focus attention on this, but they could have used it for the other as well. – DR: (@NL) Note again that their desire to separate the employment hours and wage effects of the program is very similar to our desire to separate out different margins of the impact of winning an institution. …Namely the impact on completing a course or starting a course versus other impacts and the impact of entering a specialization versus remaining impacts. …Similar decompositions for the geography outcomes. – To do: check whether any papers cite Lee using an IV approach, extending the technique and the estimation of variance. “the problem of nonrandom sample selection is well-understood in the training literature; … may be one of the reasons why most evaluations of job-training programs focus on total earnings, including zeros for those without a job, rather than on wages conditional on employment” “of the 24 studies referenced in a survey … (Heckman et al.)… Most examine annual, quarterly, or monthly earnings without discussing the sample selection problem examining rage rates.” – DR: (@NL)Note that this is relevant to our question of whether to exclude zeros in log models, etc. While there will be less unemployment in our data, it still may be a relevant influence made have a strong effect on the estimates. …previous conventional approaches to the sample selection problem (skip if desired). One may explicitly model the process determining selection, such as in Heckman (1979) … Separate equations for the wage and the propensity to be employed, where employment occurs if the latter crosses a particular threshold, in which case a wage is observed. It is reasonable to think that the treatment variable can have effects on both terms.. “sample selection bias can be seen as specification error in the conditional expectation…” The expected wage conditional on treatment exogenous variables and the selection into working (that is the underlying propensity to work variable exceeding zero) his status is equal to the true effect of the treatment an adjustment for the differences in the observable’s exogenous variables and a bias term representing the expectation of the idiosyncratic unobservables given the treatment and the exogenous variables exceeding the value necessary to induce work participation. The unobservable term needs to exceed the prediction based on the observable term for the entire term to exceed zero inducing labor force participation. One may assume the data are missing at random, perhaps conditional on a set of covariates (Rubin, 1976; essentially assuming the error terms in each equation are independent of one another, here “employment status is unrelated to the determination of wages”… This “is strictly inconsistent with standard models of labor supply that account for the participation decision (Heckman, 1974).” A more common assumption is that some exogenous variables “determine sample selection but do not have their own direct impact on the outcome of interest…. Exclusion restrictions are used in parametric and semi-parametric models…” but “there may not exist credible ’instruments… excluded from the outcome equation” – DR, aside: We can return to (our) previous papers to impose these Lee bounds! One example would be the Siskel and Ebert your reviews paper and perhaps incorporating us with subsequent approaches, considering the “selection to review” equation. Second approach “the construction of worst-case scenario bounds of the treatment effect” “Impute missing data with either the largest or smallest possible values to compute the largest and smallest possible treatment effects consistent with the data” as in Horwitz and Manski (2000a) who provide a general framework for this. Particularly useful with binary outcomes. This cannot be used when the support is unbounded. … note in their replication example they are actually using the equivalent of the bottom 5th percentile and the top 95th percentile. Strictly using a procedure would provide even wider bounds. Lee considers his approach to be a hybrid of the two previous general approaches. …end of section 2.. .a statement of the Horwitz upper bound for the treatment effect; very intuitive: “what if everyone in the treatment who dropped out would have had the largest possible wage and everyone in the control group that drop out would’ve had the smallest possible wage; this will give the upper bound.” Switching this the other way around will give a lower bound. DR, an aside thought: (@NL) Something akin to the Horwitz and M approach (or maybe Lee bounds) could be applied to our issue of swapping into institutions directly. Suppose we only focus on those who actually complied: those assigned to an institution who also went to that institution. Our concern was that this would under-represent those who had particularly strong institutional preferences. Suppose you are interested in looking at the impact of winning the lottery (for once preferred institution) itself, as that was our most simply identifiable outcome. … Let’s consider evaluating a treatment effect for those who happened to swap in. Those who swapped in might be assigned a counter-factual outcome of the lowest value of the lifetime income among those who did not get their institution of choice. Similarly, the small group who swapped out might be assigned a counterfactual outcome (had they no swapped out) representing the highest outcome value for those who did get their institution of choice. This should give us an upper bound on the treatment effect for these two groups of what we might call non-compliers. Making the opposite assumptions, precisely that those who swapped into their institution of choice would’ve had a very good counterfactual outcome (if they had not got their institution of choice) that comes from the highest outcomes for those who didn’t get their institution of choice (and also reversing this for those who swapped out of their preferred institution) would give us a lower bound for the treatment effect for this group. We can then combine these bounded treatment effects for these non-compliers with the treatment effect for the compliers to get a measure of the average treatment effect with bounds for this sort of behavior. This will also allow us to come up with estimates with bounds without having to use the instrumental variable strategy which has issues of its own. 10.2.1.0.3 Section 3: identification of bounds on treatment effects; the main meat of the model He starts with a simple example. He begins with a model with a treatment indicator and no other covariates, and a continuous outcome variable, but notes that this will clearly apply to discrete outcome variables and will also apply conditional on controls. Nest, he brings forward the statement… from the earlier selection models. In each case the latent variable must overcome a hurdle for the outcome to be observed and in fact the hurdle differs depending on the impact of the treatment itself. In general when the errors in the selection and outcome equations are correlated the difference in these means differs from the actual treatment effect. In other words through a slightly complicated story, when those who have unobservables that make them more likely to work also tend to have unobservables that would make them likely to earn more the standard difference in outcomes between control and treatment will not describe the true treatment effect. A key insight seems to be that we could identify the treatment effect if we could estimate the expected outcome given treatment and given that the unobservable component in the selection equation would lead to an observable outcome had the person not been given treatment. If so, we could subtract the observed mean control outcome from the above to yield the true treatment effect (for those who would be observed always). However, we obviously do not observe this because we only observe the outcomes for those who are treated where the selection equation is in fact positive and not “where the selection equation would have been positive had they not been treated.” However, the insight here is that this term can in fact be bounded. We do observe these outcomes for the treated people (note we are assuming without loss of generality that the treatment raises the probability of selection for this discussion) but we don’t know exactly which ones they are. In other words, we observe outcomes for more people in the treatment group than we need; we wish we could figure out what is the subset of these that would have also been observed had they not been treated, so we could compare like-to-like. The observed treatment mean is a weighted average of the thing we are seeking (to difference from the control) and “the mean for a subpopulation of marginal individuals… that are induced to be selected into the sample because of the treatment” This then gets us the upper bound for the term expressing the treatment outcome for those who would have been observed even if they had been in the control. The upper bound for this is the expected outcome for those in the treated group (who are observed of course) and who are in quantile-p or above of the outcome, where this \\(p\\) is the share of the treated population that are in the marginal group we referred to that were only induced to be selected into the sample because of the treatment. In other words the worst case scenario is that the smallest share \\(p\\) values of \\(Y\\) are in the marginal group and the largest one (which is share 1-pone are in the inframarginal group. We don’t know which observations are inframarginal and which ones are marginal. \\(p\\): the share of marginal individuals and (1-p) the share of inframarginal individuals (the latter is group we want the average outcome for). The highest could be would be the average outcomefor the largest (1-p) share of this group. We are looking for the expectation given that they are at or above at will at or above percentile p within this group. In other words we trim the lower tail of the Y distribution by the portion \\(p\\), (so what remains is the 1-p share) to get the upper bound for the inframarginal groups mean. We can then subtract the mean for the control group to get an upper bound for the treatment effect. To compute this “trimming proportion p”: this p is equal to the share of the treated group whose outcome is observed minus the share of the control group whose outcome is deserved is observed, divided by the share of the treatment group where the outcome is observed. Something like the increased likelihood of observation that is driven by the treatment, as a share of the total number as a share of the probability of observation in the treatment group. The average observed outcome for the treatment group is including too many observations; we need to difference out the share of observations that are observed only because the treatment caused them to be observed; this share is certainly no larger than the increased probability of observation in the treatment group as a share of the probability of observations the treatment group. Another much simpler way of saying this is “trimming the data by the known proportion of excess individuals” in the treatment group. (To gain bounds on the mean for the inframarginal group which we can then difference from the control-group mean get the treatment effect). Perhaps some intuition for why this improves on the Horwitz model: we don’t need to assume that those observed in the treatment group that wouldn’t have been observed in the control would’ve had the highest possible outcomes. No, we only need to assume (to get the upper bound) that these came from the highest distribution because they had to come from somewhere. These were the people in the upper tail of the relevant group but they couldn’t all have been the individual highest achiever. The model is extended to heterogeneity and heteroscedasticity. This begins with the independence of treatment assignment the “potential sample selection indicators” for either treatment or control, in other words whether that individual will have an observed outcome under treatment and whether that the individual would have an observed outcome under control, and the latent potential outcomes. Experimental or random assignment ensures that each of the potential outcomes (and the correspondence to observability under each treatment) is independent of the actual assigned treatment. The second assumption is monotonicity: treatment assignment can only affect sample selection in one direction. – DR: For our (substitution) experiments, it is in fact not clear to me whether this should necessarily be the case, as some (less generous?) people may be induced to leave because of having been asked to donate, while potentially other (more generous people) might be induced to return given that they were asked to donate. (This proposed nonmonotonicity implies that the ‘asked twice’ sample tends to weed out the less generous, which would lead to a bias against substitution, strengthening the case for our result.) - DR, aside: However, even though the paper doesn’t say it, I suspect this assumption could be weakened and you would still get some similar bounds. To put it another way, I would imagine that these bounds could be adjusted based on some reasonable ad hoc assumptions about the share of the population who is affected in either direction. – @NL: I’m coming to think that our Dutch data problems are more things involving “hurdle models”. Can this technique also be applied to such hurdle models? Next proposition 1a states that given these assumptions we can derive sharp lower and upper bounds for the average treatment effect (conditional on ‘would be observed in both states’). Note that for this estimator if the probability of observation is greater under the treatment we need to trim the treatment groups outcome distribution and if the probability of observation is greater under the control we need to trim the control group’s outcome distribution. DR, aside comment: we seem to be throwing out a bit of the data in these estimates, which would suggest that something more efficient could be generated. (The stated bounds you can estimate are exactly the same as the bounds from the previous specification, at least as I had interpreted the way they would be produced.) Their remark 2 notes that an implication is that as \\(P_0\\), that as the “difference between the relative probability of observation of an outcome under treatment versus control” tends to zero, i.e., as the probability of having an observed outcome (or the conditional probability of this) is the same for treatment and control) then there is no sample selection bias. Their estimate convergences to the estimate he calls an estimate for the “always takers subpopulation… except that taking… is selection into the [outcome-observed] sample.” So, a very vanilla estimator is acceptable if we find the same conditional probability of selection for each group, under monotonicity, which, for this case, we can test (see Remark 4 below). – (DR: To me this suggests that there might be something wrong going on here. Intuitively, If I simply observe the same rate of attrition in the treatment and control groups this shouldn’t be enough to tell me that attrition did not matter, as it could occur differentially for both groups, but it seems to be a result here; this is probably due to the assumption of monotonicity of the selection/observation term, as well as the random/exogenous assignment to each group.) Remark 3 discusses the importance of monotonicity for the bounds, saying this assumption is “minimally sufficient” (I think it would be better to say minimally sufficient for these particular bounds that he computed). To demonstrate this he gives an extreme example. Without monotonicity it could be (note: this would seem like a very unlikely outcome!) that every observation in the control group comes from the population in the treatment group that would not have been observed had they been treated and every observation in the treatment group happens to come from the set of people that would not have been observed had they been in the control group. These two “subpopulations do not overlap, so the difference in the means could not be interpreted as a causal effect.” – DR, aside : there must be some way to impose some restrictions on this even allowing for this non-monotonicity. (He notes that this can be improved upon somewhat by thinking about the total the idea that the total masses of unobserved that would’ve been observed in the other group can’t be greater than the share that is not observed in the other treatment group, but this doesn’t seem like a particularly fruitful route as it in most reasonable cases will still allow for very wide bounds.) Remark 4 suggests that if we can assume (or somehow observe?) that the conditional probabilities of selection are the same for treatment and control, we can test whether monotonicity in fact holds and the simple difference in means will be an appropriate estimate of the treatment effect. Here, the assumption implies that everyone in the treatment or control group would have been observed under the opposite treatment as well. This in fact implies that the distribution of the exogenous variables should be the same in the treatment and control groups conditional on being selected. This seems fairly intuitive, we look at whether selection seems to be occurring in different ways are on different margins for the two groups treatment versus control. Apparently for this test to have power we need that the subpopulations of “noncompliant errors in opposite directions” (quotation mine) must have distinct distributions of baselines exogenous characteristics. If these were the same then whether or not monotonicity holds the test doesn’t tell us anything. – DR: I wonder if anyone uses this test for Monotonicity under non-differential selection? Another relevant note that he bundles in this remark is that the technique here only yields estimates for those who would be with an observed outcome for either treatment or control. One could additionally try to bound this as an estimate for the entire population using the Horwitz and Manski bounds for this latter thing. However, in many contexts there are reasons that the bounded estimates they mainly use are the relevant ones, such as “the impact of the program on wage rates for those whose employment status was not affected by the program.” DR: In our substitution experiment case, the substitution patterns for those for whom attrition was not affected by the first-round-charity treatment @NL: E.g., the impact of an institution on income for those whose choice to remain in the course was not affected by their institutional assignment “Narrowing bounds using covariates” All of the above could be done conditional on a particular set of baseline characteristics such as gender or race. The average treatment effect could be estimated separately for each. (Note: and perhaps combined in a fruitful way?) One can alternately use covariates to reduce the width of these bounds. To give intuition, we can imagine a baseline covariate that perfectly predicts an individual’s wage. Because treatments are randomly assigned the maintained assumptions will still hold conditionally on this X. The results the methods can be applied separately for each value of this covariate, and for each such value the trimming procedure will actually have no impact on the estimate. DR: I think this is the “estimate and sum things up in a weighted way” procedure I thought about a moment ago. Proposition 1B gives the balance estimator for a model involving exogenous variables. Essentially, this computes the corresponding bounds estimator at each X, where the differential selection probability is computed for that particular X, the upper quantile value of the outcome is given conditional on the same X and on being in the treatment group. These are then integrated (or summed up) weighted by the distribution or the cdf of this covariate in the control group. These bounds will necessarily be sharper than the balance without controls. 10.2.1.0.4 Section 4: estimation and inference The asymptotic variance depends on components reflecting the variance of the trimmed distribution, the variance of the estimated trimming threshold, and the variance in the estimate of “how much of the distribution to trim” (the relative selection probability differential). Equation 6 formally defines the estimator Estimated bounds consistent for ‘true bounds’ under standard conditions Two ways to compute CI’s – CI’s for the ‘true bounds’ or CI’s for the TE itself. A 95% CI for the former will contain the latter with even greater probability. Imbens and Manski ‘04 can be used to derive the latter which are ’more apppropriate here’ since the object of interest is the TA and not the ’region of all rationalizable treatment effects. These are built off of a transformation of the estimate UB and LB and max estimated sd of each of these. the latter are reported by the ‘cie’ option in ‘leebounds’ Generalisation to monotonicity (without knowing direction of impact of treatment on selection)… As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative… [do similar] though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small and p0 is “close” to zero … A simple, conservative approach to combining the trimmed and untrimmed intervals is to compute their union 10.2.1.0.5 Section 5: Empirical Results Table 4 gives a step-by-step that is a good way of seeing and understanding the construction of the estimator, and where the ‘action’ is, in treimming, in components of the SE, etc. Intervals are 1/14 the width of the equivalent Horowitz/Manski bounds 10.2.1.0.5.0.1 5.2 using covariates to narrow bounds Any baseline covariate will do, as will any function of all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to be a proxy for the predicted wage potential for each individual—is constructed from a linear combination of all observed baseline characteristics. This single covariate is then discretized, so that effectively five groups are formed according to whether the predicted wage is within intervals defined by $6·75, $7, $7·50, and $8·50. (???): this is essentially what I propose we do, but using Ridge Regressions or something similar To compute the bounds for the overall average…the group-specific bounds must be averaged, weighted by the proportion (sPr Group J|S0=1,S1=1) The estimated asymptotic variance for these overall averages is the sum of (1) a weighted average of the group-specific variances and (2) the (weighted-) mean squared deviation of the group-specific estimates from the overall mean. This second term takes into account the sampling variability of the weights \\(\\rightarrow\\) result: 11% narrower bounds Interesting; possibly do similar for @NL-ed: By statistically ruling out any effect more negative than −0·037, this suggests that after 4 years, the Job Corps enabled program group members to offset at least 35% (and perhaps more) of the potential 0·058 loss in wages due to lost labour market experience that could have been caused by the program 10.2.1.1 Section 6: Conclusions: implications and applications Interesting intuitive argument: Another reason to interpret the evidence as pointing to positive wage effects is that the lower bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly negatively correlated with the propensity to be employed. From a purely theoretical standpoint, a simple labour supply model suggests that, all other things equal, those on the margin of being employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be the smallest wage that draws the individual into the labour force). In addition, the empirical evidence in Table 2 suggests that there is positive selection into employment: those who are predicted to have higher wages are more likely to be employed (i.e., U and V are positively correlated). If this is true, it seems relatively more plausible to trim the lower rather than the upper tail of the distribution to get an estimate of the treatment effect. "],
["why-experiment-design.html", "11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 11.1 Why run an experiment or study? 11.2 Causal channels and identification 11.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 11.4 Generalizability (and heterogeneity)", " 11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 11.1 Why run an experiment or study? I claim an experiment should: Have a reasonable chance of an outcome that would not have been predicted in advance. The realized outcome should meaningfully inform our understanding of the world in other words. In other words, if the outcome comes out one way it should cause us to update our beliefs about a particular hypothesis about the world in one direction (and if it comes out the other way we should update in the other direction.) Experimenter should always ask: “What uncertainty (about real-world preferences, decision-making etc.) is ‘entangled’ (ala (???)) with the results of this experiment?”… i.e., ‘how might my beliefs change depending on the results?’ (???) on twitter 11.1.1 Sitzia and Sugden on what theoretically driven experiments can and should do “Sitzia, Stefania, and Robert Sugden.”Implementing theoretical models in the laboratory, and what this can and cannot achieve.\" Journal of Economic Methodology 18.4 (2011): 323-343. This paper is a critique of how models are claimed to be “tested”, through a literal implementation, in the laboratory. They argue this misinterprets the intention of a model, and use of economic modelling in general. Ultimately, such experiments (they say) don’t really tell us one way or another about the truth or usefulness of the model for the real-world domain that was intended. Some key quotes.. My reductio ad absurdum on this is an experimenter who ‘tests mechanism-design’ by asking subjects “do you want to choose this optimal mechanism and earn $20, or this inefficient mechanism and earn $10”? – (???) on twitter They single-out two examples of well-published experiments for criticism: \"an investigation of price dispersion by John Morgan, Henrik Orzen and Martin Sefton (2006), and an investigation of information cascades by Lisa Anderson and Charles Holt (1997)\"… In each case, the experimenters create a laboratory environment that closely resembles the model itself. The only important difference between the experiment and the model is that, whereas the model world contains imaginary agents who act according to certain principles of rational choice, the laboratory contains real human beings who are free to act as they wish. The decision problems that the human subjects face are exactly the problems specified by the model. We argue that such an experiment is not, in any useful sense, a test of what the model purports to say about the target domain. Instead, it is a test of those principles of rational choice that the modeller has attributed to the model world. Those principles are not specific to that model; they are generic theoretical components that are used in many economic models across a wide range of applications. Surprisingly, these doubts are not expressed in terms of the applicability of MSNE [mixed strategy Nash Equilibrium] to the model’s target domain, pricing decisions by retail firms. The doubts are about whether experimental subjects will act according to MSNE when placed in a laboratory environment that reproduces the main features of the model. If one takes the viewpoint of the subjects themselves, there seems to be very little resemblance between the decision problems they face and those by which retail firms set their prices. The connection between the two is given by the model: the subjects’ decision problems are like those of the firms in the model, and the firms in the model are supposed to represent firms in the world. However, MOS are no more concrete than Varian in explaining how the comparative-static properties of the model relate to the real world of retail pricing. The suggestion in these passages is that the clearinghouse model’s claim to be informative about the world is strengthened if its results are confirmed in the laboratory. In this sense the experiment is informative about the world. But the experiment itself is a test of the model, not of what the model says about the world. The procedure of random and anonymous rematching of subjects is explained as a means of eliminating ‘unintended repeated game effects’, such as tacit collusion among sellers (pp. 142–3). This argument illustrates how tightly the laboratory environment is being configured to match the model. In a test of MSNE, repeated game effects are indeed a source of contamination; and MSNE is a property of Varian’s model. But in the target domain of retail trade, the same firms interact repeatedly in the same markets, with opportunities for tacit collusion. Clearly, if an experiment implemented a model in its entirety, all that it could test would be the mathematical validity of the model’s results. Provided one were confident in the modeller’s mathematics, experimental testing would be pointless. Thus, when an experiment implements almost every feature of a model, all it can test in addition to mathematical validity are those features that have not been implemented. Thus, the experiment is a test of MSNE in a specific class of games. [emphasis added] MSNE is what we will call a generic component of economic models – a piece of ready-to-use theory which economists insert into models with disparate target domain ibid Relating back to the discussion of the different conceptions of theory: Is it informative at all to run experimental tests of theoretical principles such as MSNE and Bayesian rationality, viewed as generic components of economic models? … A strict instrumentalist (taking a position that is often attributed to Friedman) might answer ‘No’ to the first question, on the grounds that tests should be directed only at the predictions of theories and not at their assumptions. Such an experimental design should not be appraised in terms of what the model purports to say about its target domain. It should be appraised in terms of what it can tell us about the relevant generic component, considered generically. When (as in the cases of MSNE and Bayesian rationality) the same theoretical component appears in many different models, an experimenter can afford to be selective in looking for a suitable design for a test Considered simply as a test of MSNE, MOS’s experiment uses extraordinarily complicated games. Many of the canonical experiments in game theory use 2×2 games. Depending on the treatment, MOS’s games are either 101×101 (for two players) or 101×101×101×101 (for four players). Payoffs to combinations of strategies are determined by a formula which, although perhaps intuitive to an economist (it replicates the demand conditions of the clearinghouse model), might not be easy for a typical subject to grasp. 11.2 Causal channels and identification Ruling out alternative hypotheses, etc 11.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 11.4 Generalizability (and heterogeneity) “But all the other papers do it!” A common response to critiques (particularly critiques of the generalizability of experimental work) is that “all the other papers have the same problem” and that excepting this critique would require rejecting all previous work too. In politics this has been referred to as “what-about-ism”. You can guess that I’m not a fan of this. I think one always needs to defend the paper and approach on its own merits. Generalisability is an important issue. Each of the other published papers that also suffers from such issues has a specific response and justification for that particular case, and if it doesn’t this is sorely lacking. I think we should be reading and publishing papers that consider, discuss, and acknowledge their own limitations, and future work can test and build on this. This should promote to robust, reproducable science. Just because I say “‘“this is something we should be concerned with doesn’t mean I’m saying”this paper has no value’. I just mean”let’s discuss reasons why this may or may not threaten internal or external validity/generalisability, and how we can design the study and analysis minimise these potential problems\"\" In writing a paper, I find it important that we the authors feel the results are credible and not overstated. So I feel like the best approach is “let’s write the best paper we can and consider every issue seriously, and then hopefully the good publication/peer-review outcome will follow”. That’s also the most motivating and least stressful way for me to work. (Rather than thinking ‘how can I sneak this paper into the best journal?’) In fact I consider peer review and high rating and use as the important outcome, not the publication itself. We live in a world where anyone can publish their work immediately on the WWW. The journals themselves are providing little or no service: it is the reviewers and editors offering feedback and evaluation that matters. A thought: Replace reviews (accept/reject/R&amp;R) with ratings (0-10)? "],
["quant-design-power.html", "12 (Experimental) Study design: Background and quantitative issues 12.1 Pre-registration and Pre-analysis plans 12.2 Sequential and adaptive designs 12.3 Efficient assignment of treatments", " 12 (Experimental) Study design: Background and quantitative issues 12.1 Pre-registration and Pre-analysis plans 12.1.1 The benefits and costs of pre-registration: a typical discussion BB: That said, I would be interested to think about the benefits – and more importantly limitations to – pre-registration. I think it could solve some of the p-hacking problems but not much else. How to not relegate exploratory analyses too far is also unclear to me. DR: I’m much more on the ‘pro’ side pre-registration and PaPs. It also helps deal with publication bias and file drawers. And p-hacking is a huge issue IMHO. But it is also good to have some consideration of the pros and cons, so this would be great. BB: RE pre-reg: yes I think it is enough that it prevents p-hacking (there could be very little cost associated with pre-reg) but I fear that it could prevent other advancements if it relegates exploratory analyses too far. DR: I don’t think it should be binary. Systems need to be worked out for adjustments to the meaning of reported estimates depending on whether they were or were not preregistered, and how many were preregistered. While reported significance levels could be adjusted in the frequentist framework, this will all presumably based on measures of the likelihood that such a result would have been estimated/reported. Thus I think this could most easily be incorporated into a Bayesian framework but I’m not saying it would be easy. Still, they have done some good work on adjustments for ‘sequential designs’. BB: I think that it could also stifle students a bit – it may reduce further the number of students who have access to funding that allows for experiments that will be able to be published if all experiments have to be high-powered. DR: Statistical power is an important issue. I was skeptical at first about the ‘dangers of underpowered studies’ but I’m coming around a bit. My thinking was that ‘we can simply make downward adjustments to the estimates reported in underpowered studies’. Anyways, we don’t want to put the cart before the horse: as Gelman said at a conference we should be supporting science not the careers of scientists. I tend to think there are strong arguments for more centralization in social science. And my impression is that we actually have too many different studies and distinct research programs being run, and too many papers being published and not carefully brought together into a framework. Going through the studies on the https://www.replicationmarkets.com/ reinforces this impression for me. Still, I think there are ways around this to enable early career people. ‘Underpowered’ experiments could be registered as part of a longer/sequential research program, perhaps collaborative and enabling meta-analysis. BB: I also don’t think it gets at publication bias very much unless pre-reg’ed studies are followed up on. Only then do you know why the study didn’t come out – and quite a lot of the time I think it will be attrition/inability to gather the necessary data. Someone could launch that journal though – the Journal of Failed Studies – to have a place for a record that they have been run and what happened to be kept. So I am pro pre-reg, I just think the system needs a bit of work. DR: If preregistration is made public and well-organize, then the ‘failed’ exercises willtbe integrated into future meta-analyses; so that’s at least a partial solution here. Agreed, we need to build better systems for incentivising pre-registration and careful data sharing. We need to give career credit to people for planning designing and reporting credible experiments and projects, even if they ‘fail’. Part this is publishing/rewarding tight null results, which actually do add a lot of value. We might also consider offering some reward careerwise to experiments that fail – in terms of being deeply inconclusive– for some arbitrary or random reason even though they were well-planned and executed. But I think it is hard to get the incentives right for the latter. 12.1.2 The hazards of specification-searching 12.2 Sequential and adaptive designs Needs to adjust significance tests for augmenting data/sequential analysis/peeking Statistics/econometrics new-statistics sagarin_2014 http://www.paugmented.com/ resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/ Yet … \\(P_{augmented}\\) may overstate type-1 error rate Statistics/econometrics response to referees, new-statistics \" A process involving stopping “whenever the nominal \\(p &lt; 0.05\\)” and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5%. Even if the subsequent data suggested a “one in a million chance of arising under the null” the overall process yields a 5%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the “likelihood of the null hypothesis” given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate. Considering the calculations in , it is clear that \\(p_{augmented}\\) should the type-1 error of the process if there is a positive probability that after an initial experiment attains p\\(&lt;0.05\\), more data is collected. A headline \\(p&lt;0.05\\) does imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded \\(p&gt;0.05\\), thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though \\(p&lt;0.05\\). Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine’s experiment). Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p&lt;0.05, more data is collected, implying that \\(p_{augmented}\\) as calculated above overstates the type I error rate that would arise from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below \\(p=0.05\\), in order to attain a type-1 error rate of 5% or less in our published corpus.\" 12.3 Efficient assignment of treatments (Links back to power analyses) 12.3.1 How many treatment arms can you ‘afford’? "],
["power.html", "13 (Experimental) Study design: (Ex-ante) Power calculations 13.1 What sort of ‘power calculations’ make sense, and what is the point? 13.2 Power calculations without real data 13.3 Power calculations using prior data", " 13 (Experimental) Study design: (Ex-ante) Power calculations 13.1 What sort of ‘power calculations’ make sense, and what is the point? 13.1.1 The ‘harm to science’ from running underpowered studies \"One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results. Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.\" verkaik2016, metzger2015 On magnitude error due to underpowered studies: https://www.pauljferraro.com/publications/2020/2/1/is-there-a-replicability-crisis-on-the-horizon-for-environmental-and-resource-economics 13.2 Power calculations without real data 13.3 Power calculations using prior data Adapt example in ‘scopingwork.Rmd’ to this "],
["experimetrics-te.html", "14 ‘Experimetrics’ and measurement of treatment effects from RCTs 14.1 Which error structure? Random effects? 14.2 Randomization inference? 14.3 Parametric and nonparametric tests of simple hypotheses 14.4 Adjustments for exogenous (but non-random) treatment assignment 14.5 IV in an experimental context to get at ‘mediators’? 14.6 Heterogeneity in an experimental context 14.7 Incorporate above: Notes on “The econometrics of randomised experiments” (Athey and Imbens)", " 14 ‘Experimetrics’ and measurement of treatment effects from RCTs 14.1 Which error structure? Random effects? 14.2 Randomization inference? 14.3 Parametric and nonparametric tests of simple hypotheses 14.4 Adjustments for exogenous (but non-random) treatment assignment 14.5 IV in an experimental context to get at ‘mediators’? 14.6 Heterogeneity in an experimental context 14.7 Incorporate above: Notes on “The econometrics of randomised experiments” (Athey and Imbens) (with an eye towards giving experiments_ Page 7 Fundamentally, most concerns with external validity are related to treatment effect heterogeneity … [ considering extrapolation between settings A and B] Units in the two settings may differ in observed or unobserved characteristics, or treatments may differ in some aspect. to assess these issues it is helpful to have … randomized experiments, in multiple settings [varying] in the distribution of characteristics of the units, and possibly … the nature of the treatments or the treatment rate, in order to assess the credibility of generalizing to other settings Shall we do Fisher’s test based on computing the distribution of differences in means randomly reassigning the “treatment”? F-tests to consider multiple outcomes for any cases? (When) shall we use covariates (esp those for interactions) in the ‘deviations from mean’ form? Considering when to use controls (and interactions?) the asymptotic variance for \\(\\hat{\\tau}}\\) is less than that of the simple difference estimator by a factor equal to \\(1-R^2\\) from including the covariates relative to not including the covariates If… the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial DR: Could there not ever be a loss from doing interactions dividing up the sample too fine in doing this interactive estimation? This should depend on the true \\(R^2\\) I think. Try to remember what is the real tradeoff? Statistics adjusted for stratification: One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance DR: Is it valid to simply say “we choose the lower value of the estimated variances”? Are they advocating this? Such a procedure seems like it would have a bias. ** Things to potential incorporate in NL HE lottery paper(s) ** (When) shall we use covariates (esp those for interactions) in the ‘deviations from mean’ form? “randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received” Consider a “partial identification or bounds analysis” to deal with noncompliance at each margin Look up “randomization-based approach to IV” (Imbens and Rosenbaum, 2005) 14.7.1 Abstract and intro randomisation-based inference as opposed to sampling based inference DR: I Disagree as the object of interest is ultimately not the experimental sample, particularly not in the lab efficiency gains from stratification into small strata, adjust se to capture these gains (We have only done this to a limited extent) (Non-compliance, intention-to-treat, and IV) Estimation and inference for heterogeneous treatment with covariates… subpopulations… Maintaining the ability to construct valid confidence (mht etc). “Conditional average treatment effects” (interaction between units) Why careful statistics are important even for randomised experiments “Randomisation approach”: potential outcomes fixed, Assignment to treatments random Example of why the randomisation-based inference approach matters: “in the conventional sampling paradigm… Controlling for observable heterogeneity using a regression model” required for the assumptions to be justified with this approach. With randomisation approach it makes more sense to put data into strata by covariates, analyse within-group experiments and average results. Recommend small strata but not too small as variances cannot be estimated within pairs DR: Section 10 on heterogeneity is particularly relevant for us Other experimetrics methodology surveys mentioned (Duflo et al ’06, Glennerster and T ’13, Glennerster ’16); present one is more theoretical 14.7.2 randomised experiments and validity Defined as settings “where the assignment mechanism does not depend on characteristics of the units” - That seems to be “pure randomisation” … Debate about the supremacy of randomised experiments Definition of internal validity (DR:: it is a bit imprecise here). Typical argument about how external validity is no more guaranteed in observational studies then and randomised experiments “there is nothing in non-experimental methods which made some superior randomised experiments with the same population and sample size in this regard.” -DR: I think this is a bit of a strawman and a weak argument here - GR: This is the Deaton argument, very strange They argue for experiments in multiple settings varying in the characteristics of the units and perhaps the treatments to assess the credibility of generalizing to other settings. (?Graphical methods to deal with external validity issues?) Finite population versus random sample from super-population We can interpret the uncertainty as unobserved potential outcomes rather than sampling uncertainty. DR: I don’t see why these are mutually exclusive. GR: agreed DR: Viewing the sample of the full population of interest may not even work I’m considering some experiments such as those using within subject treatments The differences in these approaches matter in some settings but not others. Sometimes “…conventional sampling-based standard errors will be unnecessarily conservative” DR: This could be helpful 14.7.3 Potential outcomes/ Rubin causal model framework (covered earlier) (This is somewhat familiar by now) Potential outcomes If we do not impose limitations on interactions between units (like SUTVA) there will be a dimensionality problem \\[p\\:\\{0,1\\}^N \\times Y^{2N}\\times X^N \\rightarrow [0,1]\\] DR: I am not sure I understand this notation, particularly the \\(\\{0,1\\}\\) bit \" For randomized experiments we disallow dependence on the potential outcomes, and we assume that the functional form of the assignment mechanism is known\" (this goes further than “completely randomized”) 14.7.4 3.2 Classification of assignment mechanisms (Formula for probabilities of each assignment combination of control and treatment given for each one) Completely Randomized: \\(N_t\\) Units drawn at random from a population of \\(N\\) to receive the treatment, remaining \\(N_c\\) get control. Stratified randomized: Partition into \\(G\\) strata based on covariates values, “Disallowing assignments that are likely to be uninformative about the treatment effects of interest” Paired randomized (Extreme stratification) Cluster randomized: Treatments assigned randomly to entire clusters. Maybe cheaper to implement and more valid in the presence of interactions between units within but not across clusters. 14.7.5 The analysis of Completely randomized experiments Exact p-values for sharp null hypotheses (Fisher etc) “Sharp”: “Under which we can an for all the missing potential outcomes from the observed data” … So we can infer the distribution of any statistics under the Null. E.g., \\(H0\\), The treatment has no effect \\(Y_i(0)=Y_i(1)\\forall i\\), vs \\(Ha\\), At least one unit i has \\(Y_i(0)\\neq Y_i(1)\\) Difference in means by treatment status: Calculate the probability over the randomization distribution of a value with as large an absolute value as the one observed given the actual assignments. This is done by reassigning what we call the “treatments” to all possible combinations (keeping the number of treated units constant) and calculating the “placebo” treatment effect. Calculate the fraction of assignment vectors with statistic at least as large (in absolute value) as the observed one. DR: What is the statistic called and is there preprogrammed code? Is it the Fisher’s exact test? Can do for means or means of the ranks by treatment status (rank sum?) or any stat. Latter is less sensitive to outliers and thick-tailed distributions With multiple outcomes, multiple comparisons issues Use statistics it takes into account all the outcomes (e.g., F-stat, calculate exact P value using the ‘Fisher randomization distribution’ as in Young, ’16) Or use adjustments to P values e.g., Bonferroni or tighter bounds (Which are still more conservative than the Fisher thing); Romano ea survey (2010) Rosenbaum ’92 on estimating treatment effects based on rank statistics (DR: I don’t get this at all) 14.7.6 Randomization inference for Average treatment effects Neyman wanted to estimate the ATE for the sample at hand \\[\\tau=\\frac{1}{N}\\Sum_{i=1..N}{(Y_i(1)-Y_i(0)i)}=\\bar{Y}(1)-\\bar{Y}(0)\\] DR: Again, this is really not what we care about particularly not in a small-scale experiment. Proposed the estimator “Difference in average outcomes by treatment status” (DR: Same as in last section) Defining \\(D_i\\), a term representing “assignment minus the average assignment” Allows a restatement of the estimator which makes it clear that this is unbiased for the average treatment effect \\(\\tau\\) \\[\\hat{tau}=\\tau+\\frac{1}{N}\\Sum_{i=1..N}{(D_i(\\frac{N}{N_t}Y_i(1)+\\frac{N}{N_c}Y_i(0)}\\] Sampling variance of \\(\\hat{\\tau}\\) over the randomization distribution decomposed as \\[V{\\hat{\\tau})=\\frac{S_c^2}{N_c}+\\frac{S_t^2}{N_t}-\\frac{S_{tc}^2}{N}\\] Where \\(S_c^2\\) and \\(S_t^2\\) Are the variances of the control and treated outcomes, and \\(S_{tc}^2\\) is the variance of the unit level treatment effect (DR: This must be related to the covariance) We can estimate rhe first two terms but not the latter term as we have no observations with both a control and a treatment. In practice researchers ignore the third term, which leads to an upward bias for the sample treatment effect but an unbiased estimator of the population ATE DR: Any intuition for this? We still need to make large sample approximations to construct confidence intervals for the ATE. DR: Does this yield any practical strategy for us to use? 14.7.7 Quantile treatment effect (Infinite population context) Usefulness: Uncover “Treatment effects in the tails” Results robust to thick tails S-th quantile treatment effect defined as the difference in quantiles between the \\(Y_i(1)\\) and\\(Y_i(0)\\) distributions: \\[\\tau_s=q_{Y(1)}(s)-q_{Y(0)}(s)\\] … this is distinct from the “quantile of the differences”: \\(q_{Y(1)-Y(0)}(s)\\), which is in general not identified DR: the letter is truly more interesting; we care about the distribution of the impact of the treatment and not so much about the impact of the treatment on the distribution of outcomes. the two are equal if there is “perfect rank correlation between the two potential outcomes” (DR: I think this simply means that the unit ranked n’th if not treated would also be the unit ranked n’th if treated … no crossing over). Making lemonade: they argue here that the (identifiable) difference in quantiles would be more interesting to a policymaker considering exposing all units to the treatment … (DR: presumably because she should not care who get the particular outcome but only about the distribution of outcomes, a common axiom for social welfare functions). Estimates and tests: use the difference in quantiles as a statistic in and exact P value computation … results for such exact tests are quite different than those based on estimated effects and standard errors because “Quantile estimates are far from normally distributed.” 14.7.8 Covariates (if not stratified) in completely randomized experiments (They strongly recommend stratifying instead of ex post controls.) Why use controls if a simple difference in means is unbiased for the ATE? “incorporating covariates may make analyses more informative” (greater precision) Can incorporate covariates in exact P value analysis, or estimate average treatment effects within subpopulations and average these up appropriately DR: How to do these things in practice? correcting for compromised randomization … which may occur because of missing data and selective attrition They give an example with the data from Lalonde where they estimate the average treatment effect for two groups those with and without prior earnings. They then add these up weighted by the estimated probability of being in each group. DR: I understand this correctly, as they do not define all variables also, how do they compute the standard error of the combined estimator here?! this seems more like an interaction than a standard control here, which would allow a different intercept (control outcome) but not a different treatment effect. ? se of \\[\\hat{p}(\\bar{Y}_t|Y^{t-1}=0-\\bar{Y}_c|Y^{t-1}=0)+(1-\\hat{p})(\\bar{Y}_t|Y^{t-1}=1-\\bar{Y}_c|Y^{t-1}=1)\\] DR: I think there is a simple formula for difference in means that could be applied to this 14.7.9 Randomization inference and regression estimators They urge caution in using reg. “Since randomization does not justify the models, almost anything can happen” (Freedman 08) But using only “indicator variables based on partitioning the covariate space” preserves many of the finite simple properties of simple comparisons of means. Regression estimators for average treatment effects With a single variable, the least-squares estimate of \\(\\tau\\) is identical to the simple difference in means: \\[\\hat{tau}_{ols} = \\bar{Y^o}_t - \\bar{Y}^o_c\\] The intercept is the control value of course: \\(\\hat{\\alpha}_{ols}=\\bar{Y}^o-\\hat{\\tau_{ols}}\\bar{W}=\\bar{Y^0}_c\\). Conceptually important: the unbiasedness claim in the Neyman analysis is conceptually different from the one in conventional regression analysis: in the first case the repeated sampling paradigm keeps the potential outcomes fixed and varies the assignments, whereas in the latter the realized outcomes and assignments are fixed but different units with different residuals, but the same treatment status, are sampled. Redefining the residual in randomisation-based inference terms Now the error term has a clear meaning as the difference between potential outcomes and their population expectation [DR: I think they mean the expectation conditional on treatment] The randomization implies that the average residuals for treated and control units are zero … DR: They mean it implies mean independence (?) but not full independence, heteroskedasticity still likely Because the general robust variance estimator has no natural degrees-of-freedom adjustment [DR: ??], these standard [Randomisation-based?] robust variance estimators differs slightly from the Neyman unbiased variance estimator \\(\\hat{V}_{neyman}\\) \\(\\hat{V}_{robust} =\\frac{s^2_c}{N_c}\\frac{N_c-1}{N_c}+\\frac{s^2_t}{N_t}\\frac{N_t-1}{N_t}\\) Compared to the previously stated estimator for the TE variance for the sample (which we argued overstates the true sample TE variance) \\(\\hat{V}_{neyman} =\\frac{s^2_c}{N_c}+\\frac{s^2_t}{N_t}\\) The Eicker-Huber-White variance estimator is not unbiased, and in settings where one of the treatment arms is rare, the difference may matter They give an example where it does not matter. DR: This point seems ignorable for most of our designs, as we intentionally avoid such rare arms (but in NL lottery maybe) 14.7.10 Regression Estimators with Additional Covariates [DR: seems important] For now they continue to focus on ‘pure randomisation’, not stratified nor merely exogenous conditional on observables Can include these additively: \\[Y^{obs}_i=\\alpha+\\tau W_i + \\beta&#39;\\dot{X}_i + \\epsilon_i\\] Can allow a ‘full set of interactions’ \\(Y^{obs}_i=\\alpha+\\tau W_i + \\beta&#39;\\dot{X}_i + \\gamma&#39;\\dot{X}_i W_i + \\epsilon_i\\) DR: They do not do much discussion here of whether to do additive or full interactions; maybe it comes later (causal trees etc) In general the least squares estimates based on these regression functions are not unbiased for the average treatment effects over the randomization distribution given the finite population. DR: Why not? Intuition? Which regression function of the ones above is referred to here? DR: The discussion below suggests it will still be consistent (asymptotically unbiased) There is one exception. If the covariates are all indicators and they partition the population, and we estimate the model with a full set of interactions, Equation (5.4), then the least squares estimate of \\(\\tau\\) is unbiased for the average treatment effect If \\(\\bar{X}\\) is the average value of \\(X_i\\) in the sample, then =_1{X}+_0(1−{X})$, and \\(\\hat{\\gamma}=\\hat{\\tau}_1-\\hat{\\tau}_0\\) With large sample approximations we can ‘say something about the case with multivalued covariates’ … “\\(\\tau\\) [DR: estimated how?] is asymptotically unbiased for the average treatment effect …” the asymptotic variance for \\(\\hat{\\tau}}\\) is less than that of the simple difference estimator by a factor equal to \\(1-R^2\\) from including the covariates relative to not including the covariates DR: This motivates the use of covariates even in a randomized design, and even if we don’t take the ‘model of the covariates’ seriously. “results do not rely on the regression model being true in the sense that the conditional expectation of Y obs i is actually linear in the covariates and the treatment indicator in the population” DR: Is this for the linear controls model or for the full interactions model? However, … If… the covariates have very skewed distributions, the finite sample bias in the linear regression estimates may be substantial DR: Intuition? The presence of non-zero values for γ imply treatment effect heterogeneity. Best argument for using only binary/categorical interactions: interpretation “Only if the covariates partition the population do these \\(\\gamma\\) have a clear interpretation as differences in average treatment effects.” DR: Could there not ever be a loss from including interactions and dividing up the sample too fine in doing this interactive estimation? This should depend on the true \\(R^2\\) I think. Try to remember what is the real tradeoff. ** 6 The Analysis of Stratified and paired randomized experiments ** 14.7.11 Stratified randomized experiments: analysis Case for stratification capture the gains from ex post regression adjustment without the potential costs of linear regression, and the potential costs of linear regression, and therefore stratification is generally preferable over regression adjustment Within this stratum we can estimate the average effect as the difference in average outcomes for treated and control units: \\(\\tauˆg = \\bar{Y}^{obs}_{t,g} − \\bar{Y}^{obs}_{c,g}\\), and we can estimate the within-stratum variance, using the Neyman results, as \\(\\hat{V}(\\hat{\\tau}) =\\frac{s^2_{t,g}}{N_{t,g}} + \\frac{s^2_{c,g}}{N_{c,g}}\\) where the g-subscript indexes the stratum [They wrote ‘j’ but I think its a typo] Next just average weighted by stratum shares: \\(\\hat{\\tau} = \\sum_{g=1..G}{\\hat{tau}_g \\frac{N_g}{N}\\) with estimated variance \\(\\sum_{g=1..G}\\)(_g)()^2$ DR: Presumably they mean the above mentioned Neyman variance Also note the squared term in the variance estimation, this may be how they computed the variance in the above empirical example “Special case”: proportion treat units the same in all strata \\(\\rightarrow\\) ATE estimator equals difference in means by treatment status: \\(\\hat{\\tau} = \\sum_{g=1..G}{\\hat{tau}_g \\frac{N_g}{N}=\\bar{Y}^{obs}_t-\\bar{Y}^{obs}_c\\) … same as estimator for completely randomized experiment But the estimated variance for the latter will be overly conservative. DR: But I thought stratifying sometime ends up yielding a larger estimated variance? ##Paired randomized experiments: analysis (Skipping note-taking for now) 14.7.12 7 The Design of randomised experiments and the benefits of stratification … Our recommendation is that one should always stratify as much as possible, up to the point that each stratum contains at least two treated and two control units 14.7.13 7.1 Power calculations DR: This section is fairly basic and trivial, largely what we already know we largely focus on the formulation where the output is the minimum sample size required to find treatment effects of a pre-specified size with a pre-specified probability DR: My usual formulation DR: Why are they doing these calculations based on the t-statistic, when they recommend using other measures? DR: They claim equal sample sizes is “typically close to optimal” in cases without homoskedasticity. I think this is pure speculation. 14.7.14 Stratified randomized experiments: Benefits Stratifying does not remove any bias, it simply leads more precise inferences than complete randomization confusion in the literature concerning the benefits of stratification in small samples if this correlation is weak [between the stratifying variables and the outcome] in fact there is no tradeoff. We present formal results that show that in terms of expected-squared-error, stratification (with the same treatment probabilities in each stratum) cannot be worse than complete randomization. if one stratifies on a covariate that is independent of all other variables, then stratification is obviously equivalent to complete randomization. Ex ante, committing to stratification can only improve precision, not lower it Qualifications to this: Ex-post, given the joint distribution of the covariates in the sample, a particular stratification may be inferior to complete randomization. … Second, the result requires that the sample can be viewed as a (stratified) random sample from an infinitely large population… guarantees that outcomes within strata cannot be negatively correlated. (Note) The lack of any finite sample cost … contrasts with … regression adjustment. [which] may increase the finite sample variance, and in fact it will strictly increase the variance for any sample size, if the covariates have no predictive power at all. Although there is no cost to stratification in terms of the variance, there is a cost in terms of estimation of the variance. Still One can always use the variance that ignores the stratification: this is conservative if the stratification did in fact reduce the variance DR: Is it valid to simply say “we choose the lower value of the estimated variances”? Are they advocating this? Such a procedure seems like it would have a bias. exact variance for a completely randomized experiment can be written as … variance for the corresponding stratified randomized experiment is… the difference in the two variances is \\(V_C − V_S =... \\geq 0\\) DR: I am curious how these terms are derived and compared if the strata we draw from are small, say litters of puppies, it may well be that the within-stratum correlation is negative, but that is not possible if all the strata are large: in that case the correlation has to be non-negative DR: unless sutva violated perhaps (?) consider two estimators for the variance [both unbiased] \\(\\hat{V}_C=\\frac{s^2_{t,g}}{N_{t,g}} + \\frac{s^2_{c,g}}{N_{c,g}}\\) &gt; the natural estimator for the variance under the completely randomized experiment is: \\(\\hat{V}_c=\\frac{s^2_{t}}{N_{t}} + \\frac{s^2_{c,g}}{N_{c,g}}\\) or a stratified randomized experiment the natural variance estimator, taking into account the stratification, is: \\(\\hat{V}_S=\\frac{N_f}{N_f+N_m}\\Big(\\frac{s^2_{fc}}{N_{fc}}\\frac{s^2_{ft}}{N_{ft}}\\Big)+\\frac{N_f}{N_f+N_m}\\Big(\\frac{s^2_{mc}}{N_{mc}}\\frac{s^2_{mt}}{N_{mt}}\\Big)\\) Hence, \\(E\\hat{V}_S\\leq\\hat{V}_C\\). DR: Because we know both are unbiased and we know the true variance of \\(\\hat{V}_C\\) is larger. Nevertheless, the reverse may hold in a particular sample where the stratification is not related to the potential outcomes … the two variances are identical in expectation but the \\(var\\Big(hat{V}_S\\Big) &lt; var\\Big(hat{V}_C\\Big)\\) DR: This seems contradictory at first but I think it’s correct. The expectation of the estimated variance can be smaller or identical, while the variance of the estimated variance can still be larger. 14.7.15 Re-randomization Basically, they argue that if the first pre-implementation experiment comes out very unbalanced, you can randomize again – this will be an indirect method of stratifying. P-values could/should be adjusted to take into account that you are basically stratifying imprecisely. 14.7.16 Analysis of Clustered Randomised Experiments our main recommendation is to include analyses that are based on the cluster as the unit of analysis. Although more sophisticated analyses may be more informative than simple analyses using the clusters as units, it is rare that these differences in precision are substantial, and a cluster-based analysis has the virtue of great transparency DR: skipping most of this section for now 14.7.17 Noncompliance in randomized experiments (DR: Relevant to NL lottery, not to charity experiments) randomization that validates comparisons by treatment status does not validate comparisons by post-treatment variables such as the treatment received. DR: good quote for Nlmed Responses to noncompliance: ITT LATE Partial identification or bounds analysis Latter: “to obtain the range of values for the average causal effect of the receipt of treatment for the full population.” Another approach, not further discussed here, is the randomization-based approach to instrumental variables developed in Imbens and Rosenbaum (2005). [check into that] They recommend against: &gt; The first of these is an as-treated analysis, where units are compared by the treatment received; this relies on an unconfoundedness or selectionon-observables assumption. A second type of analysis is a per protocol analysis, where units are dropped who do not receive the treatment they were assigned to. We need some additional notation in this section. DR: Skipping full note-taking on this for now but COME BACK TO IT as it is very relevant to NL Med; the bounds analysis could be particularly interesting 14.7.18 Heterogenous Treatment Effects and Pretreatment Variables Crump et al setup (?) Multiple splits and tests may lead to overstated statistical significance for differences in TE’s. Bonferroni “overly conservative in an environment where many covariates are correlated with one another” List, Shaikh, and Xu (2016) propose an approach accounting for this; it uses bootstrapping, and requires pre-specifying list of tests to conduct ** 10.3 Estimating Treatment Effect Heterogeneity ** Parametric estimators, ‘all interactions’ (presumably with a correction as noted above) Nonparametric estimator of \\(\\tau(x)\\) The approach of List, Shaikh, and Xu (2016) works for an arbitrary set of null hypotheses, so the researcher could generate a long list of hypotheses using the causal tree approach restricted to different subsets of covariates, and then test them with a correction for multiple testing. Since in datasets with many covariates, there are often many ways to describe what are essentially the same sub-groups, we expect a lot of correlation in test statistics, reducing the magnitude of the correction for multiple hypothesis testing. 14.7.19 10.3.1 Data-driven Subgroup Analysis: Recursive Partitioning for Treatment Effects Partition sample by “region of covariate space” Determine which partition produces subgroups that differ the most in terms of treatment effects. The method avoids introducing biases in the estimated average treatment effects and allows for valid confidence intervals using “sample splitting,” or “honest” estimation Output of the method … is a set of subgroups, selected to optimize for treatment effect heterogeneity (to minimize expected mean-squared error of treatment effects), together with treatment effect estimates and standard errors for each subgroup. If instead… &gt; we estimate the average treatment effect on the two subsamples using the same sample, the fact that this particular split led to a high value of the criterion would often imply that the average treatment effect estimate is biased. But here ,,, &gt; The treatment effect estimates are unbiased on the two subsamples, and the corresponding confidence intervals are valid, even in settings with a large number of pretreatment variables or covariates. Because unit level TE is not observed, it is difficult to use standard protocols … suggest transforming outcome from Y_i^{obs} to \\(Y_i^\\ast=Y_i^{obs}\\frac{W_i−p}{p(1−p)}\\) … “so that standard methods for recursive partitioning based on prediction apply” Which implies \\(E[Y_i^\\ast|X_i=x]=\\tau(x)=E[Y_i(1)−Y_i(0)|X_i = x]\\) DR: Are these p’s conditional on the x’s? Probably it doesn’t matter here as they are assuming pure randomisation. AI criterion focuses directly on the expected squared error of the treatment effect estimator … which turns out to depend both on the t-statistic and on the fit measures. … further modified to anticipate … that the treatment effects will be re-estimated on an independent sample after the subgroups are selected This penalises too small groups and too much variance, (in general) rewards explain outcomes but not treatment effect heterogeneity…enables a lower-variance estimate of the treatment effect. Wager and W argue for inflating SE’s rather than partitioning - DR: I see an advantage there, as the AI approach throws away data 14.7.20 10.3.2 Non-Parametric Estimation of Treatment Effect Heterogeneity Many allow descriptive evidence and prediction, but few methods available that allow for confidence intervals K-nearest neighbors, hurdle methods Do not prioritise ‘more important’ covariates … can work well and provide satisfactory coverage of confidence intervals with one or two covariates, but performance deteriorates quickly after that. The output of the nonparametric estimator is a treatment effect for an arbitrary x. The estimates generally must be further summarized or visualized since the model produces a distinct prediction for each x. A key problem with kernels and nearest neighbor matching is that all covariates are treated symmetrically; if one unit is close to another in 20 dimensions, the units are probably not particularly similar in any given dimension. We would ideally like to prioritize dimensions that are most important for heterogeneous treatment effects, as is done in many machine learning methods, including the highly successful random forest algorithm. But these are often “bias-dominated asymptotically” … except the ones proposed by Wager and Athey (2015) :) asymptotically normal and centered on the true value of the treatment effect,… consistent estimator for the asymptotic variance. Averages over the many “trees” of the form developed in Athey and Imbens (2016) … different subsamples are used for each tree [plus some randomness] Each tree is “honest,” in that one subsample is used to determine a partition and [another] to estimate treatment effects within the leaves. Unlike the case of a single tree, no data is “wasted” because each observation is used to determine the partition in some trees and used to estimate treatment effects in other trees, and subsampling is already an inherent part of the method. What does this mean?: can obtain nominal coverage with more covariates than K-Nearest Neighbour matching or kernel methods, (but still “eventually becomes bias-dominated when the number of covariates grows” … but “much more robust to irrelevant covariates than kernels or nearest neighbor matching.”) Also, approaches fitting separately for treatment and control Also, Bayesian perspectives on this: Green and Kern (2011), Hill (2012), others … but unknown asymptotic properties (DR: do we care?) 14.7.21 10.3.3 Treatment Effect Heterogeneity Using Regularized Regression Lasso-like (Imai and Ratkovic (2013), etc.) With few important covariates (a ‘sparse’ model), can derive valid CI’s w/o sample-splitting Some proposed modeling heterogeneity separately for treatment and control;… can be inefficient if the covariates that affect the level of outcomes are distinct from those that affect treatment effect heterogeneity. alternative … incorporate interactions … as covariates, and then allow LASSO to select which covariates are important. 14.7.22 10.3.4 Comparison of Methods Lasso: more sparsity restrictions, better handle linear or polynomial relationships between covariates and outcomes; outputs a regression; but CI’s justified only under strict conditions Random forest methods … are more localized, … capture complex, multi-dimensional interactions among covariates, or highly nonlinear interactions. Less sensitive to sparsity, CI’s do not ‘deteriorate’ as covariates grow (but MSE of predictions suffer) Inference more justifiable by random assignment (Lasso requires stronger assumptions) "],
["metaanalysis.html", "15 Making inferences from previous work; Meta-analysis, combining studies 15.1 Notes: Christensen et al 2019, ch 5, ’Using all evidence, registration and meta-analysis 15.2 Excerpts and notes from ‘Doing Meta-Analysis in R: A Hands-on Guide’ (Harrer et al) 15.3 Other notes, links, and commentary", " 15 Making inferences from previous work; Meta-analysis, combining studies My opinion on why this is so important (unfold): it is lame how often I see ‘new experiments’ and ‘new studies’ that tread most of the ground as old studies, spend lots of money, get a publication and … ignore or pay lip service to the previous findings. There is tons of data out there that can inform new questions and bring better through re-analysis and combination with other data. Otherwise we are not actually building progress. This is why I became involved with a project I called ‘ExpArchive’, later working with projects such as GESIS’ X-Econ to try to encourage and facilitate data sharing in experimental economics, as well as the innovationsinfundraising.org project, which is now collaborating with the Lily Institute’s “revolutionizing philanthropy research” (RPR) project. 15.1 Notes: Christensen et al 2019, ch 5, ’Using all evidence, registration and meta-analysis how the research community can systematically collect, organize, and analyze a body of research work Limitations to the ‘narrative literature review’: subjectivity, too much info to narrate 15.1.1 The origins [and importance] of study [pre-]registration … Make details of planned and ongoing studies available to the community …. including those not (yet) published Required by FDA in 1997, many players in medical community followed soon after Turner ea (08) and others documented massive publication bias and misrepresentation … but registration far from fully enforced (Mathieu ea ’09) found 46% clealy registered, and discrepancies between registered and published outcomes ! 15.1.2 Social science study registries Jameel 2009, AEA 2013, 2100 registrations to date RIDIE, EGAP, AsPredicted, OSF allowing a DOI (25,000+) 15.1.3 Meta-analsis Key references: Borenstein ea ’09, Cooper, Hedges, and V ’09 15.1.3.1 Selecting studies \"some scholarly discretion regarding which measures are ‘close enough’ to be included… contemperanous meta-analyses on the same topic finding opposit e conclusions ‘asses the robustness… to different inclusion conditions’… see Doucouliagos ea ’17 on inclusion options My opinion: this is the key barrier to meta-analysis in social science! How do we weight studies using different methodologies and in different contexts? The Bayesian Random Effects approach seems to offer some help here (this not to be confused with the random effects panel-data models discussed in standard Econometrics texts). 15.1.3.2 Assembling estimates Which statistic to collect? Studies \\(j \\in J, j= 1..N_j\\) Relevant estimate of stat from each study is \\(\\hat{ \\beta_j}\\) with SE \\(\\hat{\\sigma_j}\\) Papers report several estimates (e.g., in robustness checks): which to choose, esp if author’s preferred approach differs from other scholars. Ex from Hsiang, B, Miguel, ’13: links between extreme climate and violence how to classify outcomes… interpersonal and intergroup… normalised as pct changes wrt the meanoutcome in that dataset how to standardice climate varn measures… chose SD from local area mean (DR: this choice implicitly reflects a behavioural assumption) \\(\\rightarrow\\) ‘pct change in a conflict outcome as a fncn of a 1 SD schock to local climate’ 15.1.4 Combining estimates ‘Fixed-effect meta-analysis approach’: assumes a single true effect’ DR: I’m not sure I agree on this assesment of why this is unlikely to be true in practice… ‘differences in measures’ (etc) seem to be a different issue Equal weight approach: (Simply the average across studies… ugh) Precision-weighted approach: \\[\\hat{\\beta}_{PW}= \\sum_{j}p_j\\hat{\\beta}_j/ \\sum_{j}p_j\\] where \\(p_j\\) is the estimated precision for study \\(j\\): \\(\\frac{1}{\\hat{\\sigma_i}^2}\\) Thus the weight \\(\\omega_j\\) placed on study \\(j\\) is proportional to it’s precision. ‘implies weight in proportion to sample size’? I think that’s loosely worded, it must be nonlinear. \\(\\rightarrow\\) This minimises the variance in the resulting meta-analytical estimate: \\[var(\\hat{\\beta}_{PW}) =\\sum_j \\omega_j\\hat{\\sigma_j}^2 = \\frac{1}{\\sum_j(p_j)}\\] ‘inclusion of additional estimates always reduces the SE of \\(\\hat{\\beta_{PW}}\\) [in expectation].’ … so more estimtes can’t hurt as long as you know their precision. (they give a numerical example here with 3 estimates) 15.1.5 Heterogeneous estimates… 15.1.5.1 WLS estimate (Stanley and Doucouliagos ’15) Interpreted as ‘an estimate of the average of potentially heterogenous estimates’ This may feel like a more familiar to Economists but it is also seems to be far less useful than the Bayesian approach. 15.1.5.2 Random-effects (more common) Focus here on hierarchical Bayesian approach (Gelman and Hill ’06; Gelman ea ’13) ‘The magnitude and precision of the common component represents the generalizable conclusions we might draw from a literature’ … continuing from above notation ‘cross-study differences we observe might not be driven solely by sampling variability… [even with] infinite data, they would not converge to the exact same [estimate]’ True Treatment Effect (TE) \\(\\beta_j\\) for study j drawn from a normal distribution… \\[\\beta_j \\sim N(\\mu, \\tau^2)\\] ‘Hyperparameters’ \\(\\mu\\) determines central tendency of findings… \\(\\tau\\) the extent of hety across contexts. Considering \\(\\tau\\) vs \\(\\mu\\) is informative in itself. And a large \\(\\mu\\) may suggest looking into sample splits for hety on obsl lines. Uniform prior for \\(\\mu\\) \\(\\rightarrow\\) conditional posterior: \\[\\mu|\\tau,y \\sim N(\\hat{\\mu}, V_{\\mu})\\] where the estimated common effect \\(\\hat{\\mu}\\) is \\[\\hat{\\mu}= \\frac{\\sum_{j}(1/(\\hat{\\sigma}^2_j+\\hat{\\tau}^2))\\hat{\\beta}} {\\sum_{j}(1/(\\hat{\\sigma}^2_j+\\hat{\\tau}^2))}\\] (Similar to precision-weighted approach but now the between-study dispersion is incorporated into the weights) and where the estimated variance of the generalizable component \\(V_\\mu\\) is: \\[Var(\\hat{\\mu})= \\frac{1}{\\sum_j\\big(1/(\\hat{\\sigma_i}^2 + \\hat{tau}^2)}\\] Confusion/correction? Is this the estimated variance or the variance of the estimate? and how do we estimate some of the components of these, like \\(\\hat{\\tau}\\)? Intuitively, if estimated [TE] in all studies are near one another and have relatively wide and overlapping [CI’s], then most of the difference in estimates is likely the result of sampling variation [and \\(\\tau\\)] is likely to be close to zero. DR: But if the TE have wide CI’s, do we have power to idfy btwn-study hety? … I guess that’s what the ‘estimated TE are all near each other’ gives us? … Alternatively, if there is extensive variation in the estimated ATEs but each is precise… \\(\\tau\\) is likely to be relatively large. Coding meta-analyses in R “A Review of Meta-Analysis Packages in R” offers a helpful guide to the various packages, such as metafor. Doing Meta-Analysis in R: A Hands-on Guide appears extremely helpful; see, e.g., their chapter Bayesian Meta-Analysis in R using the brms package The \\(I^2\\) stat is a measure of the proportion of total variation attributed to cross-study variation; if \\(\\hat{\\sigma}_j\\) is the same across all studies we have: \\(I^2(.) = \\hat{\\tau}^2/(\\hat{\\tau}^2 + \\hat{\\sigma}^2)\\) 15.2 Excerpts and notes from ‘Doing Meta-Analysis in R: A Hands-on Guide’ (Harrer et al) Some notes follow excerpting and commenting on Doing Meta-Analysis in R: A Hands-on Guide Note that installation of the required packages can be tricky here. For Mac Catalina with R 4.0 I followed the instructions HERE #devtools::install_github(&quot;MathiasHarrer/dmetar&quot;) #...I did not &#39;update new packages&#39; #install.packages(&quot;extraDistr&quot;) library(meta) #install.packages(&quot;brms&quot;) library(brms) library(dmetar) library(extraDistr) 15.2.1 Pooling effect sizes FE model calculates weighted average: FIX THESE FORMULAS \\[\\hat{\\theta_F} = \\frac{\\sum\\limits_{k=1}^K \\hat{\\theta_k} \\] \\[\\hat{\\sigma^2_k}=\\sum\\limits_{k=1}^K \\frac{1}{K}\\hat{\\sigma}^2_k \\] note that this process does not ‘dig in’ to the raw data, it just needs the summary statistics, neither does the “RE model” they refer to: Both of these models only require an effect size, and a dispersion (variance) estimate for each study, of which the inverse is taken. This is why the methods are often called generic inverse-variance methods. Nor the Bayesian models, apparently (they use the same ‘madata’ dataset) 15.2.2 Bayesian Meta-analysis “The model we apply in Bayesian Meta-Analysis is a so-called Bayesian Hierarchical Model… every meta-analytical model inherently possesses a multilevel, and thus ‘hierarchical’, structure.” The setup Underlying RE model (as before) Study-specific estimate: \\[ \\hat\\theta_k \\sim \\mathcal{N}(\\theta_k,\\sigma_k^2) \\] True study-specific effects distributed: \\[ \\theta_k \\sim \\mathcal{N}(\\mu,\\tau^2) \\] … simplified to the ‘marginal’ form: \\[ \\hat\\theta_k | \\mu, \\tau, \\sigma_k \\sim \\mathcal{N}(\\mu,\\sigma_k^2 + \\tau^2)\\] And now we specify priors for these parameters, ‘making it Bayesian’ \\[(\\mu, \\tau^2) \\sim p(.)\\] \\[ \\tau^2 &gt; 0 \\] Estimation will… involve[] Markov Chain Monte Carlo based sampling procedures, such as the Gibbs Sampler. In the brms package we will be using in this chapter, the No-U-Turn Sampler, or NUTS (Hoffman and Gelman 2014), is used. Why use Bayesian? to “directly model the uncertainty when estimating [the between-study variance] \\(\\tau^2\\)” “have been found to be superior in estimating the between-study heterogeneity and pooled effect, particularly when the number of included studies is small” “produce full posterior distributions for both \\(\\mu\\) and \\(\\tau\\)” … so we can make legitimate statements about the probabilities of true parameters “allow us to integrate prior knowledge and assumptions when calculating meta-analyses” (including methodological uncertainty perhaps) Setting weakly informative’ priors for the mean and cross-study variance of the TE sizes It has been argued that a good approach is to use weakly informative priors (Williams, Rast, and Bürkner 2018) [rather than ‘non-informative priors’!] For \\(\\mu\\): include distributions which represent that we do indeed have some confidence that some values are more credible than others, while still not making any overly specific statements about the exact true value of the parameter. … In most applied cases, it seems reasonable to assume that the true effect size we want to estimate must lie somewhere between, for example, Cohen’s \\(d=-2.0\\) and \\(d=2.0\\), but will unlikely be hovering around \\(d=50\\). A good starting point for our \\(\\mu\\) prior may therefore be a normal distribution with mean \\(0\\) and variance \\(1\\). This means that we grant a 95% prior probability that the true pooled effect size \\(\\mu\\) lies between \\(d=-2.0\\) and \\(d=2.0\\): \\[ \\mu \\sim \\mathcal{N}(0,1)\\] For \\(\\tau^2\\) must be non-negative, but might be very close to zero. Recommended distribution for this case (for variances in general): Half-Cauchy prior (a censored Cauchy) \\(\\mathcal{HC}(x_0,s)\\) with location parameter \\(x_0\\) (peak on x-axis) and \\(s\\), scaling parameter ‘how heavy-tailed’ Half-Cauchy distribution for varying \\(s\\), with \\(x_0=0\\): HC is ’heavy-tailed;… gives some probability to very high values but low values are still more likely. One might consider \\(s=0.3\\) \\(s\\) corresponds to the std deviation here? … so an SD of the effect size about 1/3 of it’s mean size? Checking the share of this distribution below 0.3… phcauchy(0.3, sigma = 0.3) #cumulative share of distribution below 0.3 ... is 1/2 ... with sigma=0.3 ## [1] 0.5 … But they go for the ‘more conservative’ \\(s=0.5\\). In general, it is advised to always conduct sensitivity analyses with different prior specifications to check if they affect the results substantially Complete model: \\[ \\hat\\theta_k \\sim \\mathcal{N}(\\theta_k,\\sigma_k^2) \\] \\[ \\theta_k \\sim \\mathcal{N}(\\mu,\\tau^2) \\] \\[ \\mu \\sim \\mathcal{N}(0,1)\\] \\[ \\tau \\sim \\mathcal{HC}(0,0.5)\\] 15.2.2.1 Bayesian Meta-Analysis in R using the brms package You specify the priors as a vector of elements, each of which invokes the ‘prior’ function, which makes some sort of data frame. The priors function takes a distribution function with parameters, and a ‘class’. priors &lt;- c(prior(normal(0,1), class = Intercept), prior(cauchy(0,0.5), class = sd)) A quick look at the data we’re using here: str(ThirdWave[,1:3]) ## tibble [18 × 3] (S3: tbl_df/tbl/data.frame) ## $ Author: chr [1:18] &quot;Call et al.&quot; &quot;Cavanagh et al.&quot; &quot;DanitzOrsillo&quot; &quot;de Vibe et al.&quot; ... ## $ TE : num [1:18] 0.709 0.355 1.791 0.182 0.422 ... ## $ seTE : num [1:18] 0.261 0.196 0.346 0.118 0.145 ... To actually run the model, he uses the following code: This requires careful installation of packages. See here for Mac OS Catalina, R 4.9 instructions. 30 iterations is no doubt far too few, but the run process is too slow to do many more here. I find it surprising how long this procedure takes to run, this simulation, given that the actual data used (estimates and SE’s) is rather small. m.brm &lt;- brm(TE|se(seTE) ~ 1 + (1|Author), data = ThirdWave, prior = priors, iter = 30) ## Compiling the C++ model ## Start sampling ## ## SAMPLING FOR MODEL &#39;d621f94cb5a5107aad70f85d900cd165&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: WARNING: No variance estimation is ## Chain 1: performed for num_warmup &lt; 20 ## Chain 1: ## Chain 1: Iteration: 1 / 30 [ 3%] (Warmup) ## Chain 1: Iteration: 3 / 30 [ 10%] (Warmup) ## Chain 1: Iteration: 6 / 30 [ 20%] (Warmup) ## Chain 1: Iteration: 9 / 30 [ 30%] (Warmup) ## Chain 1: Iteration: 12 / 30 [ 40%] (Warmup) ## Chain 1: Iteration: 15 / 30 [ 50%] (Warmup) ## Chain 1: Iteration: 16 / 30 [ 53%] (Sampling) ## Chain 1: Iteration: 18 / 30 [ 60%] (Sampling) ## Chain 1: Iteration: 21 / 30 [ 70%] (Sampling) ## Chain 1: Iteration: 24 / 30 [ 80%] (Sampling) ## Chain 1: Iteration: 27 / 30 [ 90%] (Sampling) ## Chain 1: Iteration: 30 / 30 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.005682 seconds (Warm-up) ## Chain 1: 0.006664 seconds (Sampling) ## Chain 1: 0.012346 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;d621f94cb5a5107aad70f85d900cd165&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.1e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: WARNING: No variance estimation is ## Chain 2: performed for num_warmup &lt; 20 ## Chain 2: ## Chain 2: Iteration: 1 / 30 [ 3%] (Warmup) ## Chain 2: Iteration: 3 / 30 [ 10%] (Warmup) ## Chain 2: Iteration: 6 / 30 [ 20%] (Warmup) ## Chain 2: Iteration: 9 / 30 [ 30%] (Warmup) ## Chain 2: Iteration: 12 / 30 [ 40%] (Warmup) ## Chain 2: Iteration: 15 / 30 [ 50%] (Warmup) ## Chain 2: Iteration: 16 / 30 [ 53%] (Sampling) ## Chain 2: Iteration: 18 / 30 [ 60%] (Sampling) ## Chain 2: Iteration: 21 / 30 [ 70%] (Sampling) ## Chain 2: Iteration: 24 / 30 [ 80%] (Sampling) ## Chain 2: Iteration: 27 / 30 [ 90%] (Sampling) ## Chain 2: Iteration: 30 / 30 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.008605 seconds (Warm-up) ## Chain 2: 0.005664 seconds (Sampling) ## Chain 2: 0.014269 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;d621f94cb5a5107aad70f85d900cd165&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 9e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: WARNING: No variance estimation is ## Chain 3: performed for num_warmup &lt; 20 ## Chain 3: ## Chain 3: Iteration: 1 / 30 [ 3%] (Warmup) ## Chain 3: Iteration: 3 / 30 [ 10%] (Warmup) ## Chain 3: Iteration: 6 / 30 [ 20%] (Warmup) ## Chain 3: Iteration: 9 / 30 [ 30%] (Warmup) ## Chain 3: Iteration: 12 / 30 [ 40%] (Warmup) ## Chain 3: Iteration: 15 / 30 [ 50%] (Warmup) ## Chain 3: Iteration: 16 / 30 [ 53%] (Sampling) ## Chain 3: Iteration: 18 / 30 [ 60%] (Sampling) ## Chain 3: Iteration: 21 / 30 [ 70%] (Sampling) ## Chain 3: Iteration: 24 / 30 [ 80%] (Sampling) ## Chain 3: Iteration: 27 / 30 [ 90%] (Sampling) ## Chain 3: Iteration: 30 / 30 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.009106 seconds (Warm-up) ## Chain 3: 0.005403 seconds (Sampling) ## Chain 3: 0.014509 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;d621f94cb5a5107aad70f85d900cd165&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.2e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: WARNING: No variance estimation is ## Chain 4: performed for num_warmup &lt; 20 ## Chain 4: ## Chain 4: Iteration: 1 / 30 [ 3%] (Warmup) ## Chain 4: Iteration: 3 / 30 [ 10%] (Warmup) ## Chain 4: Iteration: 6 / 30 [ 20%] (Warmup) ## Chain 4: Iteration: 9 / 30 [ 30%] (Warmup) ## Chain 4: Iteration: 12 / 30 [ 40%] (Warmup) ## Chain 4: Iteration: 15 / 30 [ 50%] (Warmup) ## Chain 4: Iteration: 16 / 30 [ 53%] (Sampling) ## Chain 4: Iteration: 18 / 30 [ 60%] (Sampling) ## Chain 4: Iteration: 21 / 30 [ 70%] (Sampling) ## Chain 4: Iteration: 24 / 30 [ 80%] (Sampling) ## Chain 4: Iteration: 27 / 30 [ 90%] (Sampling) ## Chain 4: Iteration: 30 / 30 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.005404 seconds (Warm-up) ## Chain 4: 0.004644 seconds (Sampling) ## Chain 4: 0.010048 seconds (Total) ## Chain 4: m.brm ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: TE | se(seTE) ~ 1 + (1 | Author) ## Data: ThirdWave (Number of observations: 18) ## Samples: 4 chains, each with iter = 30; warmup = 15; thin = 1; ## total post-warmup samples = 60 ## ## Group-Level Effects: ## ~Author (Number of levels: 18) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.29 0.10 0.12 0.46 0.98 62 75 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.58 0.09 0.45 0.78 1.02 40 39 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The formula for the model is specified using ‘regression formula notation’ As there is no ‘predictor variable’ here (unless it’s meta-regression), x is replaced with 1 But we want to give studies with greater precision of the effect size estimate a greather weight. Done using y|se(se_y) For the random effects terms he adds (1|study) to the predictor part. prior: Plugs in the priors created above plug in the priors object we created previously here. iter: Number of iterations of MCMC algorithm… the more complex your model, the higher this number should be. [DR: but what’s a rule of thumb here?] 15.3 Other notes, links, and commentary A high quality meta-analysis should:- Have a pre-registered protocol- Appropriately deal with dependent effect sizes- Explore effect size heterogeneity - Have a clear methods description- Report COIs- Publish data and code https://t.co/cHj11wv5vm — Dan Quintana ((???)) November 18, 2019 "],
["bayesian-approaches.html", "16 Bayesian approaches 16.1 My (David Reinstein’s) uses for Bayesian approaches (brainstorm) 16.2 ‘Statistical thinking’ (McElreath) and AJ Kurtz ‘recoded’ (bookdown): highlights and notes 16.3 Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep” 16.4 Other resources and notes to integrate", " 16 Bayesian approaches I take notes on several different resources/texts below. Ultimately I’ll try to integrate these into a single set of notes. 16.1 My (David Reinstein’s) uses for Bayesian approaches (brainstorm) 16.1.1 Meta-analysis of previous evidence Of prior work, especially on motivators of (effective) charitable giving and responses to effectiveness information Of my own series’ of experiments (potentially joint with prior work) 16.1.2 Inference, particularly about ‘null effects’ When/what can we say about the ‘absence of an effect’ How to integrate into inferences from diagnostic testing (e.g., common-trend assumption)? 16.1.3 ‘Policy’ and business implications and recommendations In particular, in a charitable giving social-media fundraising context, we might consider whether it is worth offering ‘seed contributions’ to encourage giving on existing pages. If so, ‘which pages should we seed and how much?’ 16.1.4 Theory-driven inference about optimizing agents, esp. in strategic settings Especially in the context od ‘predicted contributions to public goods… and 2nd order beliefs’ 16.1.5 Experimental design Optimal treatment assignment, with previous observables and a track record Sequential designs Bayesian Power calculation #sessionInfo() Package loadings from Kurtz: pacman::p_unload(pacman::p_loaded(), character.only = TRUE) ggplot2::theme_set(ggplot2::theme_grey()) bayesplot::color_scheme_set(&quot;blue&quot;) library(tidyverse) #adds in next chapter 16.2 ‘Statistical thinking’ (McElreath) and AJ Kurtz ‘recoded’ (bookdown): highlights and notes McElreath’s course and text looks great. I’m taking selective notes here; I’ll try to incorporate content from both text and youtube video lectures. AJ Kurtz has re-written the code using the brms package, which he finds superior. More crucially for me, he redoes the code using ggplot and tidyverse? I’m planning to through this here, adding my own notes, questions, and considerations and (hopefully) incorporating some of my own work. I’ve also forked Kurtz’s repo here, which I may play with. 16.2.1 The Golem of Prague (Chapter 1) Don’t let your model or approach turn into a Golem you can’t control. Don’t ‘believe the model’; continuously validate it. The map is not the territory. ‘Statistical decision trees’ lend a false sense of security… and almost never fit the actual case we are dealing with. (fig 1.1) Statistical models are non-unique maps to ‘process models’ which are non-unique maps to hypotheses. (He offers the example of neutral evolutionary selection’ example.) This makes strict falsification impossible: How can you falsify a hypothesis/theory if it corresponds to a wide set of process models and statistical models, many of which overlap other hypotheses? But this warning is at least as relevant for Bayesian analyses, which must be based on specifically defined (term) models of the DGP etc. Thus he recommends caution and continuous (?) interplay between the model and the data. (See next chapter … ‘small worlds and large worlds’.) He also suggests we refer not to ‘Confidence intervals’ or even ‘Credible intervals’, but to ‘Consistent intervals’ … as in ‘these intervals are consistent with the model and data’. And… [so you should] ‘…Explicitly compare predictions of more than one model’ Rethinking: Is NHST falsificationist? (#fig:failure_of_falsification.png)From McElreath video lecture 1 Null hypothesis significance testing, NHST, is often identified with the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify a null hypothesis, not the actual research hypothesis. So the falsification is being done to something other than the explanatory model. This seems the reverse from Karl Popper’s philosophy. I.e., scientists have turned things upside down; originally the idea was that you had substitute of hypotheses that you would want to falsify and now we try to falsify silly null hypotheses that “nothing is going on”. You should try to really build a hypothesis and test it not just reject that nothing is going on. 16.2.1.1 Book’s foci Bayesian data analysis Multilevel modeling Model comparison using information criteria 16.2.2 Small Worlds and Large Worlds (Ch 2) … The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20) We imagine a bag filled with four marbles, each of which is blue or white. “So, if we’re willing to code the marbles as 0 =”white\" 1 = “blue”, we can arrange the possibility data in a tibble as follows.\" I.e., we can consider the five possible worlds, in each of which the bag has a different number of white and blue marbles, and represent each of these worlds as a column vector: d &lt;- tibble(p_1 = 0, p_2 = rep(1:0, times = c(1, 3)), p_3 = rep(1:0, times = c(2, 2)), p_4 = rep(1:0, times = c(3, 1)), p_5 = 1) d p_1 p_2 p_3 p_4 p_5 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 We visualize this in the plot below, where each column is one ‘world’: d %&gt;% gather() %&gt;% #make it long, with an ket variable for the possibility &#39;world&#39; mutate(x = rep(1:4, times = 5), #an index for &#39;which ball&#39; possibility = rep(1:5, each = 4)) %&gt;% #distributing the &#39;which world&#39; index ggplot(aes(x = x, y = possibility, fill = value %&gt;% as.character())) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(.75, 4.25), ylim = c(.75, 5.25)) + theme(legend.position = &quot;none&quot;) Simple combinatorics (permutations rule) tells us how many ‘ways’ we can draw 1, 2, and 3 marbles… Here we think about ‘which’ marble is drawn, and not just ‘which color’ it is. We can draw marble 1-4, the first time, then 1-4 the second time, and then 1-4 the third time… so possibilities=marbles ^ draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) %&gt;% knitr::kable() draw marbles possibilities 1 4 4 2 4 16 3 4 64 Next, there is a huge amount of code explaining how to make the ‘garden of forking paths’ diagrams. I’m basically going to skip all that code, and paste in a few images. You can find all the code HERE Suppose there is only one blue ball and three white balls, possibility ‘2’ above. For this world, we see the full ‘garden of forking paths’ — the number of ways to select 1, 2, and 3 balls (with replacement) — below. Every path starting from the center is a possible (sequence of) draws. Figure 16.1: All possible draws of three balls Now the inferential exercise: we want to know (the likelihood) of each of the five possible ‘worlds’. As we draw data we know we are proceeding along one of some subset of the forking paths. For example, under possible world 2, if we draw Blue, then White, then Blue, this could have occured with any of the following paths (consider a draw of each of the white balls as distinct): Figure 16.2: All possible draws of three balls We see that under World 2 there are 3 ways of getting this sequence. 3 out of \\(4^3\\) equally likely paths under World 2, or a \\(3/64\\) chance (about 5%). We can do similar for the other possible worlds; multiplying the ‘ways to produce each draw’ in the path yields the ‘total ways to produce the path’, under each world. # if we make two custom functions, here, it will simplify the code within `mutate()`, below n_blue &lt;- function(x) { rowSums(x == &quot;b&quot;) } n_white &lt;- function(x) { rowSums(x == &quot;w&quot;) } t &lt;- # for the first four columns, `p_` indexes position tibble(p_1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), p_2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), p_3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), p_4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(`draw 1: blue` = n_blue(.), `draw 2: white` = n_white(.), `draw 3: blue` = n_blue(.)) %&gt;% mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 draw 1: blue draw 2: white draw 3: blue ways to produce w w w w 0 4 0 0 b w w w 1 3 1 3 b b w w 2 2 2 8 b b b w 3 1 3 9 b b b b 4 0 4 0 Among of all possible worlds, we see the most number of ways to get B-W-B in a world 4; with three blues and one white – here there are 9 ways in total to get B-W-B. Under world 4 this sequence occurs 9/64, or roughly 14% of the time. We can see this in the following plot. (We leave out the worlds with only one color ball, as these will have no paths that produce B-W-B). Below, each partitioned section represents one world, and the paths in that world that could produce B-W-B are shown. Figure 16.3: All possible draws of three balls Three paths, versus 8 paths, versus nine paths… Does this reveal world 4 to be the most likely contents of the present bag? Not necessarily. Suppose we knew ex-ante, from the factory, that ‘99 bags out of 100 have equal numbers of whites and blues.’ Then, it would be much more likely that this bag was from an equal-color bag (world 3), even though this draw is more likely conditional on the bag being from world 4. We need will to consider the base-rate probabilities as well. This in turn motivates the standard ‘false positive/false negative HIV test’ example. 16.2.3 Using prior information We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25) This seems to easy to be true, but our garden illustration helps us understand why it is the case. 16.2.3.1 “Multiply in” new information First consider, what if we had another draw from the bag, how would this adjust the ‘number of paths’ for each world? Remember, each draw is independent (replacement). We simply record the number of ways (permutations) that could lead to this draw in each world, and we multiply the previous count by this number. You can consider this visually in seeing how ‘adding an additional fork at the end of each path’ changes the count. This is given in the table below: t &lt;- t %&gt;% rename(`previous counts` = `ways to produce`, `ways to produce` = `draw 1: blue`) %&gt;% select(p_1:p_4, `ways to produce`, `previous counts`) %&gt;% mutate(`new count` = `ways to produce` * `previous counts`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 ways to produce previous counts new count w w w w 0 0 0 b w w w 1 3 3 b b w w 2 8 16 b b b w 3 9 27 b b b b 4 0 0 How to incorporate prior information about the probability of each world? Suppose your friend in the factory tells you (reliably) that “we produce 3 bags with (just) 1 blue for every 2 bags with equal counts, for every 1 bag with 3 blues.” We can think of the ‘factory choosing which bag to produce’ as another draw, thus another path. Here the sequence in which the information is recieved shouldn’t matter. The draws are independent (we presume). We can thus multiply the number of paths for each marbles-in-bag world by the (relative) frequency with which the factory ‘draws’ that bag… as shown below: t &lt;- t %&gt;% select(p_1:p_4, `new count`) %&gt;% rename(`prior count` = `new count`) %&gt;% mutate(`factory count` = c(0, 3:0)) %&gt;% mutate(`new count` = `prior count` * `factory count`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 prior count factory count new count w w w w 0 0 0 b w w w 3 3 9 b b w w 16 2 32 b b b w 27 1 27 b b b b 0 0 0 16.2.4 From counts to probability. 16.3 Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep” Content from notes from this lecture 16.3.1 Why and when use Bayesian (MCMC) methods? 16.3.1.1 Pros No need for asymptotics … good when sample sizes are small Incorporate previous information You can consider the ‘robustness to other priors’ Fit complex nonstandard models … e.g., with difficult functional forms or likelihood settings (more computation, less thinking) Easy to make predictions (e.g., simulate scenarios) after estimation Incorporate evidence, results, expert judgement (‘restrictions’ with some lee-way?) (ISn’t this the same as number 2?) Cleaner treatment/imputation of missing values … these are just parameters 16.3.1.2 Cons Must specify prior distributions … allows subjective judgement Different way of thinking about stats and inference; probability distributions and simulations, not much about p-values, point estimates and standard errors … path dependence Computational cost This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 16.3.1.3 Why more popular today? Starting from around 2005 in Political Science and Sociology Computational revolution comes from Markov chain Monte Carlo (MCMC) methods … don’t need analytical solutions Software implementations – many in R, specialised software like EWinBugs, JAGS, STAN; also increasingly in Stata 16.3.2 Theory Bayes theorem … inverting conditional probability thing … ‘inversion’ to make inferences about the parameters In Bayesian stats the parameters (and sometimes missing values) are random variables, we make probability statements about them \\[P(A|B)=P(B|A)P(A)/P(B)\\] Frequentist: Point estimates, unknown fixed parameters, data from a hyol repeataable random sample Bayesian: Fixed data (from the experiment), parameters are random variables … results based on probability distributions about rthese Classical statistics: likelihood of data given parameter: \\(p(y|\\theta)\\) Bayes we want, \\(p(\\theta|y) = p(y|\\theta)p(\\theta)/p(y)\\) \\(p(y)\\) is a ‘constant’ in our estimation … the data is fixed. So it’s proportional to \\(p(\\theta|y) = p(y|\\theta)\\times p(\\theta)\\) \\(p(y|\\theta)\\) is what we max when we do ML $ p()$: prior distribution capturing beliefs about \\(\\theta\\) 16.3.2.1 So how do we estimate it? Specify a probability model, a distribution for Y (likelihood function) and the priors for \\(\\theta\\) Solve (find) the posterior distribution \\(p(\\theta|Y)\\) and summarise the parameters of interest In practice, step 2 is usually done via MCMC simulation rather than analytically. … via simulations, I approach the ‘true’ value on \\(\\theta\\) (Given ‘regularity conditions’) 16.3.2.2 Linear regression model example \\[Y = x&#39;\\beta+\\epsilon\\] with n obs only random term is epsilon … natural candidate is a normal distribution, so \\(Y \\sim N(x&#39;\\beta,\\sigma^2_e)\\) So we want to find \\(p(\\beta, \\sigma^2_\\epsilon|Y,X)\\). This depends on the choices of \\(p(\\beta)\\) and \\(p(\\epsilon)\\). Could choose conjugate priors, leading to a particular joint posterior, you can solve it analytically. Can yield a joint posterior. Instead, let’s assume that the latter (variance) parameter is known, you can show that the posterior for \\(\\beta\\) is also normally distributed. (Conjugate) Similarly, if we assume \\(\\beta\\) is known, if the variance term had an inverse gamma distribution (prior), so will the posterior. In these conjugate priors, the posterior mean will be a weighted average of the priors and the data. 16.3.2.3 Gibbs Needs closed form conditional posterior for every parameter. What Gibbs sampler does is break the parameter space into sets of parameters Choose starting values, \\(\\theta^0_1,...\\theta^0_k\\) sample from the first parameter’s distribution given the others … the second one, … the k’th one . Repeat step 2 … thousands of times (starting with the parameters from the previous iteration) Eventually ‘we obtain samples of \\(p(\\theta|y)\\)’ But if we don’t have a closed form, we cannot simply sample from known distributions in each step E.g., in case of Logit distribution. 16.3.2.4 Metropolis Hastings Choose ‘proposal distribution’ to sample parameter values (a candidate like normal, uniform) Start w a prelim guess for parameter values \\(\\theta_0\\) At iteration t sample a proposal \\(\\theta_t\\) from \\(p(\\theta_t|\\theta_{t-1})\\) ?? what does this come from? If \\(p(\\theta_t|y)&gt;p(\\theta_{t-1}|y)\\) accept it as the new value of \\(\\theta\\). ??? how is this computed if we don’t have conjugate closed-form posteriors? Otherwise flip a coin with probability r = (ratio of those probabilities) if coin tosses heads, accept as new theta, otherwise stay at previous theta allows algorithm to avoid getting stuck at local maxima Commonly used proposal: random walk sample: \\(\\theta_t=\\theta_{t-1}+z_t\\), \\(z_t \\sim f\\) ?? I do this because there is no analytical way to derive this, unlike in the conjugate case, where we might use the Gibbs can combine Gibbs with Metropolis steps; relevant to some problems 16.3.2.5 Assessing convergence previous … ‘eyeballing’ formal: single-chain tests (Geweke/Heidel) … is the last part of the chain stable (stationary)… compare simulation at middle and end, is there much variation? multiple-chain test… (starting from different values), do they end similar … Gelman-Rubin diagnosting \\(\\hat{R}\\) typically either a very long chain and use GH convergence, or multiple shorter chains and use \\(\\hat{R}\\) Gabriel: Gelman-Rubin is probably preferred; more conservative ?? What am I iterating towards? Converging on what? 16.3.2.6 Assesing ‘fit’ in Bayesian No r-squared Typical measure is ‘posterior predictive comparisons’ \\(p(y_{replicated}|y_{observed}= ...\\) Simulate data from estimated parameters Compare to observed data Use an overall fit measure to assess model fit E.g., percent correct predictions (binary), whether the true data is within the 95% CI of the replicates, deviance For each replicate Choose statistic D, compare the replicated \\(D(y^s_{replicated})\\) against \\(D(y^s_{observed})\\) Quantify the discrepancy … percent of correct predictions, proportion of times replicated y is below true y … compute ‘bayesian p-value’s’ Systematic differences between replicate and actual data indicate model limitations (?? what are reasonable values here??) 16.3.3 Comparing models … Equivalent of ‘likelihood’ ‘Deviance Information Criterion’ (most used); specific for MCMC simulations: compares expected LL of the model (of the data given the estimated parameters; average here across much of the later points in the chain) against the llhd at the posterior parameter mean. Always select model with lowest DIC. Bayes Factor (less used): Ratio of llhd of the models; higher BF means model is more supported; BF&gt;10 seen to provide strong evidence for model w higher value 16.3.4 On choosing priors Most social scientists use non-informative or vague priors; i.e., large variance… e.g., \\(\\beta \\sim N(0,1000)\\) But its often useful to incorporate information into your priors Small pilot to test, \\(\\rightarrow\\) data \\(Y_1\\), another study gives data \\(Y_2\\); repeated application of Bayes theorem gives the posterior. Same result whether you obtained these together, or whether you did one and then updated (e.g., via an MCMC, starting with the first one as a prior) Conjugate priors (mentioned before) Jeffrey’s priors (??) 16.3.5 Implementation If you don’t need to do fancy things, and don’t want to (?) generate the full posterior distribution (or something) Some Stata/R commands that make Bayesian look frequentist. In Jags and Winbugs, we only have to specify the prior… rest is done for us Jags is great … you only need to do self-coding with lots of data and super complicated models as it can freeze up We went through it the fancy way in Probit.R Then the easy way with ‘script probit Jags.R’ 16.3.6 Generate predictions from a WinBUGS model You can just generate these outcomes … Prediction: generate a new observation #note, he is doing one per iteration, but since these are convergent it would be basically the same if you just chose a random iteration and did all the draws from that one 16.3.7 Missing data case One solution – multiple imputation choose imputation model to predict missings, generate many copies of orig data set, imputing missibg value for each 2 more steps here Need a model for X|alpha, because missing variables are random variables 16.3.8 Stata Has some rather simple implementations; e.g., just using commands like bayes: regress y x 16.3.9 R mcmc pac Also simple code; great for standard use Speedup with parallelization; see “script for parallel probit.R” and “parallelprobit.R” More advanced: C++; can integrate it with Rcpp, or even use Exeter’s ISCA cluster summary(cars) ## speed dist ## Min. : 4.0 Min. : 2 ## 1st Qu.:12.0 1st Qu.: 26 ## Median :15.0 Median : 36 ## Mean :15.4 Mean : 43 ## 3rd Qu.:19.0 3rd Qu.: 56 ## Max. :25.0 Max. :120 16.4 Other resources and notes to integrate Hey stats twitter: got a very sharp psych UG student wanting to dive into Bayes. Many resources are too technical (i.e., not good teaching texts for UG level, but useful references). Where should I point her? — Tom Carpenter ((???)) February 1, 2020 "],
["references.html", "17 List of references", " 17 List of references Angrist J. D., and J S Pischke. 2008. “Mostly Harmless Econometrics : An Empiricist ’ S Companion.” Massachusettts I Nstitute of Technology and the London School of Economics, no. March: 290. https://doi.org/10.1017/CBO9781107415324.004. Gentzkow, Matthew. 2013. “Code and Data for the Social Sciences : A Practitioner ’ S Guide.” Heckman, James, Rodrigo Pinto, and James Heckman. 2013. “Econometric Mediation Analyses : Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs,” no. 7552. Kennedy, Peter. 2003. A Guide to Econometrics. MIT press. Tibshirani, Robert. n.d. “Statistical Learning with Sparsity the Lasso and Generalizations.” Wooldridge, Jeffrey M. 2002. Econometric Analysis of Cross Section and Panel Data. 2. The MIT press. https://doi.org/10.1515/humr.2003.021. Wooldridge, J M. 2008. Introductory Econometrics: A Modern Approach. South-Western Pub. "]
]
