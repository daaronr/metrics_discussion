[
["introduction.html", "Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus 1 Introduction 1.1 (Conceptual: approaches to statistics/inference and causality)[#conceptual] 1.2 Getting, cleaning and using data; project management and coding 1.3 Basic regression and statistical inference: Common mistakes and issues 1.4 LDV and discrete choice modeling 1.5 Robustness and diagnostics, with integrity 1.6 Control strategies and prediction; Machine Learning approaches 1.7 IV and its many issues 1.8 Other paths to observational identification 1.9 Causal pathways: Mediation modeling and its massive limitations 1.10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 1.11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 1.12 (Experimental) Study design: Background and quantitative issues 1.13 (Experimental) Study design: (Ex-ante) Power calculations 1.14 [‘Experimetrics’ and measurement of treatment effects from RCTs] (#experimetrics_te) 1.15 Making inferences from previous work; Meta-analysis, combining studies 1.16 The Bayesian approach 1.17 Some key resources and references", " Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus Dr. David Reinstein, 2020-05-22 Abstract This ‘book’ organizes my notes and helps others understand it and learn from it 1 Introduction My goal in putting this resource is to focus on the practical tools I use and the challenges I (David Reinstein) face. But I am open to collaboration with others on this. My focus: Microeconomics, behavioral economics, focus on charitable giving and ‘returns to education’ type of straightforward problems. (Minimal focus on structural approaches.) What I care about: Where we can add value to real econometric (and statistical and experimental design) practice? The data I focus on: Observational (esp. web-scraped and API data and national surveys/admin data) Experimental: esp. where with multiple crossed arms, and where the ‘cleanest design’ may not be possible I will assume familiarity with most basic statistical concepts like ‘bias’, ‘consistency’, and ‘null hypothesis testing.’ However, I will focus on some concepts that seem to often be misunderstood and mis-applied. If you are involved with this project, you can find a brief guide (somewhat WIP) on how to add content (HERE)[https://daaronr.github.io/ea_giving_barriers/bookdown-appendix.html]. This is from a different project but the setup is basically the same. A note to potential research assistants and student collaborators (unfold): 1.1 (Conceptual: approaches to statistics/inference and causality)[#conceptual] Bayesian vs. frequentist approaches Folder: bayesian Notes: bayes_notes 1.1.1 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 1.1.1.1 DAGs and Potential outcomes 1.1.2 Theory, restrictions, and ‘structural vs reduced form’ 1.2 Getting, cleaning and using data; project management and coding This will build on my content here, and integrate with it. 1.2.1 Data: What/why/where/how 1.2.2 Organizing a project 1.2.3 Dynamic documents (esp Rmd/bookdown) 1.2.4 Good coding practices 1.2.4.1 New tools and approaches to data (esp ‘tidyverse’) 1.2.4.2 Style and consistency Indenting, snake-case, etc 1.2.4.3 Using functions, variable lists, etc., for clean, concise, readable code 1.2.5 Data sharing and integrity 1.3 Basic regression and statistical inference: Common mistakes and issues 1.3.1 “Bad control” (“colliders”) Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita) 1.3.2 Choices of lhs and rhs variables Missing data Choice of control variables and interactions Which outcome variable/variables 1.3.3 Functional form Logs and exponentials Nonlinear modeling (and interpreting coefficients) 1.3.3.1 ‘Testing for nonlinear terms’ Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18 http://datacolada.org/62 1.3.4 OLS and heterogeneity OLS does not identify the ATE http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT Modeling heterogeneity: the limits of Quantile re regression 1.3.5 “Null effects” “While the classical statistical framework is not terribly clear about when one should”“accept”\" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a \"“zero or minimal effect”\" one would want to find these closely bounded around zero under a variety of conditions for generalizability. In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks”.\" Harms-lakens-18 1.3.5.1 Confidence intervals and Bayesian credible intervals 1.3.5.2 Comparing relative parameters E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men”. This doesn’t cut it: we need to see a statistical test for the difference in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing). 1.3.6 Multiple hypothesis testing (MHT) See (???) 1.3.7 Interaction terms and pitfalls 1.3.7.1 ‘Moderators’ Confusion with nonlinearity Moderators: Heterogeneity mixed with nonlinearity/corners In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in’. Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative 1.3.7.2 MHT 1.3.8 Choice of test statistics (including nonparametric) 1.3.9 How to display and write about regression results and tests 1.3.10 Bayesian interpretations of results (see ‘the Bayesian Approach’) 1.4 LDV and discrete choice modeling 1.5 Robustness and diagnostics, with integrity 1.5.1 (How) can diagnostic tests make sense? Where is the burden of proof? 1.5.2 Estimating standard errors 1.5.3 Sensitivity analysis: Interactive presentation 1.6 Control strategies and prediction; Machine Learning approaches 1.6.1 Machine Learning (statistical learning): Lasso, Ridge, and more 1.6.2 Limitations to inference from learning approaches 1.7 IV and its many issues 1.7.1 Instrument validity Exogeneity vs. exclusion Very hard to ‘powerfully test’ 1.7.2 Heterogeneity and LATE Basic consideration: what does IV identify and when:? Focusing on a binary endogenous ‘treatment’ variable With heterogeneity With imperfect compliance With one-way compliance 1.7.3 Weak instruments, other issues With a ‘weak instrument’ … why does that matter? 1.7.4 Reference to the use of IV in experiments/mediation 1.8 Other paths to observational identification 1.8.1 Fixed effects and differencing 1.8.2 DiD FE/DiD does not rule out a correlated dynamic unobservable, causing a bias 1.8.3 RD 1.8.4 Time-series-ish panel approaches to micro 1.8.4.1 Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ 1.9 Causal pathways: Mediation modeling and its massive limitations An applied review 1.10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 1.10.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ “Conditional on positive”/“intensive margin” analysis ignores selection “Conditional on positive”/“intensive margin” analysis ignores selection identification issue See Angrist and Pischke on “Good CoP, bad CoP”. See also bounding approaches such as (???) AngristJ.D.2008a, 1.10.1.1 Bounding approaches (Lee, Manski, etc) See Notes on Lee bounds 1.11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 1.11.1 Why run an experiment or study? Sugden and Sitzia critique here, give more motivation 1.11.2 Causal channels and identification Ruling out alternative hypotheses, etc 1.11.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 1.11.4 Generalizability (and heterogeneity) 1.12 (Experimental) Study design: Background and quantitative issues 1.12.1 Pre-registration and Pre-analysis plans 1.12.1.1 The hazards of specification-searching 1.12.2 Sequential and adaptive designs Needs to adjust significance tests for augmenting data/sequential analysis/peeking Statistics/econometrics new-statistics sagarin_2014 http://www.paugmented.com/ resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/ Yet … P_augmented may overstate type-1 error rate Statistics/econometrics response to referees, new-statistics \" A process involving stopping \"“whenever the nominal \\(p.0.5\\)”\" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5%. Even if the subsequent data suggested a \"“one in a million chance of arising under the null”\" the overall process yields a 5%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the \"“likelihood of the null hypothesis”\" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate. Considering the calculations in , it is clear that \\(p_{augmented}\\) should the type-1 error of the process if there is a positive probability that after an initial experiment attains p\\(&lt;0.05\\), more data is collected. A headline \\(p&lt;0.05\\) does imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded \\(p&gt;0.05\\), thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though \\(p&lt;0.05\\). Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine’s experiment). Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p&lt;0.05, more data is collected, implying that \\(p_{augmented}\\) as calculated above overstates the type I error rate that would arise from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below \\(p=0.05\\), in order to attain a type-1 error rate of 5% or less in our published corpus.\" 1.12.3 Efficient assignment of treatments (Links back to power analyses) 1.13 (Experimental) Study design: (Ex-ante) Power calculations 1.13.1 What sort of ‘power calculations’ make sense, and what is the point? 1.13.1.1 The ‘harm to science’ from running underpowered studies \"One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results. Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.\" verkaik2016, metzger2015 1.13.2 Power calculations without real data 1.13.3 Power calculations using prior data 1.14 [‘Experimetrics’ and measurement of treatment effects from RCTs] (#experimetrics_te) 1.14.1 Which error structure? Random effects? 1.14.2 Randomization inference? 1.14.3 Parametric and nonparametric tests of simple hypotheses 1.14.4 Adjustments for exogenous (but non-random) treatment assignment 1.14.5 IV in an experimental context to get at ‘mediators’? 1.14.6 Heterogeneity in an experimental context 1.15 Making inferences from previous work; Meta-analysis, combining studies 1.15.1 Publication bias 1.15.2 Combining a few (your own) studies/estimates 1.15.3 Full meta-analyses Models to address publication biases 1.16 The Bayesian approach 1.17 Some key resources and references (Angrist J. D. and Pischke 2008) ‘The Mixtape’ (Cunningham) (Kennedy 2003) (Tibshirani, n.d.) OSF guides Christensen ea “Transparent and Reproducable Social Science Research” (Gentzkow 2013; Wooldridge 2002, 2008) An Introduction to Statistical Learning with Applications in R R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org Statistical Rethinking: A Bayesian Course with Examples in R and Stan 1.17.1 Consider: Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017 Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction . Cambridge University Press, 2015 Judea Pearl Imbens: Potential Outcomes versus DAGs References "],
["conceptual.html", "2 Conceptual: approaches to statistics/inference and causality 2.1 Bayesian vs. frequentist approaches 2.2 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 2.3 Theory, restrictions, and ‘structural vs reduced form’", " 2 Conceptual: approaches to statistics/inference and causality 2.1 Bayesian vs. frequentist approaches Folder: bayesian Notes: bayes_notes 2.1.1 Interpretation of CI’s (aside) The fact that 95% of all (correct) CIs contain the true value does not mean that 95% of those that exclude zero do so correctly. You could have (say) 59% correct coverage for 10% excluding zero and 99% for 90% including zero. — Stephen John Senn ((???)) January 21, 2020 2.2 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 2.2.1 DAGs and Potential outcomes 2.3 Theory, restrictions, and ‘structural vs reduced form’ "],
["data-sci.html", "3 Getting, cleaning and using data 3.1 Data: What/why/where/how 3.2 Organizing a project 3.3 Dynamic documents (esp Rmd/bookdown) 3.4 Project management tools, esp. Git/Github 3.5 Good coding practices 3.6 Additional tips (integrate)", " 3 Getting, cleaning and using data This will build on my content here, and integrate with it. Some key resources are in a continually updated airtable HERE See especially: R for data science Advanced R bookdown: Authoring Books and Technical Documents with R Markdown: [OSF: ‘PhD Toolkit on Transparent, Open, Reproducible Research’ https://osf.io/g8yjz/](OSF: ‘PhD Toolkit on Transparent, Open, Reproducible Research’ https://osf.io/g8yjz/) Happy Git and GitHub for the useR “Data science for business” “Code and Data for the Social Sciences” (Gentzkow/Shapiro) 3.1 Data: What/why/where/how 3.2 Organizing a project 3.3 Dynamic documents (esp Rmd/bookdown) Some guidelines from a particular project: Appendix: Tech for creating, editing and collaborating on this ‘Bookdown’ web book/project (and starting your own) 3.3.1 Managing references/citations A letter to my co-authors… Hi all. Hope you are doing well. I’ve just invited you to a shared Zotero group managing my bibliography/references. I think this should be useful. (I prefer Zotero to Mendeley because it’s open source and… I forgot the other reason.) On my computer it synchronizes with a .bib (bibtex) file in a dropbox folder … For latex files we just refer to this as normal. In the Rmd files/bookdown (producing output like EA barriers or Metrics notes (present book) this is referenced in the YAML header to the index.Rmd file as bibliography: [reinstein_bibtex.bib] Then, to keep this file, I have a “download block” included in that same file (the first line with ‘dropbox’ is the key one). The download code follows (remove the ‘eval=FALSE’ to get it to actually run)… tryCatch( #trycatch lets us &#39;try&#39; to execute and if there is an error, it does the thing *after* the braces, rather than crashing { download.file(url = &quot;https://www.dropbox.com/s/3i8bjrgo8u08v5w/reinstein_bibtex.bib?raw=1&quot;, des tfile = &quot;reinstein_bibtex.bib&quot;) #download the bibtex database download.file(url = &quot;https://raw.githubusercontent.com/daaronr/dr-rstuff/master/bookdown_template/support/tufte_plus.css&quot;, destfile = here(&quot;support&quot;, &quot;tufte_plus.css&quot;)) #this downloads the style file }, error = function(e) { print(&quot;you are not online, so we can&#39;t download&quot;) } ) A fairly comprehensive discussion of tools for citation in R-markdown: A Roundup of R Tools for Handling BibTeX 3.4 Project management tools, esp. Git/Github (More to be added/linked here) See ‘Git and GitHub’ here… watch this space For students and research assistants, I've been sending first time git users/developers to this:https://t.co/P6KQpXHCWI+ https://t.co/q4R4Ei5Biw — Nathan Lane ((???)) September 14, 2019 3.5 Good coding practices 3.5.1 New tools and approaches to data (esp ‘tidyverse’) From Kurtz: If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in section 5.6.1 from Grolemund and Wickham’s R for Data Science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2. 3.5.2 Style and consistency lower_snake_case Use lower_snake_case to name all objects (that’s my preference anyways) unless there’s a strong reason to do otherwise. This includes: file_names.txt folder_names function_names (with few exceptions) names_of_data_objects_like_vectors names_of_data_output_objects_like_correlation_coefficients ex_df1 In R you probably should keep data frame names short to avoid excessive typing And by all that is holy, never put spaces or slashes in file or object names! This can make it very hard to process across systems… there are various ways of referring to spaces and other white space. 3.5.2.1 Indenting and spacing 3.5.3 Using functions, variable lists, etc., for clean, concise, readable code 3.5.4 Mapping over lists to produce results 3.5.5 Building results based on ‘lists of filters’ of the data set We can store a filter as a character vector and then apply it selection_statement &lt;- &quot;Species == &#39;setosa&#39;&quot; iris %&gt;% filter(rlang::eval_tidy(rlang::parse_expr(selection_statement))) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3&nbsp;&nbsp; 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5&nbsp;&nbsp; 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5&nbsp;&nbsp; 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3&nbsp;&nbsp; 1.4 0.1 setosa 4.3 3&nbsp;&nbsp; 1.1 0.1 setosa 5.8 4&nbsp;&nbsp; 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 5.4 3.4 1.7 0.2 setosa 5.1 3.7 1.5 0.4 setosa 4.6 3.6 1&nbsp;&nbsp; 0.2 setosa 5.1 3.3 1.7 0.5 setosa 4.8 3.4 1.9 0.2 setosa 5&nbsp;&nbsp; 3&nbsp;&nbsp; 1.6 0.2 setosa 5&nbsp;&nbsp; 3.4 1.6 0.4 setosa 5.2 3.5 1.5 0.2 setosa 5.2 3.4 1.4 0.2 setosa 4.7 3.2 1.6 0.2 setosa 4.8 3.1 1.6 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 5.5 4.2 1.4 0.2 setosa 4.9 3.1 1.5 0.2 setosa 5&nbsp;&nbsp; 3.2 1.2 0.2 setosa 5.5 3.5 1.3 0.2 setosa 4.9 3.6 1.4 0.1 setosa 4.4 3&nbsp;&nbsp; 1.3 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5&nbsp;&nbsp; 3.5 1.3 0.3 setosa 4.5 2.3 1.3 0.3 setosa 4.4 3.2 1.3 0.2 setosa 5&nbsp;&nbsp; 3.5 1.6 0.6 setosa 5.1 3.8 1.9 0.4 setosa 4.8 3&nbsp;&nbsp; 1.4 0.3 setosa 5.1 3.8 1.6 0.2 setosa 4.6 3.2 1.4 0.2 setosa 5.3 3.7 1.5 0.2 setosa 5&nbsp;&nbsp; 3.3 1.4 0.2 setosa Making this a function for later use: filter_parse = function(df, x) { {{df}} %&gt;% filter(rlang::eval_tidy(rlang::parse_expr({{x}}))) } iris %&gt;% filter_parse(selection_statement) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3&nbsp;&nbsp; 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5&nbsp;&nbsp; 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5&nbsp;&nbsp; 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3&nbsp;&nbsp; 1.4 0.1 setosa 4.3 3&nbsp;&nbsp; 1.1 0.1 setosa 5.8 4&nbsp;&nbsp; 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 5.4 3.4 1.7 0.2 setosa 5.1 3.7 1.5 0.4 setosa 4.6 3.6 1&nbsp;&nbsp; 0.2 setosa 5.1 3.3 1.7 0.5 setosa 4.8 3.4 1.9 0.2 setosa 5&nbsp;&nbsp; 3&nbsp;&nbsp; 1.6 0.2 setosa 5&nbsp;&nbsp; 3.4 1.6 0.4 setosa 5.2 3.5 1.5 0.2 setosa 5.2 3.4 1.4 0.2 setosa 4.7 3.2 1.6 0.2 setosa 4.8 3.1 1.6 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 5.5 4.2 1.4 0.2 setosa 4.9 3.1 1.5 0.2 setosa 5&nbsp;&nbsp; 3.2 1.2 0.2 setosa 5.5 3.5 1.3 0.2 setosa 4.9 3.6 1.4 0.1 setosa 4.4 3&nbsp;&nbsp; 1.3 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5&nbsp;&nbsp; 3.5 1.3 0.3 setosa 4.5 2.3 1.3 0.3 setosa 4.4 3.2 1.3 0.2 setosa 5&nbsp;&nbsp; 3.5 1.6 0.6 setosa 5.1 3.8 1.9 0.4 setosa 4.8 3&nbsp;&nbsp; 1.4 0.3 setosa 5.1 3.8 1.6 0.2 setosa 4.6 3.2 1.4 0.2 setosa 5.3 3.7 1.5 0.2 setosa 5&nbsp;&nbsp; 3.3 1.4 0.2 setosa selection_statement &lt;- &quot;Species == &#39;setosa&#39;&quot; We can do the same for a list of character vectors of filter statements, and apply each filter from the list to the dataframe, and then the output function: sel_st &lt;- c(&quot;Species == &#39;setosa&#39;&quot;, &quot;Species == &#39;virginica&#39;&quot;) map(iris, selection_statement) ## $Sepal.Length ## NULL ## ## $Sepal.Width ## NULL ## ## $Petal.Length ## NULL ## ## $Petal.Width ## NULL ## ## $Species ## NULL sel_st %&gt;% map(~ filter_parse(iris, .x)) ## [[1]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa ## ## [[2]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.3 3.3 6.0 2.5 virginica ## 2 5.8 2.7 5.1 1.9 virginica ## 3 7.1 3.0 5.9 2.1 virginica ## 4 6.3 2.9 5.6 1.8 virginica ## 5 6.5 3.0 5.8 2.2 virginica ## 6 7.6 3.0 6.6 2.1 virginica ## 7 4.9 2.5 4.5 1.7 virginica ## 8 7.3 2.9 6.3 1.8 virginica ## 9 6.7 2.5 5.8 1.8 virginica ## 10 7.2 3.6 6.1 2.5 virginica ## 11 6.5 3.2 5.1 2.0 virginica ## 12 6.4 2.7 5.3 1.9 virginica ## 13 6.8 3.0 5.5 2.1 virginica ## 14 5.7 2.5 5.0 2.0 virginica ## 15 5.8 2.8 5.1 2.4 virginica ## 16 6.4 3.2 5.3 2.3 virginica ## 17 6.5 3.0 5.5 1.8 virginica ## 18 7.7 3.8 6.7 2.2 virginica ## 19 7.7 2.6 6.9 2.3 virginica ## 20 6.0 2.2 5.0 1.5 virginica ## 21 6.9 3.2 5.7 2.3 virginica ## 22 5.6 2.8 4.9 2.0 virginica ## 23 7.7 2.8 6.7 2.0 virginica ## 24 6.3 2.7 4.9 1.8 virginica ## 25 6.7 3.3 5.7 2.1 virginica ## 26 7.2 3.2 6.0 1.8 virginica ## 27 6.2 2.8 4.8 1.8 virginica ## 28 6.1 3.0 4.9 1.8 virginica ## 29 6.4 2.8 5.6 2.1 virginica ## 30 7.2 3.0 5.8 1.6 virginica ## 31 7.4 2.8 6.1 1.9 virginica ## 32 7.9 3.8 6.4 2.0 virginica ## 33 6.4 2.8 5.6 2.2 virginica ## 34 6.3 2.8 5.1 1.5 virginica ## 35 6.1 2.6 5.6 1.4 virginica ## 36 7.7 3.0 6.1 2.3 virginica ## 37 6.3 3.4 5.6 2.4 virginica ## 38 6.4 3.1 5.5 1.8 virginica ## 39 6.0 3.0 4.8 1.8 virginica ## 40 6.9 3.1 5.4 2.1 virginica ## 41 6.7 3.1 5.6 2.4 virginica ## 42 6.9 3.1 5.1 2.3 virginica ## 43 5.8 2.7 5.1 1.9 virginica ## 44 6.8 3.2 5.9 2.3 virginica ## 45 6.7 3.3 5.7 2.5 virginica ## 46 6.7 3.0 5.2 2.3 virginica ## 47 6.3 2.5 5.0 1.9 virginica ## 48 6.5 3.0 5.2 2.0 virginica ## 49 6.2 3.4 5.4 2.3 virginica ## 50 5.9 3.0 5.1 1.8 virginica It works nicely if you have a list of filters aligned with a list of names or other objects ‘specific to each filter’ (Code below: adapt to public data and explain) bal_sx &lt;- map2(subsets_sx_dur, subsets_sx_dur_name, function(x, y) { filter_parse(sa, x) %&gt;% dplyr::filter(stage == &quot;2&quot;) %&gt;% tabyl(treat_1, treat_2, show_missing_levels = FALSE) %&gt;% adornme_not(cap = paste(y, &quot;; 1st and 2nd stage treatments&quot;)) } ) 3.5.6 Coding style and indenting in Stata (one approach) I indent every line except clear, import, save, merge (‘file operations’) except where these occur as part of a loop, in which case I put in an ‘important comment’ noting these operations lines that call other do files important comments/flags/to-do’s I only put ‘small todo’ elements having to do with code in a code file itself (and even then there may be better places). If we are going to put todos I suggest we include #todo to search for these later (and R has a utility to collect these in a nice way… maybe Stata does too. Whenever there are more than 20 lines of something prosaic that cannot/has not been put into a loop or function, I suggest we put it in a separate ‘do’ file and call that do file (with no indent). That’s what I do here, giving a brief description and a ‘back link’. Sometimes I put all those do files into an separate folder. 3.6 Additional tips (integrate) When you have to upgrade R on Mac, how to preserve package installations - twitter thread Are you teaching/learning #r #rstats &amp; want to teach/learn the latest #tidyverse #tidyr tools? e.g. bind_rows, pivot_wider/pivot_longer, the join family (full_join, inner_join…) Check out my slides on “Advanced data manipulation” here 😺https://t.co/dr9VNx7MFf — Amy Willis ((???)) November 16, 2019 “This paper does a thorough job setting out the rationale, design, and implementation of the workflowr package” says (???) (???) in his review of this #softwaretool article introducing workflowr by (???) and co-authors https://t.co/ZXmQkDhFuD #OpenScience pic.twitter.com/e77SVo8PhO — F1000Research ((???)) November 17, 2019 "],
["reg-follies.html", "4 Basic regression and statistical inference: Common mistakes and issues 4.1 Basic regression and statistical inference: Common mistakes and issues briefly listed", " 4 Basic regression and statistical inference: Common mistakes and issues 4.1 Basic regression and statistical inference: Common mistakes and issues briefly listed Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Identification Random effects estimators show a lack of robustness Specification Clustering SE is more standard practice OLS/IV estimators not ‘mean effect’ in presence of heterogeneity Power calculations/underpowered Selection bias due to attrition Selection bias due to missing variables – impute these as a solution Signs of p-hacking and specification-hunting Weak diagnostic/identification tests Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness -(Some notes on multi-level modeling) and RE linked in this Twitter thread With heterogeneity the simple OLS estimator is not the ‘mean effect’ P_augmented may overstate type-1 error rate Impact size from regression of “log 1+gift amount” Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Weak IV bias Bias from selecting instruments and estimating using the same data 4.1.1 Bad control From MHE: some variables are bad controls and should not be included in a regression model even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notational experiment at hand. That is, bad controls might just as well be dependent variables too.\" – They could also be interpreted as endogenous variables. Example of looking at a regression of wages in schooling, controlling for college degree completion: Once we acknowledge the fact that college affects occupation, comparison of wages by college degree status within occupation are no longer apples to apples, even if college degree completion is randomly assigned.\" – The question here was whether to control for the category of occupation, not the college degree. It is also incorrect to say that the conditional comparison captures the part of the effect of college that is ‘not explained by occupation’ … so we would do better to control only for variables that are not themselves caused by education.\" 4.1.2 “Bad control” (“colliders”) Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita) 4.1.3 Choices of lhs and rhs variables Missing data Choice of control variables and interactions Which outcome variable/variables 4.1.4 Functional form Logs and exponentials Nonlinear modeling (and interpreting coefficients) 4.1.4.1 ‘Testing for nonlinear terms’ Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18 http://datacolada.org/62 4.1.5 OLS and heterogeneity 4.1.5.1 OLS does not identify the ATE (-) In general, with heterogeneity, OLS does not identify the ATE. It weights observations from different parts of the sample differently. Parts with greater residual variation in the treatment (outcome) variable are more (less) heavily weighted. E.g., if the treatment is binary, the estimator will most heavily weight those parts of the sample where the probability of treatment is closest to 1/2. The formula is … Some intuition Why is this the case? The OLS type estimators we are taught in Econometrics are ‘BLUE’ under the assumption of a single homogenous ‘effect’ (the ‘slope’… although the discussion itself is often agnostic as to whether this represents a causal effect). It is ‘best’ in a minimizing MSE sense under certain assumptions; in particular, we must also know the true functional form and the set of variables to be included. See ‘overfitting’ issues. In order to have the estimate of the true slope that minimizes the squared errors, OLS (and related estimators like FGLS; as well as 2SLS in a more complicated sense) weights some observations more than others. The ‘influence’ of an observation on the estimated slope depends on the nature of the variation in the dependent and independent variables in the region that observation is drawn from. Think of drawing a line through a set of points that were drawn with some noise from the true distribution. If you drew it based on a bunch of points (from a region where) the treatment varies very little and the outcomes have a lot of noise, the line you draw will be very sensitive to the latter noise and thus unreliable. So, would optimally ‘down-weight’ these observations in drawing the line. However, if the actual slope varies by region, this also means you are under-representing certain regions, and thus getting a biased estimate of the average slope. How can we deal with this? If we think that the treatment effect varies with observable variables, we could include ‘interactions’; essentially making separate estimates of the slope for each share of the population (but potentially allowing other control variables to have a homogenous effects, and pooled or clustered estimation of underlying variance.) …Although we may want to consider both overfitting here and the idea that there may be some shared component, so the fully-interacted model may be sub-optimal. See mixed modeling (?) However, this does not tell us how to recover the average of these slopes (approximately, the ATE). Should we weight each of the slopes by the share of the population that this group represents? Mechanically, the standard way of estimating and representing these interactions and economics has been with simple dummies (0,1) for each compared group. This yields a ‘base group’ (e.g., males aged 35-60) – this obviously does not recover the average slope– as well as the ‘adjustment’ coefficients. Another way of expressing interactions, particularly helpful with multi-level interactions is called ‘effect coding’: each group is coded as a ‘difference from 0’ (e.g,. males are -1/2 and females +1/2), before doing the interactions. This could allow for a more straightforward interpretation: at each level, the uninteracted term represents the average treatment effects, and the interacted terms represent adjustments relative to this average. But under which conditions is this in fact the case? [insert here]. WB blog - your-go-to regression-specification is -biased-here-s-simple-way-fix-it A key paper: http://www.jcsuarez.com/Files/Suarez_Serrato-BFE.pdf In particular, we compare treatment effect estimates using a fixed effects estimator (FE) to the average treatment effect (ATE) by replicating eight influential papers from the American Economic Review published between 2004 and 2009.1 Using these examples, we consider a randomized experiment in Section 1 as a case study and, in Section 3, we show generally that heterogeneous treatment effects are common and that the FE and ATE are often different in statistically and economically significant degrees. In all but one paper, there is at least one statistically significant source of treatment effect heterogeneity. In five papers, this heterogeneity induces the ATE to be statistically different from the FE estimate at the 5% level (7 of 8 are statistically different at the 10% level). Five of these differences are economically significant, which we define as an absolute difference exceeding 10%. Based upon these results, we conclude that methods that consistently estimate the ATE offer more interpretable results than standard FE models By “FE” here I think they mean group dummies; they are focused on cross-sectional and not panel data! While fixed effects permit different mean outcomesamong groups, the estimates of treatment effects are typically required to be the same; in more colloquial terms, the intercepts of the conditional expectation functions may differ, but not the slopes DGP \\[y_i = x_i \\beta_{g(i)} + \\mathbf{z_i}&#39; \\gamma + \\epsilon_i\\] where $y_i is the outcome for observation i among N [N what?], \\(x_i\\) is treatment or another variable of interest, and \\(z_i\\) contains control variables, including group-specific fixed effects. The treatment effects aregroup-specific for each of the \\(g=1,...,G\\) groups, where group membership is known for each observation. Defining ATE \\[\\beta^{ATE}=\\sum_g \\pi_g \\beta_g \\] where the \\(\\pi\\) terms are population frequencies The use of interaction terms is delicate… Modeling heterogeneity: the limits of Quantile re regression 4.1.6 “Null effects” “While the classical statistical framework is not terribly clear about when one should”“accept”\" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a \"“zero or minimal effect”\" one would want to find these closely bounded around zero under a variety of conditions for generalizability. In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks”.\" Harms-lakens-18 4.1.6.1 Confidence intervals and Bayesian credible intervals 4.1.6.2 Comparing relative parameters E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men”. This doesn’t cut it: we need to see a statistical test for the difference in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing). 4.1.7 Multiple hypothesis testing (MHT) See (???) 4.1.8 Interaction terms and pitfalls See also ‘effect coding’ 4.1.8.1 ‘Moderators’ Confusion with nonlinearity Moderators: Heterogeneity mixed with nonlinearity/corners In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in’. Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative 4.1.8.2 MHT 4.1.9 Choice of test statistics (including nonparametric) (Or get to this in the experimetrics section) 4.1.10 How to display and write about regression results and tests 4.1.11 Bayesian interpretations of results "],
["robust-diag.html", "5 Robustness and diagnostics, with integrity; Open Science resources 5.1 (How) can diagnostic tests make sense? Where is the burden of proof? 5.2 Estimating standard errors 5.3 Sensitivity analysis: Interactive presentation 5.4 Supplement: open science resources, tools and considerations 5.5 Diagnosing p-hacking (see also meta-analysis)", " 5 Robustness and diagnostics, with integrity; Open Science resources 5.1 (How) can diagnostic tests make sense? Where is the burden of proof? Where a particular assumption is critical to identification and inference …Failure to reject the violation of an assumptionis not sufficient to give us confidence that it is satisfied and the results are credible. At several points the authors cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles). I am concerned with the interpretation of diagnostic testing, both in model selection, and in the defense of the exclusion restrictions or identification assumptions. It is problematic, when the basic consistency of the estimator (or a main finding of the paper) critically depends on such tests failing to reject a null hypothesis, to merely state that the ‘test failed to reject, therefore we maintain the null hypothesis’. How powerful are these tests? I.e. what is the probability of a false negative Type II error? How large a bias would be compatible with reasonable confidence intervals for these tests? 5.2 Estimating standard errors 5.3 Sensitivity analysis: Interactive presentation 5.4 Supplement: open science resources, tools and considerations I'm in psychology research (just finished PhD) and I want to get into #OpenScience to make sure I'm following best practices. But this is something that wasn't explicitly taught to me. What are some good resources? Thanks! (???) (???) (???) (???) — Alessa Teunisse ((???)) April 21, 2020 A couple of months ago I made a guide on how to use Binder to make our #RStats code #reproducible. E.g., Binder will make your code runnable using the versions of R and r-packages used when you analyzed your data. https://t.co/srYNazwy0q #reproducibility #openscience pic.twitter.com/gcTlVFpaY5 — Erik Marsja ((???)) December 17, 2019 5.5 Diagnosing p-hacking (see also meta-analysis) Ever wonder “Were those results p-hacked?” Brodeur et al. propose a useful new check (“speccheck” on Stata. R/etc. coming soon). #ASSA2020 pic.twitter.com/NCZ1jZTaO5 — Eva Vivalt ((???)) January 4, 2020 "],
["control-ml.html", "6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches 6.1 Machine Learning (statistical learning): Lasso, Ridge, and more 6.2 Notes Hastie: Statistical Learning with Sparsity 6.3 Notes: Mullainathan", " 6 Control strategies and prediction, Machine Learning (Statistical Learning) approaches ‘Identification’ of causal effects with a control strategy not credible Essentially a ‘control strategy’ is “control for all or most of the reasonable determinants of the independent variable so as to make the remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest”. All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., (???). 6.1 Machine Learning (statistical learning): Lasso, Ridge, and more 6.1.1 Limitations to inference from learning approaches 6.2 Notes Hastie: Statistical Learning with Sparsity google books link 6.2.1 Introduction One form of simplicity is sparsity, the central theme of this book. Loosely speaking, a sparse statistical model is one in which only a relatively small number of parameters (or predictors) play an important role. “the \\(\\ell_1\\) norm is special” (abs value). Other norms yield nonconvex problems, hard to minimize. “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems. Examples from gene mapping 6.2.1.1 Book roadmap Chapter 2 … lasso for linear regression, and a simple coordinate descent algorithm for its computation. Chapter 3 application of \\(\\ell_1\\) [lasso-type] penalties to generalized linear models such as multinomial and survival models, as well as support vector machines. [?] Chapter 4: Generalized penalties such as the elastic net and group lasso are discussed in Chapter 4. Chapter 5: numerical methods for optimization (skip for now] Chapter 6: statistical inference for fitted (lasso) models, including the bootstrap, Bayesian methods and more recent stuff Chapter 7: Sparse matrix decomposition [?] (Skip?) Ch 8: sparse multivariate analysis of that (Skip?) Ch 9: Graphical models and their selection (Skip?) Ch 10: compressed sensing (Skip?) Ch 11: a survey of theoretical results for the lasso (Skip?) 6.2.2 Ch2: Lasso for linear models N samples (?N observations), want to approx the response variable using a linear combination of the predoctors OLS minimizes squared-error loss but Prediction accuracy OLS unbiased but ‘often has large variance’ prediction accuracy can be improved by shrinking coefficients (even to zero) yielding biased but perhaps better predictive estimators Interpretation: too many predictors hard to interpret DR: I do not care about this for fitting background noise in experiments 6.2.2.1 2.2 The Lasso Estimator Lasso bounds the sum of the abs values of coefficients, an \"$_1\" constraint. Lasso is OLS subject to \\(\\sum_{j=1..p}{\\abs(\\beta_j)}\\leq t\\) “compactly” \\(||\\beta||_1\\leq t\\) with notation for the “\\(\\ell_1\\) norm” Bound \\(t\\) acts as a ‘budget’, must be specified by an ‘external procedure’ such as cross-validation typically we must standardize the predictors $** so that each column is centered with unit variance … as well as the outcome variables (?) … can ignore intercept DR: Not clear here whether standardisation is necessary for the procedure to be valid or just convenient for explaining and deriving its properties. Aside: Can re-write Lasso minimization st constraint as a Lagrangian. \\(\\lambda\\) plays the same role as \\(t\\) in the constraint. Thus we can speak of the solution to the Lagrangian minimisation problem \\(\\hat{\\beta)_{\\lambda}\\) which also solves the bound problem with \\(t=||\\hat_{\\lambda}||_1\\). Aside: We often remove the \\(1/2n\\) term at the beginning of the minimization problem. Same minimization, minimizing sum of squared deviations rather than something like an average of this. Express (Karush-Kuhn-Tucker) optimisation conditions for this … Example from Thomas (1990) on crime data Typically … lasso is most useful for much larger problems, including “wide” data for which \\(p&gt;&gt;N\\) Fig 2.1: Lasso vs ridge regression; coefficients of each for a set of considered variables plotted against their respective norms (as shares of maximal bound on coefficient sum measure, i.e., ols, for each) Note ridge regression penalises squared sums of betas Fig 2.2., in \\(\\beta_1,\\beta_2\\) space illustrates the difference well: contour lines of Resid SS elipses, ‘budget constraint’ for each (disc vs diamond) (Note: lasso bound was chosen via cross-validation) No analytical statistical inference after lasso (some being developed?), bootstrap is common lasso sets two of the five coefficients to zero, and tends to shrink the coefficients of the others toward zero relative to the full least-squares estimate. DR: analytically and intuitively, I do not yet understand why lasso should shrink coefficients but not all the way to zero. The penalty is linear in the coefficient size, so I would think the solution would be bang-bang, either drop a coeficient or leave it unchanged. But it is not. Adding an increment to a \\(\\hat{\\beta}\\) when it is below the OLS estimate should have a linear effect on the RSS (according to my memory and according to Sebastian). But that would mean that shrinking one parameter always yields a better benefit to cost ratio. Thus I should shrink each parameter to zero before beginning to shrink any others. This cannot be right! I looked up this derivative wrt the beta vector (one needs to set this to 0 to get the ols estimates stackexchange \\(\\frac{d RSS}{d \\beta}=-2X^{T}(y-X\\beta}\\) or \\(−\\frac{d e&#39;e}{d b}=2X′y+2X′Xb\\) The answer to this question: while the impact of changing each coefficient on SSR is in fact constant (a constant own-derivative), there is also an impact of changing one coefficient on the other derivatives. As one coefficient shrinks to zero the marginal impact of the other coefficients on the SSR may (will?) increase. - At the same time, we need that the effect of increasing it from zero need not be infinite, so it might not outweigh the linear penalty, thus some coefficients might be set to zero Relaxed lasso the least-squares fit on the subset of the three predictors tends to expand the lasso estimates away from zero. The nonzero estimates from the lasso tend to be biased toward zero, so the debiasing in the right panel can often improve the prediction error of the model. This two-stage process is also known as the relaxed lasso (Meinshausen 2007). DR: When is this likely to help/hurt relative to pure lasso? Stackexchange discussion Contrasts a ‘relaxed-lasso’ from a ‘lars-ols’ Aside: which seems better for Control variable selection for prediction/reducing noise to enable better inference of treatment effects? Ridge? better than Lasso here? We do not care about interpreting the predictors here… so if we allow \\(\\beta\\)‘s to be shrunk towards zero for each coefficient maybe that should yield better prediction than making them exactly zero? On the other hand if we know the true model is ‘parsimonious’ (as in the genes problem) it might boost efficiency to allow inference about coefficients that should be exactly zero (edited) 6.2.2.2 2.3 Cross-Validation and Inference Generalization ability accuracy for predicting independent test data from the same population … find the value of t that does best **Cross-validation procedure* randomly divide … dataset into K groups. “Typical choices … might be 5 or 10, and sometimes N.” One ‘test’, remaining K-1 ‘training’ Apply lasso to training data for a range of t values, use each fitted model to predict the responses in the test set, recording mean-squared prediction errors for each value of t. Repeat the previous step K times each time, one of the K groups is the test data, remaining K − 1 are training data. yields K different estimates of the prediction error over a range of t values. Average K estimates of prediction error for each value of t \\(\\rightarrow\\) cross-validation error curve. Fig 2.3 plots an example with K=10 splits for cross validation … of the estimated MS prediction error vs the relative bound \\(\\tilde{t}\\)(summed absolute value of Lasso betas divided by summed abs value of OLS betas). Also draw dotted line at the 1-std-error rule choice of \\(\\tilde{t\\)} Number of nonzero coefficients plotted at top 6.2.2.3 2.4 Computation of the Lasso solution DR: I think I will skip this for now least angle/LARS is mentioned at the bottom as a ‘homotopy method’ which “produce the entire path of solutions in a sequential fashion, starting at zero” 6.2.2.4 2.5 Degrees of freedom … Jumping to 6.2.2.5 2.10 Some perspective Good properties of the Lasso (\\(\\ell_1\\) penalty) Natural interpretation (enforce sparsity and simplicity) Statistical efficiency … if the underlying true signal is sparse (but if it is not sparse “no method can do well relative to the Bayes error”) Computational efficiency, as \\(\\ell_1\\) penalties are convex 6.2.3 Chapter 3: Generalized linear models 6.2.4 Chapter 4: Generalizations of the Lasso penalty lasso does not handle highly correlated variables very well; the coefficient paths tend to be erratic and can sometimes show wild behavior. The elastic net makes a compromise between the ridge and the lasso penalties (Zou and Hastie 2005)1] is a parameter that can be varied. For an individual coefficient the penalty is \\(\\frac{1}{2} (1-\\alpha)\\beta_j^2 + \\alpha|\\beta_j|\\) (a convex combo of the lasso and ridge penalties) multiplied by a ‘regularization weight’ \\(\\lambda&gt;0\\) which plays the same role (I think) as in lasso elastic net is also strictly convex 6.3 Notes: Mullainathan The fundamental insight behind these breakthroughs is as much statistical as computational. Machine intelligence became possible once researchers stopped approaching intelligence tasks procedurally and began tackling them empirically. Face recognition algorithms, for example, do not consist of hard-wired rules to scan for certain pixel combinations, based on human understanding of what constitutes a face. Instead, these algorithms use a large dataset of photos labeled as having a face or not to estimate a function f (x) that predicts the presence y of a face from pixels x (p2) &gt; supervised- machine learning, the focus of this article) revolves around the problem of prediction: produce predictions of y from x … manages to fit complex and very flexible functional forms to the data without simply overfitting; it finds functions that work well out-of-sample danger in using these tools is taking an algorithm built for [predicting \\(y\\)-] and presuming their [parameters \\(\\beta\\)] - have the properties we typically associate with estimation output One category of such applications appears when using new kinds of data for traditional questions; for example, in measuring economic activity using satellite images or in classifying industries using corporate 10-K filings. Making sense of complex data such as images and text often involves a prediction pre-processing step This middle category is most relevant for me In another category of applications, the key object of interest is actually a parameter … but the inference procedures (often implicitly) contain a prediction task. For example, the first stage of a linear instrumental variables regression is effectively prediction. The same is true when estimating heterogeneous treatment effects, testing for effects on multiple outcomes in experiments, and flexibly controlling for observed confounders A final category is in direct policy applications. Deciding which teacher to hire implicitly involves a prediction task (what added value will a given teacher have?), one that is intimately tied to the causal question of the value of an additional teacher. (p3) A useful (interactive?) example: We consider 10,000 randomly selected owner-occupied units from the 2011 metropolitan sample of the American Housing Survey. In addition to the values of each unit, we also include 150 variables that contain information about the unit and its location, such as the number of rooms, the base area, and the census region within the United States. To compare different prediction techniques, we evaluate how well each approach predicts (log) unit value on a separate hold-out set of 41,808 units from the same sample. All details on the sample and our empirical exercise can be found in an online appendix available with this paper at http://e-jep.org In-sample performance may overstate performance; this is especially true for certain machine learning algorithms like random forests that have a strong tendency to overfit. Second, on out-of-sample performance, machine learning algorithms such as random forests can do significantly better than ordinary least squares, even at moderate sample sizes and with a limited number of covariates (p4) algorithms are fitted on the same, randomly drawn training sample of 10,000 units and evaluated on the 41,808 remaining held-out units. Simply including all pairwise interactions would be infeasible as it produces more regressors than data points (especially considering that some variables are categorical Machine learning searches for these interactions automatically (p5) Shallow Regression Tree Predicting House Values not sure what’s going on here. is this the random forest thing? The prediction function takes the form of a tree that splits in two at every node. At each node of the tree, the value of a single variable (say, number of bathrooms) determines whether the left (less than two bathrooms) or the right (two or more) child node is considered next. When a terminal node-a leaf—is reached, a prediction is returned. An So how does machine learning manage to do out-of-sample prediction? The first part of the solution is regularization. In the tree case, instead of choosing the -best” overall tree, we could choose the best tree among those of a certain depth. (p5) Tree depth is an example of a regularizer. It measures the complexity of a function. As we regularize less, we do a better job at approximating the in-sample variation, but for the same reason, the wedge between in-sample and out-of-sample (p6) how do we choose the level of regularization (-tune the algorithm”)? This is the second key insight: empirical tuning. (p6) -tuning within the training sample In empirical tuning, we create an out-of-sample experiment inside the original sample. We fit on one part of the data and ask which level of regularization leads to the best performance on the other part of the data.4 We can increase the efficiency of this procedure through cross-validation: we randomly partition the sample into equally sized subsamples (-folds”). The estimation process then involves successively holding out one of the folds for evaluation while fitting the prediction function for a range of regularization parameters on all remaining folds. Finally, we pick the parameter with the best estimated average performance.5 The (p6) -! This procedure works because prediction quality is observable: both predictions y- and outcomes y are observed. Contrast this with parameter estimation, where typically we must rely on assumptions about the data-generating process to ensure consistency (p7) Some Machine Learning Algorithms Function class - (and its parametrization) Regularizer R( f ) Global/parametric predictors Linear -′x (and generalizations) Subset selection| (p7) -very useful table Some Machine Learning Algorithms Function class - (and its parametrization) Regularizer R( f ) Global/parametric predictors Linear -′x (and generalizations) Subset selection||β| (p7) Random forest (linear combination of trees (p7) -kernel in an ml framework! Kernel regression (p6) -but can we make inferences about the structure? hypothesis testing? Regularization combines with the observability of prediction quality to allow us to fit flexible functional forms and still find generalizable structure. (p7) Picking the prediction function then involves two steps: The first step is, conditional on a level of complexity, to pick the best in-sample loss-minimizing function.8 The second step is to estimate the optimal level of complexity using empirical tuning (as we saw in cross-validating the depth of the tree). (p8) -but they forgot to mention that others are shrunk linear regression in which only a small number of predictors from all possible variables are chosen to have nonzero values: the absolute-value regularizer encourages a coefficient vector where many are exactly zero. (p4) -why no ridge or elastic net? LASSO (p8) -ensembles usually win contests While it may be unsurprising that such ensembles perform well on average- after all, they can cover a wider array of functional forms-it may be more surprising that they come on top in virtually every prediction competition (p8) -neural nets broadly explained neural nets are popular prediction algorithms for image recognition tasks. For one standard implementation in binary prediction, the underlying function class is that of nested logistic regressions: The final prediction is a logistic transformation of a linear combination of variables (-neurons”) that are themselves such logistic transformations, creating a layered hierarchy of logit regressions. The complexity of these functions is controlled by the number of layers, the number of neurons per layer, and their connectivity (that is, how many variables from one level enter each logistic regression on the next) (p9) These choices about how to represent the features will interact with the regularizer and function class: A linear model can reproduce the log base area per room from log base area and log room number easily, while a regression tree would require many splits to do so. (p9) In a traditional estimator, replacing one set of variables by a set of transformed variables from which it could be reconstructed would not change the predictions, because the set of functions being chosen from has not changed. But with regularization, including these variables can improve predictions because-at any given level of regularization-the set of functions might change (p9) -!! Economic theory and content expertise play a crucial role in guiding where the algorithm looks for structure first. This is the sense in which -simply throw it all in- is an unreasonable way to understand or run these machine learning algorithms (p9) -I need hear of using adjusted r square for this Should out-ofsample performance be estimated using some known correction for overfitting (such as an adjusted R2 when it is available) or using cross-validation (p9) -big unknowns available finite-sample guidance on its implementation-such as heuristics for the number of folds (usually five to ten) or the -one standard-error rule” for tuning the LASSO (Hastie, Tibshirani, and Friedman 2009)-has a more ad-hoc flavor (p9) firewall principle: none of the data involved in fitting the prediction function-which includes crossvalidation to tune the algorithm—is used to evaluate the prediction function that is produced (p10) -how? First, econometrics can guide design choices, such as the number of folds or the function class (p10) with the fitted function. Why not also use it to learn something about the -underlying model (p10) -!! the lack of standard errors on the coefficients. Even when machine-learning predictors produce familiar output like linear functions, forming these standard errors can be more complicated than seems at first glance as they would have to account for the model selection itself. In fact, Leeb and P-tscher (2006, 2008) develop conditions under which it is impossible to obtain (uniformly) consistent estimates of the distribution of model parameters after data-driven selection (p11) -lasso chosen variables are unstable because of multicollinearity. a problem for making inferences from estimated coefficients the variables are correlated with each other (say the number of rooms of a house and its square-footage), then such variables are substitutes in predicting house prices. Similar predictions can be produced using very different variables. Which variables are actually chosen depends on the specific finite sample (p11) this creates an Achilles- heel: more functions mean a greater chance that two functions with very different (p12) coefficients can produce similar prediction quality (p12) In econometric terms, while the lack of standard errors illustrates the limitations to making inference after model selection, the challenge here is (uniform) model selection consistency itself (p12) -is this equally a problem for non sparsity based procedures like ridge? First, it encourages the choice of less complex, but wrong models. Even if the best model uses interactions of number of bathrooms with number of rooms, regularization may lead to a choice of a simpler (but worse) model that uses only number of fireplaces. Second, it can bring with it a cousin of omitted variable bias, where we are typically concerned with correlations between observed variables and unobserved ones. Here, when regularization excludes some variables, even a correlation between observed variables and other observed (but excluded) ones can create bias in the estimated coefficients (p12) Some econometric results also show the converse: when there is structure, it will be recovered at least asymptotically (for example, for prediction consistency of LASSO-type estimators in an approximately sparse linear framework, see Belloni, Chernozhukov, and Hansen 2011). (p12) -unrealistic for micro economic applications Zhao and Yu (2006) who establish asymptotic model-selection consistency for the LASSO. Besides assuming that the true model is -sparse”—only a few variables are relevant-they also require the “irrepresentable condition” between observables: loosely put, none of the irrelevant covariates can be even moderately related to the set of relevant ones. In practice, these assumptions are strong. (p13) Machine learning can deal with unconventional data that is too high-dimensional for standard estimation methods, including image and language information that we conventionally had not even thought of as data we can work with, let alone include in a regression (p13) satellite data (p13) they provide us with a large x vector of image-based data; these images are then matched (in what we hope is a representative sample) to yield data which form the y variable. This translation of satellite images to yield measures is a prediction problem (p13) particularly relevant where reliable data on economic outcomes are missing, such as in tracking and targeting poverty in developing countries (Blumenstock 2016 (p13) cell-phone data to measure wealth (p13) Google Street View to measure block-level income in New York City and Boston (p13) online posts can be made meaningful by labeling them with machine learning (p14) extract similarity of firms from their 10-K business description texts, generating new time-varying industry classifications for these firms (p14) and imputing even in traditional datasets. In this vein, Feigenbaum (2015a, b) applies machine-learning classifiers to match individuals in historical records (p13) -the first prediction applications New Data (p14) Prediction in the Service of Estimation (p14) linear instrumental variables understood as a two-stage procedure (p14) The first stage is typically handled as an estimation step. But this is effectively a prediction task: only the predictions x- enter the second stage; the coefficients in the first stage are merely a means to these fitted values. Understood this way, the finite-sample biases in instrumental variables are a consequence of overfitting (p14) -ll overfitting. Overfitting means that the in-sample fitted values x- pick up not only the signal -′z, but also the noise δ. As a consequence, xˆ is biased towards x, and the second-stage instrumental variable estimate - - is thus biased towards the ordinary least squares estimate of y on x. Since overfit will be larger when sample size is low, the number of instruments is high, or the instruments are weak, we can see why biases arise in these cases (p14) same techniques applied here result in split-sample instrumental variables (Angrist and Krueger 1995) and -jackknife” instrumental variables (Angrist, Imbens, and Krueger 1999) (p15) -worth referencing In particular, a set of papers has already introduced regularization into the first stage in a high-dimensional setting, including the LASSO (Belloni, Chen, Chernozhukov, and Hansen 2012) and ridge regression (Carrasco 2012; Hansen and Kozbur 2014). More recent extensions include nonlinear functional forms, all the way to neural nets (Hartford, Leyton-Brown, and Taddy 2016 (p15) Practically, even when there appears to be only a few instruments, the problem is effectively high-dimensional because there are many degrees of freedom in how instruments are actually constructed (p15) -a note of caution It allows us to let the data explicitly pick effective specifications, and thus allows us to recover more of the variation and construct stronger instruments, provided that predictions are constructed and used in a way that preserves the exclusion restriction (p15) -this seems similar to my idea of regularising on a subset Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016) take care of high-dimensional controls in treatment effect estimation by solving two simultaneous prediction problems, one in the outcome and one in the treatment equation (p15) the problem of verifying balance between treatment and control groups (such as when there is attrition (p15) -! Or consider the seemingly different problem of testing for effects on many outcomes. Both can be viewed as prediction problems (Ludwig, Mullainathan, and Spiess 2017). If treatment assignment can be predicted better than chance from pretreatment covariates, this is a sign of imbalance. If treatment assignment can be predicted from a set of outcomes, the treatment must have had an effect (p15) prediction task of mapping unit-level attributes to individual effect estimates (p15) Athey and Imbens (2016) use sample-splitting to obtain valid (conditional) inference on (p16) treatment effects that are estimated using decision trees, (p16) -look into the implication for treatment assignment with heterogeneity heterogenous treatment effects can be used to assign treatments; Misra and Dub- (2016) illustrate this on the problem of price targeting, applying Bayesian regularized methods to a large-scale experiment where prices were randomly assigned (p16) -caveat Suppose the algorithm chooses a tree that splits on education but not on age. Conditional on this tree, the estimated coefficients are consistent. But that does not imply that treatment effects do not also vary by age, as education may well covary with age; on other draws of the data, in fact, the same procedure could have chosen a tree that split on age instead (p16) Prediction in Policy (p16) -no .. can we predict who will gain most from admission? but even if we can what can we report? Prediction in Policy "],
["iv-and-its-many-issues-1.html", "7 IV and its many issues 7.1 Instrument validity 7.2 Heterogeneity and LATE 7.3 Weak instruments, other issues 7.4 Reference to the use of IV in experiments/mediation", " 7 IV and its many issues 7.1 Instrument validity Exogeneity vs. exclusion Very hard to ‘powerfully test’ IV not credible? Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a direct effect on the outcome (only an indirect effect through the endogenous variable 7.2 Heterogeneity and LATE Basic consideration: what does IV identify and when:? Focusing on a binary endogenous ‘treatment’ variable With heterogeneity With imperfect compliance With one-way compliance 7.3 Weak instruments, other issues With a ‘weak instrument’ … why does that matter? 7.4 Reference to the use of IV in experiments/mediation "],
["other-paths-to-observational-identification-1.html", "8 Other paths to observational identification 8.1 Fixed effects and differencing 8.2 DiD 8.3 RD 8.4 Time-series-ish panel approaches to micro", " 8 Other paths to observational identification 8.1 Fixed effects and differencing 8.2 DiD FE/DiD does not rule out a correlated dynamic unobservable, causing a bias 8.3 RD 8.4 Time-series-ish panel approaches to micro 8.4.1 Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ "],
["mediators.html", "9 Causal pathways - mediators 9.1 Mediators (and selection and Roy models): a review, considering two research applications 9.2 DR initial thoughts (for NL education paper) 9.3 Econometric Mediation Analyses (Heckman and Pinto) 9.4 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity 9.5 Antonakis approaches", " 9 Causal pathways - mediators 9.1 Mediators (and selection and Roy models): a review, considering two research applications Originally focused on issues relevant to Parey et al project on ‘returns to HE institution’ using data from the Netherlands (flagged as @NL); also relevant to Reinstein et al work on substitution in charitable giving (flagged as @subst). 9.2 DR initial thoughts (for NL education paper) Here were my initial thoughts as pertaining to our paper on the returns to university. Suppose we observe treatment \\(T\\) (e.g., ‘allowed to enter first-choice institution and course’), intermediate outcome \\(M\\) (e.g., completion of degree in first-choice course and institution), and final outcome \\(Y\\) (e.g., lifetime income.) Alternately, in the “substitution between charities” (@subst) context… (unfold) The treatment \\(T\\) is ‘asked to donate in the first round’ (in Reinstein, Riener and Vance-McMullen, henceforth ‘RRV’ experiments, and perhaps in Schmitz 2019)’, a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?), the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others), the intermediate outcome \\(M\\) is the amount given to that (first-round) charity, and the final outcome \\(Y\\) is the amount given to that charity (or other charities) in round 2 (experiments “3”: other charities in that round ). The treatment \\(T\\) (may) directly affect the final outcome \\(Y\\). Do: show a diagram here \\[T\\rightarrow Y\\] \\(T\\) also may affect an intermediate outcome \\(M\\). \\[T \\rightarrow M\\] The intermediate outcome also may affect the final outcome \\(Y\\). \\[M \\rightarrow Y\\] With exogenous variation in \\(T\\) and \\(M\\) (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions. With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of \\(T\\) on \\(Y\\). We could also compute the share of this effect that occurs via the intermediate effect, i.e., \\(T \\rightarrow M\\rightarrow Y\\). This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients. However, there are two major challenges to this estimation. We (may) have a valid instrument for (exogenous variation in) \\(T\\) only, and \\(M\\) may arise through a process involving selection on unobserved variables. Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects. Thus we consult the relevant literature, discussed below. The most influential paper in Economics has been (Heckman, Pinto, and Heckman 2013). It is cited in more recent applied work such as Fagereng, 2018 (unfold). … We follow Heckman et al. (2013) and Heckman and Pinto (2015) in using mediation analysis. The goal of this analysis is to disentangle the average causal effect on outcomes that operate through two channels: a) Indirect effects arising from the effect of treatment on measured mediators, and b) direct effects that operate through channels other than changes in the measured mediators (including changes in mediators that are not observed by the analyst and changes in the mapping between mediators and outcomes). It is therefore necessary to assume that the mediators we do not observe are uncorrelated with both \\(\\mathbf{X}\\) and the measured mediators for all values of \\(D\\). Antonakis, coming from the Psychology and Leadership disciplines, considers the mediation question in a much simpler set of models. 9.3 Econometric Mediation Analyses (Heckman and Pinto) Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs Relevance to Parey et al We have an instrument for admission to one’s first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these ‘intermediate outcomes’, including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these. a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well 9.3.1 Summary and key modeling There is a ‘production function’ cf income as a function of human capital, opportunities, etc. cf donation as a function of income, prices, mood, framing, etc. Treatments (e.g., RCTs) may affect outcomes through the following channels: observable or proxied inputs Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood unobservable/unmeasured inputs cf human capital, social connections cf unobservable generosity, wealth, or temporary mood the production function itself, the ‘map between inputs and outputs for treatment group members’ Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital ‘matter more’ at some institutions? Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else? If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs. RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs ... [nor distinguish effects from changes in production functions]. Here “we can test some of the strong assumptions implicitly invoked”. “Direct effects” as commonly stated refer to the impact of both channels 2 and 3 above. DR: Channel 2 isn’t really a direct effect imho (what was this?) Standard potential outcomes framework: \\[Y=DY_{1}+(1-D)Y_{0}\\] \\[ATE=E(Y_{1}-Y_{0})\\] Production function \\[Y_{d}=f_{d}(\\mathbf{\\mathbf{{\\theta}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] ... the function under treatment \\(d\\); of proxied and unobserved inputs that occur under state \\(d\\), and baseline variables. The production function implies: \\[ATE=E\\Big(f_{1}(\\mathbf{\\mathbf{{\\theta}}}_{1}^{p},\\mathbf{\\mathbf{{\\theta}}}_{1}^{u},\\mathbf{{X}})-f_{0}(\\mathbf{\\mathbf{{\\theta}}}_{0}^{p},\\mathbf{\\mathbf{{\\theta}}}_{0}^{u},\\mathbf{{X}})\\Big)\\] We also consider counterfactual outputs, fixing treatment status and proxied inputs: \\[Y_{d,\\bar{\\theta_{d}}^{p}}=f_{d}(\\mathbf{\\mathbf{{\\bar{{\\theta}}}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] This allows us to decompose (‘as in the mediation literature’): \\[ATE(d)=IE(d)+DE(d)\\] IE, Indirect effect: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment statuses) DE, Direct effect: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed at one of the two treatment statuses) HP further decompose the direct effect into: \\(DE&#39;(d,d&#39;)\\): The impact of letting the treatment vary the map only (fixing the rest at one of the two appropriate values) \\(DE&#39;&#39;(d,d&#39;)\\): The impact of letting the treatment vary the unmeasured inputs only (fixing the rest at one of the two appropriate values) They use this to give two further ways of decomposing the ATE. 9.3.2 Common assumptions and their implications “The standard literature on mediation analysis in psychology regresses outputs on mediator inputs” ... often adopts the strong assumptions of: no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy) and Cf ‘winning institution’ impacts human capital, social networks, etc identically for everyone; e.g., not a greater effect for men then for women, nor a greater effect for those entering particular specializations. full invariance of the production function: \\(f_{1}=f_{0}\\). ... which implies \\(Y_{d}=f(\\mathbf{\\theta}_{d}^{p},d,\\mathbf{X})\\). Sequential ignorability (Imai et al, 10, ’11): Essentially, independent randomization of both treatment status and measured inputs. Cf ‘winning institution’ does not effect the specialization entered nor the location of residence, nor are both determined by a third factor. This sentence is hard to follow: In other words, input \\(\\theta_{d&#39;}^{p}\\) is statistically independent of potential outputs when treatment is fixed at \\(D=d\\) and measured inputs are fixed at \\(\\bar{\\theta_{d&#39;}^{p}}\\) conditional on treatment assignment \\(D\\) and same preprogram characteristics \\(X\\). This assumption yields the ‘mediation formulas’: \\begin{aligned} E(IE(d)|X)= &amp; E(Y|^{p}=t,D=d,X){{}} &amp; (9)\\ E(DE(d)|X)= &amp; {}expe_{} &amp; (10) \\end{aligned} (??F is presumably the distribution over the observables; where did the unobservables go? They are in the expectations, I guess.) Difference from RCT What RCT doesn’t do: [sequential ignorability] translates into ... no confounding effects on both treatments and measured inputs ... does not follow from a randomized assignment of treatment ...[which] ensures independence between treatment status and counterfactual inputs/outputs ... [but not] between proxied inputs \\(\\theta_{d}^{p}\\) and unmeasured inputs \\(\\theta_{d}^{u}\\). [Thus not between counterfactual outputs and measured inputs is assumed in condition (ii).] Cf, randomizing ‘win first-choice institution’ does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution. What RCT does do: RCT ensures “independence between treatment status and counterfactual inputs/outputs”, thus identifying ’treatment effects for proxied inputs and for outputs. CF, we can identify the impact of the treatment ‘win first chosen institution’ on proxied input like ‘enters a specialization’ and on outputs like ‘income in observed years.’ 9.4 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity Summary ... 4000+ families targeted, incentive to relocate from projects to better neighbourhoods. Easy to identify impact of vouchers Challenge (here) is to assess impact of neighborhoods on outcomes. Method here to decompose the TEOT into unambiguously interpreted effects. Method applicable to ‘unordered choice models with categorical instrumental variables and multiple treatments’ Finds significant causal effect on labour market outcomes Relevance to Parey et al We also have an instrument (DUO lottery numbers) cleanly identifying the effect of the ‘opportunity to do something’ (in our case, to enter the course at your preferred institution). However, we also want to measure the impact of choices ‘encouraged’ by the instrument, such as (i) attending the first choice course and institution and (ii) completing this course. We also deal with unordered choices (i. enter course and institution, enter course at other institution, enter other course at institution, enter neither) (ii. choice of medical specialisation) The geographic outcome is relevant to our second paper (impact on ‘lives close to home’) Introduction The causal link between neighborhood characteristics and resident’s outcomes has seldom been assessed. Treatments: Control (no voucher) Experimental: could use voucher to lease in low-poverty neighborhood Section 8: Could use voucher in any () neighborhood Many papers evaluate the ITT or TOT effects of MTO. ITT: effect of being offered voucher estimated as difference in average outcome of experimental vs control families TOT: effect for ‘voucher compliers’ (assuming no effect of simply being offered voucher on those who don’t use it) estimated as ITT/compliance rate [ITT and TOT] are the most useful parameters to investigate the effects of offering [EA] rent subsidising vouchers to families. Identification strategy brief Vouchers as IVs for choice among 3 neighborhood alternatives (no relocation, relocate bad, relocate good) Cf @NL: enter course and fp-institution, enter course at other institution, do not enter course Neighborhood causal effects as difference in counterfactual outcomes among 3 categories Challenge: “MTO vouchers are insufficient to identify the expected outcomes for all possible counterfactual relocation decisions” ... “compliance with the terms of the program was highly selective [Clampet-Lundquist and M, 08]” Solution: Uses theory and ‘tools of causal inference. Invokes SARP to identify ’set of counterfactual relocation choices that are economically justifiable’ Identifying assumption: “the overall quality of the neighborhood is not directly caused by the unobserved family variables even though neighborhood quality correlates with these unobserved family variables due to network sorting” ‘Partition sample ... into unobserved subsets associated with economically justified counterfactual relocation choices and estimate the causal effect of neighborhood relocation conditioned on these partition sets.’ [what does this mean?] Results in brief “Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes ... 65% higher than the TOT effect for adult earnings.” Framework: first for binary/binary (simplification) First, for binary outcomes (simplified) \\(Z_{\\omega}\\): whether family \\(\\omega\\) receives a voucher (cf institution-winning lottery number) \\(T_{\\omega}\\): whether family \\(\\omega\\) relocates (cf enters first choice institution and course) Counterfactuals \\(T_{\\omega}(z)\\): relocation decision \\(\\omega\\) would choose if it had been assigned voucher \\(z\\in{0,1}\\)’: vector of potential relocation decisions (cf education choices) for each voucher assignment (cf lottery number) Can partition into never-takers, compliers, always takers, and defiers \\((Y_{\\omega}(0);Y_{\\omega}(1\\))): (Potential counterfactual) outcomes (cf income, residence, etc) when relocation decision is fixed at 0 and 1, respectively Key ( standard) identification assumption: instrument independent of counterfactual variables \\[(Y_{\\omega}(0),Y_{\\omega}(1),T_{\\omega}(0),T_{\\omega}(1))\\perp Z_{\\omega}\\] Standard result 1: ITT \\[\\begin{aligned} ITT=E(Y_{\\omega}|Z_{\\omega}=1)-(Y_{\\omega}|Z_{\\omega}=0)\\\\ =E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)P(S_{\\omega}=[0,1])+E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[1,0]&#39;)P(S_{\\omega}=[0,1])\\end{aligned}\\] i.e., ITT computation yields the sum of the ‘causal effect for compliers’ and the ’causal effect for defiers, weighted by the probability of each. Standard result 2: LATE \\[\\begin{aligned} LATE=\\frac{{ITT}}{P(T_{\\omega}=1|Z_{\\omega}=1)-P(T_{\\omega}=1|Z_{\\omega}=0)}= &amp; &amp; E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)\\\\ if\\:P(S_{\\omega}=[0,1])=0\\end{aligned}\\] i.e., the LATE, computed as the ITT divided by the ‘first stage’ impact of the instrument, is the causal effect for compliers if there are no defiers. Framework for MTO multiple treatment groups, multiple choices \\(Z_{\\omega}\\in\\{z_{1,}z_{2,}z_{3}\\}\\) for no voucher, experimental voucher, and section 8 voucher, respectively \\(T_{\\omega}\\in\\{1,2,3\\}\\) ... no relocation, low poverty neighborhood relocation, high poverty relocation \\(T_{\\omega}(z)\\): relocation decision for family \\(\\omega\\) if assigned voucher \\(z\\) \\(\\rightarrow\\) Response type for each family \\(\\omega\\) is a three-dimensional vector: \\[S_{\\omega}=[T_{\\omega}(z_{1}),T_{\\omega}(z_{2}),T_{\\omega}(z_{3})]\\]. \\(\\rightarrow\\) ITT computation now measures a weighted sum of effects across a subset of those response types whose responses vary between the assignments being compared. Cf: Considering the ‘treatments’: ‘1: enter other course at fp-inst, ’2: enter course at fp-inst’, ‘3: enter course at non-fp inst’ (I ignore other course at other institution for now) Looking among those who won the course lottery (so we have a binary instrument: wininst \\(Z_{\\omega}\\in{0,1\\}}\\) Our reduced-form estimates (regressions on the ‘lottery number wins institution’ dummy) measures the probablility-weighted sum of: impact of institution within course ($T_{}=$2 versus 3); for those who would ‘fully comply’ (enter course at institution if \\(Z_{\\omega}=1\\), enter course at other institution if 0) impact of the course at fp-institution versus second-best course at fp-institution for ‘institution-loving’ noncompliers; those who would enter the course only if they get the fp-institution and otherwise another course at the same institution effects for perverse defiers 9.5 Antonakis approaches Insert notes here References "],
["selection-cop.html", "10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 10.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ 10.2 Bounding approaches (Lee, Manski, etc)", " 10 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 10.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ “Conditional on positive”/“intensive margin” analysis ignores selection “Conditional on positive”/“intensive margin” analysis ignores selection identification issue See Angrist and Pischke on “Good CoP, bad CoP”. See also bounding approaches such as (???) AngristJ.D.2008a, 10.2 Bounding approaches (Lee, Manski, etc) See Notes on Lee bounds 10.2.1 Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD Notes David Reinstein 10.2.1.0.1 Introduction even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research Intuitive trimming procedure for bounding average treatment effects in the presence of sample selection… Requires neither exclusion restrictions nor a bounded support for the outcome of interest.\" (Also) applicable to “nonrandom sample selection/attrition”, as well as to the ‘conditional on positive’/hurdle/mediation effect discussed here analyses and evaluations typically focus on \"reduced form impacts on total earnings, a first-order issue for cost-benefit analysis. Unfortunately, exclusively studying the effect on total earnings leaves open the question of whether any earnings gains are achieved through raising individuals hypothesis wage rates (price affects or hours of work (quantity effects). Important methodological point to constantly bring up: “even a randomized experiment cannot guarantee the treatment and control individuals will be comparable conditional on being employed.” Claims that standard “parametric or semi-parametric methods for correcting sample selection require exclusion restrictions that have little justification in this case.” Notes that most of the baseline variables could affect employment probabilities or have a direct impact on wage rates. Summary of the method: “…amounts to first identifying the excess number of individuals who were induced to be selected (employed) because of the treatment and then trimming the upper and lower tails of the outcome… distribution by this number, yielding a worst-case scenario bound.” Uses same assumptions as in “conventional models for sample selection” regressor of interest is independent of the errors in the outcome and selection models selection equations – this is ensured by random assignment. “the selection equation can be written as a standard latent variable binary response model” – what meaningful restriction does this impose? He proves this procedure “yields the tightest bounds for the average treatment effect that are consistent with the observed data.” The bounds estimator is shown to be \\(\\sqrt(n)\\) consistent and asymptotically normal with an intuitive expression for its asymptotic variance which depends on the variance of the trimmed outcome and the trimming threshold, an estimated quantifiable; (and an added term accounting for the estimation of which quantile to trim on) Note for charity experiment (unfold) (@subst) – DR, Note, charity data: We can make confidence statements over the bounds themselves. Will this procedure be easy to bring into our code? – In our (charity) experiment we in fact do have upper bounds on the outcome variable. Could this yield even greater efficiency? Note for the Netherlands data: (unfold, @NL) it is not immediately clear how this could be adapted to instrumental variables; we shall see. Can we recover something meaningful from the reduced form model they are? Can it be applied to the (instrumental variables) estimates to disentangle the impact of changing courses from the impact of the institution itself? In Lee’s paper, the estimate seems to give very narrow and informative bounds even though they have a great many people who do not earn any wages as a share of the population, about 54%. These are much narrower than the bounds proposed by Horowitz and M then what those bounds produce. &lt;!- ask (???) whether his Horowitz/Manski estimator incorporated the natural bounds on the outcome. –&gt; 10.2.1.0.2 The National Job Corps Study and Sample Selection [prior approaches] In the experiment discussed here those in the control group were embargoed from the program for three years but could join afterwards, thus “when I use the phrase ‘effect of the program’ I am referring to this reduced-form treatment effect”, i.e., the intent to treat effect. – “some subpopulations were randomized into the program group with differing, but known probabilities. Thus analyzing the data requires the use of design weights.” Note: (\\(???)) this bears some resemblance to our Dutch data situation, and we can probably use examples from analyses of these programs. We can check their code against ours. – Note also that they impute means of the baseline variables with their means; this seems to be an accepted practice. Lee notes that he focuses exclusively on the “sample selection on wages caused by employment” and not the attrition/nonresponse problem, to focus attention on this, but they could have used it for the other as well. – DR: (@NL) Note again that their desire to separate the employment hours and wage effects of the program is very similar to our desire to separate out different margins of the impact of winning an institution. …Namely the impact on completing a course or starting a course versus other impacts and the impact of entering a specialization versus remaining impacts. …Similar decompositions for the geography outcomes. – To do: check whether any papers cite Lee using an IV approach, extending the technique and the estimation of variance. “the problem of nonrandom sample selection is well-understood in the training literature; … may be one of the reasons why most evaluations of job-training programs focus on total earnings, including zeros for those without a job, rather than on wages conditional on employment” “of the 24 studies referenced in a survey … (Heckman et al.)… Most examine annual, quarterly, or monthly earnings without discussing the sample selection problem examining rage rates.” – DR: (@NL)Note that this is relevant to our question of whether to exclude zeros in log models, etc. While there will be less unemployment in our data, it still may be a relevant influence made have a strong effect on the estimates. …previous conventional approaches to the sample selection problem (skip if desired). One may explicitly model the process determining selection, such as in Heckman (1979) … Separate equations for the wage and the propensity to be employed, where employment occurs if the latter crosses a particular threshold, in which case a wage is observed. It is reasonable to think that the treatment variable can have effects on both terms.. “sample selection bias can be seen as specification error in the conditional expectation…” The expected wage conditional on treatment exogenous variables and the selection into working (that is the underlying propensity to work variable exceeding zero) his status is equal to the true effect of the treatment an adjustment for the differences in the observable’s exogenous variables and a bias term representing the expectation of the idiosyncratic unobservables given the treatment and the exogenous variables exceeding the value necessary to induce work participation. The unobservable term needs to exceed the prediction based on the observable term for the entire term to exceed zero inducing labor force participation. One may assume the data are missing at random, perhaps conditional on a set of covariates (Rubin, 1976; essentially assuming the error terms in each equation are independent of one another, here “employment status is unrelated to the determination of wages”… This “is strictly inconsistent with standard models of labor supply that account for the participation decision (Heckman, 1974).” A more common assumption is that some exogenous variables “determine sample selection but do not have their own direct impact on the outcome of interest…. Exclusion restrictions are used in parametric and semi-parametric models…” but “there may not exist credible ’instruments… excluded from the outcome equation” – DR, aside: We can return to (our) previous papers to impose these Lee bounds! One example would be the Siskel and Ebert your reviews paper and perhaps incorporating us with subsequent approaches, considering the “selection to review” equation. Second approach “the construction of worst-case scenario bounds of the treatment effect” “Impute missing data with either the largest or smallest possible values to compute the largest and smallest possible treatment effects consistent with the data” as in Horwitz and Manski (2000a) who provide a general framework for this. Particularly useful with binary outcomes. This cannot be used when the support is unbounded. … note in their replication example they are actually using the equivalent of the bottom 5th percentile and the top 95th percentile. Strictly using a procedure would provide even wider bounds. Lee considers his approach to be a hybrid of the two previous general approaches. …end of section 2.. .a statement of the Horwitz upper bound for the treatment effect; very intuitive: “what if everyone in the treatment who dropped out would have had the largest possible wage and everyone in the control group that drop out would’ve had the smallest possible wage; this will give the upper bound.” Switching this the other way around will give a lower bound. DR, an aside thought: (@NL) Something akin to the Horwitz and M approach (or maybe Lee bounds) could be applied to our issue of swapping into institutions directly. Suppose we only focus on those who actually complied: those assigned to an institution who also went to that institution. Our concern was that this would under-represent those who had particularly strong institutional preferences. Suppose you are interested in looking at the impact of winning the lottery (for once preferred institution) itself, as that was our most simply identifiable outcome. … Let’s consider evaluating a treatment effect for those who happened to swap in. Those who swapped in might be assigned a counter-factual outcome of the lowest value of the lifetime income among those who did not get their institution of choice. Similarly, the small group who swapped out might be assigned a counterfactual outcome (had they no swapped out) representing the highest outcome value for those who did get their institution of choice. This should give us an upper bound on the treatment effect for these two groups of what we might call non-compliers. Making the opposite assumptions, precisely that those who swapped into their institution of choice would’ve had a very good counterfactual outcome (if they had not got their institution of choice) that comes from the highest outcomes for those who didn’t get their institution of choice (and also reversing this for those who swapped out of their preferred institution) would give us a lower bound for the treatment effect for this group. We can then combine these bounded treatment effects for these non-compliers with the treatment effect for the compliers to get a measure of the average treatment effect with bounds for this sort of behavior. This will also allow us to come up with estimates with bounds without having to use the instrumental variable strategy which has issues of its own. 10.2.1.0.3 Section 3: identification of bounds on treatment effects; the main meat of the model He starts with a simple example. He begins with a model with a treatment indicator and no other covariates, and a continuous outcome variable, but notes that this will clearly apply to discrete outcome variables and will also apply conditional on controls. Nest, he brings forward the statement… from the earlier selection models. In each case the latent variable must overcome a hurdle for the outcome to be observed and in fact the hurdle differs depending on the impact of the treatment itself. In general when the errors in the selection and outcome equations are correlated the difference in these means differs from the actual treatment effect. In other words through a slightly complicated story, when those who have unobservables that make them more likely to work also tend to have unobservables that would make them likely to earn more the standard difference in outcomes between control and treatment will not describe the true treatment effect. A key insight seems to be that we could identify the treatment effect if we could estimate the expected outcome given treatment and given that the unobservable component in the selection equation would lead to an observable outcome had the person not been given treatment. If so, we could subtract the observed mean control outcome from the above to yield the true treatment effect (for those who would be observed always). However, we obviously do not observe this because we only observe the outcomes for those who are treated where the selection equation is in fact positive and not “where the selection equation would have been positive had they not been treated.” However, the insight here is that this term can in fact be bounded. We do observe these outcomes for the treated people (note we are assuming without loss of generality that the treatment raises the probability of selection for this discussion) but we don’t know exactly which ones they are. In other words, we observe outcomes for more people in the treatment group than we need; we wish we could figure out what is the subset of these that would have also been observed had they not been treated, so we could compare like-to-like. The observed treatment mean is a weighted average of the thing we are seeking (to difference from the control) and “the mean for a subpopulation of marginal individuals… that are induced to be selected into the sample because of the treatment” This then gets us the upper bound for the term expressing the treatment outcome for those who would have been observed even if they had been in the control. The upper bound for this is the expected outcome for those in the treated group (who are observed of course) and who are in quantile-p or above of the outcome, where this \\(p\\) is the share of the treated population that are in the marginal group we referred to that were only induced to be selected into the sample because of the treatment. In other words the worst case scenario is that the smallest share \\(p\\) values of \\(Y\\) are in the marginal group and the largest one (which is share 1-pone are in the inframarginal group. We don’t know which observations are inframarginal and which ones are marginal. \\(p\\): the share of marginal individuals and (1-p) the share of inframarginal individuals (the latter is group we want the average outcome for). The highest could be would be the average outcomefor the largest (1-p) share of this group. We are looking for the expectation given that they are at or above at will at or above percentile p within this group. In other words we trim the lower tail of the Y distribution by the portion \\(p\\), (so what remains is the 1-p share) to get the upper bound for the inframarginal groups mean. We can then subtract the mean for the control group to get an upper bound for the treatment effect. To compute this “trimming proportion p”: this p is equal to the share of the treated group whose outcome is observed minus the share of the control group whose outcome is deserved is observed, divided by the share of the treatment group where the outcome is observed. Something like the increased likelihood of observation that is driven by the treatment, as a share of the total number as a share of the probability of observation in the treatment group. The average observed outcome for the treatment group is including too many observations; we need to difference out the share of observations that are observed only because the treatment caused them to be observed; this share is certainly no larger than the increased probability of observation in the treatment group as a share of the probability of observations the treatment group. Another much simpler way of saying this is “trimming the data by the known proportion of excess individuals” in the treatment group. (To gain bounds on the mean for the inframarginal group which we can then difference from the control-group mean get the treatment effect). Perhaps some intuition for why this improves on the Horwitz model: we don’t need to assume that those observed in the treatment group that wouldn’t have been observed in the control would’ve had the highest possible outcomes. No, we only need to assume (to get the upper bound) that these came from the highest distribution because they had to come from somewhere. These were the people in the upper tail of the relevant group but they couldn’t all have been the individual highest achiever. The model is extended to heterogeneity and heteroscedasticity. This begins with the independence of treatment assignment the “potential sample selection indicators” for either treatment or control, in other words whether that individual will have an observed outcome under treatment and whether that the individual would have an observed outcome under control, and the latent potential outcomes. Experimental or random assignment ensures that each of the potential outcomes (and the correspondence to observability under each treatment) is independent of the actual assigned treatment. The second assumption is monotonicity: treatment assignment can only affect sample selection in one direction. – DR: For our (substitution) experiments, it is in fact not clear to me whether this should necessarily be the case, as some (less generous?) people may be induced to leave because of having been asked to donate, while potentially other (more generous people) might be induced to return given that they were asked to donate. (This proposed nonmonotonicity implies that the ‘asked twice’ sample tends to weed out the less generous, which would lead to a bias against substitution, strengthening the case for our result.) - DR, aside: However, even though the paper doesn’t say it, I suspect this assumption could be weakened and you would still get some similar bounds. To put it another way, I would imagine that these bounds could be adjusted based on some reasonable ad hoc assumptions about the share of the population who is affected in either direction. – @NL: I’m coming to think that our Dutch data problems are more things involving “hurdle models”. Can this technique also be applied to such hurdle models? Next proposition 1a states that given these assumptions we can derive sharp lower and upper bounds for the average treatment effect (conditional on ‘would be observed in both states’). Note that for this estimator if the probability of observation is greater under the treatment we need to trim the treatment groups outcome distribution and if the probability of observation is greater under the control we need to trim the control group’s outcome distribution. DR, aside comment: we seem to be throwing out a bit of the data in these estimates, which would suggest that something more efficient could be generated. (The stated bounds you can estimate are exactly the same as the bounds from the previous specification, at least as I had interpreted the way they would be produced.) Their remark 2 notes that an implication is that as \\(P_0\\), that as the “difference between the relative probability of observation of an outcome under treatment versus control” tends to zero, i.e., as the probability of having an observed outcome (or the conditional probability of this) is the same for treatment and control) then there is no sample selection bias. Their estimate convergences to the estimate he calls an estimate for the “always takers subpopulation… except that taking… is selection into the [outcome-observed] sample.” So, a very vanilla estimator is acceptable if we find the same conditional probability of selection for each group, under monotonicity, which, for this case, we can test (see Remark 4 below). – (DR: To me this suggests that there might be something wrong going on here. Intuitively, If I simply observe the same rate of attrition in the treatment and control groups this shouldn’t be enough to tell me that attrition did not matter, as it could occur differentially for both groups, but it seems to be a result here; this is probably due to the assumption of monotonicity of the selection/observation term, as well as the random/exogenous assignment to each group.) Remark 3 discusses the importance of monotonicity for the bounds, saying this assumption is “minimally sufficient” (I think it would be better to say minimally sufficient for these particular bounds that he computed). To demonstrate this he gives an extreme example. Without monotonicity it could be (note: this would seem like a very unlikely outcome!) that every observation in the control group comes from the population in the treatment group that would not have been observed had they been treated and every observation in the treatment group happens to come from the set of people that would not have been observed had they been in the control group. These two “subpopulations do not overlap, so the difference in the means could not be interpreted as a causal effect.” – DR, aside : there must be some way to impose some restrictions on this even allowing for this non-monotonicity. (He notes that this can be improved upon somewhat by thinking about the total the idea that the total masses of unobserved that would’ve been observed in the other group can’t be greater than the share that is not observed in the other treatment group, but this doesn’t seem like a particularly fruitful route as it in most reasonable cases will still allow for very wide bounds.) Remark 4 suggests that if we can assume (or somehow observe?) that the conditional probabilities of selection are the same for treatment and control, we can test whether monotonicity in fact holds and the simple difference in means will be an appropriate estimate of the treatment effect. Here, the assumption implies that everyone in the treatment or control group would have been observed under the opposite treatment as well. This in fact implies that the distribution of the exogenous variables should be the same in the treatment and control groups conditional on being selected. This seems fairly intuitive, we look at whether selection seems to be occurring in different ways are on different margins for the two groups treatment versus control. Apparently for this test to have power we need that the subpopulations of “noncompliant errors in opposite directions” (quotation mine) must have distinct distributions of baselines exogenous characteristics. If these were the same then whether or not monotonicity holds the test doesn’t tell us anything. – DR: I wonder if anyone uses this test for Monotonicity under non-differential selection? Another relevant note that he bundles in this remark is that the technique here only yields estimates for those who would be with an observed outcome for either treatment or control. One could additionally try to bound this as an estimate for the entire population using the Horwitz and Manski bounds for this latter thing. However, in many contexts there are reasons that the bounded estimates they mainly use are the relevant ones, such as “the impact of the program on wage rates for those whose employment status was not affected by the program.” DR: In our substitution experiment case, the substitution patterns for those for whom attrition was not affected by the first-round-charity treatment @NL: E.g., the impact of an institution on income for those whose choice to remain in the course was not affected by their institutional assignment “Narrowing bounds using covariates” All of the above could be done conditional on a particular set of baseline characteristics such as gender or race. The average treatment effect could be estimated separately for each. (Note: and perhaps combined in a fruitful way?) One can alternately use covariates to reduce the width of these bounds. To give intuition, we can imagine a baseline covariate that perfectly predicts an individual’s wage. Because treatments are randomly assigned the maintained assumptions will still hold conditionally on this X. The results the methods can be applied separately for each value of this covariate, and for each such value the trimming procedure will actually have no impact on the estimate. DR: I think this is the “estimate and sum things up in a weighted way” procedure I thought about a moment ago. Proposition 1B gives the balance estimator for a model involving exogenous variables. Essentially, this computes the corresponding bounds estimator at each X, where the differential selection probability is computed for that particular X, the upper quantile value of the outcome is given conditional on the same X and on being in the treatment group. These are then integrated (or summed up) weighted by the distribution or the cdf of this covariate in the control group. These bounds will necessarily be sharper than the balance without controls. 10.2.1.0.4 Section 4: estimation and inference The asymptotic variance depends on components reflecting the variance of the trimmed distribution, the variance of the estimated trimming threshold, and the variance in the estimate of “how much of the distribution to trim” (the relative selection probability differential). Equation 6 formally defines the estimator Estimated bounds consistent for ‘true bounds’ under standard conditions Two ways to compute CI’s – CI’s for the ‘true bounds’ or CI’s for the TE itself. A 95% CI for the former will contain the latter with even greater probability. Imbens and Manski ‘04 can be used to derive the latter which are ’more apppropriate here’ since the object of interest is the TA and not the ’region of all rationalizable treatment effects. These are built off of a transformation of the estimate UB and LB and max estimated sd of each of these. the latter are reported by the ‘cie’ option in ‘leebounds’ Generalisation to monotonicity (without knowing direction of impact of treatment on selection)… As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative… [do similar] though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small and p0 is “close” to zero … A simple, conservative approach to combining the trimmed and untrimmed intervals is to compute their union 10.2.1.0.5 Section 5: Empirical Results Table 4 gives a step-by-step that is a good way of seeing and understanding the construction of the estimator, and where the ‘action’ is, in treimming, in components of the SE, etc. Intervals are 1/14 the width of the equivalent Horowitz/Manski bounds 10.2.1.0.5.0.1 5.2 using covariates to narrow bounds Any baseline covariate will do, as will any function of all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to be a proxy for the predicted wage potential for each individual—is constructed from a linear combination of all observed baseline characteristics. This single covariate is then discretized, so that effectively five groups are formed according to whether the predicted wage is within intervals defined by $6·75, $7, $7·50, and $8·50. (???): this is essentially what I propose we do, but using Ridge Regressions or something similar To compute the bounds for the overall average…the group-specific bounds must be averaged, weighted by the proportion (sPr Group J|S0=1,S1=1) The estimated asymptotic variance for these overall averages is the sum of (1) a weighted average of the group-specific variances and (2) the (weighted-) mean squared deviation of the group-specific estimates from the overall mean. This second term takes into account the sampling variability of the weights \\(\\rightarrow\\) result: 11% narrower bounds Interesting; possibly do similar for @NL-ed: By statistically ruling out any effect more negative than −0·037, this suggests that after 4 years, the Job Corps enabled program group members to offset at least 35% (and perhaps more) of the potential 0·058 loss in wages due to lost labour market experience that could have been caused by the program 10.2.1.1 Section 6: Conclusions: implications and applications Interesting intuitive argument: Another reason to interpret the evidence as pointing to positive wage effects is that the lower bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly negatively correlated with the propensity to be employed. From a purely theoretical standpoint, a simple labour supply model suggests that, all other things equal, those on the margin of being employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be the smallest wage that draws the individual into the labour force). In addition, the empirical evidence in Table 2 suggests that there is positive selection into employment: those who are predicted to have higher wages are more likely to be employed (i.e., U and V are positively correlated). If this is true, it seems relatively more plausible to trim the lower rather than the upper tail of the distribution to get an estimate of the treatment effect. "],
["why-experiment-design.html", "11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 11.1 Why run an experiment or study? 11.2 Causal channels and identification 11.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 11.4 Generalizability (and heterogeneity)", " 11 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 11.1 Why run an experiment or study? I claim an experiment should: Have a reasonable chance of an outcome that would not have been predicted in advance. The realized outcome should meaningfully inform our understanding of the world in other words. In other words, if the outcome comes out one way it should cause us to update our beliefs about a particular hypothesis about the world in one direction (and if it comes out the other way we should update in the other direction.) Experimenter should always ask: “What uncertainty (about real-world preferences, decision-making etc.) is ‘entangled’ (ala (???)) with the results of this experiment?”… i.e., ‘how might my beliefs change depending on the results?’ (???) on twitter 11.1.1 Sitzia and Sugden on what theoretically driven experiments can and should do “Sitzia, Stefania, and Robert Sugden.”Implementing theoretical models in the laboratory, and what this can and cannot achieve.\" Journal of Economic Methodology 18.4 (2011): 323-343. This paper is a critique of how models are claimed to be “tested”, through a literal implementation, in the laboratory. They argue this misinterprets the intention of a model, and use of economic modelling in general. Ultimately, such experiments (they say) don’t really tell us one way or another about the truth or usefulness of the model for the real-world domain that was intended. Some key quotes.. My reductio ad absurdum on this is an experimenter who ‘tests mechanism-design’ by asking subjects “do you want to choose this optimal mechanism and earn $20, or this inefficient mechanism and earn $10”? – (???) on twitter They single-out two examples of well-published experiments for criticism: \"an investigation of price dispersion by John Morgan, Henrik Orzen and Martin Sefton (2006), and an investigation of information cascades by Lisa Anderson and Charles Holt (1997)\"… In each case, the experimenters create a laboratory environment that closely resembles the model itself. The only important difference between the experiment and the model is that, whereas the model world contains imaginary agents who act according to certain principles of rational choice, the laboratory contains real human beings who are free to act as they wish. The decision problems that the human subjects face are exactly the problems specified by the model. We argue that such an experiment is not, in any useful sense, a test of what the model purports to say about the target domain. Instead, it is a test of those principles of rational choice that the modeller has attributed to the model world. Those principles are not specific to that model; they are generic theoretical components that are used in many economic models across a wide range of applications. Surprisingly, these doubts are not expressed in terms of the applicability of MSNE [mixed strategy Nash Equilibrium] to the model’s target domain, pricing decisions by retail firms. The doubts are about whether experimental subjects will act according to MSNE when placed in a laboratory environment that reproduces the main features of the model. If one takes the viewpoint of the subjects themselves, there seems to be very little resemblance between the decision problems they face and those by which retail firms set their prices. The connection between the two is given by the model: the subjects’ decision problems are like those of the firms in the model, and the firms in the model are supposed to represent firms in the world. However, MOS are no more concrete than Varian in explaining how the comparative-static properties of the model relate to the real world of retail pricing. The suggestion in these passages is that the clearinghouse model’s claim to be informative about the world is strengthened if its results are confirmed in the laboratory. In this sense the experiment is informative about the world. But the experiment itself is a test of the model, not of what the model says about the world. The procedure of random and anonymous rematching of subjects is explained as a means of eliminating ‘unintended repeated game effects’, such as tacit collusion among sellers (pp. 142–3). This argument illustrates how tightly the laboratory environment is being configured to match the model. In a test of MSNE, repeated game effects are indeed a source of contamination; and MSNE is a property of Varian’s model. But in the target domain of retail trade, the same firms interact repeatedly in the same markets, with opportunities for tacit collusion. Clearly, if an experiment implemented a model in its entirety, all that it could test would be the mathematical validity of the model’s results. Provided one were confident in the modeller’s mathematics, experimental testing would be pointless. Thus, when an experiment implements almost every feature of a model, all it can test in addition to mathematical validity are those features that have not been implemented. Thus, the experiment is a test of MSNE in a specific class of games. [emphasis added] MSNE is what we will call a generic component of economic models – a piece of ready-to-use theory which economists insert into models with disparate target domain ibid Relating back to the discussion of the different conceptions of theory: Is it informative at all to run experimental tests of theoretical principles such as MSNE and Bayesian rationality, viewed as generic components of economic models? … A strict instrumentalist (taking a position that is often attributed to Friedman) might answer ‘No’ to the first question, on the grounds that tests should be directed only at the predictions of theories and not at their assumptions. Such an experimental design should not be appraised in terms of what the model purports to say about its target domain. It should be appraised in terms of what it can tell us about the relevant generic component, considered generically. When (as in the cases of MSNE and Bayesian rationality) the same theoretical component appears in many different models, an experimenter can afford to be selective in looking for a suitable design for a test Considered simply as a test of MSNE, MOS’s experiment uses extraordinarily complicated games. Many of the canonical experiments in game theory use 2×2 games. Depending on the treatment, MOS’s games are either 101×101 (for two players) or 101×101×101×101 (for four players). Payoffs to combinations of strategies are determined by a formula which, although perhaps intuitive to an economist (it replicates the demand conditions of the clearinghouse model), might not be easy for a typical subject to grasp. 11.2 Causal channels and identification Ruling out alternative hypotheses, etc 11.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 11.4 Generalizability (and heterogeneity) “But all the other papers do it!” A common response to critiques (particularly critiques of the generalizability of experimental work) is that “all the other papers have the same problem” and that excepting this critique would require rejecting all previous work too. In politics this has been referred to as “what-about-ism”. You can guess that I’m not a fan of this. I think one always needs to defend the paper and approach on its own merits. Generalisability is an important issue. Each of the other published papers that also suffers from such issues has a specific response and justification for that particular case, and if it doesn’t this is sorely lacking. I think we should be reading and publishing papers that consider, discuss, and acknowledge their own limitations, and future work can test and build on this. This should promote to robust, reproducable science. Just because I say “‘“this is something we should be concerned with doesn’t mean I’m saying”this paper has no value’. I just mean”let’s discuss reasons why this may or may not threaten internal or external validity/generalisability, and how we can design the study and analysis minimise these potential problems\"\" In writing a paper, I find it important that we the authors feel the results are credible and not overstated. So I feel like the best approach is “let’s write the best paper we can and consider every issue seriously, and then hopefully the good publication/peer-review outcome will follow”. That’s also the most motivating and least stressful way for me to work. (Rather than thinking ‘how can I sneak this paper into the best journal?’) In fact I consider peer review and high rating and use as the important outcome, not the publication itself. We live in a world where anyone can publish their work immediately on the WWW. The journals themselves are providing little or no service: it is the reviewers and editors offering feedback and evaluation that matters. A thought: Replace reviews (accept/reject/R&amp;R) with ratings (0-10)? "],
["quant-design-power.html", "12 (Experimental) Study design: Background and quantitative issues 12.1 Pre-registration and Pre-analysis plans 12.2 Other notes, links, and commentary", " 12 (Experimental) Study design: Background and quantitative issues 12.1 Pre-registration and Pre-analysis plans 12.1.1 The benefits and costs of pre-registration: a typical discussion BB: That said, I would be interested to think about the benefits – and more importantly limitations to – pre-registration. I think it could solve some of the p-hacking problems but not much else. How to not relegate exploratory analyses too far is also unclear to me. DR: I’m much more on the ‘pro’ side pre-registration and PaPs. It also helps deal with publication bias and file drawers. And p-hacking is a huge issue IMHO. But it is also good to have some consideration of the pros and cons, so this would be great. BB: RE pre-reg: yes I think it is enough that it prevents p-hacking (there could be very little cost associated with pre-reg) but I fear that it could prevent other advancements if it relegates exploratory analyses too far. DR: I don’t think it should be binary. Systems need to be worked out for adjustments to the meaning of reported estimates depending on whether they were or were not preregistered, and how many were preregistered. While reported significance levels could be adjusted in the frequentist framework, this will all presumably based on measures of the likelihood that such a result would have been estimated/reported. Thus I think this could most easily be incorporated into a Bayesian framework but I’m not saying it would be easy. Still, they have done some good work on adjustments for ‘sequential designs’. BB: I think that it could also stifle students a bit – it may reduce further the number of students who have access to funding that allows for experiments that will be able to be published if all experiments have to be high-powered. DR: Statistical power is an important issue. I was skeptical at first about the ‘dangers of underpowered studies’ but I’m coming around a bit. My thinking was that ‘we can simply make downward adjustments to the estimates reported in underpowered studies’. Anyways, we don’t want to put the cart before the horse: as Gelman said at a conference we should be supporting science not the careers of scientists. I tend to think there are strong arguments for more centralization in social science. And my impression is that we actually have too many different studies and distinct research programs being run, and too many papers being published and not carefully brought together into a framework. Going through the studies on the https://www.replicationmarkets.com/ reinforces this impression for me. Still, I think there are ways around this to enable early career people. ‘Underpowered’ experiments could be registered as part of a longer/sequential research program, perhaps collaborative and enabling meta-analysis. BB: I also don’t think it gets at publication bias very much unless pre-reg’ed studies are followed up on. Only then do you know why the study didn’t come out – and quite a lot of the time I think it will be attrition/inability to gather the necessary data. Someone could launch that journal though – the Journal of Failed Studies – to have a place for a record that they have been run and what happened to be kept. So I am pro pre-reg, I just think the system needs a bit of work. DR: If preregistration is made public and well-organize, then the ‘failed’ exercises willtbe integrated into future meta-analyses; so that’s at least a partial solution here. Agreed, we need to build better systems for incentivising pre-registration and careful data sharing. We need to give career credit to people for planning designing and reporting credible experiments and projects, even if they ‘fail’. Part this is publishing/rewarding tight null results, which actually do add a lot of value. We might also consider offering some reward careerwise to experiments that fail – in terms of being deeply inconclusive– for some arbitrary or random reason even though they were well-planned and executed. But I think it is hard to get the incentives right for the latter. ### The hazards of specification-searching ## Sequential and adaptive designs Needs to adjust significance tests for augmenting data/sequential analysis/peeking Statistics/econometrics new-statistics sagarin_2014 http://www.paugmented.com/ resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/ Yet ... $P_{augmented}$ may *overstate* type-1 error rate Statistics/econometrics response to referees, new-statistics &quot; A process involving stopping &quot;whenever the nominal $p &lt; 0.05$&quot; and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5\\%. Even if the subsequent data suggested a &quot;one in a million chance of arising under the null&quot; the overall process yields a 5\\%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the &quot;likelihood of the null hypothesis&quot; given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate. Considering the calculations in \\ref{sagarin2014}, it is clear that $p_{augmented}$ should \\textit{overstate} the type-1 error of the process if there is a positive probability that after an initial experiment attains p$&lt;0.05$, more data is collected. A headline $p&lt;0.05$ does \\textit{not} imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded $p&gt;0.05$, thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though $p&lt;0.05$. Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine&#39;s experiment). Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p&lt;0.05, more data is collected, implying that $p_{augmented}$ as calculated above overstates the type I error rate that would arise from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below $p=0.05$, in order to attain a type-1 error rate of 5% or less in our published corpus.&quot; ## Efficient assignment of treatments (Links back to power analyses) ### How many treatment arms can you &#39;afford&#39;? &lt;!-- A guiding principle might be: &quot;Will we have statistical power to identify a small true effect from this pairing? If not, we drop the pairing.&quot; A caveat to this is that we may be able to pool some of the pairings to answer certain questions, but then it is only worth having the distinct variations that are being pooled if that doing so gives us power to answer some other question. --&gt; &lt;!--chapter:end:experiments_and_study_design/quant_design_power.Rmd--&gt; # (Experimental) Study design: (Ex-ante) Power calculations {#power} ## What sort of &#39;power calculations&#39; make sense, and what is the point? ### The &#39;harm to science&#39; from running underpowered studies &gt; &quot;One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results. &gt; Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.&quot; verkaik2016, metzger2015 \\ On magnitude error due to underpowered studies: https://www.pauljferraro.com/publications/2020/2/1/is-there-a-replicability-crisis-on-the-horizon-for-environmental-and-resource-economics ## Power calculations without real data ## Power calculations using prior data Adapt example in &#39;scopingwork.Rmd&#39; to this &lt;!--chapter:end:experiments_and_study_design/power_calc.Rmd--&gt; # &#39;Experimetrics&#39; and measurement of treatment effects from RCTs {#experimetrics_te} ## Which error structure? Random effects? ## Randomization inference? ## Parametric and nonparametric tests of simple hypotheses ## Adjustments for exogenous (but non-random) treatment assignment ## IV in an experimental context to get at &#39;mediators&#39;? ## Heterogeneity in an experimental context &lt;!--chapter:end:experiments_and_study_design/experimetrics_te.Rmd--&gt; # Making inferences from previous work; Meta-analysis, combining studies {#metaanalysis} My opinion on why this is so important (unfold): \\BeginKnitrBlock{fold}&lt;div class=&quot;fold&quot;&gt; it is lame how often I see &#39;new experiments&#39; and &#39;new studies&#39; that tread most of the ground as old studies, spend lots of money, get a publication and ... ignore or pay lip service to the previous findings. There is tons of data out there that can inform new questions and bring better through re-analysis and combination with other data. Otherwise we are not actually building progress. This is why I became involved with a project I called &#39;ExpArchive&#39;, later working with projects such as GESIS&#39; X-Econ to try to encourage and facilitate data sharing in experimental economics, as well as the innovationsinfundraising.org project, which is now collaborating with the Lily Institute&#39;s &quot;revolutionizing philanthropy research&quot; (RPR) project. &lt;/div&gt;\\EndKnitrBlock{fold} ## Notes: Christensen et al 2019, ch 5, &#39;Using all evidence, registration and meta-analysis &gt; how the research community can systematically collect, organize, and analyze a body of research work - Limitations to the &#39;narrative literature review&#39;: subjectivity, too much info to narrate ### The origins [and importance] of study [pre-]registration ... Make details of planned and ongoing studies available to the community .... including those not (yet) published - Required by FDA in 1997, many players in medical community followe d soon after - Turner ea (08) and others documented massive publication bias and misrepresentation ... but registration far from fully enforced (Mathieu ea &#39;09) found 46% clealy registered, and discrepancies between registered and published outcomes ! ### Social science study registries Jameel 2009, AEA 2013, 2100 registrations to date RIDIE, EGAP, AsPredicted, OSF allowing a DOI (25,000+) ### Meta-analsis Key references: Borenstein ea &#39;09, Cooper, Hedges, and V &#39;09 #### Selecting studies &quot;some scholarly discretion regarding which measures are &#39;close enough&#39; to be included... contemperanous meta-analyses on the same topic finding opposit e conclusions &#39;asses the robustness... to diffrent inclusion conditions&#39;... see Doucouliagos ea &#39;17 on inclusion options &lt;div class=&quot;marginnote&quot;&gt; My opinion: this is the key barrier to meta-analysis in social science! How do we weight studies using different methodologies and in different contexts? The Bayesian Random Effects approach seems to offer some help here (this not to be confused with the random effects panel-data models discussed in standard Econometrics texts). &lt;/div&gt; #### Assembling estimates - Which statistic to collect? \\ Studies $j \\in J, j= 1..N_j$ Relevant estimate of stat from each study is $\\hat{ \\beta_j}$ with SE $\\hat{\\sigma_j}$ - Papers report several estimates (e.g., in robustness checks): which to choose, esp if author&#39;s preferred approach differs from other scholars. \\ *Ex from Hsiang, B, Miguel, &#39;13*: links between extreme climate and violence - how to classify outcomes... interpersonal and intergroup... normalised as pct changes wrt the meanoutcome in that dataset - how to standardice climate varn measures... chose SD from local area mean (DR: this choice implicitly reflects a behavioural assumption) $\\rightarrow$ &#39;pct change in a conflict outcome as a fncn of a 1 SD schock to local climate&#39; ### Combining estimates &#39;Fixed-effect meta-analysis approach&#39;: assumes a single true effect&#39; &lt;div class=&quot;marginnote&quot;&gt; DR: I&#39;m not sure I agree on theis assesment of *why* this is unlikely to be true in practice... &#39;differences in measures&#39; (etc) seem to be a different issue &lt;/div&gt; *Equal weight approach*: (Simply the average across studies... ugh) \\ *Precision-weighted approach*: $$\\hat{\\beta}_{PW}= \\sum_{j}p_j\\hat{\\beta}_j/ \\sum_{j}p_j$$ where $p_j$ is the estimated precision for study $j$: $\\frac{1}{\\hat{\\sigma_i}^2}$ Thus the weight $\\omega_j$ placed on study $j$ is proportional to it&#39;s precision. &lt;div class=&quot;marginnote&quot;&gt; &#39;implies weight in proportion to sample size&#39;? I think that&#39;s loosely worded, it must be nonlinear. &lt;/div&gt; $\\rightarrow$ This minimises the variance in the resulting meta-analtical estimate: $$Var(\\hat{\\beta}_{PW) =\\sum_j \\omega_j^\\hat{\\sigma_j}^2=\\frac{1}{\\sum_j(p_j)}$$ &#39;inclusion of additional estimates always reduces the SE of $\\hat{\\beta_{PW}}$ [in expectation].&#39; ... so more estimtes can&#39;t hurt as long as you know their precision. (they give a numerical example here with 3 estimates) &lt;!-- Todo: add R code explicitly doing these calculations --&gt; ### Heterogeneous estimates... #### WLS estimate (Stanley and Doucouliagos &#39;15) Interpreted as &#39;an estimate of the average of potentially heterogenous estimates&#39; This may feel like a more familiar to Economists but it is also seems to be far less useful than the Bayesian approach. \\ #### Random-effects (more common) *Focus here on hierarchical Bayesian approach* (Gelman and Hill &#39;06; Gelman ea &#39;13) &#39;The magnitude and precision of the common component represents the generalizable conclusions we might draw from a literature&#39; ... continuing from above notation &#39;cross-study differences we observe might not be driven solely by sampling variability... [even with] infinite data, they would not converge to the exact same [estimate] &#39; True Treatment Effect (TE) $\\beta_j$ for study j drawn from a normal distribution... $$\\beta_j \\sim N(\\mu, \\tau^2)$$ &#39;Hyperparameters&#39; $\\mu$ determines central tendency of findings... $\\tau$ the extent of hety across contexts. Considering $\\tau$ vs $\\mu$ is informative in itself. And a large $\\mu$ may suggest looking into sample splits for hety on obsl lines. \\ Uniform prior for $\\mu$ $\\rightarrow$ conditional posterior: $$\\mu|\\tau,y \\sim N(\\hat{\\mu}, V_{\\mu})$$ where the estimated common effect $\\hat{\\mu}$ is $$\\hat{\\mu}= \\frac{\\sum_{j}(1/(\\hat{\\sigma}^2_j+\\hat{\\tau}^2))\\hat{\\beta}} {\\sum_{j}(1/(\\hat{\\sigma}^2_j+\\hat{\\tau}^2))}$$ (Similar to precision-weighted approach but now the between-study dispersion is incorporated into the weights) and where the estimated variance of the generalizable component $V_\\mu$ is: $$Var(\\hat{\\mu})= \\frac{1}{\\sum_j\\big(1/(\\hat{\\sigma_i}^2 + \\hat{tau}^2)}$$ &lt;div class=&quot;marginnote&quot;&gt; Confusion/correction? Is this the estimated variance or the variance of the estimate? &lt;/div&gt; - and how do we estimate some of the components of these, like $\\hat{\\tau}$? &gt; Intuitively, if estimated [TE] in all studies are near one another and have relatively wide and overlapping [CI&#39;s], then most of the difference in estimates is likely the result of sampling variation [and $\\tau$] is likely to be close to zero. &lt;div class=&quot;marginnote&quot;&gt; DR: But if the TE have wide CI&#39;s, do we have power to idfy btwn-study hety? ... I guess that&#39;s what the &#39;estimated TE are all near each other&#39; gives us? &lt;/div&gt; ... Alternatively, if there is extensive variation in the estimated ATEs but each is precise... $\\tau$ is likely to be relatively large. \\BeginKnitrBlock{note}&lt;div class=&quot;note&quot;&gt;Coding meta-analyses in R &quot;A Review of Meta-Analysis Packages in R&quot; offers a helpful guide to the various packages, such as `metafor`. [Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) appears extremely helpful; see, e.g., their chapter [Bayesian Meta-Analysis in R using the brms package](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-meta-analysis-in-r-using-the-brms-package.html) &lt;/div&gt;\\EndKnitrBlock{note} &lt;!-- TODO: some code exercises should be put or linked here? Perhaps drawn from the above references? --&gt; &lt;/div&gt; \\ The $I^2$ stat is a measure of the proportion of total variation attributed to cross-study variation; if $\\hat{\\sigma}_j$ is the same across all studies we have: $I^2(.) = \\hat{\\tau}^2/(\\hat{\\tau}^2 + \\hat{\\sigma}^2)$ &lt;!-- *DR: more detail would be welcome here. Material from [this syllabus]() may be helpful. https://docs.google.com/document/d/1oImg-ojUFqak5KyZ-ETD2qGvkvUgx8Ym6b8gG4GwfM8/edit?usp=drivesdk --&gt; ## Excerpts and notes from &#39;Doing Meta-Analysis in R: A Hands-on Guide&#39; (Harrer et al) {#doing-meta} Some notes follow excerpting and commenting on [Doing Meta-Analysis in R: A Hands-on Guide](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) ```r #devtools::install_github(&quot;MathiasHarrer/dmetar&quot;) #install.packages(&quot;extraDistr&quot;) library(meta) # I followed a lot of steps before this worked! ... https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started ... #install.packages(&quot;brms&quot;) library(brms) library(dmetar) library(extraDistr) 12.1.2 Pooling effect sizes FE model calculates weighted average: FIX THESE FORMULAS \\[\\hat{\\theta_F} = \\frac{\\sum\\limits_{k=1}^K \\hat{\\theta_k} \\] \\[\\hat{\\sigma^2_k}=\\sum\\limits_{k=1}^K \\frac{1}{K}\\hat{\\sigma}^2_k \\] note that this process does not ‘dig in’ to the raw data, it just needs the summary statistics, neither does the “RE model” they refer to: Both of these models only require an effect size, and a dispersion (variance) estimate for each study, of which the inverse is taken. This is why the methods are often called generic inverse-variance methods. Nor the Bayesian models, apparently (they use the same ‘madata’ dataset) 12.1.3 Bayesian Meta-analysis “The model we apply in Bayesian Meta-Analysis is a so-called Bayesian Hierarchical Model… every meta-analytical model inherently possesses a multilevel, and thus ‘hierarchical’, structure.” The setup Underlying RE model (as before) Study-specific estimate: \\[ \\hat\\theta_k \\sim \\mathcal{N}(\\theta_k,\\sigma_k^2) \\] True study-specific effects distributed: \\[ \\theta_k \\sim \\mathcal{N}(\\mu,\\tau^2) \\] … simplified to the ‘marginal’ form: \\[ \\hat\\theta_k | \\mu, \\tau, \\sigma_k \\sim \\mathcal{N}(\\mu,\\sigma_k^2 + \\tau^2)\\] And now we specify priors for these parameters, ‘making it Bayesian’ \\[(\\mu, \\tau^2) \\sim p(.)\\] \\[ \\tau^2 &gt; 0 \\] Estimation will… involve[] Markov Chain Monte Carlo based sampling procedures, such as the Gibbs Sampler. In the brms package we will be using in this chapter, the No-U-Turn Sampler, or NUTS (Hoffman and Gelman 2014), is used. Why use Bayesian? to “directly model the uncertainty when estimating [the between-study variance] \\(\\tau^2\\)” “have been found to be superior in estimating the between-study heterogeneity and pooled effect, particularly when the number of included studies is small” “produce full posterior distributions for both \\(\\mu\\) and \\(\\tau\\)” … so we can make legitimate statements about the probabilities of true parameters “allow us to integrate prior knowledge and assumptions when calculating meta-analyses” (including methodological uncertainty perhaps) Setting weakly informative’ priors for the mean and cross-study variance of the TE sizes It has been argued that a good approach is to use weakly informative priors (Williams, Rast, and Bürkner 2018) [rather than ‘non-informative priors’!] For \\(\\mu\\): include distributions which represent that we do indeed have some confidence that some values are more credible than others, while still not making any overly specific statements about the exact true value of the parameter. … In most applied cases, it seems reasonable to assume that the true effect size we want to estimate must lie somewhere between, for example, Cohen’s \\(d=-2.0\\) and \\(d=2.0\\), but will unlikely be hovering around \\(d=50\\). A good starting point for our \\(\\mu\\) prior may therefore be a normal distribution with mean \\(0\\) and variance \\(1\\). This means that we grant a 95% prior probability that the true pooled effect size \\(\\mu\\) lies between \\(d=-2.0\\) and \\(d=2.0\\): \\[ \\mu \\sim \\mathcal{N}(0,1)\\] For \\(\\tau^2\\) must be non-negative, but might be very close to zero. Recommended distribution for this case (for variances in general): Half-Cauchy prior (a censored Cauchy) \\(\\mathcal{HC}(x_0,s)\\) with location parameter \\(x_0\\) (peak on x-axis) and \\(s\\), scaling parameter ‘how heavy-tailed’ Half-Cauchy distribution for varying \\(s\\), with \\(x_0=0\\): HC is ’heavy-tailed;… gives some probability to very high values but low values are still more likely. One might consider \\(s=0.3\\) \\(s\\) corresponds to the std deviation here? … so an SD of the effect size about 1/3 of it’s mean size? Checking the share of this distribution below 0.3… phcauchy(0.3, sigma = 0.3) #cumulative share of distribution below 0.3 ... is 1/2 ... with sigma=0.3 ## [1] 0.5 … But they go for the ‘more conservative’ \\(s=0.5\\). In general, it is advised to always conduct sensitivity analyses with different prior specifications to check if they affect the results substantially Complete model: \\[ \\hat\\theta_k \\sim \\mathcal{N}(\\theta_k,\\sigma_k^2) \\] \\[ \\theta_k \\sim \\mathcal{N}(\\mu,\\tau^2) \\] \\[ \\mu \\sim \\mathcal{N}(0,1)\\] \\[ \\tau \\sim \\mathcal{HC}(0,0.5)\\] 12.1.3.1 Bayesian Meta-Analysis in R using the brms package You specify the priors as a vector of elements, each of which invokes the ‘prior’ function, which makes some sort of data frame. The priors function takes a distribution function with parameters, and a ‘class’. priors &lt;- c(prior(normal(0,1), class = Intercept), prior(cauchy(0,0.5), class = sd)) A quick look at the data we’re using here: str(ThirdWave[,1:3]) ## tibble [18 × 3] (S3: tbl_df/tbl/data.frame) ## $ Author: chr [1:18] &quot;Call et al.&quot; &quot;Cavanagh et al.&quot; &quot;DanitzOrsillo&quot; &quot;de Vibe et al.&quot; ... ## $ TE : num [1:18] 0.709 0.355 1.791 0.182 0.422 ... ## $ seTE : num [1:18] 0.261 0.196 0.346 0.118 0.145 ... To actually run the model, he uses the following code: (it seems to require Xcode to run on my mac) The formula for the model is specified using ‘regression formula notation’ As there is no ‘predictor variable’ here (unless it’s meta-regression), x is replaced with 1 But we want to give studies with greater precision of the effect size estimate a greather weight. Done using y|se(se_y) For the random effects terms he adds (1|study) to the predictor part. prior: Plugs in the priors created above plug in the priors object we created previously here. iter: Number of iterations of MCMC algorithm… the more complex your model, the higher this number should be. [DR: but what’s a rule of thumb here?] 12.2 Other notes, links, and commentary A high quality meta-analysis should:- Have a pre-registered protocol- Appropriately deal with dependent effect sizes- Explore effect size heterogeneity - Have a clear methods description- Report COIs- Publish data and code https://t.co/cHj11wv5vm — Dan Quintana ((???)) November 18, 2019 "],
["bayesian-approaches.html", "13 Bayesian approaches 13.1 My (David Reinstein’s) uses for Bayesian approaches (brainstorm) 13.2 ‘Statistical thinking’ (McElreath) and AJ Kurtz ‘recoded’ (bookdown): highlights and notes 13.3 Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep” 13.4 Other resources and notes to integrate", " 13 Bayesian approaches I take notes on several different resources/texts below. Ultimately I’ll try to integrate these into a single set of notes. 13.1 My (David Reinstein’s) uses for Bayesian approaches (brainstorm) 13.1.1 Meta-analysis of previous evidence Of prior work, especially on motivators of (effective) charitable giving and responses to effectiveness information Of my own series’ of experiments (potentially joint with prior work) 13.1.2 Inference, particularly about ‘null effects’ When/what can we say about the ‘absence of an effect’ How to integrate into inferences from diagnostic testing (e.g., common-trend assumption)? 13.1.3 ‘Policy’ and business implications and recommendations E.g., for ‘which pages to seed’ 13.1.4 Theory-driven inference about optimizing agents, esp. in strategic settings Especially in ‘predicted contributions to public goods’ settings and 2nd order beliefs 13.1.5 Experimental design Optimal treatment assignment, with previous observables and a track record Sequential designs Bayesian Power calculation #sessionInfo() Package loadings from Kurtz: 13.2 ‘Statistical thinking’ (McElreath) and AJ Kurtz ‘recoded’ (bookdown): highlights and notes McElreath’s course and text looks great. I’m taking selective notes here; I’ll try to incorporate content from both text and youtube video lectures. AJ Kurtz has re-written the code using the brms package, which he finds superior. More crucially for me, he redoes the code using ggplot and tidyverse? I’m planning to through this here, adding my own notes, questions, and considerations and (hopefully) incorporating some of my own work. I’ve also forked Kurtz’s repo here, which I may play with. 13.2.1 The Golem of Prague (Chapter 1) Don’t let your model or approach turn into a Golem you can’t control. Don’t ‘believe the model’; continuously validate it. The map is not the territory. ‘Statistical decision trees’ lend a false sense of security… and almost never fit the actual case we are dealing with. (fig 1.1) Statistical models are non-unique maps to ‘Process models’ which are non-unique maps to Hypotheses. (‘Nuetral evolutionary selection’ example.) This makes strict falsification impossible … How can you falsify a hypothesis/theory if it corresponds to a wide set of process models and statistical models, many of which overlap other hypotheses? But this warning is at least as relevant for Bayesian analyses, which must be based on specifically defined (term) models of the DGP etc. Thus he recommends caution and continuous (?) interplay between the model and the data. (See next chapter … ‘small worlds and large worlds’.) He also suggests we refer not to ‘Confidence intervals’ or even ‘Credible intervals’, but to ‘Consistent intervals’ … as in ‘these intervals are consistent with the model and data’. And… &gt; [so you should] ‘…Explicitly compare predictions of more than one model’ 13.2.1.1 Rethinking: Is NHST falsificationist? (#fig:failure_of_falsification.png)From McElreath video lecture 1 Null hypothesis significance testing, NHST, is often identified with the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify a null hypothesis, not the actual research hypothesis. So the falsification is being done to something other than the explanatory model. This seems the reverse from Karl Popper’s philosophy. I.e., scientists have turned things upside down; originally the idea was that you had substitute of hypotheses that you would want to falsify and now we try to falsify silly null hypotheses that “nothing is going on”. You should try to really build a hypothesis and test it not just reject that nothing is going on. 13.2.1.2 Book’s foci Bayesian data analysis Multilevel modeling Model comparison using information criteria 13.2.2 Small Worlds and Large Worlds (Ch 2) … The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20) We imagine a bag filled with four marbles, each of which is blue or white. “So, if we’re willing to code the marbles as 0 =”white\" 1 = “blue”, we can arrange the possibility data in a tibble as follows.\" I.e., we can consider the five possible worlds, in each of which the bag has a different number of white and blue marbles, and represent each of these worlds as a column vector: d &lt;- tibble(p_1 = 0, p_2 = rep(1:0, times = c(1, 3)), p_3 = rep(1:0, times = c(2, 2)), p_4 = rep(1:0, times = c(3, 1)), p_5 = 1) d p_1 p_2 p_3 p_4 p_5 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 We visualize this in the plot below, where each column is one ‘world’: d %&gt;% gather() %&gt;% #make it long, with an ket variable for the possibility &#39;world&#39; mutate(x = rep(1:4, times = 5), #an index for &#39;which ball&#39; possibility = rep(1:5, each = 4)) %&gt;% #distributing the &#39;which world&#39; index ggplot(aes(x = x, y = possibility, fill = value %&gt;% as.character())) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(.75, 4.25), ylim = c(.75, 5.25)) + theme(legend.position = &quot;none&quot;) Simple combinatorics (permutations rule) tells us how many ‘ways’ we can draw 1, 2, and 3 marbles… Here we think about ‘which’ marble is drawn, and not just ‘which color’ it is. We can draw marble 1-4, the first time, then 1-4 the second time, and then 1-4 the third time… so possibilities=marbles ^ draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) %&gt;% knitr::kable() draw marbles possibilities 1 4 4 2 4 16 3 4 64 Next, there is a huge amount of code explaining how to make the ‘garden of forking paths’ diagrams. I’m basically going to skip all that code, and paste in a few images. You can find all the code HERE Suppose there is only one blue ball and three white balls, possibility ‘2’ above. For this world, we see the full ‘garden of forking paths’ — the number of ways to select 1, 2, and 3 balls (with replacement) — below. Every path starting from the center is a possible (sequence of) draws. Figure 13.1: All possible draws of three balls Now the inferential exercise: we want to know (the likelihood) of each of the five possible ‘worlds’. As we draw data we know we are proceeding along one of some subset of the forking paths. For example, under possible world 2, if we draw Blue, then White, then Blue, this could have occured with any of the following paths (consider a draw of each of the white balls as distinct): Figure 13.2: All possible draws of three balls We see that under World 2 there are 3 ways of getting this sequence. 3 out of \\(4^3\\) equally likely paths under World 2, or a \\(3/64\\) chance (about 5%). We can do similar for the other possible worlds; multiplying the ‘ways to produce each draw’ in the path yields the ‘total ways to produce the path’, under each world. # if we make two custom functions, here, it will simplify the code within `mutate()`, below n_blue &lt;- function(x) { rowSums(x == &quot;b&quot;) } n_white &lt;- function(x) { rowSums(x == &quot;w&quot;) } t &lt;- # for the first four columns, `p_` indexes position tibble(p_1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), p_2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), p_3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), p_4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(`draw 1: blue` = n_blue(.), `draw 2: white` = n_white(.), `draw 3: blue` = n_blue(.)) %&gt;% mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 draw 1: blue draw 2: white draw 3: blue ways to produce w w w w 0 4 0 0 b w w w 1 3 1 3 b b w w 2 2 2 8 b b b w 3 1 3 9 b b b b 4 0 4 0 Among of all possible worlds, we see the most number of ways to get B-W-B in a world 4; with three blues and one white – here there are 9 ways in total to get B-W-B. Under world 4 this sequence occurs 9/64, or roughly 14% of the time. We can see this in the following plot. (We leave out the worlds with only one color ball, as these will have no paths that produce B-W-B). Below, each partitioned section represents one world, and the paths in that world that could produce B-W-B are shown. Figure 13.3: All possible draws of three balls Three paths, versus 8 paths, versus nine paths… Does this reveal world 4 to be the most likely contents of the present bag? Not necessarily. Suppose we knew ex-ante, from the factory, that ‘99 bags out of 100 have equal numbers of whites and blues.’ Then, it would be much more likely that this bag was from an equal-color bag (world 3), even though this draw is more likely conditional on the bag being from world 4. We need will to consider the base-rate probabilities as well. This in turn motivates the standard ‘false positive/false negative HIV test’ example. 13.2.3 Using prior information We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25) This seems to easy to be true, but our garden illustration helps us understand why it is the case. 13.2.3.1 “Multiply in” new information First consider, what if we had another draw from the bag, how would this adjust the ‘number of paths’ for each world? Remember, each draw is independent (replacement). We simply record the number of ways (permutations) that could lead to this draw in each world, and we multiply the previous count by this number. You can consider this visually in seeing how ‘adding an additional fork at the end of each path’ changes the count. This is given in the table below: t &lt;- t %&gt;% rename(`previous counts` = `ways to produce`, `ways to produce` = `draw 1: blue`) %&gt;% select(p_1:p_4, `ways to produce`, `previous counts`) %&gt;% mutate(`new count` = `ways to produce` * `previous counts`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 ways to produce previous counts new count w w w w 0 0 0 b w w w 1 3 3 b b w w 2 8 16 b b b w 3 9 27 b b b b 4 0 0 How to incorporate prior information about the probability of each world? Suppose your friend in the factory tells you (reliably) that ’we produce 3 bags with (just) 1 blue for every 2 bags with equal counts, for every 1 bag with 3 blues. We can think of the ‘factory choosing which bag to produce’ as another draw, thus another path. Here the sequence in which the information is recieved shouldn’t matter. The draws are independent (we presume). We can thus multiply the number of paths for each marbles-in-bag world by the (relative) frequency with which the factory ‘draws’ that bag… as shown below: t &lt;- t %&gt;% select(p_1:p_4, `new count`) %&gt;% rename(`prior count` = `new count`) %&gt;% mutate(`factory count` = c(0, 3:0)) %&gt;% mutate(`new count` = `prior count` * `factory count`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 prior count factory count new count w w w w 0 0 0 b w w w 3 3 9 b b w w 16 2 32 b b b w 27 1 27 b b b b 0 0 0 13.2.4 From counts to probability. 13.3 Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep” Content from notes from this lecture 13.3.1 Why and when use Bayesian (MCMC) methods? 13.3.1.1 Pros No need for asymptotics … good when sample sizes are small Incorporate previous information You can consider the ‘robustness to other priors’ Fit complex nonstandard models … e.g., with difficult functional forms or likelihood settings (more computation, less thinking) Easy to make predictions (e.g., simulate scenarios) after estimation Incorporate evidence, results, expert judgement (‘restrictions’ with some lee-way?) (ISn’t this the same as number 2?) Cleaner treatment/imputation of missing values … these are just parameters 13.3.1.2 Cons Must specify prior distributions … allows subjective judgement Different way of thinking about stats and inference; probability distributions and simulations, not much about p-values, point estimates and standard errors … path dependence Computational cost This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 13.3.1.3 Why more popular today? Starting from around 2005 in Political Science and Sociology Computational revolution comes from Markov chain Monte Carlo (MCMC) methods … don’t need analytical solutions Software implementations – many in R, specialised software like EWinBugs, JAGS, STAN; also increasingly in Stata 13.3.2 Theory Bayes theorem … inverting conditional probability thing … ‘inversion’ to make inferences about the parameters In Bayesian stats the parameters (and sometimes missing values) are random variables, we make probability statements about them \\[P(A|B)=P(B|A)P(A)/P(B)\\] Frequentist: Point estimates, unknown fixed parameters, data from a hyol repeataable random sample Bayesian: Fixed data (from the experiment), parameters are random variables … results based on probability distributions about rthese Classical statistics: likelihood of data given parameter: \\(p(y|\\theta)\\) Bayes we want, \\(p(\\theta|y) = p(y|\\theta)p(\\theta)/p(y)\\) \\(p(y)\\) is a ‘constant’ in our estimation … the data is fixed. So it’s proportional to \\(p(\\theta|y) = p(y|\\theta)\\times p(\\theta)\\) \\(p(y|\\theta)\\) is what we max when we do ML $ p()$: prior distribution capturing beliefs about \\(\\theta\\) 13.3.2.1 So how do we estimate it? Specify a probability model, a distribution for Y (likelihood function) and the priors for \\(\\theta\\) Solve (find) the posterior distribution \\(p(\\theta|Y)\\) and summarise the parameters of interest In practice, step 2 is usually done via MCMC simulation rather than analytically. … via simulations, I approach the ‘true’ value on \\(\\theta\\) (Given ‘regularity conditions’) 13.3.2.2 Linear regression model example \\[Y = x&#39;\\beta+\\epsilon\\] with n obs only random term is epsilon … natural candidate is a normal distribution, so \\(Y \\sim N(x&#39;\\beta,\\sigma^2_e)\\) So we want to find \\(p(\\beta, \\sigma^2_\\epsilon|Y,X)\\). This depends on the choices of \\(p(\\beta)\\) and \\(p(\\epsilon)\\). Could choose conjugate priors, leading to a particular joint posterior, you can solve it analytically. Can yield a joint posterior. Instead, let’s assume that the latter (variance) parameter is known, you can show that the posterior for \\(\\beta\\) is also normally distributed. (Conjugate) Similarly, if we assume \\(\\beta\\) is known, if the variance term had an inverse gamma distribution (prior), so will the posterior. In these conjugate priors, the posterior mean will be a weighted average of the priors and the data. 13.3.2.3 Gibbs Needs closed form conditional posterior for every parameter. What Gibbs sampler does is break the parameter space into sets of parameters Choose starting values, \\(\\theta^0_1,...\\theta^0_k\\) sample from the first parameter’s distribution given the others … the second one, … the k’th one . Repeat step 2 … thousands of times (starting with the parameters from the previous iteration) Eventually ‘we obtain samples of \\(p(\\theta|y)\\)’ But if we don’t have a closed form, we cannot simply sample from known distributions in each step E.g., in case of Logit distribution. 13.3.2.4 Metropolis Hastings Choose ‘proposal distribution’ to sample parameter values (a candidate like normal, uniform) Start w a prelim guess for parameter values \\(\\theta_0\\) At iteration t sample a proposal \\(\\theta_t\\) from \\(p(\\theta_t|\\theta_{t-1})\\) ?? what does this come from? If \\(p(\\theta_t|y)&gt;p(\\theta_{t-1}|y)\\) accept it as the new value of \\(\\theta\\). ??? how is this computed if we don’t have conjugate closed-form posteriors? Otherwise flip a coin with probability r = (ratio of those probabilities) if coin tosses heads, accept as new theta, otherwise stay at previous theta allows algorithm to avoid getting stuck at local maxima Commonly used proposal: random walk sample: \\(\\theta_t=\\theta_{t-1}+z_t\\), \\(z_t \\sim f\\) ?? I do this because there is no analytical way to derive this, unlike in the conjugate case, where we might use the Gibbs can combine Gibbs with Metropolis steps; relevant to some problems 13.3.2.5 Assessing convergence previous … ‘eyeballing’ formal: single-chain tests (Geweke/Heidel) … is the last part of the chain stable (stationary)… compare simulation at middle and end, is there much variation? multiple-chain test… (starting from different values), do they end similar … Gelman-Rubin diagnosting \\(\\hat{R}\\) typically either a very long chain and use GH convergence, or multiple shorter chains and use \\(\\hat{R}\\) Gabriel: Gelman-Rubin is probably preferred; more conservative ?? What am I iterating towards? Converging on what? 13.3.2.6 Assesing ‘fit’ in Bayesian No r-squared Typical measure is ‘posterior predictive comparisons’ \\(p(y_{replicated}|y_{observed}= ...\\) Simulate data from estimated parameters Compare to observed data Use an overall fit measure to assess model fit E.g., percent correct predictions (binary), whether the true data is within the 95% CI of the replicates, deviance For each replicate Choose statistic D, compare the replicated \\(D(y_s_{replicated})\\) against $D(y_s_{observed}) Quantify the discrepancy … percent of correct predictions, proportion of times replicated y is below true y … compute ‘bayesian p-value’s’ Systematic differences between replicate and actual data indicate model limitations (?? what are reasonable values here??) 13.3.3 Comparing models … Equivalent of ‘likelihood’ ‘Deviance Information Criterion’ (most used); specific for MCMC simulations: compares expected LL of the model (of the data given the estimated parameters; average here across much of the later points in the chain) against the llhd at the posterior parameter mean. Always select model with lowest DIC. Bayes Factor (less used): Ratio of llhd of the models; higher BF means model is more supported; BF&gt;10 seen to provide strong evidence for model w higher value 13.3.4 On choosing priors Most social scientists use non-informative or vague priors; i.e., large variance… e.g., \\(\\beta \\sim N(0,1000)\\) But its often useful to incorporate information into your priors Small pilot to test, \\(\\rightarrow\\) data \\(Y_1\\), another study gives data \\(Y_2\\); repeated application of Bayes theorem gives the posterior. Same result whether you obtained these together, or whether you did one and then updated (e.g., via an MCMC, starting with the first one as a prior) Conjugate priors (mentioned before) Jeffrey’s priors (??) 13.3.5 Implementation If you don’t need to do fancy things, and don’t want to (?) generate the full posterior distribution (or something) Some Stata/R commands that make Bayesian look frequentist. In Jags and Winbugs, we only have to specify the prior… rest is done for us Jags is great … you only need to do self-coding with lots of data and super complicated models as it can freeze up We went through it the fancy way in Probit.R Then the easy way with ‘script probit Jags.R’ 13.3.6 Generate predictions from a WinBUGS model You can just generate these outcomes … Prediction: generate a new observation #note, he is doing one per iteration, but since these are convergent it would be basically the same if you just chose a random iteration and did all the draws from that one 13.3.7 Missing data case One solution – multiple imputation choose imputation model to predict missings, generate many copies of orig data set, imputing missibg value for each 2 more steps here Need a model for X|alpha, because missing variables are random variables 13.3.8 Stata Has some rather simple implementations; e.g., just using commands like bayes: regress y x 13.3.9 R mcmc pac Also simple code; great for standard use Speedup with parallelization; see “script for parallel probit.R” and “parallelprobit.R” More advanced: C++; can integrate it with Rcpp, or even use Exeter’s ISCA cluster summary(cars) ## speed dist ## Min. : 4.0 Min. : 2 ## 1st Qu.:12.0 1st Qu.: 26 ## Median :15.0 Median : 36 ## Mean :15.4 Mean : 43 ## 3rd Qu.:19.0 3rd Qu.: 56 ## Max. :25.0 Max. :120 13.4 Other resources and notes to integrate Hey stats twitter: got a very sharp psych UG student wanting to dive into Bayes. Many resources are too technical (i.e., not good teaching texts for UG level, but useful references). Where should I point her? — Tom Carpenter ((???)) February 1, 2020 Angrist J. D., and J S Pischke. 2008. “Mostly Harmless Econometrics : An Empiricist ’ S Companion.” Massachusettts I Nstitute of Technology and the London School of Economics, no. March: 290. https://doi.org/10.1017/CBO9781107415324.004. Gentzkow, Matthew. 2013. “Code and Data for the Social Sciences : A Practitioner ’ S Guide.” Heckman, James, Rodrigo Pinto, and James Heckman. 2013. “Econometric Mediation Analyses : Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs,” no. 7552. Kennedy, Peter. 2003. A Guide to Econometrics. MIT press. Tibshirani, Robert. n.d. “Statistical Learning with Sparsity the Lasso and Generalizations.” Wooldridge, Jeffrey M. 2002. Econometric Analysis of Cross Section and Panel Data. 2. The MIT press. https://doi.org/10.1515/humr.2003.021. Wooldridge, J M. 2008. Introductory Econometrics: A Modern Approach. South-Western Pub. "]
]
