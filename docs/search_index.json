[
["introduction.html", "Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus 1 Introduction 1.1 Conceptual 1.2 Getting, cleaning and using data 1.3 Basic regression and statistical inference: Common mistakes and issues 1.4 LDV and discrete choice modeling 1.5 Robustness and diagnostics, with integrity 1.6 Control strategies and prediction; Machine Learning approaches 1.7 IV and its many issues 1.8 Causal pathways: Mediation modeling and its massive limitations 1.9 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 1.10 Other paths to observational identification 1.11 (Ex-ante) Power calculations 1.12 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 1.13 (Experimental) Study design: Background and quantitative issues 1.14 ‘Experimetrics’ and measurement of treatment effects from RCTs 1.15 The Bayesian approach 1.16 Making inferences from previous work; Meta-analysis, combining studies 1.17 Some key resources and references", " Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioral, and Experimental focus Dr. David Reinstein, 2020-04-25 Abstract This ‘book’ organizes my notes and helps others understand it and learn from it 1 Introduction Focus on the practical tools I use and the challenges I (David Reinstein) face Microeconomics, behavioral economics, focus on charitable giving and ‘returns to education’ type of straightforward problems. (Limited to no focus on structural approaches.) Where we can add value to real econometric practice?? Data: Observational (esp. web-scraped and API data and national surveys/admin data} Experimental: esp. where with multiple crossed arms, and where the ‘cleanest design’ may not be possible Assume familiarity with most basic statistical concepts like ‘bias’, ‘consistency’, and ‘null hypothesis testing.’ However, I will focus on some concepts that seem to often be misunderstood and mis-applied. 1.1 Conceptual 1.1.1 Bayesian vs. frequentist approaches Folder: bayesian Notes: bayes_notes 1.1.2 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 1.1.2.1 DAGs and Potential outcomes 1.1.3 Theory, restrictions, and ‘structural vs reduced form’ 1.2 Getting, cleaning and using data This will build on my content here, and integrate with it. 1.2.1 Data: What/why/where/how 1.2.2 Good coding practices 1.2.2.1 Organizing a project 1.2.2.2 Dynamic documents (esp Rmd/bookdown) 1.2.3 New tools and approaches to data (esp ‘tidyverse’) 1.2.3.1 Style and consistency Indenting, snake-case, etc 1.2.3.2 Using functions, variable lists, etc., for clean, concise, readable code 1.2.4 Data sharing and integrity 1.3 Basic regression and statistical inference: Common mistakes and issues 1.3.1 “Bad control” (“colliders”) Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita) 1.3.2 Choices of lhs and rhs variables Missing data Choice of control variables and interactions Which outcome variable/variables 1.3.3 Functional form Logs and exponentials Nonlinear modeling (and interpreting coefficients) 1.3.3.1 ‘Testing for nonlinear terms’ Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18 http://datacolada.org/62 1.3.4 OLS and heterogeneity OLS does not identify the ATE http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT Modeling heterogeneity: the limits of Quantile re regression 1.3.5 “Null effects” “While the classical statistical framework is not terribly clear about when one should”“accept”\" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a \"“zero or minimal effect”\" one would want to find these closely bounded around zero under a variety of conditions for generalizability. In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks”.\" Harms-lakens-18 1.3.5.1 Confidence intervals and Bayesian credible intervals 1.3.5.2 Comparing relative parameters E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men”. This doesn’t cut it: we need to see a statistical test for the difference in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing). 1.3.6 Multiple hypothesis testing (MHT) See (???) 1.3.7 Interaction terms and pitfalls 1.3.7.1 ‘Moderators’ Confusion with nonlinearity Moderators: Heterogeneity mixed with nonlinearity/corners In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in’. Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative 1.3.7.2 MHT 1.3.8 Choice of test statistics (including nonparametric) 1.3.9 How to display and write about regression results and tests 1.3.10 Bayesian interpretations of results (see ‘the Bayesian Approach’) 1.4 LDV and discrete choice modeling 1.5 Robustness and diagnostics, with integrity 1.5.1 (How) can diagnostic tests make sense? Where is the burden of proof? 1.5.2 Estimating standard errors 1.5.3 Sensitivity analysis: Interactive presentation 1.6 Control strategies and prediction; Machine Learning approaches 1.6.1 Machine Learning (statistical learning): Lasso, Ridge, and more 1.6.2 Limitations to inference from learning approaches 1.7 IV and its many issues 1.7.1 Instrument validity Exogeneity vs. exclusion Very hard to ‘powerfully test’ 1.7.2 Heterogeneity and LATE Basic consideration: what does IV identify and when:? Focusing on a binary endogenous ‘treatment’ variable With heterogeneity With imperfect compliance With one-way compliance 1.7.3 Weak instruments, other issues With a ‘weak instrument’ … why does that matter? 1.7.4 Reference to the use of IV in experiments/mediation 1.8 Causal pathways: Mediation modeling and its massive limitations An applied review 1.9 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 1.9.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ “Conditional on positive”/“intensive margin” analysis ignores selection “Conditional on positive”/“intensive margin” analysis ignores selection identification issue See Angrist and Pischke on “Good CoP, bad CoP”. See also bounding approaches such as (???) AngristJ.D.2008a, 1.9.1.1 Bounding approaches (Lee, Manski, etc) See Notes on Lee bounds 1.10 Other paths to observational identification 1.10.1 Fixed effects and differencing 1.10.2 DiD FE/DiD does not rule out a correlated dynamic unobservable, causing a bias 1.10.3 RD 1.10.4 Time-series-ish panel approaches to micro 1.10.4.1 Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ 1.11 (Ex-ante) Power calculations 1.11.1 What sort of ‘power calculations’ make sense, and what is the point? 1.11.1.1 The ‘harm to science’ from running underpowered studies \"One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results. Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.\" verkaik2016, metzger2015 1.11.2 Power calculations without real data 1.11.3 Power calculations using prior data 1.12 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 1.12.1 Why run an experiment or study? Sugden and Sitzia critique here, give more motivation 1.12.2 Causal channels and identification Ruling out alternative hypotheses, etc 1.12.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 1.12.4 Generalizability (and heterogeneity) 1.13 (Experimental) Study design: Background and quantitative issues 1.13.1 Pre-registration and Pre-analysis plans 1.13.1.1 The hazards of specification-searching 1.13.2 Sequential and adaptive designs Needs to adjust significance tests for augmenting data/sequential analysis/peeking Statistics/econometrics new-statistics sagarin_2014 http://www.paugmented.com/ resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/ Yet … P_augmented may overstate type-1 error rate Statistics/econometrics response to referees, new-statistics \" A process involving stopping \"“whenever the nominal \\(p.0.5\\)”\" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5%. Even if the subsequent data suggested a \"“one in a million chance of arising under the null”\" the overall process yields a 5%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the \"“likelihood of the null hypothesis”\" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate. Considering the calculations in , it is clear that \\(p_{augmented}\\) should the type-1 error of the process if there is a positive probability that after an initial experiment attains p\\(&lt;0.05\\), more data is collected. A headline \\(p&lt;0.05\\) does imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded \\(p&gt;0.05\\), thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though \\(p&lt;0.05\\). Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine’s experiment). Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p&lt;0.05, more data is collected, implying that \\(p_{augmented}\\) as calculated above overstates the type I error rate that would arise from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below \\(p=0.05\\), in order to attain a type-1 error rate of 5% or less in our published corpus.\" 1.13.3 Efficient assignment of treatments (Links back to power analyses) 1.14 ‘Experimetrics’ and measurement of treatment effects from RCTs 1.14.1 Which error structure? Random effects? 1.14.2 Randomization inference? 1.14.3 Parametric and nonparametric tests of simple hypotheses 1.14.4 Adjustments for exogenous (but non-random) treatment assignment 1.14.5 IV in an experimental context to get at ‘mediators’? 1.14.6 Heterogeneity in an experimental context 1.15 The Bayesian approach 1.16 Making inferences from previous work; Meta-analysis, combining studies 1.16.1 Publication bias 1.16.2 Combining a few (your own) studies/estimates 1.16.3 Full meta-analyses Models to address publication biases 1.17 Some key resources and references (Angrist J. D. and Pischke 2008) ‘The Mixtape’ (Cunningham) (Kennedy 2003) (Tibshirani, n.d.) OSF guides Christensen ea “Transparent and Reproducable Social Science Research” (Gentzkow 2013; Wooldridge 2002, 2008) An Introduction to Statistical Learning with Applications in R R for Data Science Garrett Grolemund Hadley Wickhamr r4ds.org Statistical Rethinking: A Bayesian Course with Examples in R and Stan 1.17.1 Consider: Paul R. Rosenbaum. Observation and Experiment: An Introduction to Causal Inference . Harvard University Press, 2017 Guido Imbens and Donald Rubin. Causal Inference for Statistics, Social and Biomedical Sciences: An Introduction . Cambridge University Press, 2015 Judea Pearl Imbens: Potential Outcomes versus DAGs References "],
["reg-follies.html", "2 Basic regression and statistical inference: Common mistakes and issues 2.1 Basic regression and statistical inference: Common mistakes and issues briefly listed 2.2 Bad control", " 2 Basic regression and statistical inference: Common mistakes and issues 2.1 Basic regression and statistical inference: Common mistakes and issues briefly listed Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Identification Random effects estimators show a lack of robustness Specification Clustering SE is more standard practice OLS/IV estimators not ‘mean effect’ in presence of heterogeneity Power calculations/underpowered Selection bias due to attrition Selection bias due to missing variables – impute these as a solution Signs of p-hacking and specification-hunting Weak diagnostic/identification tests Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness With heterogeneity the simple OLS estimator is not the ‘mean effect’ P_augmented may overstate type-1 error rate Impact size from regression of “log 1+gift amount” Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Weak IV bias Bias from selecting instruments and estimating using the same data 2.2 Bad control From MEH: some variables are bad controls and should not be included in a regression model even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notational experiment at hand. That is, bad controls might just as well be dependent variables too.\" – They could also be interpreted as endogenous variables. Example of looking at a regression of wages in schooling, controlling for college degree completion: Once we acknowledge the fact that college affects occupation, comparison of wages by college degree status within occupation are no longer apples to apples, even if college degree completion is randomly assigned.\" – The question here was whether to control for the category of occupation, not the college degree. It is also incorrect to say that the conditional comparison captures the part of the effect of college that is ‘not explained by occupation’ … so we would do better to control only for variables that are not themselves caused by education.\" 2.2.1 “Bad control” (“colliders”) Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita) 2.2.2 Choices of lhs and rhs variables Missing data Choice of control variables and interactions Which outcome variable/variables 2.2.3 Functional form Logs and exponentials Nonlinear modeling (and interpreting coefficients) 2.2.3.1 ‘Testing for nonlinear terms’ Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18 http://datacolada.org/62 2.2.4 OLS and heterogeneity OLS does not identify the ATE http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT Modeling heterogeneity: the limits of Quantile re regression 2.2.5 “Null effects” “While the classical statistical framework is not terribly clear about when one should”“accept”\" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a \"“zero or minimal effect”\" one would want to find these closely bounded around zero under a variety of conditions for generalizability. In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks”.\" Harms-lakens-18 2.2.5.1 Confidence intervals and Bayesian credible intervals 2.2.5.2 Comparing relative parameters E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men”. This doesn’t cut it: we need to see a statistical test for the difference in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing). 2.2.6 Multiple hypothesis testing (MHT) See (???) 2.2.7 Interaction terms and pitfalls 2.2.7.1 ‘Moderators’ Confusion with nonlinearity Moderators: Heterogeneity mixed with nonlinearity/corners In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in’. Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative 2.2.7.2 MHT 2.2.8 Choice of test statistics (including nonparametric) (Or get to this in the experimetrics section) 2.2.9 How to display and write about regression results and tests 2.2.10 Bayesian interpretations of results "],
["robust-diag.html", "3 Robustness and diagnostics, with integrity 3.1 (How) can diagnostic tests make sense? Where is the burden of proof? 3.2 Estimating standard errors 3.3 Sensitivity analysis: Interactive presentation", " 3 Robustness and diagnostics, with integrity 3.1 (How) can diagnostic tests make sense? Where is the burden of proof? Where a particular assumption is critical to identification and inference …Failure to reject the violation of an assumptionis not sufficient to give us confidence that it is satisfied and the results are credible. At several points the authors cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles). 3.2 Estimating standard errors 3.3 Sensitivity analysis: Interactive presentation "],
["control-ml.html", "4 Control strategies and prediction, Machine Learning (Statistical Learning) approaches 4.1 Machine Learning (statistical learning): Lasso, Ridge, and more 4.2 Notes Hastie: Statistical Learning with Sparsity 4.3 Notes: Mullainathan", " 4 Control strategies and prediction, Machine Learning (Statistical Learning) approaches ‘Identification’ of causal effects with a control strategy not credible Essentially a ‘control strategy’ is “control for all or most of the reasonable determinants of the independent variable so as to make the remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest”. All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., (???). 4.1 Machine Learning (statistical learning): Lasso, Ridge, and more 4.1.1 Limitations to inference from learning approaches 4.2 Notes Hastie: Statistical Learning with Sparsity google books link 4.2.1 Introduction One form of simplicity is sparsity, the central theme of this book. Loosely speaking, a sparse statistical model is one in which only a relatively small number of parameters (or predictors) play an important role. “the \\(\\ell_1\\) norm is special” (abs value). Other norms yield nonconvex problems, hard to minimize. “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems. Examples from gene mapping 4.2.1.1 Book roadmap Chapter 2 … lasso for linear regression, and a simple coordinate descent algorithm for its computation. Chapter 3 application of \\(\\ell_1\\) [lasso-type] penalties to generalized linear models such as multinomial and survival models, as well as support vector machines. [?] Chapter 4: Generalized penalties such as the elastic net and group lasso are discussed in Chapter 4. Chapter 5: numerical methods for optimization (skip for now] Chapter 6: statistical inference for fitted (lasso) models, including the bootstrap, Bayesian methods and more recent stuff Chapter 7: Sparse matrix decomposition [?] (Skip?) Ch 8: sparse multivariate analysis of that (Skip?) Ch 9: Graphical models and their selection (Skip?) Ch 10: compressed sensing (Skip?) Ch 11: a survey of theoretical results for the lasso (Skip?) 4.2.2 Ch2: Lasso for linear models N samples (?N observations), want to approx the response variable using a linear combination of the predoctors OLS minimizes squared-error loss but Prediction accuracy OLS unbiased but ‘often has large variance’ prediction accuracy can be improved by shrinking coefficients (even to zero) yielding biased but perhaps better predictive estimators Interpretation: too many predictors hard to interpret DR: I do not care about this for fitting background noise in experiments 4.2.2.1 2.2 The Lasso Estimator Lasso bounds the sum of the abs values of coefficients, an \"$_1\" constraint. Lasso is OLS subject to \\(\\sum_{j=1..p}{\\abs(\\beta_j)}\\leq t\\) “compactly” \\(||\\beta||_1\\leq t\\) with notation for the “\\(\\ell_1\\) norm” Bound \\(t\\) acts as a ‘budget’, must be specified by an ‘external procedure’ such as cross-validation typically we must standardize the predictors $** so that each column is centered with unit variance … as well as the outcome variables (?) … can ignore intercept DR: Not clear here whether standardisation is necessary for the procedure to be valid or just convenient for explaining and deriving its properties. Aside: Can re-write Lasso minimization st constraint as a Lagrangian. \\(\\lambda\\) plays the same role as \\(t\\) in the constraint. Thus we can speak of the solution to the Lagrangian minimisation problem \\(\\hat{\\beta)_{\\lambda}\\) which also solves the bound problem with \\(t=||\\hat_{\\lambda}||_1\\). Aside: We often remove the \\(1/2n\\) term at the beginning of the minimization problem. Same minimization, minimizing sum of squared deviations rather than something like an average of this. Express (Karush-Kuhn-Tucker) optimisation conditions for this … Example from Thomas (1990) on crime data Typically … lasso is most useful for much larger problems, including “wide” data for which \\(p&gt;&gt;N\\) Fig 2.1: Lasso vs ridge regression; coefficients of each for a set of considered variables plotted against their respective norms (as shares of maximal bound on coefficient sum measure, i.e., ols, for each) Note ridge regression penalises squared sums of betas Fig 2.2., in \\(\\beta_1,\\beta_2\\) space illustrates the difference well: contour lines of Resid SS elipses, ‘budget constraint’ for each (disc vs diamond) (Note: lasso bound was chosen via cross-validation) No analytical statistical inference after lasso (some being developed?), bootstrap is common lasso sets two of the five coefficients to zero, and tends to shrink the coefficients of the others toward zero relative to the full least-squares estimate. DR: analytically and intuitively, I do not yet understand why lasso should shrink coefficients but not all the way to zero. The penalty is linear in the coefficient size, so I would think the solution would be bang-bang, either drop a coeficient or leave it unchanged. But it is not. Adding an increment to a \\(\\hat{\\beta}\\) when it is below the OLS estimate should have a linear effect on the RSS (according to my memory and according to Sebastian). But that would mean that shrinking one parameter always yields a better benefit to cost ratio. Thus I should shrink each parameter to zero before beginning to shrink any others. This cannot be right! I looked up this derivative wrt the beta vector (one needs to set this to 0 to get the ols estimates stackexchange \\(\\frac{d RSS}{d \\beta}=-2X^{T}(y-X\\beta}\\) or \\(−\\frac{d e&#39;e}{d b}=2X′y+2X′Xb\\) The answer to this question: while the impact of changing each coefficient on SSR is in fact constant (a constant own-derivative), there is also an impact of changing one coefficient on the other derivatives. As one coefficient shrinks to zero the marginal impact of the other coefficients on the SSR may (will?) increase. - At the same time, we need that the effect of increasing it from zero need not be infinite, so it might not outweigh the linear penalty, thus some coefficients might be set to zero Relaxed lasso the least-squares fit on the subset of the three predictors tends to expand the lasso estimates away from zero. The nonzero estimates from the lasso tend to be biased toward zero, so the debiasing in the right panel can often improve the prediction error of the model. This two-stage process is also known as the relaxed lasso (Meinshausen 2007). DR: When is this likely to help/hurt relative to pure lasso? Stackexchange discussion Contrasts a ‘relaxed-lasso’ from a ‘lars-ols’ Aside: which seems better for Control variable selection for prediction/reducing noise to enable better inference of treatment effects? Ridge? better than Lasso here? We do not care about interpreting the predictors here… so if we allow \\(\\beta\\)‘s to be shrunk towards zero for each coefficient maybe that should yield better prediction than making them exactly zero? On the other hand if we know the true model is ‘parsimonious’ (as in the genes problem) it might boost efficiency to allow inference about coefficients that should be exactly zero (edited) 4.2.2.2 2.3 Cross-Validation and Inference Generalization ability accuracy for predicting independent test data from the same population … find the value of t that does best **Cross-validation procedure* randomly divide … dataset into K groups. “Typical choices … might be 5 or 10, and sometimes N.” One ‘test’, remaining K-1 ‘training’ Apply lasso to training data for a range of t values, use each fitted model to predict the responses in the test set, recording mean-squared prediction errors for each value of t. Repeat the previous step K times each time, one of the K groups is the test data, remaining K − 1 are training data. yields K different estimates of the prediction error over a range of t values. Average K estimates of prediction error for each value of t \\(\\rightarrow\\) cross-validation error curve. Fig 2.3 plots an example with K=10 splits for cross validation … of the estimated MS prediction error vs the relative bound \\(\\tilde{t}\\)(summed absolute value of Lasso betas divided by summed abs value of OLS betas). Also draw dotted line at the 1-std-error rule choice of \\(\\tilde{t\\)} Number of nonzero coefficients plotted at top 4.2.2.3 2.4 Computation of the Lasso solution DR: I think I will skip this for now least angle/LARS is mentioned at the bottom as a ‘homotopy method’ which “produce the entire path of solutions in a sequential fashion, starting at zero” 4.2.2.4 2.5 Degrees of freedom … Jumping to 4.2.2.5 2.10 Some perspective Good properties of the Lasso (\\(\\ell_1\\) penalty) Natural interpretation (enforce sparsity and simplicity) Statistical efficiency … if the underlying true signal is sparse (but if it is not sparse “no method can do well relative to the Bayes error”) Computational efficiency, as \\(\\ell_1\\) penalties are convex 4.2.3 Chapter 3: Generalized linear models 4.2.4 Chapter 4: Generalizations of the Lasso penalty lasso does not handle highly correlated variables very well; the coefficient paths tend to be erratic and can sometimes show wild behavior. The elastic net makes a compromise between the ridge and the lasso penalties (Zou and Hastie 2005)1] is a parameter that can be varied. For an individual coefficient the penalty is \\(\\frac{1}{2} (1-\\alpha)\\beta_j^2 + \\alpha|\\beta_j|\\) (a convex combo of the lasso and ridge penalties) multiplied by a ‘regularization weight’ \\(\\lambda&gt;0\\) which plays the same role (I think) as in lasso elastic net is also strictly convex 4.3 Notes: Mullainathan Page 1 The fundamental insight behind these breakthroughs is as much statistical as computational. Machine intelligence became possible once researchers stopped approaching intelligence tasks procedurally and began tackling them empirically. Face recognition algorithms, for example, do not consist of hard-wired rules to scan for certain pixel combinations, based on human understanding of what constitutes a face. Instead, these algorithms use a large dataset of photos labeled as having a face or not to estimate a function f (x) that predicts the presence y of a face from pixels x (p2) supervised- machine learning, the focus of this article) revolves around the problem of prediction: produce predictions of y from x (p2) manages to fit complex and very flexible functional forms to the data without simply overfitting; it finds functions that work well out-of-sample (p2) danger in using these tools is taking an algorithm built for y-, and presuming their β - have the properties we typically associate with estimation output (p2) One category of such applications appears when using new kinds of data for traditional questions; for example, in measuring economic activity using satellite images or in classifying industries using corporate 10-K filings. Making sense of complex data such as images and text often involves a prediction pre-processing step (p2) -this is the most relevant for me In another category of applications, the key object of interest is actually a parameter -, but the inference procedures (often implicitly) contain a prediction task. For example, the first stage of a linear instrumental variables regression is effectively prediction. The same is true when estimating heterogeneous treatment effects, testing for effects on multiple outcomes in experiments, and flexibly controlling for observed confounders (p2) A final category is in direct policy applications. Deciding which teacher to hire implicitly involves a prediction task (what added value will a given teacher have?), one that is intimately tied to the causal question of the value of an additional teacher. (p3) -useful example… interactive? We consider 10,000 randomly selected owner-occupied units from the 2011 metropolitan sample of the American Housing Survey. In addition to the values of each unit, we also include 150 variables that contain information about the unit and its location, such as the number of rooms, the base area, and the census region within the United States. To compare different prediction techniques, we evaluate how well each approach predicts (log) unit value on a separate hold-out set of 41,808 units from the same sample. All details on the sample and our empirical exercise can be found in an online appendix available with this paper at http://e-jep.org (p3) In-sample performance may overstate performance; this is especially true for certain machine learning algorithms like random forests that have a strong tendency to overfit. Second, on out-of-sample performance, machine learning algorithms such as random forests can do significantly better than ordinary least squares, even at moderate sample sizes and with a limited number of covariates (p4) algorithms are fitted on the same, randomly drawn training sample of 10,000 units and evaluated on the 41,808 remaining held-out units. The (p4) Simply including all pairwise interactions would be infeasible as it produces more regressors than data points (especially considering that some variables are categorical (p4) Machine learning searches for these interactions automatically (p5) Shallow Regression Tree Predicting House Values (p5) -not sure what’s going on here. is this the random forest thing? The prediction function takes the form of a tree that splits in two at every node. At each node of the tree, the value of a single variable (say, number of bathrooms) determines whether the left (less than two bathrooms) or the right (two or more) child node is considered next. When a terminal node-a leaf—is reached, a prediction is returned. An (p5) So how does machine learning manage to do out-of-sample prediction? The first part of the solution is regularization. In the tree case, instead of choosing the -best” overall tree, we could choose the best tree among those of a certain depth. (p5) Tree depth is an example of a regularizer. It measures the complexity of a function. As we regularize less, we do a better job at approximating the in-sample variation, but for the same reason, the wedge between in-sample and out-of-sample (p6) how do we choose the level of regularization (-tune the algorithm”)? This is the second key insight: empirical tuning. (p6) -tuning within the training sample In empirical tuning, we create an out-of-sample experiment inside the original sample. We fit on one part of the data and ask which level of regularization leads to the best performance on the other part of the data.4 We can increase the efficiency of this procedure through cross-validation: we randomly partition the sample into equally sized subsamples (-folds”). The estimation process then involves successively holding out one of the folds for evaluation while fitting the prediction function for a range of regularization parameters on all remaining folds. Finally, we pick the parameter with the best estimated average performance.5 The (p6) -! This procedure works because prediction quality is observable: both predictions y- and outcomes y are observed. Contrast this with parameter estimation, where typically we must rely on assumptions about the data-generating process to ensure consistency (p7) Some Machine Learning Algorithms Function class - (and its parametrization) Regularizer R( f ) Global/parametric predictors Linear -′x (and generalizations) Subset selection| (p7) -very useful table Some Machine Learning Algorithms Function class - (and its parametrization) Regularizer R( f ) Global/parametric predictors Linear -′x (and generalizations) Subset selection||β| (p7) Random forest (linear combination of trees (p7) -kernel in an ml framework! Kernel regression (p6) -but can we make inferences about the structure? hypothesis testing? Regularization combines with the observability of prediction quality to allow us to fit flexible functional forms and still find generalizable structure. (p7) Picking the prediction function then involves two steps: The first step is, conditional on a level of complexity, to pick the best in-sample loss-minimizing function.8 The second step is to estimate the optimal level of complexity using empirical tuning (as we saw in cross-validating the depth of the tree). (p8) -but they forgot to mention that others are shrunk linear regression in which only a small number of predictors from all possible variables are chosen to have nonzero values: the absolute-value regularizer encourages a coefficient vector where many are exactly zero. (p4) -why no ridge or elastic net? LASSO (p8) -ensembles usually win contests While it may be unsurprising that such ensembles perform well on average- after all, they can cover a wider array of functional forms-it may be more surprising that they come on top in virtually every prediction competition (p8) -neural nets broadly explained neural nets are popular prediction algorithms for image recognition tasks. For one standard implementation in binary prediction, the underlying function class is that of nested logistic regressions: The final prediction is a logistic transformation of a linear combination of variables (-neurons”) that are themselves such logistic transformations, creating a layered hierarchy of logit regressions. The complexity of these functions is controlled by the number of layers, the number of neurons per layer, and their connectivity (that is, how many variables from one level enter each logistic regression on the next) (p9) These choices about how to represent the features will interact with the regularizer and function class: A linear model can reproduce the log base area per room from log base area and log room number easily, while a regression tree would require many splits to do so. (p9) In a traditional estimator, replacing one set of variables by a set of transformed variables from which it could be reconstructed would not change the predictions, because the set of functions being chosen from has not changed. But with regularization, including these variables can improve predictions because-at any given level of regularization-the set of functions might change (p9) -!! Economic theory and content expertise play a crucial role in guiding where the algorithm looks for structure first. This is the sense in which -simply throw it all in- is an unreasonable way to understand or run these machine learning algorithms (p9) -I need hear of using adjusted r square for this Should out-ofsample performance be estimated using some known correction for overfitting (such as an adjusted R2 when it is available) or using cross-validation (p9) -big unknowns available finite-sample guidance on its implementation-such as heuristics for the number of folds (usually five to ten) or the -one standard-error rule” for tuning the LASSO (Hastie, Tibshirani, and Friedman 2009)-has a more ad-hoc flavor (p9) firewall principle: none of the data involved in fitting the prediction function-which includes crossvalidation to tune the algorithm—is used to evaluate the prediction function that is produced (p10) -how? First, econometrics can guide design choices, such as the number of folds or the function class (p10) with the fitted function. Why not also use it to learn something about the -underlying model (p10) -!! the lack of standard errors on the coefficients. Even when machine-learning predictors produce familiar output like linear functions, forming these standard errors can be more complicated than seems at first glance as they would have to account for the model selection itself. In fact, Leeb and P-tscher (2006, 2008) develop conditions under which it is impossible to obtain (uniformly) consistent estimates of the distribution of model parameters after data-driven selection (p11) -lasso chosen variables are unstable because of multicollinearity. a problem for making inferences from estimated coefficients the variables are correlated with each other (say the number of rooms of a house and its square-footage), then such variables are substitutes in predicting house prices. Similar predictions can be produced using very different variables. Which variables are actually chosen depends on the specific finite sample (p11) this creates an Achilles- heel: more functions mean a greater chance that two functions with very different (p12) coefficients can produce similar prediction quality (p12) In econometric terms, while the lack of standard errors illustrates the limitations to making inference after model selection, the challenge here is (uniform) model selection consistency itself (p12) -is this equally a problem for non sparsity based procedures like ridge? First, it encourages the choice of less complex, but wrong models. Even if the best model uses interactions of number of bathrooms with number of rooms, regularization may lead to a choice of a simpler (but worse) model that uses only number of fireplaces. Second, it can bring with it a cousin of omitted variable bias, where we are typically concerned with correlations between observed variables and unobserved ones. Here, when regularization excludes some variables, even a correlation between observed variables and other observed (but excluded) ones can create bias in the estimated coefficients (p12) Some econometric results also show the converse: when there is structure, it will be recovered at least asymptotically (for example, for prediction consistency of LASSO-type estimators in an approximately sparse linear framework, see Belloni, Chernozhukov, and Hansen 2011). (p12) -unrealistic for micro economic applications Zhao and Yu (2006) who establish asymptotic model-selection consistency for the LASSO. Besides assuming that the true model is -sparse”—only a few variables are relevant-they also require the “irrepresentable condition” between observables: loosely put, none of the irrelevant covariates can be even moderately related to the set of relevant ones. In practice, these assumptions are strong. (p13) Machine learning can deal with unconventional data that is too high-dimensional for standard estimation methods, including image and language information that we conventionally had not even thought of as data we can work with, let alone include in a regression (p13) satellite data (p13) they provide us with a large x vector of image-based data; these images are then matched (in what we hope is a representative sample) to yield data which form the y variable. This translation of satellite images to yield measures is a prediction problem (p13) particularly relevant where reliable data on economic outcomes are missing, such as in tracking and targeting poverty in developing countries (Blumenstock 2016 (p13) cell-phone data to measure wealth (p13) Google Street View to measure block-level income in New York City and Boston (p13) online posts can be made meaningful by labeling them with machine learning (p14) extract similarity of firms from their 10-K business description texts, generating new time-varying industry classifications for these firms (p14) and imputing even in traditional datasets. In this vein, Feigenbaum (2015a, b) applies machine-learning classifiers to match individuals in historical records (p13) -the first prediction applications New Data (p14) Prediction in the Service of Estimation (p14) linear instrumental variables understood as a two-stage procedure (p14) The first stage is typically handled as an estimation step. But this is effectively a prediction task: only the predictions x- enter the second stage; the coefficients in the first stage are merely a means to these fitted values. Understood this way, the finite-sample biases in instrumental variables are a consequence of overfitting (p14) -ll overfitting. Overfitting means that the in-sample fitted values x- pick up not only the signal -′z, but also the noise δ. As a consequence, xˆ is biased towards x, and the second-stage instrumental variable estimate - - is thus biased towards the ordinary least squares estimate of y on x. Since overfit will be larger when sample size is low, the number of instruments is high, or the instruments are weak, we can see why biases arise in these cases (p14) same techniques applied here result in split-sample instrumental variables (Angrist and Krueger 1995) and -jackknife” instrumental variables (Angrist, Imbens, and Krueger 1999) (p15) -worth referencing In particular, a set of papers has already introduced regularization into the first stage in a high-dimensional setting, including the LASSO (Belloni, Chen, Chernozhukov, and Hansen 2012) and ridge regression (Carrasco 2012; Hansen and Kozbur 2014). More recent extensions include nonlinear functional forms, all the way to neural nets (Hartford, Leyton-Brown, and Taddy 2016 (p15) Practically, even when there appears to be only a few instruments, the problem is effectively high-dimensional because there are many degrees of freedom in how instruments are actually constructed (p15) -a note of caution It allows us to let the data explicitly pick effective specifications, and thus allows us to recover more of the variation and construct stronger instruments, provided that predictions are constructed and used in a way that preserves the exclusion restriction (p15) -this seems similar to my idea of regularising on a subset Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016) take care of high-dimensional controls in treatment effect estimation by solving two simultaneous prediction problems, one in the outcome and one in the treatment equation (p15) the problem of verifying balance between treatment and control groups (such as when there is attrition (p15) -! Or consider the seemingly different problem of testing for effects on many outcomes. Both can be viewed as prediction problems (Ludwig, Mullainathan, and Spiess 2017). If treatment assignment can be predicted better than chance from pretreatment covariates, this is a sign of imbalance. If treatment assignment can be predicted from a set of outcomes, the treatment must have had an effect (p15) prediction task of mapping unit-level attributes to individual effect estimates (p15) Athey and Imbens (2016) use sample-splitting to obtain valid (conditional) inference on (p16) treatment effects that are estimated using decision trees, (p16) -look into the implication for treatment assignment with heterogeneity heterogenous treatment effects can be used to assign treatments; Misra and Dub- (2016) illustrate this on the problem of price targeting, applying Bayesian regularized methods to a large-scale experiment where prices were randomly assigned (p16) -caveat Suppose the algorithm chooses a tree that splits on education but not on age. Conditional on this tree, the estimated coefficients are consistent. But that does not imply that treatment effects do not also vary by age, as education may well covary with age; on other draws of the data, in fact, the same procedure could have chosen a tree that split on age instead (p16) Prediction in Policy (p16) -no .. can we predict who will gain most from admission? but even if we can what can we report? Prediction in Policy "],
["iv-and-its-many-issues-1.html", "5 IV and its many issues 5.1 Instrument validity 5.2 Heterogeneity and LATE 5.3 Weak instruments, other issues 5.4 Reference to the use of IV in experiments/mediation", " 5 IV and its many issues 5.1 Instrument validity Exogeneity vs. exclusion Very hard to ‘powerfully test’ IV not credible? Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a direct effect on the outcome (only an indirect effect through the endogenous variable 5.2 Heterogeneity and LATE Basic consideration: what does IV identify and when:? Focusing on a binary endogenous ‘treatment’ variable With heterogeneity With imperfect compliance With one-way compliance 5.3 Weak instruments, other issues With a ‘weak instrument’ … why does that matter? 5.4 Reference to the use of IV in experiments/mediation "],
["causal-pathways-mediation-modeling-and-its-massive-limitations-1.html", "6 Causal pathways: Mediation modeling and its massive limitations 6.1 Mediators (and selection and Roy models): a review, considering two research applications 6.2 DR initial thoughts (for NL education paper) 6.3 Econometric Mediation Analyses (Heckman and Pinto) 6.4 Summary and key modeling 6.5 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity Summary Relevance to Parey et al Introduction Identification strategy brief Results in brief Framework: first for binary/binary (simplification) Framework for MTO multiple treatment groups, multiple choices", " 6 Causal pathways: Mediation modeling and its massive limitations 6.1 Mediators (and selection and Roy models): a review, considering two research applications (Originally focused on issues relevant to Parey et al) 6.2 DR initial thoughts (for NL education paper) Here were my initial thoughts as pertaining to our paper on the returns to university. Suppose we observe treatment \\(T\\) (e.g., ‘allowed to enter first-choice institution and course’), intermediate outcome \\(M\\) (e.g., completion of degree in first-choice course and institution), and final outcome \\(Y\\) (e.g., lifetime income.) Alternately, in the “substitution between charities” context… (unfold) The treatment \\(T\\) is ‘asked to donate in the first round’ (in Reinstein, Riener and Vance-McMullen, henceforth ‘RRV’ experiments, and perhaps in Schmitz 2019)’, a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?), the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others), the intermediate outcome \\(M\\) is the amount given to that (first-round) charity, and the final outcome \\(Y\\) is the amount given to that charity (or other charities) in round 2 (experiments “3”: other charities in that round ). The treatment \\(T\\) (may) directly affect the final outcome \\(Y\\). Do: show a diagram here \\[T\\rightarrow Y\\] \\(T\\) also may affect an intermediate outcome \\(M\\). \\[T \\rightarrow M\\] The intermediate outcome also may affect the final outcome \\(Y\\). \\[M \\rightarrow Y\\] With exogenous variation in \\(T\\) and \\(M\\) (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions. With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of \\(T\\) on \\(Y\\). We could also compute the share of this effect that occurs via the intermediate effect, i.e., \\(T \\rightarrow M\\rightarrow Y\\). This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients. However, there are two major challenges to this estimation. We (may) have a valid instrument for (exogenous variation in) \\(T\\) only, and \\(M\\) may arise through a process involving selection on unobserved variables. Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects. Thus we consult the relevant literature, discussed below. The most influential paper in Economics has been (Heckman, Pinto, and Heckman 2013). It is cited in more recent applied work such as Fagereng, 2018 (unfold). … We follow Heckman et al. (2013) and Heckman and Pinto (2015) in using mediation analysis. The goal of this analysis is to disentangle the average causal effect on outcomes that operate through two channels: a) Indirect effects arising from the effect of treatment on measured mediators, and b) direct effects that operate through channels other than changes in the measured mediators (including changes in mediators that are not observed by the analyst and changes in the mapping between mediators and outcomes). It is therefore necessary to assume that the mediators we do not observe are uncorrelated with both \\(\\mathbf{X}\\) and the measured mediators for all values of \\(D\\). Antonakis, coming from the Psychology and Leadership disciplines, considers the mediation question in a much simpler set of models. 6.3 Econometric Mediation Analyses (Heckman and Pinto) Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs Relevance to Parey et al We have an instrument for admission to one’s first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these ‘intermediate outcomes’, including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these. a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well 6.4 Summary and key modeling There is a ‘production function’ cf income as a function of human capital, opportunities, etc. cf donation as a function of income, prices, mood, framing, etc. Treatments (e.g., RCTs) may affect outcomes through the following channels: observable or proxied inputs Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood unobservable/unmeasured inputs cf human capital, social connections cf unobservable generosity, wealth, or temporary mood the production function itself, the ‘map between inputs and outputs for treatment group members’ Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital ‘matter more’ at some institutions? Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else? If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs. RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs ... [nor distinguish effects from changes in production functions]. Here “we can test some of the strong assumptions implicitly invoked”. “Direct effects” as commonly stated refer to the impact of both channels 2 and 3 above. DR: Channel 2 isn’t really a direct effect imho (what was this?) Standard potential outcomes framework: \\[Y=DY_{1}+(1-D)Y_{0}\\] \\[ATE=E(Y_{1}-Y_{0})\\] Production function \\[Y_{d}=f_{d}(\\mathbf{\\mathbf{{\\theta}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] ... the function under treatment \\(d\\); of proxied and unobserved inputs that occur under state \\(d\\), and baseline variables. The production function implies: \\[ATE=E\\Big(f_{1}(\\mathbf{\\mathbf{{\\theta}}}_{1}^{p},\\mathbf{\\mathbf{{\\theta}}}_{1}^{u},\\mathbf{{X}})-f_{0}(\\mathbf{\\mathbf{{\\theta}}}_{0}^{p},\\mathbf{\\mathbf{{\\theta}}}_{0}^{u},\\mathbf{{X}})\\Big)\\] We also consider counterfactual outputs, fixing treatment status and proxied inputs: \\[Y_{d,\\bar{\\theta_{d}}^{p}}=f_{d}(\\mathbf{\\mathbf{{\\bar{{\\theta}}}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] This allows us to decompose (‘as in the mediation literature’): ATE(d)=IE(d)+DE(d) IE, Indirect effect: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment statuses) DE, Direct effect: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed at one of the two treatment statuses) HP further decompose the direct effect into: \\(DE&#39;(d,d&#39;)\\): The impact of letting the treatment vary the map only (fixing the rest at one of the two appropriate values) \\(DE&#39;&#39;(d,d&#39;)\\): The impact of letting the treatment vary the unmeasured inputs only (fixing the rest at one of the two appropriate values) They use this to give two further ways of decomposing the ATE. Common assumptions “The standard literature on mediation analysis in psychology regresses outputs on mediator inputs” ... often adopts the strong assumptions of: no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy) and1 full invariance of the production function: \\(f_{1}=f_{0}\\). ... which implies \\(Y_{d}=f(\\mathbf{\\theta}_{d}^{p},d,\\mathbf{X})\\). Sequential ignorability (Imai et al, 10, ’11): Essentially, independent randomization of both treatment status and measured inputs.2 This sentence is hard to follow: In other words, input \\(\\theta_{d&#39;}^{p}\\) is statistically independent of potential outputs when treatment is fixed at \\(D=d\\) and measured inputs are fixed at \\(\\bar{\\theta_{d&#39;}^{p}}\\) conditional on treatment assignment \\(D\\) and same preprogram characteristics \\(X\\). This assumption yields the ‘mediation formulas’: \\begin{aligned} E(IE(d)|X)= &amp; E(Y|^{p}=t,D=d,X){{}} &amp; (9)\\ E(DE(d)|X)= &amp; {}expe_{} &amp; (10) \\end{aligned} (??F is presumably the distribution over the observables; where did the unobservables go? They are in the expectations, I guess.) Difference from RCT What RCT doesn’t do: \\[sequential ignorability\\] translates into ... no confounding effects on both treatments and measured inputs ... does not follow from a randomized assignment of treatment ...\\[which\\] ensures independence between treatment status and counterfactual inputs/outputs ... \\[but *not*\\] between proxied inputs \\(\\theta_{d}^{p}\\) and unmeasured inputs \\(\\theta_{d}^{u}\\). \\[Thus *not* between counterfactual outputs and measured inputs is assumed in condition (ii).\\] Cf, randomizing ‘win first-choice institution’ does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution. What RCT does do: RCT ensures “independence between treatment status and counterfactual inputs/outputs”, thus identifying ’treatment effects for proxied inputs and for outputs. CF, we can identify the impact of the treatment ‘win first chosen institution’ on proxied input like ‘enters a specialization’ and on outputs like ‘income in observed years.’ 6.5 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity Summary ... 4000+ families targeted, incentive to relocate from projects to better neighbourhoods. Easy to identify impact of vouchers Challenge (here) is to assess impact of neighborhoods on outcomes. Method here to decompose the TEOT into unambiguously interpreted effects. Method applicable to ‘unordered choice models with categorical instrumental variables and multiple treatments’ Finds significant causal effect on labour market outcomes Relevance to Parey et al We also have an instrument (DUO lottery numbers) cleanly identifying the effect of the ‘opportunity to do something’ (in our case, to enter the course at your preferred institution). However, we also want to measure the impact of choices ‘encouraged’ by the instrument, such as (i) attending the first choice course and institution and (ii) completing this course. We also deal with unordered choices (i. enter course and institution, enter course at other institution, enter other course at institution, enter neither) (ii. choice of medical specialisation) The geographic outcome is relevant to our second paper (impact on ‘lives close to home’) Introduction The causal link between neighborhood characteristics and resident’s outcomes has seldom been assessed. Treatments: Control (no voucher) Experimental: could use voucher to lease in low-poverty neighborhood Section 8: Could use voucher in any () neighborhood Many papers evaluate the ITT or TOT effects of MTO. ITT: effect of being offered voucher estimated as difference in average outcome of experimental vs control families TOT: effect for ‘voucher compliers’ (assuming no effect of simply being offered voucher on those who don’t use it) estimated as ITT/compliance rate \\[ITT and TOT\\] are the most useful parameters to investigate the effects of offering \\[EA\\] rent subsidising vouchers to families. Identification strategy brief Vouchers as IVs for choice among 3 neighborhood alternatives (no relocation, relocate bad, relocate good) \\[Cf: enter course and fp-institution, enter course at other institution, do not enter course\\] Neighborhood causal effects as difference in counterfactual outcomes among 3 categories Challenge: “MTO vouchers are insufficient to identify the expected outcomes for all possible counterfactual relocation decisions” ... “compliance with the terms of the program was highly selective \\[Clampet-Lundquist and M, 08\\]” Solution: Uses theory and ‘tools of causal inference. Invokes SARP to identify ’set of counterfactual relocation choices that are economically justifiable’ Identifying assumption: “the overall quality of the neighborhood is not directly caused by the unobserved family variables even though neighborhood quality correlates with these unobserved family variables due to network sorting” ‘Partition sample ... into unobserved subsets associated with economically justified counterfactual relocation choices and estimate the causal effect of neighborhood relocation conditioned on these partition sets.’ \\[*what does this mean?\\]* Results in brief “Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes ... 65% higher than the TOT effect for adult earnings.” Framework: first for binary/binary (simplification) First, for binary outcomes (simplified) \\(Z_{\\omega}\\): whether family \\(\\omega\\) receives a voucher (cf institution-winning lottery number) \\(T_{\\omega}\\): whether family \\(\\omega\\) relocates (cf enters first choice institution and course) Counterfactuals \\(T_{\\omega}(z)\\): relocation decision \\(\\omega\\) would choose if it had been assigned voucher \\(z\\in{0,1}\\)’: vector of potential relocation decisions (cf education choices) for each voucher assignment (cf lottery number) Can partition into never-takers, compliers, always takers, and defiers \\((Y_{\\omega}(0);Y_{\\omega}(1\\))): (Potential counterfactual) outcomes (cf income, residence, etc) when relocation decision is fixed at 0 and 1, respectively Key ( standard) identification assumption: instrument independent of counterfactual variables \\[(Y_{\\omega}(0),Y_{\\omega}(1),T_{\\omega}(0),T_{\\omega}(1))\\perp Z_{\\omega}\\] Standard result 1: ITT \\[\\begin{aligned} ITT=E(Y_{\\omega}|Z_{\\omega}=1)-(Y_{\\omega}|Z_{\\omega}=0)\\\\ =E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)P(S_{\\omega}=[0,1])+E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[1,0]&#39;)P(S_{\\omega}=[0,1])\\end{aligned}\\] i.e., ITT computation yields the sum of the ‘causal effect for compliers’ and the ’causal effect for defiers, weighted by the probability of each. Standard result 2: LATE \\[\\begin{aligned} LATE=\\frac{{ITT}}{P(T_{\\omega}=1|Z_{\\omega}=1)-P(T_{\\omega}=1|Z_{\\omega}=0)}= &amp; &amp; E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)\\\\ if\\:P(S_{\\omega}=[0,1])=0\\end{aligned}\\] i.e., the LATE, computed as the ITT divided by the ‘first stage’ impact of the instrument, is the causal effect for compliers if there are no defiers. Framework for MTO multiple treatment groups, multiple choices \\(Z_{\\omega}\\in\\{z_{1,}z_{2,}z_{3}\\}\\) for no voucher, experimental voucher, and section 8 voucher, respectively \\(T_{\\omega}\\in\\{1,2,3\\}\\) ... no relocation, low poverty neighborhood relocation, high poverty relocation \\(T_{\\omega}(z)\\): relocation decision for family \\(\\omega\\) if assigned voucher \\(z\\) \\(\\ensuremath{\\rightarrow}\\)Response type for each family \\(\\omega\\) is that a three-dimensional vector: \\(S_{\\omega}=[T_{\\omega}(z_{1}),T_{\\omega}(z_{2}),T_{\\omega}(z_{3})]\\). \\(\\ensuremath{\\rightarrow}\\) ITT computation now measures a weighted sum of effects across a subset of those response types whose responses vary between the assignments being compared. Cf: Considering the ‘treatments’: ‘1: enter other course at fp-inst, ’2: enter course at fp-inst’, ‘3: enter course at non-fp inst’ (I ignore other course at other institution for now) Looking among those who won the course lottery (so we have a binary instrument: wininst \\(Z_{\\omega}\\in{0,1\\}}\\) Our reduced-form estimates (regressions on the ‘lottery number wins institution’ dummy) measures the probablility-weighted sum of: impact of institution within course ($T_{}=$2 versus 3); for those who would ‘fully comply’ (enter course at institution if \\(Z_{\\omega}=1\\), enter course at other institution if 0) impact of the course at fp-institution versus second-best course at fp-institution for ‘institution-loving’ noncompliers; those who would enter the course only if they get the fp-institution and otherwise another course at the same institution effects for perverse defiers References "],
["causal-pathways-selection-corners-hurdles-and-conditional-on-estimates-1.html", "7 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 7.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ 7.2 Bounding approaches (Lee, Manski, etc)", " 7 Causal pathways: selection, corners, hurdles, and ‘conditional on’ estimates 7.1 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ “Conditional on positive”/“intensive margin” analysis ignores selection “Conditional on positive”/“intensive margin” analysis ignores selection identification issue See Angrist and Pischke on “Good CoP, bad CoP”. See also bounding approaches such as (???) AngristJ.D.2008a, 7.2 Bounding approaches (Lee, Manski, etc) See Notes on Lee bounds 7.2.1 Notes: Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects, David Lee, 2009, RESTUD Notes David Reinstein 7.2.1.0.1 Introduction even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research Intuitive trimming procedure for bounding average treatment effects in the presence of sample selection… Requires neither exclusion restrictions nor a bounded support for the outcome of interest.\" (Also) applicable to “nonrandom sample selection/attrition”, as well as to the ‘conditional on positive’/hurdle/mediation effect discussed here analyses and evaluations typically focus on \"reduced form impacts on total earnings, a first-order issue for cost-benefit analysis. Unfortunately, exclusively studying the effect on total earnings leaves open the question of whether any earnings gains are achieved through raising individuals hypothesis wage rates (price affects or hours of work (quantity effects). Important methodological point to constantly bring up: “even a randomized experiment cannot guarantee the treatment and control individuals will be comparable conditional on being employed.” Claims that standard “parametric or semi-parametric methods for correcting sample selection require exclusion restrictions that have little justification in this case.” Notes that most of the baseline variables could affect employment probabilities or have a direct impact on wage rates. Summary of the method: “…amounts to first identifying the excess number of individuals who were induced to be selected (employed) because of the treatment and then trimming the upper and lower tails of the outcome… distribution by this number, yielding a worst-case scenario bound.” Uses same assumptions as in “conventional models for sample selection” regressor of interest is independent of the errors in the outcome and selection models selection equations – this is ensured by random assignment. “the selection equation can be written as a standard latent variable binary response model” – what meaningful restriction does this impose? He proves this procedure “yields the tightest bounds for the average treatment effect that are consistent with the observed data.” The bounds estimator is shown to be \\(\\sqrt(n)\\) consistent and asymptotically normal with an intuitive expression for its asymptotic variance which depends on the variance of the trimmed outcome and the trimming threshold, an estimated quantifiable; (and an added term accounting for the estimation of which quantile to trim on) Note for charity experiment (unfold) (@subst) – DR, Note, charity data: We can make confidence statements over the bounds themselves. Will this procedure be easy to bring into our code? – In our (charity) experiment we in fact do have upper bounds on the outcome variable. Could this yield even greater efficiency? Note for the Netherlands data: (unfold, @NL) it is not immediately clear how this could be adapted to instrumental variables; we shall see. Can we recover something meaningful from the reduced form model they are? Can it be applied to the (instrumental variables) estimates to disentangle the impact of changing courses from the impact of the institution itself? In Lee’s paper, the estimate seems to give very narrow and informative bounds even though they have a great many people who do not earn any wages as a share of the population, about 54%. These are much narrower than the bounds proposed by Horowitz and M then what those bounds produce. &lt;!- ask (???) whether his Horowitz/Manski estimator incorporated the natural bounds on the outcome. –&gt; 7.2.1.0.2 The National Job Corps Study and Sample Selection [prior approaches] In the experiment discussed here those in the control group were embargoed from the program for three years but could join afterwards, thus “when I use the phrase ‘effect of the program’ I am referring to this reduced-form treatment effect”, i.e., the intent to treat effect. – “some subpopulations were randomized into the program group with differing, but known probabilities. Thus analyzing the data requires the use of design weights.” Note: (\\(???)) this bears some resemblance to our Dutch data situation, and we can probably use examples from analyses of these programs. We can check their code against ours. – Note also that they impute means of the baseline variables with their means; this seems to be an accepted practice. Lee notes that he focuses exclusively on the “sample selection on wages caused by employment” and not the attrition/nonresponse problem, to focus attention on this, but they could have used it for the other as well. – DR: (@NL) Note again that their desire to separate the employment hours and wage effects of the program is very similar to our desire to separate out different margins of the impact of winning an institution. …Namely the impact on completing a course or starting a course versus other impacts and the impact of entering a specialization versus remaining impacts. …Similar decompositions for the geography outcomes. – To do: check whether any papers cite Lee using an IV approach, extending the technique and the estimation of variance. “the problem of nonrandom sample selection is well-understood in the training literature; … may be one of the reasons why most evaluations of job-training programs focus on total earnings, including zeros for those without a job, rather than on wages conditional on employment” “of the 24 studies referenced in a survey … (Heckman et al.)… Most examine annual, quarterly, or monthly earnings without discussing the sample selection problem examining rage rates.” – DR: (@NL)Note that this is relevant to our question of whether to exclude zeros in log models, etc. While there will be less unemployment in our data, it still may be a relevant influence made have a strong effect on the estimates. …previous conventional approaches to the sample selection problem (skip if desired). One may explicitly model the process determining selection, such as in Heckman (1979) … Separate equations for the wage and the propensity to be employed, where employment occurs if the latter crosses a particular threshold, in which case a wage is observed. It is reasonable to think that the treatment variable can have effects on both terms.. “sample selection bias can be seen as specification error in the conditional expectation…” The expected wage conditional on treatment exogenous variables and the selection into working (that is the underlying propensity to work variable exceeding zero) his status is equal to the true effect of the treatment an adjustment for the differences in the observable’s exogenous variables and a bias term representing the expectation of the idiosyncratic unobservables given the treatment and the exogenous variables exceeding the value necessary to induce work participation. The unobservable term needs to exceed the prediction based on the observable term for the entire term to exceed zero inducing labor force participation. One may assume the data are missing at random, perhaps conditional on a set of covariates (Rubin, 1976; essentially assuming the error terms in each equation are independent of one another, here “employment status is unrelated to the determination of wages”… This “is strictly inconsistent with standard models of labor supply that account for the participation decision (Heckman, 1974).” A more common assumption is that some exogenous variables “determine sample selection but do not have their own direct impact on the outcome of interest…. Exclusion restrictions are used in parametric and semi-parametric models…” but “there may not exist credible ’instruments… excluded from the outcome equation” – DR, aside: We can return to (our) previous papers to impose these Lee bounds! One example would be the Siskel and Ebert your reviews paper and perhaps incorporating us with subsequent approaches, considering the “selection to review” equation. Second approach “the construction of worst-case scenario bounds of the treatment effect” “Impute missing data with either the largest or smallest possible values to compute the largest and smallest possible treatment effects consistent with the data” as in Horwitz and Manski (2000a) who provide a general framework for this. Particularly useful with binary outcomes. This cannot be used when the support is unbounded. … note in their replication example they are actually using the equivalent of the bottom 5th percentile and the top 95th percentile. Strictly using a procedure would provide even wider bounds. Lee considers his approach to be a hybrid of the two previous general approaches. …end of section 2.. .a statement of the Horwitz upper bound for the treatment effect; very intuitive: “what if everyone in the treatment who dropped out would have had the largest possible wage and everyone in the control group that drop out would’ve had the smallest possible wage; this will give the upper bound.” Switching this the other way around will give a lower bound. DR, an aside thought: (@NL) Something akin to the Horwitz and M approach (or maybe Lee bounds) could be applied to our issue of swapping into institutions directly. Suppose we only focus on those who actually complied: those assigned to an institution who also went to that institution. Our concern was that this would under-represent those who had particularly strong institutional preferences. Suppose you are interested in looking at the impact of winning the lottery (for once preferred institution) itself, as that was our most simply identifiable outcome. … Let’s consider evaluating a treatment effect for those who happened to swap in. Those who swapped in might be assigned a counter-factual outcome of the lowest value of the lifetime income among those who did not get their institution of choice. Similarly, the small group who swapped out might be assigned a counterfactual outcome (had they no swapped out) representing the highest outcome value for those who did get their institution of choice. This should give us an upper bound on the treatment effect for these two groups of what we might call non-compliers. Making the opposite assumptions, precisely that those who swapped into their institution of choice would’ve had a very good counterfactual outcome (if they had not got their institution of choice) that comes from the highest outcomes for those who didn’t get their institution of choice (and also reversing this for those who swapped out of their preferred institution) would give us a lower bound for the treatment effect for this group. We can then combine these bounded treatment effects for these non-compliers with the treatment effect for the compliers to get a measure of the average treatment effect with bounds for this sort of behavior. This will also allow us to come up with estimates with bounds without having to use the instrumental variable strategy which has issues of its own. 7.2.1.0.3 Section 3: identification of bounds on treatment effects; the main meat of the model He starts with a simple example. He begins with a model with a treatment indicator and no other covariates, and a continuous outcome variable, but notes that this will clearly apply to discrete outcome variables and will also apply conditional on controls. Nest, he brings forward the statement… from the earlier selection models. In each case the latent variable must overcome a hurdle for the outcome to be observed and in fact the hurdle differs depending on the impact of the treatment itself. In general when the errors in the selection and outcome equations are correlated the difference in these means differs from the actual treatment effect. In other words through a slightly complicated story, when those who have unobservables that make them more likely to work also tend to have unobservables that would make them likely to earn more the standard difference in outcomes between control and treatment will not describe the true treatment effect. A key insight seems to be that we could identify the treatment effect if we could estimate the expected outcome given treatment and given that the unobservable component in the selection equation would lead to an observable outcome had the person not been given treatment. If so, we could subtract the observed mean control outcome from the above to yield the true treatment effect (for those who would be observed always). However, we obviously do not observe this because we only observe the outcomes for those who are treated where the selection equation is in fact positive and not “where the selection equation would have been positive had they not been treated.” However, the insight here is that this term can in fact be bounded. We do observe these outcomes for the treated people (note we are assuming without loss of generality that the treatment raises the probability of selection for this discussion) but we don’t know exactly which ones they are. In other words, we observe outcomes for more people in the treatment group than we need; we wish we could figure out what is the subset of these that would have also been observed had they not been treated, so we could compare like-to-like. The observed treatment mean is a weighted average of the thing we are seeking (to difference from the control) and “the mean for a subpopulation of marginal individuals… that are induced to be selected into the sample because of the treatment” This then gets us the upper bound for the term expressing the treatment outcome for those who would have been observed even if they had been in the control. The upper bound for this is the expected outcome for those in the treated group (who are observed of course) and who are in quantile-p or above of the outcome, where this \\(p\\) is the share of the treated population that are in the marginal group we referred to that were only induced to be selected into the sample because of the treatment. In other words the worst case scenario is that the smallest share \\(p\\) values of \\(Y\\) are in the marginal group and the largest one (which is share 1-pone are in the inframarginal group. We don’t know which observations are inframarginal and which ones are marginal. \\(p\\): the share of marginal individuals and (1-p) the share of inframarginal individuals (the latter is group we want the average outcome for). The highest could be would be the average outcomefor the largest (1-p) share of this group. We are looking for the expectation given that they are at or above at will at or above percentile p within this group. In other words we trim the lower tail of the Y distribution by the portion \\(p\\), (so what remains is the 1-p share) to get the upper bound for the inframarginal groups mean. We can then subtract the mean for the control group to get an upper bound for the treatment effect. To compute this “trimming proportion p”: this p is equal to the share of the treated group whose outcome is observed minus the share of the control group whose outcome is deserved is observed, divided by the share of the treatment group where the outcome is observed. Something like the increased likelihood of observation that is driven by the treatment, as a share of the total number as a share of the probability of observation in the treatment group. The average observed outcome for the treatment group is including too many observations; we need to difference out the share of observations that are observed only because the treatment caused them to be observed; this share is certainly no larger than the increased probability of observation in the treatment group as a share of the probability of observations the treatment group. Another much simpler way of saying this is “trimming the data by the known proportion of excess individuals” in the treatment group. (To gain bounds on the mean for the inframarginal group which we can then difference from the control-group mean get the treatment effect). Perhaps some intuition for why this improves on the Horwitz model: we don’t need to assume that those observed in the treatment group that wouldn’t have been observed in the control would’ve had the highest possible outcomes. No, we only need to assume (to get the upper bound) that these came from the highest distribution because they had to come from somewhere. These were the people in the upper tail of the relevant group but they couldn’t all have been the individual highest achiever. The model is extended to heterogeneity and heteroscedasticity. This begins with the independence of treatment assignment the “potential sample selection indicators” for either treatment or control, in other words whether that individual will have an observed outcome under treatment and whether that the individual would have an observed outcome under control, and the latent potential outcomes. Experimental or random assignment ensures that each of the potential outcomes (and the correspondence to observability under each treatment) is independent of the actual assigned treatment. The second assumption is monotonicity: treatment assignment can only affect sample selection in one direction. – DR: For our (substitution) experiments, it is in fact not clear to me whether this should necessarily be the case, as some (less generous?) people may be induced to leave because of having been asked to donate, while potentially other (more generous people) might be induced to return given that they were asked to donate. (This proposed nonmonotonicity implies that the ‘asked twice’ sample tends to weed out the less generous, which would lead to a bias against substitution, strengthening the case for our result.) - DR, aside: However, even though the paper doesn’t say it, I suspect this assumption could be weakened and you would still get some similar bounds. To put it another way, I would imagine that these bounds could be adjusted based on some reasonable ad hoc assumptions about the share of the population who is affected in either direction. – @NL: I’m coming to think that our Dutch data problems are more things involving “hurdle models”. Can this technique also be applied to such hurdle models? Next proposition 1a states that given these assumptions we can derive sharp lower and upper bounds for the average treatment effect (conditional on ‘would be observed in both states’). Note that for this estimator if the probability of observation is greater under the treatment we need to trim the treatment groups outcome distribution and if the probability of observation is greater under the control we need to trim the control group’s outcome distribution. DR, aside comment: we seem to be throwing out a bit of the data in these estimates, which would suggest that something more efficient could be generated. (The stated bounds you can estimate are exactly the same as the bounds from the previous specification, at least as I had interpreted the way they would be produced.) Their remark 2 notes that an implication is that as \\(P_0\\), that as the “difference between the relative probability of observation of an outcome under treatment versus control” tends to zero, i.e., as the probability of having an observed outcome (or the conditional probability of this) is the same for treatment and control) then there is no sample selection bias. Their estimate convergences to the estimate he calls an estimate for the “always takers subpopulation… except that taking… is selection into the [outcome-observed] sample.” So, a very vanilla estimator is acceptable if we find the same conditional probability of selection for each group, under monotonicity, which, for this case, we can test (see Remark 4 below). – (DR: To me this suggests that there might be something wrong going on here. Intuitively, If I simply observe the same rate of attrition in the treatment and control groups this shouldn’t be enough to tell me that attrition did not matter, as it could occur differentially for both groups, but it seems to be a result here; this is probably due to the assumption of monotonicity of the selection/observation term, as well as the random/exogenous assignment to each group.) Remark 3 discusses the importance of monotonicity for the bounds, saying this assumption is “minimally sufficient” (I think it would be better to say minimally sufficient for these particular bounds that he computed). To demonstrate this he gives an extreme example. Without monotonicity it could be (note: this would seem like a very unlikely outcome!) that every observation in the control group comes from the population in the treatment group that would not have been observed had they been treated and every observation in the treatment group happens to come from the set of people that would not have been observed had they been in the control group. These two “subpopulations do not overlap, so the difference in the means could not be interpreted as a causal effect.” – DR, aside : there must be some way to impose some restrictions on this even allowing for this non-monotonicity. (He notes that this can be improved upon somewhat by thinking about the total the idea that the total masses of unobserved that would’ve been observed in the other group can’t be greater than the share that is not observed in the other treatment group, but this doesn’t seem like a particularly fruitful route as it in most reasonable cases will still allow for very wide bounds.) Remark 4 suggests that if we can assume (or somehow observe?) that the conditional probabilities of selection are the same for treatment and control, we can test whether monotonicity in fact holds and the simple difference in means will be an appropriate estimate of the treatment effect. Here, the assumption implies that everyone in the treatment or control group would have been observed under the opposite treatment as well. This in fact implies that the distribution of the exogenous variables should be the same in the treatment and control groups conditional on being selected. This seems fairly intuitive, we look at whether selection seems to be occurring in different ways are on different margins for the two groups treatment versus control. Apparently for this test to have power we need that the subpopulations of “noncompliant errors in opposite directions” (quotation mine) must have distinct distributions of baselines exogenous characteristics. If these were the same then whether or not monotonicity holds the test doesn’t tell us anything. – DR: I wonder if anyone uses this test for Monotonicity under non-differential selection? Another relevant note that he bundles in this remark is that the technique here only yields estimates for those who would be with an observed outcome for either treatment or control. One could additionally try to bound this as an estimate for the entire population using the Horwitz and Manski bounds for this latter thing. However, in many contexts there are reasons that the bounded estimates they mainly use are the relevant ones, such as “the impact of the program on wage rates for those whose employment status was not affected by the program.” DR: In our substitution experiment case, the substitution patterns for those for whom attrition was not affected by the first-round-charity treatment @NL: E.g., the impact of an institution on income for those whose choice to remain in the course was not affected by their institutional assignment “Narrowing bounds using covariates” All of the above could be done conditional on a particular set of baseline characteristics such as gender or race. The average treatment effect could be estimated separately for each. (Note: and perhaps combined in a fruitful way?) One can alternately use covariates to reduce the width of these bounds. To give intuition, we can imagine a baseline covariate that perfectly predicts an individual’s wage. Because treatments are randomly assigned the maintained assumptions will still hold conditionally on this X. The results the methods can be applied separately for each value of this covariate, and for each such value the trimming procedure will actually have no impact on the estimate. DR: I think this is the “estimate and sum things up in a weighted way” procedure I thought about a moment ago. Proposition 1B gives the balance estimator for a model involving exogenous variables. Essentially, this computes the corresponding bounds estimator at each X, where the differential selection probability is computed for that particular X, the upper quantile value of the outcome is given conditional on the same X and on being in the treatment group. These are then integrated (or summed up) weighted by the distribution or the cdf of this covariate in the control group. These bounds will necessarily be sharper than the balance without controls. 7.2.1.0.4 Section 4: estimation and inference The asymptotic variance depends on components reflecting the variance of the trimmed distribution, the variance of the estimated trimming threshold, and the variance in the estimate of “how much of the distribution to trim” (the relative selection probability differential). Equation 6 formally defines the estimator Estimated bounds consistent for ‘true bounds’ under standard conditions Two ways to compute CI’s – CI’s for the ‘true bounds’ or CI’s for the TE itself. A 95% CI for the former will contain the latter with even greater probability. Imbens and Manski ‘04 can be used to derive the latter which are ’more apppropriate here’ since the object of interest is the TA and not the ’region of all rationalizable treatment effects. These are built off of a transformation of the estimate UB and LB and max estimated sd of each of these. the latter are reported by the ‘cie’ option in ‘leebounds’ Generalisation to monotonicity (without knowing direction of impact of treatment on selection)… As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative… [do similar] though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small and p0 is “close” to zero … A simple, conservative approach to combining the trimmed and untrimmed intervals is to compute their union 7.2.1.0.5 Section 5: Empirical Results Table 4 gives a step-by-step that is a good way of seeing and understanding the construction of the estimator, and where the ‘action’ is, in treimming, in components of the SE, etc. Intervals are 1/14 the width of the equivalent Horowitz/Manski bounds 7.2.1.0.5.0.1 5.2 using covariates to narrow bounds Any baseline covariate will do, as will any function of all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to be a proxy for the predicted wage potential for each individual—is constructed from a linear combination of all observed baseline characteristics. This single covariate is then discretized, so that effectively five groups are formed according to whether the predicted wage is within intervals defined by $6·75, $7, $7·50, and $8·50. (???): this is essentially what I propose we do, but using Ridge Regressions or something similar To compute the bounds for the overall average…the group-specific bounds must be averaged, weighted by the proportion (sPr Group J|S0=1,S1=1) The estimated asymptotic variance for these overall averages is the sum of (1) a weighted average of the group-specific variances and (2) the (weighted-) mean squared deviation of the group-specific estimates from the overall mean. This second term takes into account the sampling variability of the weights \\(\\rightarrow\\) result: 11% narrower bounds Interesting; possibly do similar for @NL-ed: By statistically ruling out any effect more negative than −0·037, this suggests that after 4 years, the Job Corps enabled program group members to offset at least 35% (and perhaps more) of the potential 0·058 loss in wages due to lost labour market experience that could have been caused by the program 7.2.1.1 Section 6: Conclusions: implications and applications Interesting intuitive argument: Another reason to interpret the evidence as pointing to positive wage effects is that the lower bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly negatively correlated with the propensity to be employed. From a purely theoretical standpoint, a simple labour supply model suggests that, all other things equal, those on the margin of being employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be the smallest wage that draws the individual into the labour force). In addition, the empirical evidence in Table 2 suggests that there is positive selection into employment: those who are predicted to have higher wages are more likely to be employed (i.e., U and V are positively correlated). If this is true, it seems relatively more plausible to trim the lower rather than the upper tail of the distribution to get an estimate of the treatment effect. "],
["notes-on-bayesian-approaches-david-reinstein.html", "8 Notes on Bayesian approaches – David Reinstein 8.1 My uses for Bayesian approaches (brainstorm) 8.2 Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep” 8.3 Why and when use Bayesian (MCMC) methods? 8.4 Theory 8.5 Comparing models … Equivalent of ‘likelihood’ 8.6 On choosing priors 8.7 Implementation 8.8 Generate predictions from a WinBUGS model 8.9 Missing data case 8.10 Stata 8.11 R mcmc pac", " 8 Notes on Bayesian approaches – David Reinstein 8.1 My uses for Bayesian approaches (brainstorm) 8.1.1 Meta-analysis of previous evidence Of prior work, especially on motivators of (effective) charitable giving and responses to effectiveness information Of my own series’ of experiments (potentially joint with prior work) 8.1.2 Inference, particularly about ‘null effects’ When/what can we say about the ‘absence of an effect’ How to integrate into inferences from diagnostic testing (e.g., common-trend assumption)? 8.1.3 ‘Policy’ and business implications and recommendations E.g., for ‘which pages to seed’ 8.1.4 Theory-driven inference about optimizing agents, esp. in strategic settings Especially in ‘predicted contributions to public goods’ settings and 2nd order beliefs 8.1.5 Experimental design Optimal treatment assignment, with previous observables and a track record Sequential designs ( Bayesian Power calculation 8.2 Title: “Introduction to Bayesian analysis in R and Stata - Katz, Qstep” 8.3 Why and when use Bayesian (MCMC) methods? 8.3.1 Pros No need for asymptotics … good when sample sizes are small Incorporate previous information You can consider the ‘robustness to other priors’ Fit complex nonstandard models … e.g., with difficult functional forms or likelihood settings (more computation, less thinking) Easy to make predictions (e.g., simulate scenarios) after estimation Incorporate evidence, results, expert judgement (‘restrictions’ with some lee-way?) (ISn’t this the same as number 2?) Cleaner treatment/imputation of missing values … these are just parameters 8.3.2 Cons Must specify prior distributions … allows subjective judgement Different way of thinking about stats and inference; probability distributions and simulations, not much about p-values, point estimates and standard errors … path dependence Computational cost This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 8.3.3 Why more popular today? Starting from around 2005 in Political Science and Sociology Computational revolution comes from Markov chain Monte Carlo (MCMC) methods … don’t need analytical solutions Software implementations – many in R, specialised software like EWinBugs, JAGS, STAN; also increasingly in Stata 8.4 Theory Bayes theorem … inverting conditional probability thing … ‘inversion’ to make inferences about the parameters In Bayesian stats the parameters (and sometimes missing values) are random variables, we make probability statements about them \\[P(A|B)=P(B|A)P(A)/P(B)\\] Frequentist: Point estimates, unknown fixed parameters, data from a hyol repeataable random sample Bayesian: Fixed data (from the experiment), parameters are random variables … results based on probability distributions about rthese Classical statistics: likelihood of data given parameter: \\(p(y|\\theta)\\) Bayes we want, \\(p(\\theta|y) = p(y|\\theta)p(\\theta)/p(y)\\) \\(p(y)\\) is a ‘constant’ in our estimation … the data is fixed. So it’s proportional to \\(p(\\theta|y) = p(y|\\theta)\\times p(\\theta)\\) \\(p(y|\\theta)\\) is what we max when we do ML $ p()$: prior distribution capturing beliefs about \\(\\theta\\) 8.4.1 So how do we estimate it? Specify a probability model, a distribution for Y (likelihood function) and the priors for \\(\\theta\\) Solve (find) the posterior distribution \\(p(\\theta|Y)\\) and summarise the parameters of interest In practice, step 2 is usually done via MCMC simulation rather than analytically. … via simulations, I approach the ‘true’ value on \\(\\theta\\) (Given ‘regularity conditions’) 8.4.2 Linear regression model example \\[Y = x&#39;\\beta+\\epsilon\\] with n obs only random term is epsilon … natural candidate is a normal distribution, so \\(Y \\sim N(x&#39;\\beta,\\sigma^2_e)\\) So we want to find \\(p(\\beta, \\sigma^2_\\epsilon|Y,X)\\). This depends on the choices of \\(p(\\beta)\\) and \\(p(\\epsilon)\\). Could choose conjugate priors, leading to a particular joint posterior, you can solve it analytically. Can yield a joint posterior. Instead, let’s assume that the latter (variance) parameter is known, you can show that the posterior for \\(\\beta\\) is also normally distributed. (Conjugate) Similarly, if we assume \\(\\beta\\) is known, if the variance term had an inverse gamma distribution (prior), so will the posterior. In these conjugate priors, the posterior mean will be a weighted average of the priors and the data. 8.4.3 Gibbs Needs closed form conditional posterior for every parameter. What Gibbs sampler does is break the parameter space into sets of parameters Choose starting values, \\(\\theta^0_1,...\\theta^0_k\\) sample from the first parameter’s distribution given the others … the second one, … the k’th one . Repeat step 2 … thousands of times (starting with the parameters from the previous iteration) Eventually ‘we obtain samples of \\(p(\\theta|y)\\)’ But if we don’t have a closed form, we cannot simply sample from known distributions in each step E.g., in case of Logit distribution. 8.4.4 Metropolis Hastings Choose ‘proposal distribution’ to sample parameter values (a candidate like normal, uniform) Start w a prelim guess for parameter values \\(\\theta_0\\) At iteration t sample a proposal \\(\\theta_t\\) from \\(p(\\theta_t|\\theta_{t-1})\\) ?? what does this come from? If \\(p(\\theta_t|y)&gt;p(\\theta_{t-1}|y)\\) accept it as the new value of \\(\\theta\\). ??? how is this computed if we don’t have conjugate closed-form posteriors? Otherwise flip a coin with probability r = (ratio of those probabilities) if coin tosses heads, accept as new theta, otherwise stay at previous theta allows algorithm to avoid getting stuck at local maxima Commonly used proposal: random walk sample: \\(\\theta_t=\\theta_{t-1}+z_t\\), \\(z_t \\sim f\\) ?? I do this because there is no analytical way to derive this, unlike in the conjugate case, where we might use the Gibbs can combine Gibbs with Metropolis steps; relevant to some problems 8.4.5 Assessing convergence previous … ‘eyeballing’ formal: single-chain tests (Geweke/Heidel) … is the last part of the chain stable (stationary)… compare simulation at middle and end, is there much variation? multiple-chain test… (starting from different values), do they end similar … Gelman-Rubin diagnosting \\(\\hat{R}\\) typically either a very long chain and use GH convergence, or multiple shorter chains and use \\(\\hat{R}\\) Gabriel: Gelman-Rubin is probably preferred; more conservative ?? What am I iterating towards? Converging on what? ###Assesing ‘fit’ in Bayesian No r-squared Typical measure is ‘posterior predictive comparisons’ \\(p(y_{replicated}|y_{observed}= ...\\) Simulate data from estimated parameters Compare to observed data Use an overall fit measure to assess model fit E.g., percent correct predictions (binary), whether the true data is within the 95% CI of the replicates, deviance For each replicate Choose statistic D, compare the replicated \\(D(y_s_{replicated})\\) against $D(y_s_{observed}) Quantify the discrepancy … percent of correct predictions, proportion of times replicated y is below true y … compute ‘bayesian p-value’s’ Systematic differences between replicate and actual data indicate model limitations (?? what are reasonable values here??) 8.5 Comparing models … Equivalent of ‘likelihood’ ‘Deviance Information Criterion’ (most used); specific for MCMC simulations: compares expected LL of the model (of the data given the estimated parameters; average here across much of the later points in the chain) against the llhd at the posterior parameter mean. Always select model with lowest DIC. Bayes Factor (less used): Ratio of llhd of the models; higher BF means model is more supported; BF&gt;10 seen to provide strong evidence for model w higher value 8.6 On choosing priors Most social scientists use non-informative or vague priors; i.e., large variance… e.g., \\(\\beta \\sim N(0,1000)\\) But its often useful to incorporate information into your priors Small pilot to test, \\(\\rightarrow\\) data \\(Y_1\\), another study gives data \\(Y_2\\); repeated application of Bayes theorem gives the posterior. Same result whether you obtained these together, or whether you did one and then updated (e.g., via an MCMC, starting with the first one as a prior) Conjugate priors (mentioned before) Jeffrey’s priors (??) 8.7 Implementation If you don’t need to do fancy things, and don’t want to (?) generate the full posterior distribution (or something) Some Stata/R commands that make Bayesian look frequentist. In Jags and Winbugs, we only have to specify the prior… rest is done for us Jags is great … you only need to do self-coding with lots of data and super complicated models as it can freeze up We went through it the fancy way in Probit.R Then the easy way with ‘script probit Jags.R’ 8.8 Generate predictions from a WinBUGS model You can just generate these outcomes … Prediction: generate a new observation #note, he is doing one per iteration, but since these are convergent it would be basically the same if you just chose a random iteration and did all the draws from that one 8.9 Missing data case One solution – multiple imputation choose imputation model to predict missings, generate many copies of orig data set, imputing missibg value for each 2 more steps here Need a model for X|alpha, because missing variables are random variables 8.10 Stata Has some rather simple implementations; e.g., just using commands like ```bayes: regress y x ’’’ 8.11 R mcmc pac Also simple code; great for standard use Speedup with parallelization; see “script for parallel probit.R” and “parallelprobit.R” More advanced: C++; can integrate it with Rcpp, or even use Exeter’s ISCA cluster ``{r cars} summary(cars) ``` Angrist J. D., and J S Pischke. 2008. “Mostly Harmless Econometrics : An Empiricist ’ S Companion.” Massachusettts I Nstitute of Technology and the London School of Economics, no. March: 290. https://doi.org/10.1017/CBO9781107415324.004. Gentzkow, Matthew. 2013. “Code and Data for the Social Sciences : A Practitioner ’ S Guide.” Heckman, James, Rodrigo Pinto, and James Heckman. 2013. “Econometric Mediation Analyses : Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs,” no. 7552. Kennedy, Peter. 2003. A Guide to Econometrics. MIT press. Tibshirani, Robert. n.d. “Statistical Learning with Sparsity the Lasso and Generalizations.” Wooldridge, Jeffrey M. 2002. Econometric Analysis of Cross Section and Panel Data. 2. The MIT press. https://doi.org/10.1515/humr.2003.021. Wooldridge, J M. 2008. Introductory Econometrics: A Modern Approach. South-Western Pub. Cf ‘winning institution’ impacts human capital, social networks, etc identically for everyone; e.g., not a greater effect for men then for women, nor a greater effect for those entering particular specializations.↩︎ Cf ‘winning institution’ does not effect the specialization entered nor the location of residence, nor are both determined by a third factor.↩︎ "]
]
