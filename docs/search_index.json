[
["econometrics-statistics-and-data-science-reinstein-notes-with-a-micro-behavioural-and-experimental-focus.html", "Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus 1 Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioural, and Experimental focus", " Econometrics, statistics, and data science: Reinstein notes with a Micro, Behaviural, and Experimental focus Dr. David Reinstein, 2020-04-23 Abstract This ‘book’ organizes my notes and helps others understand it and learn from it 1 Econometrics, statistics, and data science: Reinstein notes with a Micro, Behavioural, and Experimental focus "],
["notes-introduction.html", "2 Notes introduction 2.1 Conceptual 2.2 Getting, cleaning and using data 2.3 Control strategies and prediction; Machine Learning approaches 2.4 Basic regression and statistical inference: Common mistakes and issues 2.5 LDV and discrete choice modeling 2.6 Robustness and diagnostics, with integrity 2.7 IV and its many issues 2.8 Causal pathways: mediation, hurdles, etc. 2.9 Causal pathways: mediation, hurdles, etc. 2.10 Causal pathways: mediation, hurdles, etc. 2.11 Other paths to observational identification 2.12 (Ex-ante) Power calculations 2.13 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 2.14 (Experimental) Study design: Background and quantitative issues 2.15 ‘Experimetrics’ and measurement of treatment effects from RCTs 2.16 Making inferences from previous work; Meta-analysis, combining studies", " 2 Notes introduction Focus on the practical tools I use and the challenges I (David Reinstein) face Microeconomics, behavioral economics, focus on charitable giving and ‘returns to education’ type of straightforward problems. (Limited to no focus on structural approaches.) Where we can add value to real econometric practice?? Data: Observational (esp. web-scraped and API data and national surveys/admin data} Experimental: esp. where with multiple crossed arms, and where the ‘cleanest design’ may not be possible Assume familiarity with most basic statistical concepts like ‘bias’, ‘consistency’, and ‘null hypothesis testing.’ However, I will focus on some concepts that seem to often be misunderstood and mis-applied. 2.1 Conceptual 2.1.1 Bayesian vs. frequentist approaches Folder: bayesian Notes: bayes_notes 2.1.2 Causal vs. descriptive; ‘treatment effects’ and the potential outcomes causal model 2.1.2.1 DAGs and Potential outcomes 2.1.3 Theory, restrictions, and ‘structural vs reduced form’ 2.2 Getting, cleaning and using data 2.2.1 Data: What/why/where/how 2.2.2 Good coding practices 2.2.2.1 Organizing a project 2.2.2.2 Dynamic documents (esp Rmd/bookdown) 2.2.3 New tools and approaches to data (esp ‘tidyverse’) 2.2.3.1 Style and consistency Indenting, snake-case,etc 2.2.3.2 Using functions, variable lists, etc., for clean, concise, readable code 2.2.4 Data sharing and integrity 2.3 Control strategies and prediction; Machine Learning approaches ‘Identification’ of causal effects with a control strategy not credible Identification Essentially a ‘control strategy’ is “control for all or most of the reasonable determinants of the independent variable so as to make the remaining unobservable component very small, minimizing the potential for bias in the coefficient of interest”. All of the controls must still be exogenous, otherwise this itself can lead to a bias. There is some discussion of how to validate this approach; see, e.g., (???). 2.3.1 Machine Learning (statistical learning): Lasso, Ridge, and more 2.3.2 Limitations to inference from learning approaches 2.4 Basic regression and statistical inference: Common mistakes and issues Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Identification Random effects estimators show a lack of robustness Specification Clustering SE is more standard practice OLS/IV estimators not ‘mean effect’ in presence of heterogeneity Power calculations/underpowered Selection bias due to attrition Selection bias due to missing variables – impute these as a solution Signs of p-hacking and specification-hunting Weak diagnostic/identification tests Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness Dropping zeroes in a “loglinear” model is problematic Random effects estimators show a lack of robustness With heterogeneity the simple OLS estimator is not the ‘mean effect’ P_augmented may overstate type-1 error rate Impact size from regression of “log 1+gift amount” Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ Peer effects: Self-selection, Common environment, simultaneity/reflection (Manski paper) Weak IV bias Bias from selecting instruments and estimating using the same data 2.4.1 “Bad control” (“colliders”) Endogenous control: Are the control variables you use endogenous? (E.g., because FDI may itself affect GDP per capita) 2.4.2 Choices of lhs and rhs variables Missing data Choice of control variables and interactions Which outcome variable/variables 2.4.3 Functional form Logs and exponentials Nonlinear modeling (and interpreting coefficients) 2.4.3.1 ‘Testing for nonlinear terms’ Quadratic regressions are not diagnostic regarding u-shapedness: Simonsohn18 http://datacolada.org/62 2.4.4 OLS and heterogeneity OLS does not identify the ATE http://blogs.worldbank.org/impactevaluations/your-go-regression-specification-biased-here-s-simple-way-fix-it?cid=SHR_BlogSiteShare_XX_EXT Modeling heterogeneity: the limits of Quantile re regression 2.4.5 “Null effects” “While the classical statistical framework is not terribly clear about when one should”“accept”\" a null hypothesis, we clearly should distinguish strong evidence for a small or zero effect from the evidence and consequent imprecise estimates. If our technique and identification strategy is valid, and we find estimates with confidence intervals closely down around zero, we may have some confidence that any effect, if it exists, is small, at least in this context. To more robustly assert a \"“zero or minimal effect”\" one would want to find these closely bounded around zero under a variety of conditions for generalizability. In general it is important to distinguish a lack of statistical power from a “tight” and informative null result; essentially by considering confidence intervals (or Bayesian credible intervals). See, e.g., Harms and Lakens (2018), “Making ‘null effects’ informative: statistical techniques and inferential frameworks”.\" Harms-lakens-18 2.4.5.1 Confidence intervals and Bayesian credible intervals 2.4.5.2 Comparing relative parameters E.g., “the treatment had a heterogeneous effect… we see a statistically significant positive effect for women but not for men”. This doesn’t cut it: we need to see a statistical test for the difference in these effects. (And also see caveat about multiple hypothesis testing and ex-post fishing). 2.4.6 Multiple hypothesis testing (MHT) See (???) 2.4.7 Interaction terms and pitfalls 2.4.7.1 ‘Moderators’ Confusion with nonlinearity Moderators: Heterogeneity mixed with nonlinearity/corners In the presence of nonlinearity, e.g., diminishing returns, if outcome ‘starts’ at a higher level for one group (e.g., women), it is hard to disentangle a heterogeneous response to the treatment from ‘the diminishing returns kicking in’. Related to https://datacolada.org/57 [57] Interactions in Logit Regressions: Why Positive May Mean Negative 2.4.7.2 MHT 2.4.8 Choice of test statistics (including nonparametric) (Or get to this in the experimetrics section) 2.4.9 How to display and write about regression results and tests 2.4.10 Bayesian interpretations of results 2.5 LDV and discrete choice modeling 2.6 Robustness and diagnostics, with integrity 2.6.1 (How) can diagnostic tests make sense? Where is the burden of proof? Where a particular assumption is critical to identification and inference …Failure to reject the violation of an assumptionis not sufficient to give us confidence that it is satisfied and the results are credible. At several points the authors cite insignificant statistical tests as evidence in support of a substantive model, or of evidence that they do not need to worry about certain confounds. Although the problem of induction is difficult, I find this approach inadequate. Where a negative finding is given as an important result, the authors should also show that their parameter estimate is tightly bounded around zero. Where it is cited as evidence they can ignore a confound, they should provide evidence that they can statistically bound that effect is small enough that it should not reasonably cause an issue (e.g., as using Lee or McNemar bounds for selective attrition/hurdles). 2.6.2 Estimating standard errors 2.6.3 Sensitivity analysis: Interactive presentation 2.7 IV and its many issues 2.7.1 Instrument validity Exogeneity vs. exclusion Very hard to ‘powerfully test’ IV not credible Identification Note that for an instrument to be valid it needs to both be exogenously determined (i.e., not selected in a way related to the outcome of interest) and to also not have a direct effect on the outcome (only an indirect effect through the endogenous variable 2.7.2 Heterogeneity and LATE 2.7.3 Weak instruments, other issues 2.7.4 Reference to the use of IV in experiments/mediation 2.8 Causal pathways: mediation, hurdles, etc. 2.8.1 Mediation modeling 2.8.2 ‘Corner solution’ or hurdle variables and ’Conditional on Positive “Conditional on positive”/“intensive margin” analysis ignores selection “Conditional on positive”/“intensive margin” analysis ignores selection Identification See Angrist and Pischke on “Good CoP, bad CoP”. See also bounding approaches such as (???) AngristJ.D.2008a, 2.9 Causal pathways: mediation, hurdles, etc. 2.9.1 Mediation modeling 2.9.2 ‘Corner solution’ or hurdle variables and ‘Conditional on Positive’ 2.10 Causal pathways: mediation, hurdles, etc. 2.10.1 Mediation modeling and its massive limitations An applied review 2.10.2 ‘Corner solution’ or hurdle variables and ’Conditional on Positive 2.10.2.1 Bounding approaches (Lee, Manski, etc) 2.11 Other paths to observational identification 2.11.1 Fixed effects and differencing 2.11.2 DiD FE/DiD does not rule out a correlated dynamic unobservable, causing a bias 2.11.3 RD 2.11.4 Time-series-ish panel approaches to micro 2.11.4.1 Lagged dependent variable and fixed effects –&gt; ‘Nickel bias’ 2.12 (Ex-ante) Power calculations 2.12.1 What sort of ‘power calculations’ make sense, and what is the point? 2.12.1.1 The ‘harm to science’ from running underpowered studies \"One worries about underpowered tests. Your result (may have) relatively large effect sizes that are still insignificant, which makes me wonder whether it has low power. Low powered studies undermine the reliability of our results. Button et al (2013_ point out that running lower-powered studies reduces the positive predicted value—the probability that a “pos- itive” research finding reflects a true effect—of a typical study reported to find a statistically significant result. In combination with publication bias, this could lead a large rate of type-1 error in our body of scientific knowledge (false-positive cases, where the true effect was null and the authors had a very “lucky” draw). True non-null effects will be underrepresented, as underpowered tests will too-often fail to detect (and publish) these. Furthermore, in both cases (true null, true non-null), underpowered tests will be far more likely to find a significant result when they have a random draw that estimates an effect size substantially larger than the true effect size. Thus, the published evidence base will tend to overstate the size of effects.\" verkaik2016, metzger2015 2.12.2 Power calculations without real data 2.12.3 Power calculations using prior data 2.13 (Experimental) Study design: Identifying meaningful and useful (causal) relationships and parameters 2.13.1 Why run an experiment or study? Sugden and Sitzia critique here, give more motivation 2.13.2 Causal channels and identification Ruling out alternative hypotheses, etc 2.13.3 Types of experiments, ‘demand effects’ and more artifacts of artifical setups 2.13.4 Generalizability (and heterogeneity) 2.14 (Experimental) Study design: Background and quantitative issues 2.14.1 Pre-registration and Pre-analysis plans 2.14.1.1 The hazards of specification-searching 2.14.2 Sequential and adaptive designs Needs to adjust significance tests for augmenting data/sequential analysis/peeking Statistics/econometrics new-statistics sagarin_2014 http://www.paugmented.com/ resubmit_letterJpube.tex, http://andrewgelman.com/2014/02/13/stopping-rules-bayesian-analysis/ Yet … P_augmented may overstate type-1 error rate Statistics/econometrics response to referees, new-statistics \" A process involving stopping \"“whenever the nominal \\(p.0.5\\)”\" and gathering more data otherwise (even rarely) must yield a type-1 error rate above 5%. Even if the subsequent data suggested a \"“one in a million chance of arising under the null”\" the overall process yields a 5%+ error rate. The NHST frequentist framework can not adjust ex-post to consider the \"“likelihood of the null hypothesis”\" given the observed data, in light of the shocking one-in-a-million result. While Bayesian approaches can address this, we are not highly familiar with these methods; however, we are willing to pursue this if you feel it is appropriate. Considering the calculations in , it is clear that \\(p_{augmented}\\) should the type-1 error of the process if there is a positive probability that after an initial experiment attains p\\(&lt;0.05\\), more data is collected. A headline \\(p&lt;0.05\\) does imply that this result will enter the published record. Referees may be skeptical of other parts of the design or framework or motivation. They may also choose to reject the paper specifically because of this issue; they believe the author would have continued collecting data had the result yielded \\(p&gt;0.05\\), thus they think it is better to demand more evidence or a more stringent critical value. Prompted by the referee, the author may collect more data even though \\(p&lt;0.05\\). Or, she may decide to collect more data even without a referee report/rejection demanding it, for various reasons (as we did after our Valentine’s experiment). Thus, we might imagine that there is some probability that after (e.g.) an initial experiment attaining p&lt;0.05, more data is collected, implying that \\(p_{augmented}\\) as calculated above overstates the type I error rate that would arise from these practices. As referees and editors, we should be concerned about the status of knowledge as accepted by the profession, i.e., in published papers. If we recognize the possibility of data augmentation after any paper is rejected, it might be a better practice to require a significance standard substantially below \\(p=0.05\\), in order to attain a type-1 error rate of 5% or less in our published corpus.\" 2.14.3 Efficient assignment of treatments (Links back to power analyses) 2.15 ‘Experimetrics’ and measurement of treatment effects from RCTs 2.15.1 Which error structure? Random effects? 2.15.2 Randomization inference? 2.15.3 Parametric and nonparametric tests of simple hypotheses 2.15.4 Adjustments for exogenous (but non-random) treatment assignment 2.15.5 IV in an experimental context to get at ‘mediators’? 2.15.6 Heterogeneity in an experimental context 2.16 Making inferences from previous work; Meta-analysis, combining studies 2.16.1 Publication bias 2.16.2 Combining a few (your own) studies/estimates 2.16.3 Full meta-analyses Models to address publication biases "],
["mediators-and-selection-and-roy-models-a-review-considering-two-research-applications.html", "3 Mediators (and selection and Roy models): a review, considering two research applications 3.1 DR initial thoughts (for NL education paper) 3.2 Econometric Mediation Analyses (Heckman and Pinto) 3.3 Summary and key modeling 3.4 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity Summary Relevance to Parey et al Introduction Identification strategy brief Results in brief Framework: first for binary/binary (simplification) Framework for MTO multiple treatment groups, multiple choices", " 3 Mediators (and selection and Roy models): a review, considering two research applications (Originally focused on issues relevant to Parey et al) 3.1 DR initial thoughts (for NL education paper) Here were my initial thoughts as pertaining to our paper on the returns to university. Suppose we observe treatment \\(T\\) (e.g., ‘allowed to enter first-choice institution and course’), intermediate outcome \\(M\\) (e.g., completion of degree in first-choice course and institution), and final outcome \\(Y\\) (e.g., lifetime income.) Alternately, in the “substitution between charities” context… (unfold) The treatment \\(T\\) is ‘asked to donate in the first round’ (in Reinstein, Riener and Vance-McMullen, henceforth ‘RRV’ experiments, and perhaps in Schmitz 2019)’, a greater incentive or a nudge to donate in round 1 (Heger and Slonim, 2020; others?), the inclusion of (an incentive to donate to) an additional charity in that same round (Reinstein 2006; Filiz-Ozbay and Uler; many others), the intermediate outcome \\(M\\) is the amount given to that (first-round) charity, and the final outcome \\(Y\\) is the amount given to that charity (or other charities) in round 2 (experiments “3”: other charities in that round ). The treatment \\(T\\) (may) directly affect the final outcome \\(Y\\). \\[T\\rightarrow Y\\] \\(T\\) also may affect an intermediate outcome \\(M\\). \\[T \\rightarrow M\\] The intermediate outcome also may affect the final outcome \\(Y\\). \\[M \\rightarrow Y\\] With exogenous variation in \\(T\\) and \\(M\\) (or identified instruments for each of these), we should be able to estimate each of these three relationships as functions. With homogeneous (and in a simplest case linear and separate) effects we can then use these functions to compute the total (direct plus indirect) effect of \\(T\\) on \\(Y\\). We could also compute the share of this effect that occurs via the intermediate effect, i.e., \\(T \\rightarrow M\\rightarrow Y\\). This should be merely the composition of these two functions, or, in the linear case, the product of the slope coefficients. However, there are two major challenges to this estimation. We (may) have a valid instrument for (exogenous variation in) \\(T\\) only, and \\(M\\) may arise through a process involving selection on unobserved variables. Each of the three above relationships (as well as the selection equation) may involve heterogeneous functions; i.e., differential treatment effects. Thus we consult the relevant literature, discussed below. The most influential paper in Economics has been (Heckman, Pinto, and Heckman 2013). It is cited in more recent applied work such as Fagereng, 2018 (unfold). … We follow Heckman et al. (2013) and Heckman and Pinto (2015) in using mediation analysis. The goal of this analysis is to disentangle the average causal effect on outcomes that operate through two channels: a) Indirect effects arising from the effect of treatment on measured mediators, and b) direct effects that operate through channels other than changes in the measured mediators (including changes in mediators that are not observed by the analyst and changes in the mapping between mediators and outcomes). It is therefore necessary to assume that the mediators we do not observe are uncorrelated with both \\(\\mathbf{X}\\) and the measured mediators for all values of \\(D\\). Antonakis, coming from the Psychology and Leadership disciplines, considers the mediation question in a much simpler set of models. 3.2 Econometric Mediation Analyses (Heckman and Pinto) Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs Relevance to Parey et al We have an instrument for admission to one’s first-choice institution (and course-subject). Our result show an impact of this admission on future income, for at least some groups. However, this effect could come through any of a number of channels. We observe some of these ‘intermediate outcomes’, including course enrollment, course completion, medical specialization, and location of residence, but we do not have specific instruments for each of these. a lot of work might yield an instrument for specialization; I hear there is a lottery at that level as well 3.3 Summary and key modeling There is a ‘production function’ cf income as a function of human capital, opportunities, etc. cf donation as a function of income, prices, mood, framing, etc. Treatments (e.g., RCTs) may affect outcomes through the following channels: observable or proxied inputs Cf degree obtained, specialization entered, years of study, moving away from parents, location of residence as proxy for job opportunities Cf donation in first stage (to targeted charity), measured/self reported attitudes towards charities, self-reported mood unobservable/unmeasured inputs cf human capital, social connections cf unobservable generosity, wealth, or temporary mood the production function itself, the ‘map between inputs and outputs for treatment group members’ Cf does the institution itself directly shift the income?, does it change the impact of entering a specialization, does human capital ‘matter more’ at some institutions? Cf Does he treatment affect the impact of having made the first donation on later donations , or the effect of mood on donating.. ; what else? If treatments affect unmeasured inputs in a way not statistically independent of measured inputs, this biases estimates of the effect of measured inputs. RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect of increases in measured inputs on outputs ... \\[nor distinguish effects from changes in production functions\\]. Here “we can test some of the strong assumptions implicitly invoked”. “Direct effects” as commonly stated refer to the impact of both channels 2 and 3 above. DR: Channel 2 isn’t really a direct effect imho (what was this?) Standard potential outcomes framework: \\[Y=DY_{1}+(1-D)Y_{0}\\] \\[ATE=E(Y_{1}-Y_{0})\\] Production function \\[Y_{d}=f_{d}(\\mathbf{\\mathbf{{\\theta}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] ... the function under treatment \\(d\\); of proxied and unobserved inputs that occur under state \\(d\\), and baseline variables. The production function implies: \\[ATE=E\\Big(f_{1}(\\mathbf{\\mathbf{{\\theta}}}_{1}^{p},\\mathbf{\\mathbf{{\\theta}}}_{1}^{u},\\mathbf{{X}})-f_{0}(\\mathbf{\\mathbf{{\\theta}}}_{0}^{p},\\mathbf{\\mathbf{{\\theta}}}_{0}^{u},\\mathbf{{X}})\\] We also consider counterfactual outputs, fixing treatment status and proxied inputs: \\[Y_{d,\\bar{\\theta_{d}}^{p}}=f_{d}(\\mathbf{\\mathbf{{\\bar{{\\theta}}}}}_{d}^{p},\\mathbf{\\mathbf{{\\theta}}}_{d}^{u},\\mathbf{{X}}),d\\in\\left\\{ 0,1\\right\\}\\] This allows us to decompose (‘as in the mediation literature’): ATE(d)=IE(d)+DE(d) IE, Indirect effect: allows only the proxied inputs to vary with the treatment (holds the rest fixed at one of the two treatment statuses) DE, Direct effect: allows technology and the distribution of unobservables to vary with the treatment (holds proxied inputs fixed at one of the two treatment statuses) HP further decompose the direct effect into: \\(DE&#39;(d,d&#39;)\\): The impact of letting the treatment vary the map only (fixing the rest at one of the two appropriate values) \\(DE&#39;&#39;(d,d&#39;)\\): The impact of letting the treatment vary the unmeasured inputs only (fixing the rest at one of the two appropriate values) They use this to give two further ways of decomposing the ATE. Common assumptions “The standard literature on mediation analysis in psychology regresses outputs on mediator inputs” ... often adopts the strong assumptions of: no variation in unmeasured inputs conditional on the treatment (implying the effects of these are summarized by a treatment dummy) and1 full invariance of the production function: \\(f_{1}=f_{0}\\). ... which implies \\(Y_{d}=f(\\mathbf{\\theta}_{d}^{p},d,\\mathbf{X})\\). Sequential ignorability (Imai et al, 10, ’11): Essentially, independent randomization of both treatment status and measured inputs.2 This sentence is hard to follow: In other words, input \\(\\theta_{d&#39;}^{p}\\) is statistically independent of potential outputs when treatment is fixed at \\(D=d\\) and measured inputs are fixed at \\(\\bar{\\theta_{d&#39;}^{p}}\\) conditional on treatment assignment \\(D\\) and same preprogram characteristics \\(X\\). This assumption yields the ‘mediation formulas’: \\begin{aligned} E(IE(d)|X)= &amp; E(Y|^{p}=t,D=d,X){{}} &amp; (9)\\ E(DE(d)|X)= &amp; {}expe_{} &amp; (10) \\end{aligned} (??F is presumably the distribution over the observables; where did the unobservables go? They are in the expectations, I guess.) Difference from RCT What RCT doesn’t do: \\[sequential ignorability\\] translates into ... no confounding effects on both treatments and measured inputs ... does not follow from a randomized assignment of treatment ...\\[which\\] ensures independence between treatment status and counterfactual inputs/outputs ... \\[but *not*\\] between proxied inputs \\(\\theta_{d}^{p}\\) and unmeasured inputs \\(\\theta_{d}^{u}\\). \\[Thus *not* between counterfactual outputs and measured inputs is assumed in condition (ii).\\] Cf, randomizing ‘win first-choice institution’ does not guarantee that the choice (potential choice under winning/losing institution) to enter a particular specialty is independent of (potential after winning/losing institution) unobserved human capital gains at an institution. The (potential) choiceof specialty is alsonot guaranteed choice independent of potential incomes (holding proxy inputs like specialty constant) if winning/losing institution. What RCT does do: RCT ensures “independence between treatment status and counterfactual inputs/outputs”, thus identifying ’treatment effects for proxied inputs and for outputs. CF, we can identify the impact of the treatment ‘win first chosen institution’ on proxied input like ‘enters a specialization’ and on outputs like ‘income in observed years.’ 3.4 Pinto (2015), Selection Bias in a Controlled Experiment: The Case of Moving to Opportunity Summary ... 4000+ families targeted, incentive to relocate from projects to better neighbourhoods. Easy to identify impact of vouchers Challenge (here) is to assess impact of neighborhoods on outcomes. Method here to decompose the TEOT into unambiguously interpreted effects. Method applicable to ‘unordered choice models with categorical instrumental variables and multiple treatments’ Finds significant causal effect on labour market outcomes Relevance to Parey et al We also have an instrument (DUO lottery numbers) cleanly identifying the effect of the ‘opportunity to do something’ (in our case, to enter the course at your preferred institution). However, we also want to measure the impact of choices ‘encouraged’ by the instrument, such as (i) attending the first choice course and institution and (ii) completing this course. We also deal with unordered choices (i. enter course and institution, enter course at other institution, enter other course at institution, enter neither) (ii. choice of medical specialisation) The geographic outcome is relevant to our second paper (impact on ‘lives close to home’) Introduction The causal link between neighborhood characteristics and resident’s outcomes has seldom been assessed. Treatments: Control (no voucher) Experimental: could use voucher to lease in low-poverty neighborhood Section 8: Could use voucher in any () neighborhood Many papers evaluate the ITT or TOT effects of MTO. ITT: effect of being offered voucher estimated as difference in average outcome of experimental vs control families TOT: effect for ‘voucher compliers’ (assuming no effect of simply being offered voucher on those who don’t use it) estimated as ITT/compliance rate \\[ITT and TOT\\] are the most useful parameters to investigate the effects of offering \\[EA\\] rent subsidising vouchers to families. Identification strategy brief Vouchers as IVs for choice among 3 neighborhood alternatives (no relocation, relocate bad, relocate good) \\[Cf: enter course and fp-institution, enter course at other institution, do not enter course\\] Neighborhood causal effects as difference in counterfactual outcomes among 3 categories Challenge: “MTO vouchers are insufficient to identify the expected outcomes for all possible counterfactual relocation decisions” ... “compliance with the terms of the program was highly selective \\[Clampet-Lundquist and M, 08\\]” Solution: Uses theory and ‘tools of causal inference. Invokes SARP to identify ’set of counterfactual relocation choices that are economically justifiable’ Identifying assumption: “the overall quality of the neighborhood is not directly caused by the unobserved family variables even though neighborhood quality correlates with these unobserved family variables due to network sorting” ‘Partition sample ... into unobserved subsets associated with economically justified counterfactual relocation choices and estimate the causal effect of neighborhood relocation conditioned on these partition sets.’ \\[*what does this mean?\\]* Results in brief “Relocating from housing projects to low poverty neighborhoods generates statistically significant results on labor market outcomes ... 65% higher than the TOT effect for adult earnings.” Framework: first for binary/binary (simplification) First, for binary outcomes (simplified) \\(Z_{\\omega}\\): whether family \\(\\omega\\) receives a voucher (cf institution-winning lottery number) \\(T_{\\omega}\\): whether family \\(\\omega\\) relocates (cf enters first choice institution and course) Counterfactuals \\(T_{\\omega}(z)\\): relocation decision \\(\\omega\\) would choose if it had been assigned voucher \\(z\\in{0,1}\\)’: vector of potential relocation decisions (cf education choices) for each voucher assignment (cf lottery number) Can partition into never-takers, compliers, always takers, and defiers \\((Y_{\\omega}(0);Y_{\\omega}(1\\))): (Potential counterfactual) outcomes (cf income, residence, etc) when relocation decision is fixed at 0 and 1, respectively Key ( standard) identification assumption: instrument independent of counterfactual variables \\[(Y_{\\omega}(0),Y_{\\omega}(1),T_{\\omega}(0),T_{\\omega}(1))\\perp Z_{\\omega}\\] Standard result 1: ITT \\[\\begin{aligned} ITT=E(Y_{\\omega}|Z_{\\omega}=1)-(Y_{\\omega}|Z_{\\omega}=0)\\\\ =E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)P(S_{\\omega}=[0,1])+E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[1,0]&#39;)P(S_{\\omega}=[0,1])\\end{aligned}\\] i.e., ITT computation yields the sum of the ‘causal effect for compliers’ and the ’causal effect for defiers, weighted by the probability of each. Standard result 2: LATE \\[\\begin{aligned} LATE=\\frac{{ITT}}{P(T_{\\omega}=1|Z_{\\omega}=1)-P(T_{\\omega}=1|Z_{\\omega}=0)}= &amp; &amp; E(Y_{\\omega}(1)-Y_{\\omega}(0)|S_{\\omega}=[0,1]&#39;)\\\\ if\\:P(S_{\\omega}=[0,1])=0\\end{aligned}\\] i.e., the LATE, computed as the ITT divided by the ‘first stage’ impact of the instrument, is the causal effect for compliers if there are no defiers. Framework for MTO multiple treatment groups, multiple choices \\(Z_{\\omega}\\in\\{z_{1,}z_{2,}z_{3}\\}\\) for no voucher, experimental voucher, and section 8 voucher, respectively \\(T_{\\omega}\\in\\{1,2,3\\}\\) ... no relocation, low poverty neighborhood relocation, high poverty relocation \\(T_{\\omega}(z)\\): relocation decision for family \\(\\omega\\) if assigned voucher \\(z\\) \\(\\ensuremath{\\rightarrow}\\)Response type for each family \\(\\omega\\) is that a three-dimensional vector: \\(S_{\\omega}=[T_{\\omega}(z_{1}),T_{\\omega}(z_{2}),T_{\\omega}(z_{3})]\\). \\(\\ensuremath{\\rightarrow}\\) ITT computation now measures a weighted sum of effects across a subset of those response types whose responses vary between the assignments being compared. Cf: Considering the ‘treatments’: ‘1: enter other course at fp-inst, ’2: enter course at fp-inst’, ‘3: enter course at non-fp inst’ (I ignore other course at other institution for now) Looking among those who won the course lottery (so we have a binary instrument: wininst \\(Z_{\\omega}\\in{0,1\\}}\\) Our reduced-form estimates (regressions on the ‘lottery number wins institution’ dummy) measures the probablility-weighted sum of: impact of institution within course ($T_{}=$2 versus 3); for those who would ‘fully comply’ (enter course at institution if \\(Z_{\\omega}=1\\), enter course at other institution if 0) impact of the course at fp-institution versus second-best course at fp-institution for ‘institution-loving’ noncompliers; those who would enter the course only if they get the fp-institution and otherwise another course at the same institution effects for perverse defiers Heckman, James, Rodrigo Pinto, and James Heckman. 2013. “Econometric Mediation Analyses : Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs,” no. 7552. Cf ‘winning institution’ impacts human capital, social networks, etc identically for everyone; e.g., not a greater effect for men then for women, nor a greater effect for those entering particular specializations.↩︎ Cf ‘winning institution’ does not effect the specialization entered nor the location of residence, nor are both determined by a third factor.↩︎ References "]
]
