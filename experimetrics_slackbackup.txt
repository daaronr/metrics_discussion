daaronr [6:08 PM]
joined #experimetrics.

daaronr [6:11 PM]
Markdown (raw)
notesatheyimbensexperimetrics.md
Markdown (raw) from Dropbox

daaronr [9:53 PM]
I need to get the actual handbook article; I expect a few things have been clarified

hannes [5:28 PM]
joined #experimetrics.

daaronr [4:06 PM]
stumped our 4 econometricians with this nonlinearity question

daaronr [9:44 PM]
Finished reading the relevant parts of Athey-Imbens (” Econometrics of Randomized Experiments). [WP version] Not as many practical tips as I would have hoped. Not much talking you through random forests or Lasso procedure. (edited)

daaronr [9:46 PM]
They also don’t seem to directly address the nonlinearity/heterogeneity issue, although I suppose nonparametric approaches may be less vulnerable to that
Not sure how much *inference* one gets out of these search and fit procedures

daaronr [11:21 AM]
@gerhard I want to read about model fitting (lasso etc) for the *controls* not interactions. What can I read (papers and stata docs), and what work of yours should I look at?
@gerhard Give me your thoughts on my nonlinear work when you’ve a moment

gerhard [11:21 AM]
joined #experimetrics by invitation from daaronr.

gerhard [12:07 PM]
look at the commando lars in stata. I run into the problem that when adding too many categorical variabels, it runs into memory problems, so I could never add more than three variables, which is definitely not satisfying.

daaronr [1:20 PM]
good read perhaps: Varian, “Big Data: New Tricks for Econometrics”

daaronr [5:53 PM]
Winfried Pohlmeier suggested reading “Hastie, T., R. Tibshirani, and M. Wainwright (2015): Statistical Learning with Sparisty The Lasso
and Generalizations, Monographs on Statistics and Probability, CRC Press.”
all pages available on google books, I think: https://books.google.co.uk/books?hl=en&lr=&id=f-A_CQAAQBAJ&oi=fnd&pg=PP1&ots=G4RMC-gZU-&sig=u_EoI3-ZcX0phwsZgLt2Ux7DXyc#v=onepage&q&f=false
Google Books
Statistical Learning with Sparsity
Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of l1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and… Show more

daaronr [10:39 AM]
Oh good, found the pdf here: https://web.stanford.edu/~hastie/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf
@gerhard

davidhughjones [10:57 AM]
joined #experimetrics.

daaronr [11:00 AM]
My notes on the Hastie sparsity book (markdown format) (edited)
Markdown (raw)
notes_hastie_statlearning.md
Markdown (raw) from Dropbox

daaronr [12:05 PM]
OFFICIAL “Control variable selection for prediction/reducing noise to enable better inference of treatment effects” thread

daaronr [1:52 PM]
David Reinstein
“the optimal value of λ does occur at one of the LARS steps” , but its not clear to me how we know which one (without doing the cross-validation)
From a thread in #analysisAug 23rd, 2017

daaronr [4:41 PM]
replied to a thread:
@gerhard, I think we may be able to deal with fitting the controls and estimating the Treatment Effect in a single step: with “glmnet” in R. See “Penalty factors” [here](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html).
… if we set the penalty to 0 for the treatment and 1 for all others, this should get what we want … I think

gerhard [10:06 PM]
Estimating heterogeneous treatment effects https://projecteuclid.org/euclid.aoas/1365527206  and https://cran.r-project.org/web/packages/FindIt/index.html
projecteuclid.org
Imai , Ratkovic : Estimating treatment effect heterogeneity in randomized program evaluation
Project Euclid - mathematics and statistics online

daaronr [5:52 PM]
Penalisation methodology ANOTHER thread:

daaronr [5:55 PM]
replied to a thread:
I think many people are using Lasso/penalisation *wrongly*; you shouldn’t just use it to *select* control variables and then run OLS with these controls. Lasso (etc) optimises to produce the best prediction by downweighting the coefficients for variables it does not drop. (Thus in our cases of interest it ‘reduces noise’). You need to *use* these predictions, i.e., keep the downweighted coefficients.  Do you agree? @davidhughjones

daaronr [11:35 AM]
@gerhard Can you find a particular quote or section in Athey/Imbens that directly justifies our centered interactions?

> In general the least squares estimates based on these regression functions are not unbiased for the average treatment effects over the randomization distribution given the finite population. [Unless] the covariates are all indicators and they partition the population, and we estimate the model with a full set of interactions

But I think the *this* is referring to a regression *with* interactions; I am not sure it refers to the regression without interactions.  I don’t see where it says there is a bias for the standard estimator without interactions
My only argument is that OLS estimators BLUE estimators of  a *homogenous* effect, and thus they (over)weight observations with more conditional variance in the treatment and less variance in the outcomes. With heterogeneous treatment effects this yields an estimate that does not represent the *average* treatment effect for the overall source population. However, I am not 100% sure that using the decentered interactions solves this problem and recovers the ATE’s. (This is the same issue we are facing with our Dutch admin data, it turns out). What do you think?

daaronr [11:55 AM]
OK, working on some language here. @gerhard Let me know what you think please!
> We pool our data across all of our experiments to perform a meta-analysis, allowing greater statistical power. For statistical inference, we consider this as a draw from a population composed of likely participants in each of our experiments, with shares corresponding to our relative sample sizes of UK students, German students, and UK nonstudents.

daaronr [12:01 PM]
…
>All regressions (except where noted) include de-meaned dummies for each experiment, and the interactions of these with the \textit{Before} treatment. This estimator recovers the \textit{average} treatment effect (ATE) for our source population in the presence of heterogeneity. In contrast, OLS estimators are more efficient if effects are homogenous, but they achieve this efficiency by (over)weighting observations (relative to shares of the source population) with higher conditional variance in the treatment and less residual variance in the outcome variable. With heterogeneous treatment effects this yields an arbitrarily weighted estimate of treatment effects (\citealp{angristpischkemhe}, p. 58), while the “fully interacted” estimator recovers the ATE (see \citealp{AtheyImbens2017}, equation 5.4).  However, our results are similar with or without these interactions, as well as with additional interactions by specific pre-determined variables (table \ref{tab:OLSDonationsPooledGender}).
(edited)

gerhard [9:50 AM]
Hi david, this sounds pretty convincing. Will read the A&P MHE p58  before commenting further

Nick [2:04 AM]
joined #experimetrics along with 2 others.

daaronr [9:49 AM]
@gerhard Read through “Tidy Data” (Wickham) and took notes and comments/questions. Would be nice to chat on it.
I’m now reading the “Review of Meta-Analysis packages in R”. Very cool. Will be useful for both our papers, and for the IIF evidence project too.

daaronr [5:01 PM]
Meta-analysis reading recommendations: Card and Krueger 1995; newer Stanlet and Doucouliagos

daaronr [5:10 PM]
Ada Gonzales is teaching a course at the EUI; I’ll ask for the finalised syllabus

daaronr [10:37 AM]
Something to use more in our analyses where there are multiple (and ordinal/likert..?) outcomes: “Use first component of polychoric principle component analysis as the dependent variable” Cronbach’s-alpha measure of ?cohesiveness of this. … Used in Seetha Menon’s paper https://docs.wixstatic.com/ugd/de08e8_121976c3255c4d4cae51fa6419e4f299.pdf
@gerhard

daaronr [12:11 PM]
@gerhard @davidhughjones @hannes See the new "critiques" table here: https://airtable.com/tblRKTrQq2YUbHhiD/viwJm1cEoDxWt4KnZ ... might be a useful resource in our own research and even for the community "common mistakes economists publish" (edited)

daaronr [3:56 PM]
@gerhard In fitting the ‘noise’ variables I think Ridge regression has something to say for itself:
> Ridge regression improves prediction error by shrinking large regression coefficients in order to reduce overfitting, but it does not perform covariate selection and therefore does not help to make the model more interpretable.
If that is the only issue it is fine; we are not trying to *interpret* these control variables

daaronr [8:05 PM]
@gerhard @Toby J @hannes @davidhughjones “Sequential analyses”: http://datacolada.org/wp-content/uploads/2015/12/5367-Lakens-EJSP-2014-Performing-high-power.pdf

One can preregister either a specific stopping rule for peeking at pre-defined intervals or a plan for doing peeking and stopping using a “spending function” for ‘using alpha’ (prob of type-1 error).

Either way, you can compute the p-values for each test that yield the appropriate net probability of a type-1 error.

Toby J [8:05 PM]
joined #experimetrics by invitation from daaronr.

daaronr [8:19 PM]
“Null hypothesis significance testing” — http://journals.sagepub.com/doi/abs/10.1177/1948550617697177 This also seems helpful

daaronr [11:55 AM]
@Toby J This may also/instead be helpful in formulating our design, deciding on a stopping rule, and formulating  and preregistering a “critical value”: http://www.paugmented.com/

daaronr [3:18 PM]
Ugh, a new robust standard error we may have to switch to at some point: https://www.researchgate.net/profile/Michal_Kolesar/publication/256037347_Robust_Standard_Errors_in_Small_Samples_Some_Practical_Advice/links/5512c27c0cf270fd7e336add.pdf @gerhard

daaronr [2:00 PM]
@gerhard Vis a vis our discussion on blocking/stratification — Athey and Imbens (“The econometrics of randomised experiments”) make a strong case for “stratification” (i.e., ‘block-randomisation’) @Toby J

> capture the gains from ex post regression adjustment without the potential costs of linear regression, and the potential costs of linear regression…

daaronr [1:34 PM]
I’m just finishing Lee (2009), ReStud on bounding estimates in the presence of attrition. Very well written and useful for bounded estimates in experiments that have attrition *as well* as things like ‘impact on wage conditional on entering the labor force’ … and (maybe?) on ‘amount donated conditional on donating’
